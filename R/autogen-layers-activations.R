## Autogenerated. Do not modify manually.


# keras.layers.Activation
# keras.src.layers.activations.activation.Activation
r"-(Applies an activation function to an output.

    Args:
        activation: Activation function. It could be a callable, or the name of
            an activation from the `keras.activations` namespace.
        **kwargs: Base layer keyword arguments, such as `name` and `dtype`.

    Example:

    >>> layer = keras.layers.Activation('relu')
    >>> layer([-3.0, -1.0, 0.0, 2.0])
    [0.0, 0.0, 0.0, 2.0]
    >>> layer = keras.layers.Activation(keras.activations.relu)
    >>> layer([-3.0, -1.0, 0.0, 2.0])
    [0.0, 0.0, 0.0, 2.0]
    )-"
#' Applies an activation function to an output.
#'
#' @description
#'
#' # Examples
#' ```python
#' layer = keras.layers.Activation('relu')
#' layer([-3.0, -1.0, 0.0, 2.0])
#' # [0.0, 0.0, 0.0, 2.0]
#' layer = keras.layers.Activation(keras.activations.relu)
#' layer([-3.0, -1.0, 0.0, 2.0])
#' # [0.0, 0.0, 0.0, 2.0]
#' ```
#'
#' @param activation Activation function. It could be a callable, or the name of
#'     an activation from the `keras.activations` namespace.
#' @param ... Base layer keyword arguments, such as `name` and `dtype`.
#' @param object Object to compose the layer with. A tensor, array, or sequential model.
#'
#' @export
#' @family activations layers
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/layers/Activation>
layer_activation <-
function (object, activation, ...)
{
    args <- capture_args2(list(input_shape = normalize_shape,
        batch_size = as_integer, batch_input_shape = normalize_shape),
        ignore = "object")
    create_layer(keras$layers$Activation, object, args)
}


# keras.layers.ELU
# keras.src.layers.activations.elu.ELU
r"-(Applies an Exponential Linear Unit function to an output.

    Formula:

    ```
    f(x) = alpha * (exp(x) - 1.) for x < 0
    f(x) = x for x >= 0
    ```

    Args:
        alpha: float, slope of negative section. Defaults to `1.0`.
        **kwargs: Base layer keyword arguments, such as `name` and `dtype`.
    )-"
#' Applies an Exponential Linear Unit function to an output.
#'
#' @description
#' Formula:
#'
#' ```
#' f(x) = alpha * (exp(x) - 1.) for x < 0
#' f(x) = x for x >= 0
#' ```
#'
#' @param alpha float, slope of negative section. Defaults to `1.0`.
#' @param ... Base layer keyword arguments, such as `name` and `dtype`.
#' @param object Object to compose the layer with. A tensor, array, or sequential model.
#'
#' @export
#' @family activations layers
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/layers/ELU>
layer_activation_elu <-
function (object, alpha = 1, ...)
{
    args <- capture_args2(list(input_shape = normalize_shape,
        batch_size = as_integer, batch_input_shape = normalize_shape),
        ignore = "object")
    create_layer(keras$layers$ELU, object, args)
}


# keras.layers.LeakyReLU
# keras.src.layers.activations.leaky_relu.LeakyReLU
r"-(Leaky version of a Rectified Linear Unit activation layer.

    This layer allows a small gradient when the unit is not active.

    Formula:

    ``` python
    f(x) = alpha * x if x < 0
    f(x) = x if x >= 0
    ```

    Example:

    ``` python
    leaky_relu_layer = LeakyReLU(negative_slope=0.5)
    input = np.array([-10, -5, 0.0, 5, 10])
    result = leaky_relu_layer(input)
    # result = [-5. , -2.5,  0. ,  5. , 10.]
    ```

    Args:
        negative_slope: Float >= 0.0. Negative slope coefficient.
          Defaults to `0.3`.
        **kwargs: Base layer keyword arguments, such as
            `name` and `dtype`.

    )-"
#' Leaky version of a Rectified Linear Unit activation layer.
#'
#' @description
#' This layer allows a small gradient when the unit is not active.
#'
#' Formula:
#'
#' ``` python
#' f(x) = alpha * x if x < 0
#' f(x) = x if x >= 0
#' ```
#'
#' # Examples
#' ``` python
#' leaky_relu_layer = LeakyReLU(negative_slope=0.5)
#' input = np.array([-10, -5, 0.0, 5, 10])
#' result = leaky_relu_layer(input)
#' # result = [-5. , -2.5,  0. ,  5. , 10.]
#' ```
#'
#' @param negative_slope Float >= 0.0. Negative slope coefficient.
#'   Defaults to `0.3`.
#' @param ... Base layer keyword arguments, such as
#'     `name` and `dtype`.
#' @param object Object to compose the layer with. A tensor, array, or sequential model.
#'
#' @export
#' @family activations layers
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/layers/LeakyReLU>
layer_activation_leaky_relu <-
function (object, negative_slope = 0.3, ...)
{
    args <- capture_args2(list(input_shape = normalize_shape,
        batch_size = as_integer, batch_input_shape = normalize_shape),
        ignore = "object")
    create_layer(keras$layers$LeakyReLU, object, args)
}


# keras.layers.PReLU
# keras.src.layers.activations.prelu.PReLU
r"-(Parametric Rectified Linear Unit activation layer.

    Formula:
    ``` python
    f(x) = alpha * x for x < 0
    f(x) = x for x >= 0
    ```
    where `alpha` is a learned array with the same shape as x.

    Args:
        alpha_initializer: Initializer function for the weights.
        alpha_regularizer: Regularizer for the weights.
        alpha_constraint: Constraint for the weights.
        shared_axes: The axes along which to share learnable parameters for the
            activation function. For example, if the incoming feature maps are
            from a 2D convolution with output shape
            `(batch, height, width, channels)`, and you wish to share parameters
            across space so that each filter only has one set of parameters,
            set `shared_axes=[1, 2]`.
        **kwargs: Base layer keyword arguments, such as `name` and `dtype`.
    )-"
#' Parametric Rectified Linear Unit activation layer.
#'
#' @description
#' Formula:
#' ``` python
#' f(x) = alpha * x for x < 0
#' f(x) = x for x >= 0
#' ```
#' where `alpha` is a learned array with the same shape as x.
#'
#' @param alpha_initializer Initializer function for the weights.
#' @param alpha_regularizer Regularizer for the weights.
#' @param alpha_constraint Constraint for the weights.
#' @param shared_axes The axes along which to share learnable parameters for the
#'     activation function. For example, if the incoming feature maps are
#'     from a 2D convolution with output shape
#'     `(batch, height, width, channels)`, and you wish to share parameters
#'     across space so that each filter only has one set of parameters,
#'     set `shared_axes=[1, 2]`.
#' @param ... Base layer keyword arguments, such as `name` and `dtype`.
#' @param object Object to compose the layer with. A tensor, array, or sequential model.
#'
#' @export
#' @family activations layers
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/layers/PReLU>
layer_activation_parametric_relu <-
function (object, alpha_initializer = "Zeros", alpha_regularizer = NULL,
    alpha_constraint = NULL, shared_axes = NULL, ...)
{
    args <- capture_args2(list(input_shape = normalize_shape,
        batch_size = as_integer, batch_input_shape = normalize_shape,
        shared_axes = function (x)
        lapply(x, as_integer)), ignore = "object")
    create_layer(keras$layers$PReLU, object, args)
}


# keras.layers.ReLU
# keras.src.layers.activations.relu.ReLU
r"-(Rectified Linear Unit activation function layer.

    Formula:
    ``` python
    f(x) = max(x,0)
    f(x) = max_value if x >= max_value
    f(x) = x if threshold <= x < max_value
    f(x) = negative_slope * (x - threshold) otherwise
    ```

    Example:
    ``` python
    relu_layer = keras.layers.activations.ReLU(
        max_value=10,
        negative_slope=0.5,
        threshold=0,
    )
    input = np.array([-10, -5, 0.0, 5, 10])
    result = relu_layer(input)
    # result = [-5. , -2.5,  0. ,  5. , 10.]
    ```

    Args:
        max_value: Float >= 0. Maximum activation value. None means unlimited.
            Defaults to `None`.
        negative_slope: Float >= 0. Negative slope coefficient.
            Defaults to `0.0`.
        threshold: Float >= 0. Threshold value for thresholded activation.
            Defaults to `0.0`.
        **kwargs: Base layer keyword arguments, such as `name` and `dtype`.
    )-"
#' Rectified Linear Unit activation function layer.
#'
#' @description
#' Formula:
#' ``` python
#' f(x) = max(x,0)
#' f(x) = max_value if x >= max_value
#' f(x) = x if threshold <= x < max_value
#' f(x) = negative_slope * (x - threshold) otherwise
#' ```
#'
#' # Examples
#' ``` python
#' relu_layer = keras.layers.activations.ReLU(
#'     max_value=10,
#'     negative_slope=0.5,
#'     threshold=0,
#' )
#' input = np.array([-10, -5, 0.0, 5, 10])
#' result = relu_layer(input)
#' # result = [-5. , -2.5,  0. ,  5. , 10.]
#' ```
#'
#' @param max_value Float >= 0. Maximum activation value. None means unlimited.
#'     Defaults to `None`.
#' @param negative_slope Float >= 0. Negative slope coefficient.
#'     Defaults to `0.0`.
#' @param threshold Float >= 0. Threshold value for thresholded activation.
#'     Defaults to `0.0`.
#' @param ... Base layer keyword arguments, such as `name` and `dtype`.
#' @param object Object to compose the layer with. A tensor, array, or sequential model.
#'
#' @export
#' @family activations layers
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU>
layer_activation_relu <-
function (object, max_value = NULL, negative_slope = 0, threshold = 0,
    ...)
{
    args <- capture_args2(list(input_shape = normalize_shape,
        batch_size = as_integer, batch_input_shape = normalize_shape),
        ignore = "object")
    create_layer(keras$layers$ReLU, object, args)
}


# keras.layers.Softmax
# keras.src.layers.activations.softmax.Softmax
r"-(Softmax activation layer.

    Formula:
    ``` python
    exp_x = exp(x - max(x))
    f(x) = exp_x / sum(exp_x)
    ```

    Example:
    >>>softmax_layer = keras.layers.activations.Softmax()
    >>>input = np.array([1.0, 2.0, 1.0])
    >>>result = softmax_layer(input)
    [0.21194157, 0.5761169, 0.21194157]


    Args:
        axis: Integer, or list of Integers, axis along which the softmax
            normalization is applied.
        **kwargs: Base layer keyword arguments, such as `name` and `dtype`.

    Call arguments:
        inputs: The inputs (logits) to the softmax layer.
        mask: A boolean mask of the same shape as `inputs`. The mask
            specifies 1 to keep and 0 to mask. Defaults to `None`.

    Returns:
        Softmaxed output with the same shape as `inputs`.
    )-"
#' Softmax activation layer.
#'
#' @description
#' Formula:
#' ``` python
#' exp_x = exp(x - max(x))
#' f(x) = exp_x / sum(exp_x)
#' ```
#'
#' # Examples
#' ```python
#' >>>softmax_layer = keras.layers.activations.Softmax()
#' # >>>input = np.array([1.0, 2.0, 1.0])
#' # >>>result = softmax_layer(input)
#' # [0.21194157, 0.5761169, 0.21194157]
#' ```
#'
#' # Call Arguments
#' - `inputs`: The inputs (logits) to the softmax layer.
#' - `mask`: A boolean mask of the same shape as `inputs`. The mask
#'     specifies 1 to keep and 0 to mask. Defaults to `None`.
#'
#' # Returns
#'     Softmaxed output with the same shape as `inputs`.
#'
#' @param axis Integer, or list of Integers, axis along which the softmax
#'     normalization is applied.
#' @param ... Base layer keyword arguments, such as `name` and `dtype`.
#' @param object Object to compose the layer with. A tensor, array, or sequential model.
#'
#' @export
#' @family activations layers
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/layers/Softmax>
layer_activation_softmax <-
function (object, axis = -1L, ...)
{
    args <- capture_args2(list(axis = as_integer, input_shape = normalize_shape,
        batch_size = as_integer, batch_input_shape = normalize_shape),
        ignore = "object")
    create_layer(keras$layers$Softmax, object, args)
}
