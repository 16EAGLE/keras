## Autogenerated. Do not modify manually.


# keras$ops$image$affine_transform
# keras.ops.image.affine_transform
# keras.src.ops.image.affine_transform
r"-(Applies the given transform(s) to the image(s).

    Args:
        image: Input image or batch of images. Must be 3D or 4D.
        transform: Projective transform matrix/matrices. A vector of length 8 or
            tensor of size N x 8. If one row of transform is
            `[a0, a1, a2, b0, b1, b2, c0, c1]`, then it maps the output point
            `(x, y)` to a transformed input point
            `(x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k)`,
            where `k = c0 x + c1 y + 1`. The transform is inverted compared to
            the transform mapping input points to output points. Note that
            gradients are not backpropagated into transformation parameters.
            Note that `c0` and `c1` are only effective when using TensorFlow
            backend and will be considered as `0` when using other backends.
        interpolation: Interpolation method. Available methods are `"nearest"`,
            and `"bilinear"`. Defaults to `"bilinear"`.
        fill_mode: Points outside the boundaries of the input are filled
            according to the given mode. Available methods are `"constant"`,
            `"nearest"`, `"wrap"` and `"reflect"`. Defaults to `"constant"`.
            - `"reflect"`: `(d c b a | a b c d | d c b a)`
                The input is extended by reflecting about the edge of the last
                pixel.
            - `"constant"`: `(k k k k | a b c d | k k k k)`
                The input is extended by filling all values beyond
                the edge with the same constant value k specified by
                `fill_value`.
            - `"wrap"`: `(a b c d | a b c d | a b c d)`
                The input is extended by wrapping around to the opposite edge.
            - `"nearest"`: `(a a a a | a b c d | d d d d)`
                The input is extended by the nearest pixel.
        fill_value: Value used for points outside the boundaries of the input if
            `fill_mode="constant"`. Defaults to `0`.
        data_format: string, either `"channels_last"` or `"channels_first"`.
            The ordering of the dimensions in the inputs. `"channels_last"`
            corresponds to inputs with shape `(batch, height, width, channels)`
            while `"channels_first"` corresponds to inputs with shape
            `(batch, channels, height, weight)`. It defaults to the
            `image_data_format` value found in your Keras config file at
            `~/.keras/keras.json`. If you never set it, then it will be
            `"channels_last"`.

    Returns:
        Applied affine transform image or batch of images.

    Examples:

    >>> x = np.random.random((2, 64, 80, 3)) # batch of 2 RGB images
    >>> transform = np.array(
    ...     [
    ...         [1.5, 0, -20, 0, 1.5, -16, 0, 0],  # zoom
    ...         [1, 0, -20, 0, 1, -16, 0, 0],  # translation
    ...     ]
    ... )
    >>> y = keras.ops.image.affine_transform(x, transform)
    >>> y.shape
    (2, 64, 80, 3)

    >>> x = np.random.random((64, 80, 3)) # single RGB image
    >>> transform = np.array([1.0, 0.5, -20, 0.5, 1.0, -16, 0, 0])  # shear
    >>> y = keras.ops.image.affine_transform(x, transform)
    >>> y.shape
    (64, 80, 3)

    >>> x = np.random.random((2, 3, 64, 80)) # batch of 2 RGB images
    >>> transform = np.array(
    ...     [
    ...         [1.5, 0, -20, 0, 1.5, -16, 0, 0],  # zoom
    ...         [1, 0, -20, 0, 1, -16, 0, 0],  # translation
    ...     ]
    ... )
    >>> y = keras.ops.image.affine_transform(x, transform,
    ...     data_format="channels_first")
    >>> y.shape
    (2, 3, 64, 80)
    )-"

# keras.src.ops.image.affine_transform
#' Applies the given transform(s) to the image(s).
#'
#' @description
#'
#' # Returns
#' Applied affine transform image or batch of images.
#'
#' # Examples
#' ```python
#' x = np.random.random((2, 64, 80, 3)) # batch of 2 RGB images
#' transform = np.array(
#'     [
#'         [1.5, 0, -20, 0, 1.5, -16, 0, 0],  # zoom
#'         [1, 0, -20, 0, 1, -16, 0, 0],  # translation
#'     ]
#' )
#' y = keras.ops.image.affine_transform(x, transform)
#' y.shape
#' # (2, 64, 80, 3)
#' ```
#'
#' ```python
#' x = np.random.random((64, 80, 3)) # single RGB image
#' transform = np.array([1.0, 0.5, -20, 0.5, 1.0, -16, 0, 0])  # shear
#' y = keras.ops.image.affine_transform(x, transform)
#' y.shape
#' # (64, 80, 3)
#' ```
#'
#' ```python
#' x = np.random.random((2, 3, 64, 80)) # batch of 2 RGB images
#' transform = np.array(
#'     [
#'         [1.5, 0, -20, 0, 1.5, -16, 0, 0],  # zoom
#'         [1, 0, -20, 0, 1, -16, 0, 0],  # translation
#'     ]
#' )
#' y = keras.ops.image.affine_transform(x, transform,
#'     data_format="channels_first")
#' y.shape
#' # (2, 3, 64, 80)
#' ```
#'
#' @param image Input image or batch of images. Must be 3D or 4D.
#' @param transform Projective transform matrix/matrices. A vector of length 8 or
#'     tensor of size N x 8. If one row of transform is
#'     `[a0, a1, a2, b0, b1, b2, c0, c1]`, then it maps the output point
#'     `(x, y)` to a transformed input point
#'     `(x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k)`,
#'     where `k = c0 x + c1 y + 1`. The transform is inverted compared to
#'     the transform mapping input points to output points. Note that
#'     gradients are not backpropagated into transformation parameters.
#'     Note that `c0` and `c1` are only effective when using TensorFlow
#'     backend and will be considered as `0` when using other backends.
#' @param interpolation Interpolation method. Available methods are `"nearest"`,
#'     and `"bilinear"`. Defaults to `"bilinear"`.
#' @param fill_mode Points outside the boundaries of the input are filled
#'     according to the given mode. Available methods are `"constant"`,
#'     `"nearest"`, `"wrap"` and `"reflect"`. Defaults to `"constant"`.
#'     - `"reflect"`: `(d c b a | a b c d | d c b a)`
#'         The input is extended by reflecting about the edge of the last
#'         pixel.
#'     - `"constant"`: `(k k k k | a b c d | k k k k)`
#'         The input is extended by filling all values beyond
#'         the edge with the same constant value k specified by
#'         `fill_value`.
#'     - `"wrap"`: `(a b c d | a b c d | a b c d)`
#'         The input is extended by wrapping around to the opposite edge.
#'     - `"nearest"`: `(a a a a | a b c d | d d d d)`
#'         The input is extended by the nearest pixel.
#' @param fill_value Value used for points outside the boundaries of the input if
#'     `fill_mode="constant"`. Defaults to `0`.
#' @param data_format string, either `"channels_last"` or `"channels_first"`.
#'     The ordering of the dimensions in the inputs. `"channels_last"`
#'     corresponds to inputs with shape `(batch, height, width, channels)`
#'     while `"channels_first"` corresponds to inputs with shape
#'     `(batch, channels, height, weight)`. It defaults to the
#'     `image_data_format` value found in your Keras config file at
#'     `~/.keras/keras.json`. If you never set it, then it will be
#'     `"channels_last"`.
#'
#' @export
#' @family ops
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/ops/image/affine_transform>
k_image_affine_transform <-
function (image, transform, interpolation = "bilinear", fill_mode = "constant",
    fill_value = 0L, data_format = "channels_last")
{
    args <- capture_args2(list(fill_value = as_integer))
    do.call(keras$ops$image$affine_transform, args)
}


# keras$ops$image$extract_patches
# keras.ops.image.extract_patches
# keras.src.ops.image.extract_patches
r"-(Extracts patches from the image(s).

    Args:
        image: Input image or batch of images. Must be 3D or 4D.
        size: Patch size int or tuple (patch_height, patch_widht)
        strides: strides along height and width. If not specified, or
            if `None`, it defaults to the same value as `size`.
        dilation_rate: This is the input stride, specifying how far two
            consecutive patch samples are in the input. For value other than 1,
            strides must be 1. NOTE: `strides > 1` is not supported in
            conjunction with `dilation_rate > 1`
        padding: The type of padding algorithm to use: `"same"` or `"valid"`.
        data_format: string, either `"channels_last"` or `"channels_first"`.
            The ordering of the dimensions in the inputs. `"channels_last"`
            corresponds to inputs with shape `(batch, height, width, channels)`
            while `"channels_first"` corresponds to inputs with shape
            `(batch, channels, height, weight)`. It defaults to the
            `image_data_format` value found in your Keras config file at
            `~/.keras/keras.json`. If you never set it, then it will be
            `"channels_last"`.

    Returns:
        Extracted patches 3D (if not batched) or 4D (if batched)

    Examples:

    >>> image = np.random.random(
    ...     (2, 20, 20, 3)
    ... ).astype("float32") # batch of 2 RGB images
    >>> patches = keras.ops.image.extract_patches(image, (5, 5))
    >>> patches.shape
    (2, 4, 4, 75)
    >>> image = np.random.random((20, 20, 3)).astype("float32") # 1 RGB image
    >>> patches = keras.ops.image.extract_patches(image, (3, 3), (1, 1))
    >>> patches.shape
    (18, 18, 27)
    )-"

# keras.src.ops.image.extract_patches
#' Extracts patches from the image(s).
#'
#' @description
#'
#' # Returns
#' Extracted patches 3D (if not batched) or 4D (if batched)
#'
#' # Examples
#' ```python
#' image = np.random.random(
#'     (2, 20, 20, 3)
#' ).astype("float32") # batch of 2 RGB images
#' patches = keras.ops.image.extract_patches(image, (5, 5))
#' patches.shape
#' # (2, 4, 4, 75)
#' image = np.random.random((20, 20, 3)).astype("float32") # 1 RGB image
#' patches = keras.ops.image.extract_patches(image, (3, 3), (1, 1))
#' patches.shape
#' # (18, 18, 27)
#' ```
#'
#' @param image Input image or batch of images. Must be 3D or 4D.
#' @param size Patch size int or tuple (patch_height, patch_widht)
#' @param strides strides along height and width. If not specified, or
#'     if `None`, it defaults to the same value as `size`.
#' @param dilation_rate This is the input stride, specifying how far two
#'     consecutive patch samples are in the input. For value other than 1,
#'     strides must be 1. NOTE: `strides > 1` is not supported in
#'     conjunction with `dilation_rate > 1`
#' @param padding The type of padding algorithm to use: `"same"` or `"valid"`.
#' @param data_format string, either `"channels_last"` or `"channels_first"`.
#'     The ordering of the dimensions in the inputs. `"channels_last"`
#'     corresponds to inputs with shape `(batch, height, width, channels)`
#'     while `"channels_first"` corresponds to inputs with shape
#'     `(batch, channels, height, weight)`. It defaults to the
#'     `image_data_format` value found in your Keras config file at
#'     `~/.keras/keras.json`. If you never set it, then it will be
#'     `"channels_last"`.
#'
#' @export
#' @family ops
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/ops/image/extract_patches>
k_image_extract_patches <-
function (image, size, strides = NULL, dilation_rate = 1L, padding = "valid",
    data_format = "channels_last")
{
    args <- capture_args2(list(size = as_integer, dilation_rate = as_integer))
    do.call(keras$ops$image$extract_patches, args)
}


# keras$ops$image$map_coordinates
# keras.ops.image.map_coordinates
# keras.src.ops.image.map_coordinates
r"-(Map the input array to new coordinates by interpolation..

    Note that interpolation near boundaries differs from the scipy function,
    because we fixed an outstanding bug
    [scipy/issues/2640](https://github.com/scipy/scipy/issues/2640).

    Args:
        input: The input array.
        coordinates: The coordinates at which input is evaluated.
        order: The order of the spline interpolation. The order must be `0` or
            `1`. `0` indicates the nearest neighbor and `1` indicates the linear
            interpolation.
        fill_mode: Points outside the boundaries of the input are filled
            according to the given mode. Available methods are `"constant"`,
            `"nearest"`, `"wrap"` and `"mirror"` and `"reflect"`. Defaults to
            `"constant"`.
            - `"constant"`: `(k k k k | a b c d | k k k k)`
                The input is extended by filling all values beyond
                the edge with the same constant value k specified by
                `fill_value`.
            - `"nearest"`: `(a a a a | a b c d | d d d d)`
                The input is extended by the nearest pixel.
            - `"wrap"`: `(a b c d | a b c d | a b c d)`
                The input is extended by wrapping around to the opposite edge.
            - `"mirror"`: `(c d c b | a b c d | c b a b)`
                The input is extended by mirroring about the edge.
            - `"reflect"`: `(d c b a | a b c d | d c b a)`
                The input is extended by reflecting about the edge of the last
                pixel.
        fill_value: Value used for points outside the boundaries of the input if
            `fill_mode="constant"`. Defaults to `0`.

    Returns:
        Output image or batch of images.

    )-"

# keras.src.ops.image.map_coordinates
#' Map the input array to new coordinates by interpolation..
#'
#' @description
#' Note that interpolation near boundaries differs from the scipy function,
#' because we fixed an outstanding bug
#' [scipy/issues/2640](https://github.com/scipy/scipy/issues/2640).
#'
#' # Returns
#'     Output image or batch of images.
#'
#' @param input The input array.
#' @param coordinates The coordinates at which input is evaluated.
#' @param order The order of the spline interpolation. The order must be `0` or
#'     `1`. `0` indicates the nearest neighbor and `1` indicates the linear
#'     interpolation.
#' @param fill_mode Points outside the boundaries of the input are filled
#'     according to the given mode. Available methods are `"constant"`,
#'     `"nearest"`, `"wrap"` and `"mirror"` and `"reflect"`. Defaults to
#'     `"constant"`.
#'     - `"constant"`: `(k k k k | a b c d | k k k k)`
#'         The input is extended by filling all values beyond
#'         the edge with the same constant value k specified by
#'         `fill_value`.
#'     - `"nearest"`: `(a a a a | a b c d | d d d d)`
#'         The input is extended by the nearest pixel.
#'     - `"wrap"`: `(a b c d | a b c d | a b c d)`
#'         The input is extended by wrapping around to the opposite edge.
#'     - `"mirror"`: `(c d c b | a b c d | c b a b)`
#'         The input is extended by mirroring about the edge.
#'     - `"reflect"`: `(d c b a | a b c d | d c b a)`
#'         The input is extended by reflecting about the edge of the last
#'         pixel.
#' @param fill_value Value used for points outside the boundaries of the input if
#'     `fill_mode="constant"`. Defaults to `0`.
#'
#' @export
#' @family ops
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/ops/image/map_coordinates>
k_image_map_coordinates <-
function (input, coordinates, order, fill_mode = "constant",
    fill_value = 0L)
{
    args <- capture_args2(list(fill_value = as_integer))
    do.call(keras$ops$image$map_coordinates, args)
}


# keras$ops$image$pad_images
# keras.ops.image.pad_images
# keras.src.ops.image.pad_images
r"-(Pad `images` with zeros to the specified `height` and `width`.

    Args:
        images: 4D Tensor of shape `(batch, height, width, channels)` or 3D
            Tensor of shape `(height, width, channels)`.
        top_padding: Number of rows of zeros to add on top.
        bottom_padding: Number of rows of zeros to add at the bottom.
        left_padding: Number of columns of zeros to add on the left.
        right_padding: Number of columns of zeros to add on the right.
        target_height: Height of output images.
        target_width: Width of output images.

    Returns:
        If `images` were 4D, a 4D float Tensor of shape
            `(batch, target_height, target_width, channels)`
        If `images` were 3D, a 3D float Tensor of shape
            `(target_height, target_width, channels)`

    Example:

    >>> images = np.random.random((15, 25, 3))
    >>> padded_images = keras.ops.image.pad_images(
    ...     images, 2, 3, target_height=20, target_width=30
    ... )
    >>> padded_images.shape
    (20, 30, 3)

    >>> batch_images = np.random.random((2, 15, 25, 3))
    >>> padded_batch = keras.ops.image.pad_images(
    ...     batch_images, 2, 3, target_height=20, target_width=30
    ... )
    >>> padded_batch.shape
    (2, 20, 30, 3))-"

# keras.src.ops.image.pad_images
#' Pad `images` with zeros to the specified `height` and `width`.
#'
#' @description
#'
#' # Returns
#' If `images` were 4D, a 4D float Tensor of shape
#'     `(batch, target_height, target_width, channels)`
#' If `images` were 3D, a 3D float Tensor of shape
#'     `(target_height, target_width, channels)`
#'
#' # Examples
#' ```python
#' images = np.random.random((15, 25, 3))
#' padded_images = keras.ops.image.pad_images(
#'     images, 2, 3, target_height=20, target_width=30
#' )
#' padded_images.shape
#' # (20, 30, 3)
#' ```
#'
#' ```python
#' batch_images = np.random.random((2, 15, 25, 3))
#' padded_batch = keras.ops.image.pad_images(
#'     batch_images, 2, 3, target_height=20, target_width=30
#' )
#' padded_batch.shape
#' # (2, 20, 30, 3)
#' ```
#'
#' @param images 4D Tensor of shape `(batch, height, width, channels)` or 3D
#'     Tensor of shape `(height, width, channels)`.
#' @param top_padding Number of rows of zeros to add on top.
#' @param bottom_padding Number of rows of zeros to add at the bottom.
#' @param left_padding Number of columns of zeros to add on the left.
#' @param right_padding Number of columns of zeros to add on the right.
#' @param target_height Height of output images.
#' @param target_width Width of output images.
#'
#' @export
#' @family ops
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/ops/image/pad_images>
k_image_pad_images <-
function (images, top_padding = NULL, left_padding = NULL, target_height = NULL,
    target_width = NULL, bottom_padding = NULL, right_padding = NULL)
keras$ops$image$pad_images(images, top_padding, left_padding,
    target_height, target_width, bottom_padding, right_padding)


# keras$ops$image$resize
# keras.ops.image.resize
# keras.src.ops.image.resize
r"-(Resize images to size using the specified interpolation method.

    Args:
        image: Input image or batch of images. Must be 3D or 4D.
        size: Size of output image in `(height, width)` format.
        interpolation: Interpolation method. Available methods are `"nearest"`,
            `"bilinear"`, and `"bicubic"`. Defaults to `"bilinear"`.
        antialias: Whether to use an antialiasing filter when downsampling an
            image. Defaults to `False`.
        data_format: string, either `"channels_last"` or `"channels_first"`.
            The ordering of the dimensions in the inputs. `"channels_last"`
            corresponds to inputs with shape `(batch, height, width, channels)`
            while `"channels_first"` corresponds to inputs with shape
            `(batch, channels, height, weight)`. It defaults to the
            `image_data_format` value found in your Keras config file at
            `~/.keras/keras.json`. If you never set it, then it will be
            `"channels_last"`.

    Returns:
        Resized image or batch of images.

    Examples:

    >>> x = np.random.random((2, 4, 4, 3)) # batch of 2 RGB images
    >>> y = keras.ops.image.resize(x, (2, 2))
    >>> y.shape
    (2, 2, 2, 3)

    >>> x = np.random.random((4, 4, 3)) # single RGB image
    >>> y = keras.ops.image.resize(x, (2, 2))
    >>> y.shape
    (2, 2, 3)

    >>> x = np.random.random((2, 3, 4, 4)) # batch of 2 RGB images
    >>> y = keras.ops.image.resize(x, (2, 2),
    ...     data_format="channels_first")
    >>> y.shape
    (2, 3, 2, 2)
    )-"

# keras.src.ops.image.resize
#' Resize images to size using the specified interpolation method.
#'
#' @description
#'
#' # Returns
#' Resized image or batch of images.
#'
#' # Examples
#' ```python
#' x = np.random.random((2, 4, 4, 3)) # batch of 2 RGB images
#' y = keras.ops.image.resize(x, (2, 2))
#' y.shape
#' # (2, 2, 2, 3)
#' ```
#'
#' ```python
#' x = np.random.random((4, 4, 3)) # single RGB image
#' y = keras.ops.image.resize(x, (2, 2))
#' y.shape
#' # (2, 2, 3)
#' ```
#'
#' ```python
#' x = np.random.random((2, 3, 4, 4)) # batch of 2 RGB images
#' y = keras.ops.image.resize(x, (2, 2),
#'     data_format="channels_first")
#' y.shape
#' # (2, 3, 2, 2)
#' ```
#'
#' @param image Input image or batch of images. Must be 3D or 4D.
#' @param size Size of output image in `(height, width)` format.
#' @param interpolation Interpolation method. Available methods are `"nearest"`,
#'     `"bilinear"`, and `"bicubic"`. Defaults to `"bilinear"`.
#' @param antialias Whether to use an antialiasing filter when downsampling an
#'     image. Defaults to `False`.
#' @param data_format string, either `"channels_last"` or `"channels_first"`.
#'     The ordering of the dimensions in the inputs. `"channels_last"`
#'     corresponds to inputs with shape `(batch, height, width, channels)`
#'     while `"channels_first"` corresponds to inputs with shape
#'     `(batch, channels, height, weight)`. It defaults to the
#'     `image_data_format` value found in your Keras config file at
#'     `~/.keras/keras.json`. If you never set it, then it will be
#'     `"channels_last"`.
#'
#' @export
#' @family ops
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/ops/image/resize>
k_image_resize <-
function (image, size, interpolation = "bilinear", antialias = FALSE,
    data_format = "channels_last")
keras$ops$image$resize(image, size, interpolation, antialias,
    data_format)
