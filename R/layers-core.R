

#' Input layer
#' 
#' Layer to be used as an entry point into a graph.
#' 
#' @param shape Shape, not including the batch size. For instance, 
#'   `shape=c(32)` indicates that the expected input will be batches
#'   of 32-dimensional vectors.
#' @param batch_shape Shapes, including the batch size. For instance, 
#'   `batch_shape=c(10, 32)` indicates that the expected input will be
#'   batches of 10 32-dimensional vectors. `batch_shape=list(NULL, 32)`
#'   indicates batches of an arbitrary number of 32-dimensional vectors.
#' @param name An optional name string for the layer. Should be unique in a 
#'   model (do not reuse the same name twice). It will be autogenerated if it 
#'   isn't provided.
#' @param dtype  The data type expected by the input, as a string (`float32`, 
#'   `float64`, `int32`...)
#' @param sparse Boolean, whether the placeholder created is meant to be sparse.
#' @param tensor Existing tensor to wrap into the `Input` layer. If set, the
#'   layer will not create a placeholder tensor.
#'   
#' @details It can either wrap an existing tensor (pass an `input_tensor` 
#'   argument) or create its a placeholder tensor (pass arguments `input_shape` 
#'   or `batch_input_shape` as well as `input_dtype`).
#'   
#' @return A tensor   
#'   
#' @export
layer_input <- function(shape = NULL, batch_shape = NULL, name = NULL,
                        dtype = NULL, sparse = FALSE, tensor = NULL) {
  keras$layers$Input(
    shape = normalize_shape(shape),
    batch_shape = normalize_shape(batch_shape),
    name = name,
    dtype = ifelse(is.null(dtype), keras$backend$floatx(), dtype),
    sparse = sparse,
    tensor = tensor
  )
}

#' Add a densely-connected NN layer to an output
#' 
#' Implements the operation: `output = activation(dot(input, kernel) + bias)` 
#' where `activation` is the element-wise activation function passed as the 
#' `activation` argument, `kernel` is a weights matrix created by the layer, and
#' `bias` is a bias vector created by the layer (only applicable if `use_bias` 
#' is `TRUE`). Note: if the input to the layer has a rank greater than 2, then 
#' it is flattened prior to the initial dot product with `kernel`.
#' 
#' @param x Model or layer
#' @param units Positive integer, dimensionality of the output space.
#' @param activation Name of activation function to use. If you don't specify 
#'   anything, no activation is applied (ie. "linear" activation: a(x) = x).
#' @param use_bias Whether the layer uses a bias vector.
#' @param kernel_initializer Initializer for the `kernel` weights matrix.
#' @param bias_initializer Initializer for the bias vector.
#' @param kernel_regularizer Regularizer function applied to the `kernel` 
#'   weights matrix.
#' @param bias_regularizer Regularizer function applied to the bias vector.
#' @param activity_regularizer Regularizer function applied to the output of the
#'   layer (its "activation")..
#' @param kernel_constraint Constraint function applied to the `kernel` weights 
#'   matrix.
#' @param bias_constraint  Constraint function applied to the bias vector.
#' @param input_shape Dimensionality of the input (integer) not including the
#'   samples axis. This argument is required when using this layer as the first
#'   layer in a model.
#'   
#'   
#' @section Input and Output Shapes:
#'   
#'   Input shape: nD tensor with shape: `(batch_size, ..., input_dim)`. The most
#'   common situation would be a 2D input with shape `(batch_size, input_dim)`.
#'   
#'   Output shape: nD tensor with shape: `(batch_size, ..., units)`. For 
#'   instance, for a 2D input with shape `(batch_size, input_dim)`, the output 
#'   would have shape `(batch_size, unit)`.
#'   
#' @export
layer_dense <- function(x, units, activation = NULL, use_bias = TRUE, 
                        kernel_initializer = 'glorot_uniform', bias_initializer = 'zeros', 
                        kernel_regularizer = NULL, bias_regularizer = NULL, activity_regularizer = NULL,
                        kernel_constraint = NULL, bias_constraint = NULL, input_shape = NULL
                        ) {
  layer <- keras$layers$Dense(
    units = as.integer(units),
    activation = resolve_keras_function(activation),
    use_bias = use_bias,
    kernel_initializer = kernel_initializer,
    bias_initializer = bias_initializer,
    kernel_regularizer = kernel_regularizer,
    bias_regularizer = bias_regularizer,
    activity_regularizer = activity_regularizer,
    kernel_constraint = kernel_constraint,
    bias_constraint = bias_constraint, 
    input_shape = normalize_shape(input_shape)
  )
  compose_layer(x, layer)
}

#' Reshapes an output to a certain shape.
#' 
#' @inheritParams layer_activation
#'   
#' @param target_shape List of integers, does not include the samples dimension 
#'   (batch size).
#'   
#' @section Input and Output Shapes:
#'   
#'   Input shape: Arbitrary, although all dimensions in the input shaped must be
#'   fixed.
#'   
#'   Output shape: `(batch_size,) + target_shape`.
#'   
#' @export
layer_reshape <- function(x, target_shape, input_shape = NULL) {
  
  # build args
  args <- list(target_shape = normalize_shape(target_shape))
  args$input_shape <- normalize_shape(input_shape)
  
  # call function
  layer <- do.call(keras$layers$Reshape, args)
  
  # compose
  compose_layer(x, layer)
}


#' Permute the dimensions of an input according to a given pattern
#' 
#' @param dims List of integers. Permutation pattern, does not include the 
#'   samples dimension. Indexing starts at 1. For instance, `(2, 1)` permutes 
#'   the first and second dimension of the input.
#'   
#' @inheritParams layer_activation
#'   
#' @section Input and Output Shapes:
#'   
#'   Input shape: Arbitrary
#'   
#'   Output shape: Same as the input shape, but with the dimensions re-ordered
#'   according to the specified pattern.
#'   
#' @note Useful for e.g. connecting RNNs and convnets together.
#'   
#' @export
layer_permute <- function(x, dims, input_shape = NULL) {
  
  # build args
  args <- list(dims = list(as.integer(dims)))
  args$input_shape <- normalize_shape(input_shape)
  
  # call function
  layer <- do.call(keras$layers$Permute, args)
  
  # compose
  compose_layer(x, layer)
}


#' Flattens an input
#' 
#' Flatten a given input, does not affect the batch size.
#' 
#' @inheritParams layer_activation
#' 
#' @export
layer_flatten <- function(x, input_shape = NULL) {
  
  # build args
  args <- list()
  args$input_shape <- normalize_shape(input_shape)
  
  # call function
  layer <- do.call(keras$layers$Flatten, args)
  
  # compose
  compose_layer(x, layer)
}

as_integer_tuple <- function(x) {
  if (is.null(x))
    x
  else
    tuple(as.list(as.integer(x)))
}

# Helper function to coerce shape arguments to tuple
normalize_shape <- function(shape) {
  
  # reflect NULL back
  if (is.null(shape))
    shape
  
  # if it's a list or a numeric vector then convert to integer
  else if (is.list(shape) || is.numeric(shape))
    shape <- lapply(shape, function(value) {
      if (!is.null(value))
        as.integer(value)
      else
        NULL
    })
  
  # coerce to tuple so it's iterable    
  tuple(shape)
}

# Helper function to compose a layer with an object of type Model or Layer
compose_layer <- function(x, layer) {
  
  # if a sequential is passed then add it to the model
  if (inherits(x, "tensorflow.contrib.keras.python.keras.models.Sequential")) {
    
    x$add(layer)
    x
    
  # if a layer is passed then wrap the layer
  } else if (inherits(x, "tensorflow.python.framework.ops.Tensor") ||
             inherits(x, "tensorflow.contrib.keras.python.keras.engine.topology.Layer")) {
    
    py_call(layer, x)
    
  # otherwie it's an unexpected type
  } else {
    
    stop("Invalid input to layer function (must be a model or a tensor)",
         call. = FALSE)
  }
}



