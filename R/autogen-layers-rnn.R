## Autogenerated. Do not modify manually.


# keras$layers$Bidirectional
# keras_core.src.layers.rnn.bidirectional.Bidirectional
r"-(Bidirectional wrapper for RNNs.

    Args:
        layer: `keras.layers.RNN` instance, such as
            `keras.layers.LSTM` or `keras.layers.GRU`.
            It could also be a `keras.layers.Layer` instance
            that meets the following criteria:
            1. Be a sequence-processing layer (accepts 3D+ inputs).
            2. Have a `go_backwards`, `return_sequences` and `return_state`
            attribute (with the same semantics as for the `RNN` class).
            3. Have an `input_spec` attribute.
            4. Implement serialization via `get_config()` and `from_config()`.
            Note that the recommended way to create new RNN layers is to write a
            custom RNN cell and use it with `keras.layers.RNN`, instead of
            subclassing `keras.layers.Layer` directly.
            When `return_sequences` is `True`, the output of the masked
            timestep will be zero regardless of the layer's original
            `zero_output_for_mask` value.
        merge_mode: Mode by which outputs of the forward and backward RNNs
            will be combined. One of `{"sum", "mul", "concat", "ave", None}`.
            If `None`, the outputs will not be combined,
            they will be returned as a list. Defaults to `"concat"`.
        backward_layer: Optional `keras.layers.RNN`,
            or `keras.layers.Layer` instance to be used to handle
            backwards input processing.
            If `backward_layer` is not provided, the layer instance passed
            as the `layer` argument will be used to generate the backward layer
            automatically.
            Note that the provided `backward_layer` layer should have properties
            matching those of the `layer` argument, in particular
            it should have the same values for `stateful`, `return_states`,
            `return_sequences`, etc. In addition, `backward_layer`
            and `layer` should have different `go_backwards` argument values.
            A `ValueError` will be raised if these requirements are not met.

    Call arguments:
        The call arguments for this layer are the same as those of the
        wrapped RNN layer. Beware that when passing the `initial_state`
        argument during the call of this layer, the first half in the
        list of elements in the `initial_state` list will be passed to
        the forward RNN call and the last half in the list of elements
        will be passed to the backward RNN call.

    Note: instantiating a `Bidirectional` layer from an existing RNN layer
    instance will not reuse the weights state of the RNN layer instance -- the
    `Bidirectional` layer will have freshly initialized weights.

    Examples:

    ```python
    model = Sequential([
        Input(shape=(5, 10)),
        Bidirectional(LSTM(10, return_sequences=True),
        Bidirectional(LSTM(10)),
        Dense(5, activation="softmax"),
    ])
    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

    # With custom backward layer
    forward_layer = LSTM(10, return_sequences=True)
    backward_layer = LSTM(10, activation='relu', return_sequences=True,
                          go_backwards=True)
    model = Sequential([
        Input(shape=(5, 10)),
        Bidirectional(forward_layer, backward_layer=backward_layer),
        Dense(5, activation="softmax"),
    ])
    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
    ```
    )-"


# keras_core.src.layers.rnn.bidirectional.Bidirectional
#' Bidirectional wrapper for RNNs.
#'
#' @description
#'
#' # Call Arguments
#' The call arguments for this layer are the same as those of the
#'     wrapped RNN layer. Beware that when passing the `initial_state`
#'     argument during the call of this layer, the first half in the
#'     list of elements in the `initial_state` list will be passed to
#'     the forward RNN call and the last half in the list of elements
#'     will be passed to the backward RNN call.
#'
#' Note: instantiating a `Bidirectional` layer from an existing RNN layer
#' instance will not reuse the weights state of the RNN layer instance -- the
#' `Bidirectional` layer will have freshly initialized weights.
#'
#' # Examples
#' ```python
#' model = Sequential([
#'     Input(shape=(5, 10)),
#'     Bidirectional(LSTM(10, return_sequences=True),
#'     Bidirectional(LSTM(10)),
#'     Dense(5, activation="softmax"),
#' ])
#' model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
#'
#' # With custom backward layer
#' forward_layer = LSTM(10, return_sequences=True)
#' backward_layer = LSTM(10, activation='relu', return_sequences=True,
#'                       go_backwards=True)
#' model = Sequential([
#'     Input(shape=(5, 10)),
#'     Bidirectional(forward_layer, backward_layer=backward_layer),
#'     Dense(5, activation="softmax"),
#' ])
#' model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
#' ```
#'
#' @param layer `keras.layers.RNN` instance, such as
#'     `keras.layers.LSTM` or `keras.layers.GRU`.
#'     It could also be a `keras.layers.Layer` instance
#'     that meets the following criteria:
#'     1. Be a sequence-processing layer (accepts 3D+ inputs).
#'     2. Have a `go_backwards`, `return_sequences` and `return_state`
#'     attribute (with the same semantics as for the `RNN` class).
#'     3. Have an `input_spec` attribute.
#'     4. Implement serialization via `get_config()` and `from_config()`.
#'     Note that the recommended way to create new RNN layers is to write a
#'     custom RNN cell and use it with `keras.layers.RNN`, instead of
#'     subclassing `keras.layers.Layer` directly.
#'     When `return_sequences` is `True`, the output of the masked
#'     timestep will be zero regardless of the layer's original
#'     `zero_output_for_mask` value.
#' @param merge_mode Mode by which outputs of the forward and backward RNNs
#'     will be combined. One of `{"sum", "mul", "concat", "ave", None}`.
#'     If `None`, the outputs will not be combined,
#'     they will be returned as a list. Defaults to `"concat"`.
#' @param backward_layer Optional `keras.layers.RNN`,
#'     or `keras.layers.Layer` instance to be used to handle
#'     backwards input processing.
#'     If `backward_layer` is not provided, the layer instance passed
#'     as the `layer` argument will be used to generate the backward layer
#'     automatically.
#'     Note that the provided `backward_layer` layer should have properties
#'     matching those of the `layer` argument, in particular
#'     it should have the same values for `stateful`, `return_states`,
#'     `return_sequences`, etc. In addition, `backward_layer`
#'     and `layer` should have different `go_backwards` argument values.
#'     A `ValueError` will be raised if these requirements are not met.
#'
#' @export
#' @family recurrent layers
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional>
layer_bidirectional <-
function (object, layer, merge_mode = "concat", weights = NULL,
    backward_layer = NULL, ...)
{
    args <- capture_args2(list(input_shape = normalize_shape,
        batch_size = as_integer, batch_input_shape = normalize_shape),
        ignore = "object")
    create_layer(keras$layers$Bidirectional, object, args)
}


# keras$layers$ConvLSTM1D
# keras_core.src.layers.rnn.conv_lstm1d.ConvLSTM1D
r"-(1D Convolutional LSTM.

    Similar to an LSTM layer, but the input transformations
    and recurrent transformations are both convolutional.

    Args:
        filters: int, the dimension of the output space (the number of filters
            in the convolution).
        kernel_size: int or tuple/list of 1 integer, specifying the size of
            the convolution window.
        strides: int or tuple/list of 1 integer, specifying the stride length
            of the convolution. `strides > 1` is incompatible with
            `dilation_rate > 1`.
        padding: string, `"valid"` or `"same"` (case-insensitive).
            `"valid"` means no padding. `"same"` results in padding evenly to
            the left/right or up/down of the input such that output has the
            same height/width dimension as the input.
        data_format: string, either `"channels_last"` or `"channels_first"`.
            The ordering of the dimensions in the inputs. `"channels_last"`
            corresponds to inputs with shape `(batch, steps, features)`
            while `"channels_first"` corresponds to inputs with shape
            `(batch, features, steps)`. It defaults to the `image_data_format`
            value found in your Keras config file at `~/.keras/keras.json`.
            If you never set it, then it will be `"channels_last"`.
        dilation_rate: int or tuple/list of 1 integers, specifying the dilation
            rate to use for dilated convolution.
        activation: Activation function to use. By default hyperbolic tangent
            activation function is applied (`tanh(x)`).
        recurrent_activation: Activation function to use for the recurrent step.
        use_bias: Boolean, whether the layer uses a bias vector.
        kernel_initializer: Initializer for the `kernel` weights matrix,
            used for the linear transformation of the inputs.
        recurrent_initializer: Initializer for the `recurrent_kernel` weights
            matrix, used for the linear transformation of the recurrent state.
        bias_initializer: Initializer for the bias vector.
        unit_forget_bias: Boolean. If `True`, add 1 to the bias of
            the forget gate at initialization.
            Use in combination with `bias_initializer="zeros"`.
            This is recommended in [Jozefowicz et al., 2015](
            http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)
        kernel_regularizer: Regularizer function applied to the `kernel` weights
            matrix.
        recurrent_regularizer: Regularizer function applied to the
            `recurrent_kernel` weights matrix.
        bias_regularizer: Regularizer function applied to the bias vector.
        activity_regularizer: Regularizer function applied to.
        kernel_constraint: Constraint function applied to the `kernel` weights
            matrix.
        recurrent_constraint: Constraint function applied to the
            `recurrent_kernel` weights matrix.
        bias_constraint: Constraint function applied to the bias vector.
        dropout: Float between 0 and 1. Fraction of the units to drop for the
            linear transformation of the inputs.
        recurrent_dropout: Float between 0 and 1. Fraction of the units to drop
            for the linear transformation of the recurrent state.
        seed: Random seed for dropout.
        return_sequences: Boolean. Whether to return the last output
            in the output sequence, or the full sequence. Default: `False`.
        return_state: Boolean. Whether to return the last state in addition
            to the output. Default: `False`.
        go_backwards: Boolean (default: `False`).
            If `True`, process the input sequence backwards and return the
            reversed sequence.
        stateful: Boolean (default False). If `True`, the last state
            for each sample at index i in a batch will be used as initial
            state for the sample of index i in the following batch.
        unroll: Boolean (default: `False`).
            If `True`, the network will be unrolled,
            else a symbolic loop will be used.
            Unrolling can speed-up a RNN,
            although it tends to be more memory-intensive.
            Unrolling is only suitable for short sequences.


    Call arguments:
        inputs: A 4D tensor.
        initial_state: List of initial state tensors to be passed to the first
            call of the cell.
        mask: Binary tensor of shape `(samples, timesteps)` indicating whether a
            given timestep should be masked.
        training: Python boolean indicating whether the layer should behave in
            training mode or in inference mode.
            This is only relevant if `dropout` or `recurrent_dropout` are set.

    Input shape:

    - If `data_format="channels_first"`:
        4D tensor with shape: `(samples, time, channels, rows)`
    - If `data_format="channels_last"`:
        4D tensor with shape: `(samples, time, rows, channels)`

    Output shape:

    - If `return_state`: a list of tensors. The first tensor is the output.
        The remaining tensors are the last states,
        each 3D tensor with shape: `(samples, filters, new_rows)` if
        `data_format='channels_first'`
        or shape: `(samples, new_rows, filters)` if
        `data_format='channels_last'`.
        `rows` values might have changed due to padding.
    - If `return_sequences`: 4D tensor with shape: `(samples, timesteps,
        filters, new_rows)` if data_format='channels_first'
        or shape: `(samples, timesteps, new_rows, filters)` if
        `data_format='channels_last'`.
    - Else, 3D tensor with shape: `(samples, filters, new_rows)` if
        `data_format='channels_first'`
        or shape: `(samples, new_rows, filters)` if
        `data_format='channels_last'`.

    References:

    - [Shi et al., 2015](http://arxiv.org/abs/1506.04214v1)
        (the current implementation does not include the feedback loop on the
        cells output).
    )-"


# keras_core.src.layers.rnn.conv_lstm1d.ConvLSTM1D
#' 1D Convolutional LSTM.
#'
#' @description
#' Similar to an LSTM layer, but the input transformations
#' and recurrent transformations are both convolutional.
#'
#' # Call Arguments
#' - `inputs`: A 4D tensor.
#' - `initial_state`: List of initial state tensors to be passed to the first
#'     call of the cell.
#' - `mask`: Binary tensor of shape `(samples, timesteps)` indicating whether a
#'     given timestep should be masked.
#' - `training`: Python boolean indicating whether the layer should behave in
#'     training mode or in inference mode.
#'     This is only relevant if `dropout` or `recurrent_dropout` are set.
#'
#' # Input Shape
#' - If `data_format="channels_first"`:
#'     4D tensor with shape: `(samples, time, channels, rows)`
#' - If `data_format="channels_last"`:
#'     4D tensor with shape: `(samples, time, rows, channels)`
#'
#' # Output Shape
#' - If `return_state`: a list of tensors. The first tensor is the output.
#'     The remaining tensors are the last states,
#'     each 3D tensor with shape: `(samples, filters, new_rows)` if
#'     `data_format='channels_first'`
#'     or shape: `(samples, new_rows, filters)` if
#'     `data_format='channels_last'`.
#'     `rows` values might have changed due to padding.
#' - If `return_sequences`: 4D tensor with shape: `(samples, timesteps,
#'     filters, new_rows)` if data_format='channels_first'
#'     or shape: `(samples, timesteps, new_rows, filters)` if
#'     `data_format='channels_last'`.
#' - Else, 3D tensor with shape: `(samples, filters, new_rows)` if
#'     `data_format='channels_first'`
#'     or shape: `(samples, new_rows, filters)` if
#'     `data_format='channels_last'`.
#'
#' # References
#' - [Shi et al., 2015](http://arxiv.org/abs/1506.04214v1)
#'     (the current implementation does not include the feedback loop on the
#'     cells output).
#'
#' @param filters int, the dimension of the output space (the number of filters
#'     in the convolution).
#' @param kernel_size int or tuple/list of 1 integer, specifying the size of
#'     the convolution window.
#' @param strides int or tuple/list of 1 integer, specifying the stride length
#'     of the convolution. `strides > 1` is incompatible with
#'     `dilation_rate > 1`.
#' @param padding string, `"valid"` or `"same"` (case-insensitive).
#'     `"valid"` means no padding. `"same"` results in padding evenly to
#'     the left/right or up/down of the input such that output has the
#'     same height/width dimension as the input.
#' @param data_format string, either `"channels_last"` or `"channels_first"`.
#'     The ordering of the dimensions in the inputs. `"channels_last"`
#'     corresponds to inputs with shape `(batch, steps, features)`
#'     while `"channels_first"` corresponds to inputs with shape
#'     `(batch, features, steps)`. It defaults to the `image_data_format`
#'     value found in your Keras config file at `~/.keras/keras.json`.
#'     If you never set it, then it will be `"channels_last"`.
#' @param dilation_rate int or tuple/list of 1 integers, specifying the dilation
#'     rate to use for dilated convolution.
#' @param activation Activation function to use. By default hyperbolic tangent
#'     activation function is applied (`tanh(x)`).
#' @param recurrent_activation Activation function to use for the recurrent step.
#' @param use_bias Boolean, whether the layer uses a bias vector.
#' @param kernel_initializer Initializer for the `kernel` weights matrix,
#'     used for the linear transformation of the inputs.
#' @param recurrent_initializer Initializer for the `recurrent_kernel` weights
#'     matrix, used for the linear transformation of the recurrent state.
#' @param bias_initializer Initializer for the bias vector.
#' @param unit_forget_bias Boolean. If `True`, add 1 to the bias of
#'     the forget gate at initialization.
#'     Use in combination with `bias_initializer="zeros"`.
#'     This is recommended in [Jozefowicz et al., 2015](
#'     http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)
#' @param kernel_regularizer Regularizer function applied to the `kernel` weights
#'     matrix.
#' @param recurrent_regularizer Regularizer function applied to the
#'     `recurrent_kernel` weights matrix.
#' @param bias_regularizer Regularizer function applied to the bias vector.
#' @param activity_regularizer Regularizer function applied to.
#' @param kernel_constraint Constraint function applied to the `kernel` weights
#'     matrix.
#' @param recurrent_constraint Constraint function applied to the
#'     `recurrent_kernel` weights matrix.
#' @param bias_constraint Constraint function applied to the bias vector.
#' @param dropout Float between 0 and 1. Fraction of the units to drop for the
#'     linear transformation of the inputs.
#' @param recurrent_dropout Float between 0 and 1. Fraction of the units to drop
#'     for the linear transformation of the recurrent state.
#' @param seed Random seed for dropout.
#' @param return_sequences Boolean. Whether to return the last output
#'     in the output sequence, or the full sequence. Default: `False`.
#' @param return_state Boolean. Whether to return the last state in addition
#'     to the output. Default: `False`.
#' @param go_backwards Boolean (default: `False`).
#'     If `True`, process the input sequence backwards and return the
#'     reversed sequence.
#' @param stateful Boolean (default False). If `True`, the last state
#'     for each sample at index i in a batch will be used as initial
#'     state for the sample of index i in the following batch.
#' @param unroll Boolean (default: `False`).
#'     If `True`, the network will be unrolled,
#'     else a symbolic loop will be used.
#'     Unrolling can speed-up a RNN,
#'     although it tends to be more memory-intensive.
#'     Unrolling is only suitable for short sequences.
#'
#' @export
#' @family recurrent layers
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/layers/ConvLSTM1D>
layer_conv_lstm_1d <-
function (object, filters, kernel_size, strides = 1L, padding = "valid",
    data_format = NULL, dilation_rate = 1L, activation = "tanh",
    recurrent_activation = "sigmoid", use_bias = TRUE, kernel_initializer = "glorot_uniform",
    recurrent_initializer = "orthogonal", bias_initializer = "zeros",
    unit_forget_bias = TRUE, kernel_regularizer = NULL, recurrent_regularizer = NULL,
    bias_regularizer = NULL, activity_regularizer = NULL, kernel_constraint = NULL,
    recurrent_constraint = NULL, bias_constraint = NULL, dropout = 0,
    recurrent_dropout = 0, seed = NULL, return_sequences = FALSE,
    return_state = FALSE, go_backwards = FALSE, stateful = FALSE,
    ...)
{
    args <- capture_args2(list(filters = as_integer, kernel_size = as_integer_tuple,
        strides = as_integer_tuple, dilation_rate = as_integer_tuple,
        input_shape = normalize_shape, batch_size = as_integer,
        batch_input_shape = normalize_shape), ignore = "object")
    create_layer(keras$layers$ConvLSTM1D, object, args)
}


# keras$layers$ConvLSTM2D
# keras_core.src.layers.rnn.conv_lstm2d.ConvLSTM2D
r"-(2D Convolutional LSTM.

    Similar to an LSTM layer, but the input transformations
    and recurrent transformations are both convolutional.

    Args:
        filters: int, the dimension of the output space (the number of filters
            in the convolution).
        kernel_size: int or tuple/list of 2 integers, specifying the size of the
            convolution window.
        strides: int or tuple/list of 2 integers, specifying the stride length
            of the convolution. `strides > 1` is incompatible with
            `dilation_rate > 1`.
        padding: string, `"valid"` or `"same"` (case-insensitive).
            `"valid"` means no padding. `"same"` results in padding evenly to
            the left/right or up/down of the input such that output has the same
            height/width dimension as the input.
        data_format: string, either `"channels_last"` or `"channels_first"`.
            The ordering of the dimensions in the inputs. `"channels_last"`
            corresponds to inputs with shape `(batch, steps, features)`
            while `"channels_first"` corresponds to inputs with shape
            `(batch, features, steps)`. It defaults to the `image_data_format`
            value found in your Keras config file at `~/.keras/keras.json`.
            If you never set it, then it will be `"channels_last"`.
        dilation_rate: int or tuple/list of 2 integers, specifying the dilation
            rate to use for dilated convolution.
        activation: Activation function to use. By default hyperbolic tangent
            activation function is applied (`tanh(x)`).
        recurrent_activation: Activation function to use for the recurrent step.
        use_bias: Boolean, whether the layer uses a bias vector.
        kernel_initializer: Initializer for the `kernel` weights matrix,
            used for the linear transformation of the inputs.
        recurrent_initializer: Initializer for the `recurrent_kernel` weights
            matrix, used for the linear transformation of the recurrent state.
        bias_initializer: Initializer for the bias vector.
        unit_forget_bias: Boolean. If `True`, add 1 to the bias of the forget
            gate at initialization.
            Use in combination with `bias_initializer="zeros"`.
            This is recommended in [Jozefowicz et al., 2015](
            http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)
        kernel_regularizer: Regularizer function applied to the `kernel` weights
            matrix.
        recurrent_regularizer: Regularizer function applied to the
            `recurrent_kernel` weights matrix.
        bias_regularizer: Regularizer function applied to the bias vector.
        activity_regularizer: Regularizer function applied to.
        kernel_constraint: Constraint function applied to the `kernel` weights
            matrix.
        recurrent_constraint: Constraint function applied to the
            `recurrent_kernel` weights matrix.
        bias_constraint: Constraint function applied to the bias vector.
        dropout: Float between 0 and 1. Fraction of the units to drop for the
            linear transformation of the inputs.
        recurrent_dropout: Float between 0 and 1. Fraction of the units to drop
            for the linear transformation of the recurrent state.
        seed: Random seed for dropout.
        return_sequences: Boolean. Whether to return the last output
            in the output sequence, or the full sequence. Default: `False`.
        return_state: Boolean. Whether to return the last state in addition
            to the output. Default: `False`.
        go_backwards: Boolean (default: `False`).
            If `True`, process the input sequence backwards and return the
            reversed sequence.
        stateful: Boolean (default False). If `True`, the last state
            for each sample at index i in a batch will be used as initial
            state for the sample of index i in the following batch.
        unroll: Boolean (default: `False`).
            If `True`, the network will be unrolled,
            else a symbolic loop will be used.
            Unrolling can speed-up a RNN,
            although it tends to be more memory-intensive.
            Unrolling is only suitable for short sequences.


    Call arguments:
        inputs: A 5D tensor.
        mask: Binary tensor of shape `(samples, timesteps)` indicating whether a
            given timestep should be masked.
        training: Python boolean indicating whether the layer should behave in
            training mode or in inference mode.
            This is only relevant if `dropout` or `recurrent_dropout` are set.
        initial_state: List of initial state tensors to be passed to the first
            call of the cell.

    Input shape:

    - If `data_format='channels_first'`:
        5D tensor with shape: `(samples, time, channels, rows, cols)`
    - If `data_format='channels_last'`:
        5D tensor with shape: `(samples, time, rows, cols, channels)`

    Output shape:

    - If `return_state`: a list of tensors. The first tensor is the output.
        The remaining tensors are the last states,
        each 4D tensor with shape: `(samples, filters, new_rows, new_cols)` if
        `data_format='channels_first'`
        or shape: `(samples, new_rows, new_cols, filters)` if
        `data_format='channels_last'`. `rows` and `cols` values might have
        changed due to padding.
    - If `return_sequences`: 5D tensor with shape: `(samples, timesteps,
        filters, new_rows, new_cols)` if data_format='channels_first'
        or shape: `(samples, timesteps, new_rows, new_cols, filters)` if
        `data_format='channels_last'`.
    - Else, 4D tensor with shape: `(samples, filters, new_rows, new_cols)` if
        `data_format='channels_first'`
        or shape: `(samples, new_rows, new_cols, filters)` if
        `data_format='channels_last'`.

    References:

    - [Shi et al., 2015](http://arxiv.org/abs/1506.04214v1)
        (the current implementation does not include the feedback loop on the
        cells output).
    )-"


# keras_core.src.layers.rnn.conv_lstm2d.ConvLSTM2D
#' 2D Convolutional LSTM.
#'
#' @description
#' Similar to an LSTM layer, but the input transformations
#' and recurrent transformations are both convolutional.
#'
#' # Call Arguments
#' - `inputs`: A 5D tensor.
#' - `mask`: Binary tensor of shape `(samples, timesteps)` indicating whether a
#'     given timestep should be masked.
#' - `training`: Python boolean indicating whether the layer should behave in
#'     training mode or in inference mode.
#'     This is only relevant if `dropout` or `recurrent_dropout` are set.
#' - `initial_state`: List of initial state tensors to be passed to the first
#'     call of the cell.
#'
#' # Input Shape
#' - If `data_format='channels_first'`:
#'     5D tensor with shape: `(samples, time, channels, rows, cols)`
#' - If `data_format='channels_last'`:
#'     5D tensor with shape: `(samples, time, rows, cols, channels)`
#'
#' # Output Shape
#' - If `return_state`: a list of tensors. The first tensor is the output.
#'     The remaining tensors are the last states,
#'     each 4D tensor with shape: `(samples, filters, new_rows, new_cols)` if
#'     `data_format='channels_first'`
#'     or shape: `(samples, new_rows, new_cols, filters)` if
#'     `data_format='channels_last'`. `rows` and `cols` values might have
#'     changed due to padding.
#' - If `return_sequences`: 5D tensor with shape: `(samples, timesteps,
#'     filters, new_rows, new_cols)` if data_format='channels_first'
#'     or shape: `(samples, timesteps, new_rows, new_cols, filters)` if
#'     `data_format='channels_last'`.
#' - Else, 4D tensor with shape: `(samples, filters, new_rows, new_cols)` if
#'     `data_format='channels_first'`
#'     or shape: `(samples, new_rows, new_cols, filters)` if
#'     `data_format='channels_last'`.
#'
#' # References
#' - [Shi et al., 2015](http://arxiv.org/abs/1506.04214v1)
#'     (the current implementation does not include the feedback loop on the
#'     cells output).
#'
#' @param filters int, the dimension of the output space (the number of filters
#'     in the convolution).
#' @param kernel_size int or tuple/list of 2 integers, specifying the size of the
#'     convolution window.
#' @param strides int or tuple/list of 2 integers, specifying the stride length
#'     of the convolution. `strides > 1` is incompatible with
#'     `dilation_rate > 1`.
#' @param padding string, `"valid"` or `"same"` (case-insensitive).
#'     `"valid"` means no padding. `"same"` results in padding evenly to
#'     the left/right or up/down of the input such that output has the same
#'     height/width dimension as the input.
#' @param data_format string, either `"channels_last"` or `"channels_first"`.
#'     The ordering of the dimensions in the inputs. `"channels_last"`
#'     corresponds to inputs with shape `(batch, steps, features)`
#'     while `"channels_first"` corresponds to inputs with shape
#'     `(batch, features, steps)`. It defaults to the `image_data_format`
#'     value found in your Keras config file at `~/.keras/keras.json`.
#'     If you never set it, then it will be `"channels_last"`.
#' @param dilation_rate int or tuple/list of 2 integers, specifying the dilation
#'     rate to use for dilated convolution.
#' @param activation Activation function to use. By default hyperbolic tangent
#'     activation function is applied (`tanh(x)`).
#' @param recurrent_activation Activation function to use for the recurrent step.
#' @param use_bias Boolean, whether the layer uses a bias vector.
#' @param kernel_initializer Initializer for the `kernel` weights matrix,
#'     used for the linear transformation of the inputs.
#' @param recurrent_initializer Initializer for the `recurrent_kernel` weights
#'     matrix, used for the linear transformation of the recurrent state.
#' @param bias_initializer Initializer for the bias vector.
#' @param unit_forget_bias Boolean. If `True`, add 1 to the bias of the forget
#'     gate at initialization.
#'     Use in combination with `bias_initializer="zeros"`.
#'     This is recommended in [Jozefowicz et al., 2015](
#'     http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)
#' @param kernel_regularizer Regularizer function applied to the `kernel` weights
#'     matrix.
#' @param recurrent_regularizer Regularizer function applied to the
#'     `recurrent_kernel` weights matrix.
#' @param bias_regularizer Regularizer function applied to the bias vector.
#' @param activity_regularizer Regularizer function applied to.
#' @param kernel_constraint Constraint function applied to the `kernel` weights
#'     matrix.
#' @param recurrent_constraint Constraint function applied to the
#'     `recurrent_kernel` weights matrix.
#' @param bias_constraint Constraint function applied to the bias vector.
#' @param dropout Float between 0 and 1. Fraction of the units to drop for the
#'     linear transformation of the inputs.
#' @param recurrent_dropout Float between 0 and 1. Fraction of the units to drop
#'     for the linear transformation of the recurrent state.
#' @param seed Random seed for dropout.
#' @param return_sequences Boolean. Whether to return the last output
#'     in the output sequence, or the full sequence. Default: `False`.
#' @param return_state Boolean. Whether to return the last state in addition
#'     to the output. Default: `False`.
#' @param go_backwards Boolean (default: `False`).
#'     If `True`, process the input sequence backwards and return the
#'     reversed sequence.
#' @param stateful Boolean (default False). If `True`, the last state
#'     for each sample at index i in a batch will be used as initial
#'     state for the sample of index i in the following batch.
#' @param unroll Boolean (default: `False`).
#'     If `True`, the network will be unrolled,
#'     else a symbolic loop will be used.
#'     Unrolling can speed-up a RNN,
#'     although it tends to be more memory-intensive.
#'     Unrolling is only suitable for short sequences.
#'
#' @export
#' @family recurrent layers
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/layers/ConvLSTM2D>
layer_conv_lstm_2d <-
function (object, filters, kernel_size, strides = 1L, padding = "valid",
    data_format = NULL, dilation_rate = 1L, activation = "tanh",
    recurrent_activation = "sigmoid", use_bias = TRUE, kernel_initializer = "glorot_uniform",
    recurrent_initializer = "orthogonal", bias_initializer = "zeros",
    unit_forget_bias = TRUE, kernel_regularizer = NULL, recurrent_regularizer = NULL,
    bias_regularizer = NULL, activity_regularizer = NULL, kernel_constraint = NULL,
    recurrent_constraint = NULL, bias_constraint = NULL, dropout = 0,
    recurrent_dropout = 0, seed = NULL, return_sequences = FALSE,
    return_state = FALSE, go_backwards = FALSE, stateful = FALSE,
    ...)
{
    args <- capture_args2(list(filters = as_integer, kernel_size = as_integer_tuple,
        strides = as_integer_tuple, dilation_rate = as_integer_tuple,
        input_shape = normalize_shape, batch_size = as_integer,
        batch_input_shape = normalize_shape), ignore = "object")
    create_layer(keras$layers$ConvLSTM2D, object, args)
}


# keras$layers$ConvLSTM3D
# keras_core.src.layers.rnn.conv_lstm3d.ConvLSTM3D
r"-(3D Convolutional LSTM.

    Similar to an LSTM layer, but the input transformations
    and recurrent transformations are both convolutional.

    Args:
        filters: int, the dimension of the output space (the number of filters
            in the convolution).
        kernel_size: int or tuple/list of 3 integers, specifying the size of the
            convolution window.
        strides: int or tuple/list of 3 integers, specifying the stride length
            of the convolution. `strides > 1` is incompatible with
            `dilation_rate > 1`.
        padding: string, `"valid"` or `"same"` (case-insensitive).
            `"valid"` means no padding. `"same"` results in padding evenly to
            the left/right or up/down of the input such that output has the same
            height/width dimension as the input.
        data_format: string, either `"channels_last"` or `"channels_first"`.
            The ordering of the dimensions in the inputs. `"channels_last"`
            corresponds to inputs with shape `(batch, steps, features)`
            while `"channels_first"` corresponds to inputs with shape
            `(batch, features, steps)`. It defaults to the `image_data_format`
            value found in your Keras config file at `~/.keras/keras.json`.
            If you never set it, then it will be `"channels_last"`.
        dilation_rate: int or tuple/list of 3 integers, specifying the dilation
            rate to use for dilated convolution.
        activation: Activation function to use. By default hyperbolic tangent
            activation function is applied (`tanh(x)`).
        recurrent_activation: Activation function to use for the recurrent step.
        use_bias: Boolean, whether the layer uses a bias vector.
        kernel_initializer: Initializer for the `kernel` weights matrix,
            used for the linear transformation of the inputs.
        recurrent_initializer: Initializer for the `recurrent_kernel` weights
            matrix, used for the linear transformation of the recurrent state.
        bias_initializer: Initializer for the bias vector.
        unit_forget_bias: Boolean. If `True`, add 1 to the bias of the forget
            gate at initialization.
            Use in combination with `bias_initializer="zeros"`.
            This is recommended in [Jozefowicz et al., 2015](
            http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)
        kernel_regularizer: Regularizer function applied to the `kernel` weights
            matrix.
        recurrent_regularizer: Regularizer function applied to the
            `recurrent_kernel` weights matrix.
        bias_regularizer: Regularizer function applied to the bias vector.
        activity_regularizer: Regularizer function applied to.
        kernel_constraint: Constraint function applied to the `kernel` weights
            matrix.
        recurrent_constraint: Constraint function applied to the
            `recurrent_kernel` weights matrix.
        bias_constraint: Constraint function applied to the bias vector.
        dropout: Float between 0 and 1. Fraction of the units to drop for the
            linear transformation of the inputs.
        recurrent_dropout: Float between 0 and 1. Fraction of the units to drop
            for the linear transformation of the recurrent state.
        seed: Random seed for dropout.
        return_sequences: Boolean. Whether to return the last output
            in the output sequence, or the full sequence. Default: `False`.
        return_state: Boolean. Whether to return the last state in addition
            to the output. Default: `False`.
        go_backwards: Boolean (default: `False`).
            If `True`, process the input sequence backwards and return the
            reversed sequence.
        stateful: Boolean (default False). If `True`, the last state
            for each sample at index i in a batch will be used as initial
            state for the sample of index i in the following batch.
        unroll: Boolean (default: `False`).
            If `True`, the network will be unrolled,
            else a symbolic loop will be used.
            Unrolling can speed-up a RNN,
            although it tends to be more memory-intensive.
            Unrolling is only suitable for short sequences.


    Call arguments:
        inputs: A 6D tensor.
        mask: Binary tensor of shape `(samples, timesteps)` indicating whether a
            given timestep should be masked.
        training: Python boolean indicating whether the layer should behave in
            training mode or in inference mode.
            This is only relevant if `dropout` or `recurrent_dropout` are set.
        initial_state: List of initial state tensors to be passed to the first
            call of the cell.

    Input shape:

    - If `data_format='channels_first'`:
        5D tensor with shape: `(samples, time, channels, *spatial_dims)`
    - If `data_format='channels_last'`:
        5D tensor with shape: `(samples, time, *spatial_dims, channels)`

    Output shape:

    - If `return_state`: a list of tensors. The first tensor is the output.
        The remaining tensors are the last states,
        each 4D tensor with shape: `(samples, filters, *spatial_dims)` if
        `data_format='channels_first'`
        or shape: `(samples, *spatial_dims, filters)` if
        `data_format='channels_last'`.
    - If `return_sequences`: 5D tensor with shape: `(samples, timesteps,
        filters, *spatial_dims)` if data_format='channels_first'
        or shape: `(samples, timesteps, *spatial_dims, filters)` if
        `data_format='channels_last'`.
    - Else, 4D tensor with shape: `(samples, filters, *spatial_dims)` if
        `data_format='channels_first'`
        or shape: `(samples, *spatial_dims, filters)` if
        `data_format='channels_last'`.

    References:

    - [Shi et al., 2015](http://arxiv.org/abs/1506.04214v1)
        (the current implementation does not include the feedback loop on the
        cells output).
    )-"


# keras_core.src.layers.rnn.conv_lstm3d.ConvLSTM3D
#' 3D Convolutional LSTM.
#'
#' @description
#' Similar to an LSTM layer, but the input transformations
#' and recurrent transformations are both convolutional.
#'
#' # Call Arguments
#' - `inputs`: A 6D tensor.
#' - `mask`: Binary tensor of shape `(samples, timesteps)` indicating whether a
#'     given timestep should be masked.
#' - `training`: Python boolean indicating whether the layer should behave in
#'     training mode or in inference mode.
#'     This is only relevant if `dropout` or `recurrent_dropout` are set.
#' - `initial_state`: List of initial state tensors to be passed to the first
#'     call of the cell.
#'
#' # Input Shape
#' - If `data_format='channels_first'`:
#'     5D tensor with shape: `(samples, time, channels, *spatial_dims)`
#' - If `data_format='channels_last'`:
#'     5D tensor with shape: `(samples, time, *spatial_dims, channels)`
#'
#' # Output Shape
#' - If `return_state`: a list of tensors. The first tensor is the output.
#'     The remaining tensors are the last states,
#'     each 4D tensor with shape: `(samples, filters, *spatial_dims)` if
#'     `data_format='channels_first'`
#'     or shape: `(samples, *spatial_dims, filters)` if
#'     `data_format='channels_last'`.
#' - If `return_sequences`: 5D tensor with shape: `(samples, timesteps,
#'     filters, *spatial_dims)` if data_format='channels_first'
#'     or shape: `(samples, timesteps, *spatial_dims, filters)` if
#'     `data_format='channels_last'`.
#' - Else, 4D tensor with shape: `(samples, filters, *spatial_dims)` if
#'     `data_format='channels_first'`
#'     or shape: `(samples, *spatial_dims, filters)` if
#'     `data_format='channels_last'`.
#'
#' # References
#' - [Shi et al., 2015](http://arxiv.org/abs/1506.04214v1)
#'     (the current implementation does not include the feedback loop on the
#'     cells output).
#'
#' @param filters int, the dimension of the output space (the number of filters
#'     in the convolution).
#' @param kernel_size int or tuple/list of 3 integers, specifying the size of the
#'     convolution window.
#' @param strides int or tuple/list of 3 integers, specifying the stride length
#'     of the convolution. `strides > 1` is incompatible with
#'     `dilation_rate > 1`.
#' @param padding string, `"valid"` or `"same"` (case-insensitive).
#'     `"valid"` means no padding. `"same"` results in padding evenly to
#'     the left/right or up/down of the input such that output has the same
#'     height/width dimension as the input.
#' @param data_format string, either `"channels_last"` or `"channels_first"`.
#'     The ordering of the dimensions in the inputs. `"channels_last"`
#'     corresponds to inputs with shape `(batch, steps, features)`
#'     while `"channels_first"` corresponds to inputs with shape
#'     `(batch, features, steps)`. It defaults to the `image_data_format`
#'     value found in your Keras config file at `~/.keras/keras.json`.
#'     If you never set it, then it will be `"channels_last"`.
#' @param dilation_rate int or tuple/list of 3 integers, specifying the dilation
#'     rate to use for dilated convolution.
#' @param activation Activation function to use. By default hyperbolic tangent
#'     activation function is applied (`tanh(x)`).
#' @param recurrent_activation Activation function to use for the recurrent step.
#' @param use_bias Boolean, whether the layer uses a bias vector.
#' @param kernel_initializer Initializer for the `kernel` weights matrix,
#'     used for the linear transformation of the inputs.
#' @param recurrent_initializer Initializer for the `recurrent_kernel` weights
#'     matrix, used for the linear transformation of the recurrent state.
#' @param bias_initializer Initializer for the bias vector.
#' @param unit_forget_bias Boolean. If `True`, add 1 to the bias of the forget
#'     gate at initialization.
#'     Use in combination with `bias_initializer="zeros"`.
#'     This is recommended in [Jozefowicz et al., 2015](
#'     http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)
#' @param kernel_regularizer Regularizer function applied to the `kernel` weights
#'     matrix.
#' @param recurrent_regularizer Regularizer function applied to the
#'     `recurrent_kernel` weights matrix.
#' @param bias_regularizer Regularizer function applied to the bias vector.
#' @param activity_regularizer Regularizer function applied to.
#' @param kernel_constraint Constraint function applied to the `kernel` weights
#'     matrix.
#' @param recurrent_constraint Constraint function applied to the
#'     `recurrent_kernel` weights matrix.
#' @param bias_constraint Constraint function applied to the bias vector.
#' @param dropout Float between 0 and 1. Fraction of the units to drop for the
#'     linear transformation of the inputs.
#' @param recurrent_dropout Float between 0 and 1. Fraction of the units to drop
#'     for the linear transformation of the recurrent state.
#' @param seed Random seed for dropout.
#' @param return_sequences Boolean. Whether to return the last output
#'     in the output sequence, or the full sequence. Default: `False`.
#' @param return_state Boolean. Whether to return the last state in addition
#'     to the output. Default: `False`.
#' @param go_backwards Boolean (default: `False`).
#'     If `True`, process the input sequence backwards and return the
#'     reversed sequence.
#' @param stateful Boolean (default False). If `True`, the last state
#'     for each sample at index i in a batch will be used as initial
#'     state for the sample of index i in the following batch.
#' @param unroll Boolean (default: `False`).
#'     If `True`, the network will be unrolled,
#'     else a symbolic loop will be used.
#'     Unrolling can speed-up a RNN,
#'     although it tends to be more memory-intensive.
#'     Unrolling is only suitable for short sequences.
#'
#' @export
#' @family recurrent layers
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/layers/ConvLSTM3D>
layer_conv_lstm_3d <-
function (object, filters, kernel_size, strides = 1L, padding = "valid",
    data_format = NULL, dilation_rate = 1L, activation = "tanh",
    recurrent_activation = "sigmoid", use_bias = TRUE, kernel_initializer = "glorot_uniform",
    recurrent_initializer = "orthogonal", bias_initializer = "zeros",
    unit_forget_bias = TRUE, kernel_regularizer = NULL, recurrent_regularizer = NULL,
    bias_regularizer = NULL, activity_regularizer = NULL, kernel_constraint = NULL,
    recurrent_constraint = NULL, bias_constraint = NULL, dropout = 0,
    recurrent_dropout = 0, seed = NULL, return_sequences = FALSE,
    return_state = FALSE, go_backwards = FALSE, stateful = FALSE,
    ...)
{
    args <- capture_args2(list(filters = as_integer, kernel_size = as_integer_tuple,
        strides = as_integer_tuple, dilation_rate = as_integer_tuple,
        input_shape = normalize_shape, batch_size = as_integer,
        batch_input_shape = normalize_shape), ignore = "object")
    create_layer(keras$layers$ConvLSTM3D, object, args)
}


# keras$layers$GRU
# keras_core.src.layers.rnn.gru.GRU
r"-(Gated Recurrent Unit - Cho et al. 2014.

    Based on available runtime hardware and constraints, this layer
    will choose different implementations (cuDNN-based or backend-native)
    to maximize the performance. If a GPU is available and all
    the arguments to the layer meet the requirement of the cuDNN kernel
    (see below for details), the layer will use a fast cuDNN implementation
    when using the TensorFlow backend.

    The requirements to use the cuDNN implementation are:

    1. `activation` == `tanh`
    2. `recurrent_activation` == `sigmoid`
    3. `dropout` == 0 and `recurrent_dropout` == 0
    4. `unroll` is `False`
    5. `use_bias` is `True`
    6. `reset_after` is `True`
    7. Inputs, if use masking, are strictly right-padded.
    8. Eager execution is enabled in the outermost context.

    There are two variants of the GRU implementation. The default one is based
    on [v3](https://arxiv.org/abs/1406.1078v3) and has reset gate applied to
    hidden state before matrix multiplication. The other one is based on
    [original](https://arxiv.org/abs/1406.1078v1) and has the order reversed.

    The second variant is compatible with CuDNNGRU (GPU-only) and allows
    inference on CPU. Thus it has separate biases for `kernel` and
    `recurrent_kernel`. To use this variant, set `reset_after=True` and
    `recurrent_activation='sigmoid'`.

    For example:

    >>> inputs = np.random.random((32, 10, 8))
    >>> gru = keras.layers.GRU(4)
    >>> output = gru(inputs)
    >>> output.shape
    (32, 4)
    >>> gru = keras.layers.GRU(4, return_sequences=True, return_state=True)
    >>> whole_sequence_output, final_state = gru(inputs)
    >>> whole_sequence_output.shape
    (32, 10, 4)
    >>> final_state.shape
    (32, 4)

    Args:
        units: Positive integer, dimensionality of the output space.
        activation: Activation function to use.
            Default: hyperbolic tangent (`tanh`).
            If you pass `None`, no activation is applied
            (ie. "linear" activation: `a(x) = x`).
        recurrent_activation: Activation function to use
            for the recurrent step.
            Default: sigmoid (`sigmoid`).
            If you pass `None`, no activation is applied
            (ie. "linear" activation: `a(x) = x`).
        use_bias: Boolean, (default `True`), whether the layer
            should use a bias vector.
        kernel_initializer: Initializer for the `kernel` weights matrix,
            used for the linear transformation of the inputs. Default:
            `"glorot_uniform"`.
        recurrent_initializer: Initializer for the `recurrent_kernel`
            weights matrix, used for the linear transformation of the recurrent
            state. Default: `"orthogonal"`.
        bias_initializer: Initializer for the bias vector. Default: `"zeros"`.
        kernel_regularizer: Regularizer function applied to the `kernel` weights
            matrix. Default: `None`.
        recurrent_regularizer: Regularizer function applied to the
            `recurrent_kernel` weights matrix. Default: `None`.
        bias_regularizer: Regularizer function applied to the bias vector.
            Default: `None`.
        activity_regularizer: Regularizer function applied to the output of the
            layer (its "activation"). Default: `None`.
        kernel_constraint: Constraint function applied to the `kernel` weights
            matrix. Default: `None`.
        recurrent_constraint: Constraint function applied to the
            `recurrent_kernel` weights matrix. Default: `None`.
        bias_constraint: Constraint function applied to the bias vector.
            Default: `None`.
        dropout: Float between 0 and 1. Fraction of the units to drop for the
            linear transformation of the inputs. Default: 0.
        recurrent_dropout: Float between 0 and 1. Fraction of the units to drop
            for the linear transformation of the recurrent state. Default: 0.
        seed: Random seed for dropout.
        return_sequences: Boolean. Whether to return the last output
            in the output sequence, or the full sequence. Default: `False`.
        return_state: Boolean. Whether to return the last state in addition
            to the output. Default: `False`.
        go_backwards: Boolean (default `False`).
            If `True`, process the input sequence backwards and return the
            reversed sequence.
        stateful: Boolean (default: `False`). If `True`, the last state
            for each sample at index i in a batch will be used as initial
            state for the sample of index i in the following batch.
        unroll: Boolean (default: `False`).
            If `True`, the network will be unrolled,
            else a symbolic loop will be used.
            Unrolling can speed-up a RNN,
            although it tends to be more memory-intensive.
            Unrolling is only suitable for short sequences.
        reset_after: GRU convention (whether to apply reset gate after or
            before matrix multiplication). `False` is `"before"`,
            `True` is `"after"` (default and cuDNN compatible).

    Call arguments:
        inputs: A 3D tensor, with shape `(batch, timesteps, feature)`.
        mask: Binary tensor of shape `(samples, timesteps)` indicating whether
            a given timestep should be masked  (optional).
            An individual `True` entry indicates that the corresponding timestep
            should be utilized, while a `False` entry indicates that the
            corresponding timestep should be ignored. Defaults to `None`.
        training: Python boolean indicating whether the layer should behave in
            training mode or in inference mode. This argument is passed to the
            cell when calling it. This is only relevant if `dropout` or
            `recurrent_dropout` is used  (optional). Defaults to `None`.
        initial_state: List of initial state tensors to be passed to the first
            call of the cell (optional, `None` causes creation
            of zero-filled initial state tensors). Defaults to `None`.
    )-"


# keras_core.src.layers.rnn.gru.GRU
#' Gated Recurrent Unit - Cho et al. 2014.
#'
#' @description
#' Based on available runtime hardware and constraints, this layer
#' will choose different implementations (cuDNN-based or backend-native)
#' to maximize the performance. If a GPU is available and all
#' the arguments to the layer meet the requirement of the cuDNN kernel
#' (see below for details), the layer will use a fast cuDNN implementation
#' when using the TensorFlow backend.
#'
#' The requirements to use the cuDNN implementation are:
#'
#' 1. `activation` == `tanh`
#' 2. `recurrent_activation` == `sigmoid`
#' 3. `dropout` == 0 and `recurrent_dropout` == 0
#' 4. `unroll` is `False`
#' 5. `use_bias` is `True`
#' 6. `reset_after` is `True`
#' 7. Inputs, if use masking, are strictly right-padded.
#' 8. Eager execution is enabled in the outermost context.
#'
#' There are two variants of the GRU implementation. The default one is based
#' on [v3](https://arxiv.org/abs/1406.1078v3) and has reset gate applied to
#' hidden state before matrix multiplication. The other one is based on
#' [original](https://arxiv.org/abs/1406.1078v1) and has the order reversed.
#'
#' The second variant is compatible with CuDNNGRU (GPU-only) and allows
#' inference on CPU. Thus it has separate biases for `kernel` and
#' `recurrent_kernel`. To use this variant, set `reset_after=True` and
#' `recurrent_activation='sigmoid'`.
#'
#' For example:
#'
#' ```python
#' inputs = np.random.random((32, 10, 8))
#' gru = keras.layers.GRU(4)
#' output = gru(inputs)
#' output.shape
#' # (32, 4)
#' gru = keras.layers.GRU(4, return_sequences=True, return_state=True)
#' whole_sequence_output, final_state = gru(inputs)
#' whole_sequence_output.shape
#' # (32, 10, 4)
#' final_state.shape
#' # (32, 4)
#' ```
#'
#' # Call Arguments
#' - `inputs`: A 3D tensor, with shape `(batch, timesteps, feature)`.
#' - `mask`: Binary tensor of shape `(samples, timesteps)` indicating whether
#'     a given timestep should be masked  (optional).
#'     An individual `True` entry indicates that the corresponding timestep
#'     should be utilized, while a `False` entry indicates that the
#'     corresponding timestep should be ignored. Defaults to `None`.
#' - `training`: Python boolean indicating whether the layer should behave in
#'     training mode or in inference mode. This argument is passed to the
#'     cell when calling it. This is only relevant if `dropout` or
#'     `recurrent_dropout` is used  (optional). Defaults to `None`.
#' - `initial_state`: List of initial state tensors to be passed to the first
#'     call of the cell (optional, `None` causes creation
#'     of zero-filled initial state tensors). Defaults to `None`.
#'
#' @param units Positive integer, dimensionality of the output space.
#' @param activation Activation function to use.
#'     Default: hyperbolic tangent (`tanh`).
#'     If you pass `None`, no activation is applied
#'     (ie. "linear" activation: `a(x) = x`).
#' @param recurrent_activation Activation function to use
#'     for the recurrent step.
#'     Default: sigmoid (`sigmoid`).
#'     If you pass `None`, no activation is applied
#'     (ie. "linear" activation: `a(x) = x`).
#' @param use_bias Boolean, (default `True`), whether the layer
#'     should use a bias vector.
#' @param kernel_initializer Initializer for the `kernel` weights matrix,
#'     used for the linear transformation of the inputs. Default:
#'     `"glorot_uniform"`.
#' @param recurrent_initializer Initializer for the `recurrent_kernel`
#'     weights matrix, used for the linear transformation of the recurrent
#'     state. Default: `"orthogonal"`.
#' @param bias_initializer Initializer for the bias vector. Default: `"zeros"`.
#' @param kernel_regularizer Regularizer function applied to the `kernel` weights
#'     matrix. Default: `None`.
#' @param recurrent_regularizer Regularizer function applied to the
#'     `recurrent_kernel` weights matrix. Default: `None`.
#' @param bias_regularizer Regularizer function applied to the bias vector.
#'     Default: `None`.
#' @param activity_regularizer Regularizer function applied to the output of the
#'     layer (its "activation"). Default: `None`.
#' @param kernel_constraint Constraint function applied to the `kernel` weights
#'     matrix. Default: `None`.
#' @param recurrent_constraint Constraint function applied to the
#'     `recurrent_kernel` weights matrix. Default: `None`.
#' @param bias_constraint Constraint function applied to the bias vector.
#'     Default: `None`.
#' @param dropout Float between 0 and 1. Fraction of the units to drop for the
#'     linear transformation of the inputs. Default: 0.
#' @param recurrent_dropout Float between 0 and 1. Fraction of the units to drop
#'     for the linear transformation of the recurrent state. Default: 0.
#' @param seed Random seed for dropout.
#' @param return_sequences Boolean. Whether to return the last output
#'     in the output sequence, or the full sequence. Default: `False`.
#' @param return_state Boolean. Whether to return the last state in addition
#'     to the output. Default: `False`.
#' @param go_backwards Boolean (default `False`).
#'     If `True`, process the input sequence backwards and return the
#'     reversed sequence.
#' @param stateful Boolean (default: `False`). If `True`, the last state
#'     for each sample at index i in a batch will be used as initial
#'     state for the sample of index i in the following batch.
#' @param unroll Boolean (default: `False`).
#'     If `True`, the network will be unrolled,
#'     else a symbolic loop will be used.
#'     Unrolling can speed-up a RNN,
#'     although it tends to be more memory-intensive.
#'     Unrolling is only suitable for short sequences.
#' @param reset_after GRU convention (whether to apply reset gate after or
#'     before matrix multiplication). `False` is `"before"`,
#'     `True` is `"after"` (default and cuDNN compatible).
#'
#' @export
#' @family recurrent layers
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU>
layer_gru <-
function (object, units, activation = "tanh", recurrent_activation = "sigmoid",
    use_bias = TRUE, kernel_initializer = "glorot_uniform", recurrent_initializer = "orthogonal",
    bias_initializer = "zeros", kernel_regularizer = NULL, recurrent_regularizer = NULL,
    bias_regularizer = NULL, activity_regularizer = NULL, kernel_constraint = NULL,
    recurrent_constraint = NULL, bias_constraint = NULL, dropout = 0,
    recurrent_dropout = 0, seed = NULL, return_sequences = FALSE,
    return_state = FALSE, go_backwards = FALSE, stateful = FALSE,
    unroll = FALSE, reset_after = TRUE, ...)
{
    args <- capture_args2(list(units = as_integer, input_shape = normalize_shape,
        batch_size = as_integer, batch_input_shape = normalize_shape),
        ignore = "object")
    create_layer(keras$layers$GRU, object, args)
}


# keras$layers$GRUCell
# keras_core.src.layers.rnn.gru.GRUCell
r"-(Cell class for the GRU layer.

    This class processes one step within the whole time sequence input, whereas
    `keras.layer.GRU` processes the whole sequence.

    Args:
        units: Positive integer, dimensionality of the output space.
        activation: Activation function to use. Default: hyperbolic tangent
            (`tanh`). If you pass None, no activation is applied
            (ie. "linear" activation: `a(x) = x`).
        recurrent_activation: Activation function to use for the recurrent step.
            Default: sigmoid (`sigmoid`). If you pass `None`, no activation is
            applied (ie. "linear" activation: `a(x) = x`).
        use_bias: Boolean, (default `True`), whether the layer
            should use a bias vector.
        kernel_initializer: Initializer for the `kernel` weights matrix,
            used for the linear transformation of the inputs. Default:
            `"glorot_uniform"`.
        recurrent_initializer: Initializer for the `recurrent_kernel`
            weights matrix, used for the linear transformation
            of the recurrent state. Default: `"orthogonal"`.
        bias_initializer: Initializer for the bias vector. Default: `"zeros"`.
        kernel_regularizer: Regularizer function applied to the `kernel` weights
            matrix. Default: `None`.
        recurrent_regularizer: Regularizer function applied to the
            `recurrent_kernel` weights matrix. Default: `None`.
        bias_regularizer: Regularizer function applied to the bias vector.
            Default: `None`.
        kernel_constraint: Constraint function applied to the `kernel` weights
            matrix. Default: `None`.
        recurrent_constraint: Constraint function applied to the
            `recurrent_kernel` weights matrix. Default: `None`.
        bias_constraint: Constraint function applied to the bias vector.
            Default: `None`.
        dropout: Float between 0 and 1. Fraction of the units to drop for the
            linear transformation of the inputs. Default: 0.
        recurrent_dropout: Float between 0 and 1. Fraction of the units to drop
            for the linear transformation of the recurrent state. Default: 0.
        reset_after: GRU convention (whether to apply reset gate after or
            before matrix multiplication). False = "before",
            True = "after" (default and cuDNN compatible).
        seed: Random seed for dropout.

    Call arguments:
        inputs: A 2D tensor, with shape `(batch, features)`.
        states: A 2D tensor with shape `(batch, units)`, which is the state
            from the previous time step.
        training: Python boolean indicating whether the layer should behave in
            training mode or in inference mode. Only relevant when `dropout` or
            `recurrent_dropout` is used.

    Example:

    >>> inputs = np.random.random((32, 10, 8))
    >>> rnn = keras.layers.RNN(keras.layers.GRUCell(4))
    >>> output = rnn(inputs)
    >>> output.shape
    (32, 4)
    >>> rnn = keras.layers.RNN(
    ...    keras.layers.GRUCell(4),
    ...    return_sequences=True,
    ...    return_state=True)
    >>> whole_sequence_output, final_state = rnn(inputs)
    >>> whole_sequence_output.shape
    (32, 10, 4)
    >>> final_state.shape
    (32, 4)
    )-"


# keras_core.src.layers.rnn.gru.GRUCell
#' Cell class for the GRU layer.
#'
#' @description
#' This class processes one step within the whole time sequence input, whereas
#' `keras.layer.GRU` processes the whole sequence.
#'
#' # Call Arguments
#' - `inputs`: A 2D tensor, with shape `(batch, features)`.
#' - `states`: A 2D tensor with shape `(batch, units)`, which is the state
#'     from the previous time step.
#' - `training`: Python boolean indicating whether the layer should behave in
#'     training mode or in inference mode. Only relevant when `dropout` or
#'     `recurrent_dropout` is used.
#'
#' # Examples
#' ```python
#' inputs = np.random.random((32, 10, 8))
#' rnn = keras.layers.RNN(keras.layers.GRUCell(4))
#' output = rnn(inputs)
#' output.shape
#' # (32, 4)
#' rnn = keras.layers.RNN(
#'    keras.layers.GRUCell(4),
#'    return_sequences=True,
#'    return_state=True)
#' whole_sequence_output, final_state = rnn(inputs)
#' whole_sequence_output.shape
#' # (32, 10, 4)
#' final_state.shape
#' # (32, 4)
#' ```
#'
#' @param units Positive integer, dimensionality of the output space.
#' @param activation Activation function to use. Default: hyperbolic tangent
#'     (`tanh`). If you pass None, no activation is applied
#'     (ie. "linear" activation: `a(x) = x`).
#' @param recurrent_activation Activation function to use for the recurrent step.
#'     Default: sigmoid (`sigmoid`). If you pass `None`, no activation is
#'     applied (ie. "linear" activation: `a(x) = x`).
#' @param use_bias Boolean, (default `True`), whether the layer
#'     should use a bias vector.
#' @param kernel_initializer Initializer for the `kernel` weights matrix,
#'     used for the linear transformation of the inputs. Default:
#'     `"glorot_uniform"`.
#' @param recurrent_initializer Initializer for the `recurrent_kernel`
#'     weights matrix, used for the linear transformation
#'     of the recurrent state. Default: `"orthogonal"`.
#' @param bias_initializer Initializer for the bias vector. Default: `"zeros"`.
#' @param kernel_regularizer Regularizer function applied to the `kernel` weights
#'     matrix. Default: `None`.
#' @param recurrent_regularizer Regularizer function applied to the
#'     `recurrent_kernel` weights matrix. Default: `None`.
#' @param bias_regularizer Regularizer function applied to the bias vector.
#'     Default: `None`.
#' @param kernel_constraint Constraint function applied to the `kernel` weights
#'     matrix. Default: `None`.
#' @param recurrent_constraint Constraint function applied to the
#'     `recurrent_kernel` weights matrix. Default: `None`.
#' @param bias_constraint Constraint function applied to the bias vector.
#'     Default: `None`.
#' @param dropout Float between 0 and 1. Fraction of the units to drop for the
#'     linear transformation of the inputs. Default: 0.
#' @param recurrent_dropout Float between 0 and 1. Fraction of the units to drop
#'     for the linear transformation of the recurrent state. Default: 0.
#' @param reset_after GRU convention (whether to apply reset gate after or
#'     before matrix multiplication). False = "before",
#'     True = "after" (default and cuDNN compatible).
#' @param seed Random seed for dropout.
#'
#' @export
#' @family recurrent layers
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRUCell>
layer_gru_cell <-
function (units, activation = "tanh", recurrent_activation = "sigmoid",
    use_bias = TRUE, kernel_initializer = "glorot_uniform", recurrent_initializer = "orthogonal",
    bias_initializer = "zeros", kernel_regularizer = NULL, recurrent_regularizer = NULL,
    bias_regularizer = NULL, kernel_constraint = NULL, recurrent_constraint = NULL,
    bias_constraint = NULL, dropout = 0, recurrent_dropout = 0,
    reset_after = TRUE, seed = NULL, ...)
{
    args <- capture_args2(list(units = as_integer, input_shape = normalize_shape,
        batch_size = as_integer, batch_input_shape = normalize_shape))
    do.call(keras$layers$GRUCell, args)
}


# keras$layers$LSTM
# keras_core.src.layers.rnn.lstm.LSTM
r"-(Long Short-Term Memory layer - Hochreiter 1997.

    Based on available runtime hardware and constraints, this layer
    will choose different implementations (cuDNN-based or backend-native)
    to maximize the performance. If a GPU is available and all
    the arguments to the layer meet the requirement of the cuDNN kernel
    (see below for details), the layer will use a fast cuDNN implementation
    when using the TensorFlow backend.
    The requirements to use the cuDNN implementation are:

    1. `activation` == `tanh`
    2. `recurrent_activation` == `sigmoid`
    3. `dropout` == 0 and `recurrent_dropout` == 0
    4. `unroll` is `False`
    5. `use_bias` is `True`
    6. Inputs, if use masking, are strictly right-padded.
    7. Eager execution is enabled in the outermost context.

    For example:

    >>> inputs = np.random.random((32, 10, 8))
    >>> lstm = keras.layers.LSTM(4)
    >>> output = lstm(inputs)
    >>> output.shape
    (32, 4)
    >>> lstm = keras.layers.LSTM(
    ...     4, return_sequences=True, return_state=True)
    >>> whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)
    >>> whole_seq_output.shape
    (32, 10, 4)
    >>> final_memory_state.shape
    (32, 4)
    >>> final_carry_state.shape
    (32, 4)

    Args:
        units: Positive integer, dimensionality of the output space.
        activation: Activation function to use.
            Default: hyperbolic tangent (`tanh`).
            If you pass `None`, no activation is applied
            (ie. "linear" activation: `a(x) = x`).
        recurrent_activation: Activation function to use
            for the recurrent step.
            Default: sigmoid (`sigmoid`).
            If you pass `None`, no activation is applied
            (ie. "linear" activation: `a(x) = x`).
        use_bias: Boolean, (default `True`), whether the layer
            should use a bias vector.
        kernel_initializer: Initializer for the `kernel` weights matrix,
            used for the linear transformation of the inputs. Default:
            `"glorot_uniform"`.
        recurrent_initializer: Initializer for the `recurrent_kernel`
            weights matrix, used for the linear transformation of the recurrent
            state. Default: `"orthogonal"`.
        bias_initializer: Initializer for the bias vector. Default: `"zeros"`.
        unit_forget_bias: Boolean (default `True`). If `True`,
            add 1 to the bias of the forget gate at initialization.
            Setting it to `True` will also force `bias_initializer="zeros"`.
            This is recommended in [Jozefowicz et al.](
            https://github.com/mlresearch/v37/blob/gh-pages/jozefowicz15.pdf)
        kernel_regularizer: Regularizer function applied to the `kernel` weights
            matrix. Default: `None`.
        recurrent_regularizer: Regularizer function applied to the
            `recurrent_kernel` weights matrix. Default: `None`.
        bias_regularizer: Regularizer function applied to the bias vector.
            Default: `None`.
        activity_regularizer: Regularizer function applied to the output of the
            layer (its "activation"). Default: `None`.
        kernel_constraint: Constraint function applied to the `kernel` weights
            matrix. Default: `None`.
        recurrent_constraint: Constraint function applied to the
            `recurrent_kernel` weights matrix. Default: `None`.
        bias_constraint: Constraint function applied to the bias vector.
            Default: `None`.
        dropout: Float between 0 and 1. Fraction of the units to drop for the
            linear transformation of the inputs. Default: 0.
        recurrent_dropout: Float between 0 and 1. Fraction of the units to drop
            for the linear transformation of the recurrent state. Default: 0.
        seed: Random seed for dropout.
        return_sequences: Boolean. Whether to return the last output
            in the output sequence, or the full sequence. Default: `False`.
        return_state: Boolean. Whether to return the last state in addition
            to the output. Default: `False`.
        go_backwards: Boolean (default: `False`).
            If `True`, process the input sequence backwards and return the
            reversed sequence.
        stateful: Boolean (default: `False`). If `True`, the last state
            for each sample at index i in a batch will be used as initial
            state for the sample of index i in the following batch.
        unroll: Boolean (default False).
            If `True`, the network will be unrolled,
            else a symbolic loop will be used.
            Unrolling can speed-up a RNN,
            although it tends to be more memory-intensive.
            Unrolling is only suitable for short sequences.

    Call arguments:
        inputs: A 3D tensor, with shape `(batch, timesteps, feature)`.
        mask: Binary tensor of shape `(samples, timesteps)` indicating whether
            a given timestep should be masked  (optional).
            An individual `True` entry indicates that the corresponding timestep
            should be utilized, while a `False` entry indicates that the
            corresponding timestep should be ignored. Defaults to `None`.
        training: Python boolean indicating whether the layer should behave in
            training mode or in inference mode. This argument is passed to the
            cell when calling it. This is only relevant if `dropout` or
            `recurrent_dropout` is used  (optional). Defaults to `None`.
        initial_state: List of initial state tensors to be passed to the first
            call of the cell (optional, `None` causes creation
            of zero-filled initial state tensors). Defaults to `None`.
    )-"


# keras_core.src.layers.rnn.lstm.LSTM
#' Long Short-Term Memory layer - Hochreiter 1997.
#'
#' @description
#' Based on available runtime hardware and constraints, this layer
#' will choose different implementations (cuDNN-based or backend-native)
#' to maximize the performance. If a GPU is available and all
#' the arguments to the layer meet the requirement of the cuDNN kernel
#' (see below for details), the layer will use a fast cuDNN implementation
#' when using the TensorFlow backend.
#' The requirements to use the cuDNN implementation are:
#'
#' 1. `activation` == `tanh`
#' 2. `recurrent_activation` == `sigmoid`
#' 3. `dropout` == 0 and `recurrent_dropout` == 0
#' 4. `unroll` is `False`
#' 5. `use_bias` is `True`
#' 6. Inputs, if use masking, are strictly right-padded.
#' 7. Eager execution is enabled in the outermost context.
#'
#' For example:
#'
#' ```python
#' inputs = np.random.random((32, 10, 8))
#' lstm = keras.layers.LSTM(4)
#' output = lstm(inputs)
#' output.shape
#' # (32, 4)
#' lstm = keras.layers.LSTM(
#'     4, return_sequences=True, return_state=True)
#' whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)
#' whole_seq_output.shape
#' # (32, 10, 4)
#' final_memory_state.shape
#' # (32, 4)
#' final_carry_state.shape
#' # (32, 4)
#' ```
#'
#' # Call Arguments
#' - `inputs`: A 3D tensor, with shape `(batch, timesteps, feature)`.
#' - `mask`: Binary tensor of shape `(samples, timesteps)` indicating whether
#'     a given timestep should be masked  (optional).
#'     An individual `True` entry indicates that the corresponding timestep
#'     should be utilized, while a `False` entry indicates that the
#'     corresponding timestep should be ignored. Defaults to `None`.
#' - `training`: Python boolean indicating whether the layer should behave in
#'     training mode or in inference mode. This argument is passed to the
#'     cell when calling it. This is only relevant if `dropout` or
#'     `recurrent_dropout` is used  (optional). Defaults to `None`.
#' - `initial_state`: List of initial state tensors to be passed to the first
#'     call of the cell (optional, `None` causes creation
#'     of zero-filled initial state tensors). Defaults to `None`.
#'
#' @param units Positive integer, dimensionality of the output space.
#' @param activation Activation function to use.
#'     Default: hyperbolic tangent (`tanh`).
#'     If you pass `None`, no activation is applied
#'     (ie. "linear" activation: `a(x) = x`).
#' @param recurrent_activation Activation function to use
#'     for the recurrent step.
#'     Default: sigmoid (`sigmoid`).
#'     If you pass `None`, no activation is applied
#'     (ie. "linear" activation: `a(x) = x`).
#' @param use_bias Boolean, (default `True`), whether the layer
#'     should use a bias vector.
#' @param kernel_initializer Initializer for the `kernel` weights matrix,
#'     used for the linear transformation of the inputs. Default:
#'     `"glorot_uniform"`.
#' @param recurrent_initializer Initializer for the `recurrent_kernel`
#'     weights matrix, used for the linear transformation of the recurrent
#'     state. Default: `"orthogonal"`.
#' @param bias_initializer Initializer for the bias vector. Default: `"zeros"`.
#' @param unit_forget_bias Boolean (default `True`). If `True`,
#'     add 1 to the bias of the forget gate at initialization.
#'     Setting it to `True` will also force `bias_initializer="zeros"`.
#'     This is recommended in [Jozefowicz et al.](
#'     https://github.com/mlresearch/v37/blob/gh-pages/jozefowicz15.pdf)
#' @param kernel_regularizer Regularizer function applied to the `kernel` weights
#'     matrix. Default: `None`.
#' @param recurrent_regularizer Regularizer function applied to the
#'     `recurrent_kernel` weights matrix. Default: `None`.
#' @param bias_regularizer Regularizer function applied to the bias vector.
#'     Default: `None`.
#' @param activity_regularizer Regularizer function applied to the output of the
#'     layer (its "activation"). Default: `None`.
#' @param kernel_constraint Constraint function applied to the `kernel` weights
#'     matrix. Default: `None`.
#' @param recurrent_constraint Constraint function applied to the
#'     `recurrent_kernel` weights matrix. Default: `None`.
#' @param bias_constraint Constraint function applied to the bias vector.
#'     Default: `None`.
#' @param dropout Float between 0 and 1. Fraction of the units to drop for the
#'     linear transformation of the inputs. Default: 0.
#' @param recurrent_dropout Float between 0 and 1. Fraction of the units to drop
#'     for the linear transformation of the recurrent state. Default: 0.
#' @param seed Random seed for dropout.
#' @param return_sequences Boolean. Whether to return the last output
#'     in the output sequence, or the full sequence. Default: `False`.
#' @param return_state Boolean. Whether to return the last state in addition
#'     to the output. Default: `False`.
#' @param go_backwards Boolean (default: `False`).
#'     If `True`, process the input sequence backwards and return the
#'     reversed sequence.
#' @param stateful Boolean (default: `False`). If `True`, the last state
#'     for each sample at index i in a batch will be used as initial
#'     state for the sample of index i in the following batch.
#' @param unroll Boolean (default False).
#'     If `True`, the network will be unrolled,
#'     else a symbolic loop will be used.
#'     Unrolling can speed-up a RNN,
#'     although it tends to be more memory-intensive.
#'     Unrolling is only suitable for short sequences.
#'
#' @export
#' @family recurrent layers
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM>
layer_lstm <-
function (object, units, activation = "tanh", recurrent_activation = "sigmoid",
    use_bias = TRUE, kernel_initializer = "glorot_uniform", recurrent_initializer = "orthogonal",
    bias_initializer = "zeros", unit_forget_bias = TRUE, kernel_regularizer = NULL,
    recurrent_regularizer = NULL, bias_regularizer = NULL, activity_regularizer = NULL,
    kernel_constraint = NULL, recurrent_constraint = NULL, bias_constraint = NULL,
    dropout = 0, recurrent_dropout = 0, seed = NULL, return_sequences = FALSE,
    return_state = FALSE, go_backwards = FALSE, stateful = FALSE,
    unroll = FALSE, ...)
{
    args <- capture_args2(list(units = as_integer, input_shape = normalize_shape,
        batch_size = as_integer, batch_input_shape = normalize_shape),
        ignore = "object")
    create_layer(keras$layers$LSTM, object, args)
}


# keras$layers$LSTMCell
# keras_core.src.layers.rnn.lstm.LSTMCell
r"-(Cell class for the LSTM layer.

    This class processes one step within the whole time sequence input, whereas
    `keras.layer.LSTM` processes the whole sequence.

    Args:
        units: Positive integer, dimensionality of the output space.
        activation: Activation function to use. Default: hyperbolic tangent
            (`tanh`). If you pass None, no activation is applied
            (ie. "linear" activation: `a(x) = x`).
        recurrent_activation: Activation function to use for the recurrent step.
            Default: sigmoid (`sigmoid`). If you pass `None`, no activation is
            applied (ie. "linear" activation: `a(x) = x`).
        use_bias: Boolean, (default `True`), whether the layer
            should use a bias vector.
        kernel_initializer: Initializer for the `kernel` weights matrix,
            used for the linear transformation of the inputs. Default:
            `"glorot_uniform"`.
        recurrent_initializer: Initializer for the `recurrent_kernel`
            weights matrix, used for the linear transformation
            of the recurrent state. Default: `"orthogonal"`.
        bias_initializer: Initializer for the bias vector. Default: `"zeros"`.
        unit_forget_bias: Boolean (default `True`). If `True`,
            add 1 to the bias of the forget gate at initialization.
            Setting it to `True` will also force `bias_initializer="zeros"`.
            This is recommended in [Jozefowicz et al.](
            https://github.com/mlresearch/v37/blob/gh-pages/jozefowicz15.pdf)
        kernel_regularizer: Regularizer function applied to the `kernel` weights
            matrix. Default: `None`.
        recurrent_regularizer: Regularizer function applied to the
            `recurrent_kernel` weights matrix. Default: `None`.
        bias_regularizer: Regularizer function applied to the bias vector.
            Default: `None`.
        kernel_constraint: Constraint function applied to the `kernel` weights
            matrix. Default: `None`.
        recurrent_constraint: Constraint function applied to the
            `recurrent_kernel` weights matrix. Default: `None`.
        bias_constraint: Constraint function applied to the bias vector.
            Default: `None`.
        dropout: Float between 0 and 1. Fraction of the units to drop for the
            linear transformation of the inputs. Default: 0.
        recurrent_dropout: Float between 0 and 1. Fraction of the units to drop
            for the linear transformation of the recurrent state. Default: 0.
        seed: Random seed for dropout.

    Call arguments:
        inputs: A 2D tensor, with shape `(batch, features)`.
        states: A 2D tensor with shape `(batch, units)`, which is the state
            from the previous time step.
        training: Python boolean indicating whether the layer should behave in
            training mode or in inference mode. Only relevant when `dropout` or
            `recurrent_dropout` is used.

    Example:

    >>> inputs = np.random.random((32, 10, 8))
    >>> rnn = keras.layers.RNN(keras.layers.LSTMCell(4))
    >>> output = rnn(inputs)
    >>> output.shape
    (32, 4)
    >>> rnn = keras.layers.RNN(
    ...    keras.layers.LSTMCell(4),
    ...    return_sequences=True,
    ...    return_state=True)
    >>> whole_sequence_output, final_state = rnn(inputs)
    >>> whole_sequence_output.shape
    (32, 10, 4)
    >>> final_state.shape
    (32, 4)
    )-"


# keras_core.src.layers.rnn.lstm.LSTMCell
#' Cell class for the LSTM layer.
#'
#' @description
#' This class processes one step within the whole time sequence input, whereas
#' `keras.layer.LSTM` processes the whole sequence.
#'
#' # Call Arguments
#' - `inputs`: A 2D tensor, with shape `(batch, features)`.
#' - `states`: A 2D tensor with shape `(batch, units)`, which is the state
#'     from the previous time step.
#' - `training`: Python boolean indicating whether the layer should behave in
#'     training mode or in inference mode. Only relevant when `dropout` or
#'     `recurrent_dropout` is used.
#'
#' # Examples
#' ```python
#' inputs = np.random.random((32, 10, 8))
#' rnn = keras.layers.RNN(keras.layers.LSTMCell(4))
#' output = rnn(inputs)
#' output.shape
#' # (32, 4)
#' rnn = keras.layers.RNN(
#'    keras.layers.LSTMCell(4),
#'    return_sequences=True,
#'    return_state=True)
#' whole_sequence_output, final_state = rnn(inputs)
#' whole_sequence_output.shape
#' # (32, 10, 4)
#' final_state.shape
#' # (32, 4)
#' ```
#'
#' @param units Positive integer, dimensionality of the output space.
#' @param activation Activation function to use. Default: hyperbolic tangent
#'     (`tanh`). If you pass None, no activation is applied
#'     (ie. "linear" activation: `a(x) = x`).
#' @param recurrent_activation Activation function to use for the recurrent step.
#'     Default: sigmoid (`sigmoid`). If you pass `None`, no activation is
#'     applied (ie. "linear" activation: `a(x) = x`).
#' @param use_bias Boolean, (default `True`), whether the layer
#'     should use a bias vector.
#' @param kernel_initializer Initializer for the `kernel` weights matrix,
#'     used for the linear transformation of the inputs. Default:
#'     `"glorot_uniform"`.
#' @param recurrent_initializer Initializer for the `recurrent_kernel`
#'     weights matrix, used for the linear transformation
#'     of the recurrent state. Default: `"orthogonal"`.
#' @param bias_initializer Initializer for the bias vector. Default: `"zeros"`.
#' @param unit_forget_bias Boolean (default `True`). If `True`,
#'     add 1 to the bias of the forget gate at initialization.
#'     Setting it to `True` will also force `bias_initializer="zeros"`.
#'     This is recommended in [Jozefowicz et al.](
#'     https://github.com/mlresearch/v37/blob/gh-pages/jozefowicz15.pdf)
#' @param kernel_regularizer Regularizer function applied to the `kernel` weights
#'     matrix. Default: `None`.
#' @param recurrent_regularizer Regularizer function applied to the
#'     `recurrent_kernel` weights matrix. Default: `None`.
#' @param bias_regularizer Regularizer function applied to the bias vector.
#'     Default: `None`.
#' @param kernel_constraint Constraint function applied to the `kernel` weights
#'     matrix. Default: `None`.
#' @param recurrent_constraint Constraint function applied to the
#'     `recurrent_kernel` weights matrix. Default: `None`.
#' @param bias_constraint Constraint function applied to the bias vector.
#'     Default: `None`.
#' @param dropout Float between 0 and 1. Fraction of the units to drop for the
#'     linear transformation of the inputs. Default: 0.
#' @param recurrent_dropout Float between 0 and 1. Fraction of the units to drop
#'     for the linear transformation of the recurrent state. Default: 0.
#' @param seed Random seed for dropout.
#'
#' @export
#' @family recurrent layers
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell>
layer_lstm_cell <-
function (units, activation = "tanh", recurrent_activation = "sigmoid",
    use_bias = TRUE, kernel_initializer = "glorot_uniform", recurrent_initializer = "orthogonal",
    bias_initializer = "zeros", unit_forget_bias = TRUE, kernel_regularizer = NULL,
    recurrent_regularizer = NULL, bias_regularizer = NULL, kernel_constraint = NULL,
    recurrent_constraint = NULL, bias_constraint = NULL, dropout = 0,
    recurrent_dropout = 0, seed = NULL, ...)
{
    args <- capture_args2(list(units = as_integer, input_shape = normalize_shape,
        batch_size = as_integer, batch_input_shape = normalize_shape))
    do.call(keras$layers$LSTMCell, args)
}


# keras$layers$RNN
# keras_core.src.layers.rnn.rnn.RNN
r"-(Base class for recurrent layers.

    Args:
        cell: A RNN cell instance or a list of RNN cell instances.
            A RNN cell is a class that has:
            - A `call(input_at_t, states_at_t)` method, returning
            `(output_at_t, states_at_t_plus_1)`. The call method of the
            cell can also take the optional argument `constants`, see
            section "Note on passing external constants" below.
            - A `state_size` attribute. This can be a single integer
            (single state) in which case it is the size of the recurrent
            state. This can also be a list/tuple of integers
            (one size per state).
            - A `output_size` attribute, a single integer.
            - A `get_initial_state(batch_size=None)`
            method that creates a tensor meant to be fed to `call()` as the
            initial state, if the user didn't specify any initial state
            via other means. The returned initial state should have
            shape `(batch_size, cell.state_size)`.
            The cell might choose to create a tensor full of zeros,
            or other values based on the cell's implementation.
            `inputs` is the input tensor to the RNN layer, with shape
            `(batch_size, timesteps, features)`.
            If this method is not implemented
            by the cell, the RNN layer will create a zero filled tensor
            with shape `(batch_size, cell.state_size)`.
            In the case that `cell` is a list of RNN cell instances, the cells
            will be stacked on top of each other in the RNN, resulting in an
            efficient stacked RNN.
        return_sequences: Boolean (default `False`). Whether to return the last
            output in the output sequence, or the full sequence.
        return_state: Boolean (default `False`).
            Whether to return the last state in addition to the output.
        go_backwards: Boolean (default `False`).
            If `True`, process the input sequence backwards and return the
            reversed sequence.
        stateful: Boolean (default `False`). If True, the last state
            for each sample at index `i` in a batch will be used as initial
            state for the sample of index `i` in the following batch.
        unroll: Boolean (default `False`).
            If True, the network will be unrolled, else a symbolic loop will be
            used. Unrolling can speed-up a RNN, although it tends to be more
            memory-intensive. Unrolling is only suitable for short sequences.
        zero_output_for_mask: Boolean (default `False`).
            Whether the output should use zeros for the masked timesteps.
            Note that this field is only used when `return_sequences`
            is `True` and `mask` is provided.
            It can useful if you want to reuse the raw output sequence of
            the RNN without interference from the masked timesteps, e.g.,
            merging bidirectional RNNs.

    Call arguments:
        inputs: Input tensor.
        initial_state: List of initial state tensors to be passed to the first
            call of the cell.
        mask: Binary tensor of shape `[batch_size, timesteps]`
            indicating whether a given timestep should be masked.
            An individual `True` entry indicates that the corresponding
            timestep should be utilized, while a `False` entry indicates
            that the corresponding timestep should be ignored.
        training: Python boolean indicating whether the layer should behave in
            training mode or in inference mode. This argument is passed
            to the cell when calling it.
            This is for use with cells that use dropout.

    Input shape:
        3-D tensor with shape `(batch_size, timesteps, features)`.

    Output shape:

    - If `return_state`: a list of tensors. The first tensor is
    the output. The remaining tensors are the last states,
    each with shape `(batch_size, state_size)`, where `state_size` could
    be a high dimension tensor shape.
    - If `return_sequences`: 3D tensor with shape
    `(batch_size, timesteps, output_size)`.

    Masking:

    This layer supports masking for input data with a variable number
    of timesteps. To introduce masks to your data,
    use a `keras.layers.Embedding` layer with the `mask_zero` parameter
    set to `True`.

    Note on using statefulness in RNNs:

    You can set RNN layers to be 'stateful', which means that the states
    computed for the samples in one batch will be reused as initial states
    for the samples in the next batch. This assumes a one-to-one mapping
    between samples in different successive batches.

    To enable statefulness:

    - Specify `stateful=True` in the layer constructor.
    - Specify a fixed batch size for your model, by passing
    If sequential model:
        `batch_input_shape=(...)` to the first layer in your model.
    Else for functional model with 1 or more Input layers:
        `batch_shape=(...)` to all the first layers in your model.
    This is the expected shape of your inputs
    *including the batch size*.
    It should be a tuple of integers, e.g. `(32, 10, 100)`.
    - Specify `shuffle=False` when calling `fit()`.

    To reset the states of your model, call `.reset_states()` on either
    a specific layer, or on your entire model.

    Note on specifying the initial state of RNNs:

    You can specify the initial state of RNN layers symbolically by
    calling them with the keyword argument `initial_state`. The value of
    `initial_state` should be a tensor or list of tensors representing
    the initial state of the RNN layer.

    You can specify the initial state of RNN layers numerically by
    calling `reset_states` with the keyword argument `states`. The value of
    `states` should be a numpy array or list of numpy arrays representing
    the initial state of the RNN layer.

    Examples:

    ```python
    from keras.layers import RNN
    from keras import ops

    # First, let's define a RNN Cell, as a layer subclass.
    class MinimalRNNCell(keras.layers.Layer):

        def __init__(self, units, **kwargs):
            super().__init__(**kwargs)
            self.units = units
            self.state_size = units

        def build(self, input_shape):
            self.kernel = self.add_weight(shape=(input_shape[-1], self.units),
                                          initializer='uniform',
                                          name='kernel')
            self.recurrent_kernel = self.add_weight(
                shape=(self.units, self.units),
                initializer='uniform',
                name='recurrent_kernel')
            self.built = True

        def call(self, inputs, states):
            prev_output = states[0]
            h = ops.matmul(inputs, self.kernel)
            output = h + ops.matmul(prev_output, self.recurrent_kernel)
            return output, [output]

    # Let's use this cell in a RNN layer:

    cell = MinimalRNNCell(32)
    x = keras.Input((None, 5))
    layer = RNN(cell)
    y = layer(x)

    # Here's how to use the cell to build a stacked RNN:

    cells = [MinimalRNNCell(32), MinimalRNNCell(64)]
    x = keras.Input((None, 5))
    layer = RNN(cells)
    y = layer(x)
    ```
    )-"


# keras_core.src.layers.rnn.rnn.RNN
#' Base class for recurrent layers.
#'
#' @description
#'
#' # Call Arguments
#' - `inputs`: Input tensor.
#' - `initial_state`: List of initial state tensors to be passed to the first
#'     call of the cell.
#' - `mask`: Binary tensor of shape `[batch_size, timesteps]`
#'     indicating whether a given timestep should be masked.
#'     An individual `True` entry indicates that the corresponding
#'     timestep should be utilized, while a `False` entry indicates
#'     that the corresponding timestep should be ignored.
#' - `training`: Python boolean indicating whether the layer should behave in
#'     training mode or in inference mode. This argument is passed
#'     to the cell when calling it.
#'     This is for use with cells that use dropout.
#'
#' # Input Shape
#' 3-D tensor with shape `(batch_size, timesteps, features)`.
#'
#' # Output Shape
#' - If `return_state`: a list of tensors. The first tensor is
#' the output. The remaining tensors are the last states,
#' each with shape `(batch_size, state_size)`, where `state_size` could
#' be a high dimension tensor shape.
#' - If `return_sequences`: 3D tensor with shape
#' `(batch_size, timesteps, output_size)`.
#'
#' Masking:
#'
#' This layer supports masking for input data with a variable number
#' of timesteps. To introduce masks to your data,
#' use a `keras.layers.Embedding` layer with the `mask_zero` parameter
#' set to `True`.
#'
#' Note on using statefulness in RNNs:
#'
#' You can set RNN layers to be 'stateful', which means that the states
#' computed for the samples in one batch will be reused as initial states
#' for the samples in the next batch. This assumes a one-to-one mapping
#' between samples in different successive batches.
#'
#' To enable statefulness:
#'
#' - Specify `stateful=True` in the layer constructor.
#' - Specify a fixed batch size for your model, by passing
#' If sequential model:
#'     `batch_input_shape=(...)` to the first layer in your model.
#' Else for functional model with 1 or more Input layers:
#'     `batch_shape=(...)` to all the first layers in your model.
#' This is the expected shape of your inputs
#' *including the batch size*.
#' It should be a tuple of integers, e.g. `(32, 10, 100)`.
#' - Specify `shuffle=False` when calling `fit()`.
#'
#' To reset the states of your model, call `.reset_states()` on either
#' a specific layer, or on your entire model.
#'
#' Note on specifying the initial state of RNNs:
#'
#' You can specify the initial state of RNN layers symbolically by
#' calling them with the keyword argument `initial_state`. The value of
#' `initial_state` should be a tensor or list of tensors representing
#' the initial state of the RNN layer.
#'
#' You can specify the initial state of RNN layers numerically by
#' calling `reset_states` with the keyword argument `states`. The value of
#' `states` should be a numpy array or list of numpy arrays representing
#' the initial state of the RNN layer.
#'
#' # Examples
#' ```python
#' from keras.layers import RNN
#' from keras import ops
#'
#' # First, let's define a RNN Cell, as a layer subclass.
#' class MinimalRNNCell(keras.layers.Layer):
#'
#'     def __init__(self, units, **kwargs):
#'         super().__init__(**kwargs)
#'         self.units = units
#'         self.state_size = units
#'
#'     def build(self, input_shape):
#'         self.kernel = self.add_weight(shape=(input_shape[-1], self.units),
#'                                       initializer='uniform',
#'                                       name='kernel')
#'         self.recurrent_kernel = self.add_weight(
#'             shape=(self.units, self.units),
#'             initializer='uniform',
#'             name='recurrent_kernel')
#'         self.built = True
#'
#'     def call(self, inputs, states):
#'         prev_output = states[0]
#'         h = ops.matmul(inputs, self.kernel)
#'         output = h + ops.matmul(prev_output, self.recurrent_kernel)
#'         return output, [output]
#'
#' # Let's use this cell in a RNN layer:
#'
#' cell = MinimalRNNCell(32)
#' x = keras.Input((None, 5))
#' layer = RNN(cell)
#' y = layer(x)
#'
#' # Here's how to use the cell to build a stacked RNN:
#'
#' cells = [MinimalRNNCell(32), MinimalRNNCell(64)]
#' x = keras.Input((None, 5))
#' layer = RNN(cells)
#' y = layer(x)
#' ```
#'
#' @param cell A RNN cell instance or a list of RNN cell instances.
#'     A RNN cell is a class that has:
#'     - A `call(input_at_t, states_at_t)` method, returning
#'     `(output_at_t, states_at_t_plus_1)`. The call method of the
#'     cell can also take the optional argument `constants`, see
#'     section "Note on passing external constants" below.
#'     - A `state_size` attribute. This can be a single integer
#'     (single state) in which case it is the size of the recurrent
#'     state. This can also be a list/tuple of integers
#'     (one size per state).
#'     - A `output_size` attribute, a single integer.
#'     - A `get_initial_state(batch_size=None)`
#'     method that creates a tensor meant to be fed to `call()` as the
#'     initial state, if the user didn't specify any initial state
#'     via other means. The returned initial state should have
#'     shape `(batch_size, cell.state_size)`.
#'     The cell might choose to create a tensor full of zeros,
#'     or other values based on the cell's implementation.
#'     `inputs` is the input tensor to the RNN layer, with shape
#'     `(batch_size, timesteps, features)`.
#'     If this method is not implemented
#'     by the cell, the RNN layer will create a zero filled tensor
#'     with shape `(batch_size, cell.state_size)`.
#'     In the case that `cell` is a list of RNN cell instances, the cells
#'     will be stacked on top of each other in the RNN, resulting in an
#'     efficient stacked RNN.
#' @param return_sequences Boolean (default `False`). Whether to return the last
#'     output in the output sequence, or the full sequence.
#' @param return_state Boolean (default `False`).
#'     Whether to return the last state in addition to the output.
#' @param go_backwards Boolean (default `False`).
#'     If `True`, process the input sequence backwards and return the
#'     reversed sequence.
#' @param stateful Boolean (default `False`). If True, the last state
#'     for each sample at index `i` in a batch will be used as initial
#'     state for the sample of index `i` in the following batch.
#' @param unroll Boolean (default `False`).
#'     If True, the network will be unrolled, else a symbolic loop will be
#'     used. Unrolling can speed-up a RNN, although it tends to be more
#'     memory-intensive. Unrolling is only suitable for short sequences.
#' @param zero_output_for_mask Boolean (default `False`).
#'     Whether the output should use zeros for the masked timesteps.
#'     Note that this field is only used when `return_sequences`
#'     is `True` and `mask` is provided.
#'     It can useful if you want to reuse the raw output sequence of
#'     the RNN without interference from the masked timesteps, e.g.,
#'     merging bidirectional RNNs.
#'
#' @export
#' @family recurrent layers
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN>
layer_rnn <-
function (object, cell, return_sequences = FALSE, return_state = FALSE,
    go_backwards = FALSE, stateful = FALSE, unroll = FALSE, zero_output_for_mask = FALSE,
    ...)
{
    args <- capture_args2(list(cell = as_integer, input_shape = normalize_shape,
        batch_size = as_integer, batch_input_shape = normalize_shape),
        ignore = "object")
    create_layer(keras$layers$RNN, object, args)
}


# keras$layers$SimpleRNN
# keras_core.src.layers.rnn.simple_rnn.SimpleRNN
r"-(Fully-connected RNN where the output is to be fed back as the new input.

    Args:
        units: Positive integer, dimensionality of the output space.
        activation: Activation function to use.
            Default: hyperbolic tangent (`tanh`).
            If you pass None, no activation is applied
            (ie. "linear" activation: `a(x) = x`).
        use_bias: Boolean, (default `True`), whether the layer uses
            a bias vector.
        kernel_initializer: Initializer for the `kernel` weights matrix,
            used for the linear transformation of the inputs. Default:
            `"glorot_uniform"`.
        recurrent_initializer: Initializer for the `recurrent_kernel`
            weights matrix, used for the linear transformation of the recurrent
            state.  Default: `"orthogonal"`.
        bias_initializer: Initializer for the bias vector. Default: `"zeros"`.
        kernel_regularizer: Regularizer function applied to the `kernel` weights
            matrix. Default: `None`.
        recurrent_regularizer: Regularizer function applied to the
            `recurrent_kernel` weights matrix. Default: `None`.
        bias_regularizer: Regularizer function applied to the bias vector.
            Default: `None`.
        activity_regularizer: Regularizer function applied to the output of the
            layer (its "activation"). Default: `None`.
        kernel_constraint: Constraint function applied to the `kernel` weights
            matrix. Default: `None`.
        recurrent_constraint: Constraint function applied to the
            `recurrent_kernel` weights matrix.  Default: `None`.
        bias_constraint: Constraint function applied to the bias vector.
            Default: `None`.
        dropout: Float between 0 and 1.
            Fraction of the units to drop for the linear transformation
            of the inputs. Default: 0.
        recurrent_dropout: Float between 0 and 1.
            Fraction of the units to drop for the linear transformation of the
            recurrent state. Default: 0.
        return_sequences: Boolean. Whether to return the last output
            in the output sequence, or the full sequence. Default: `False`.
        return_state: Boolean. Whether to return the last state
            in addition to the output. Default: `False`.
        go_backwards: Boolean (default: `False`).
            If `True`, process the input sequence backwards and return the
            reversed sequence.
        stateful: Boolean (default: `False`). If `True`, the last state
            for each sample at index i in a batch will be used as initial
            state for the sample of index i in the following batch.
        unroll: Boolean (default: `False`).
            If `True`, the network will be unrolled,
            else a symbolic loop will be used.
            Unrolling can speed-up a RNN,
            although it tends to be more memory-intensive.
            Unrolling is only suitable for short sequences.

    Call arguments:
        sequence: A 3D tensor, with shape `[batch, timesteps, feature]`.
        mask: Binary tensor of shape `[batch, timesteps]` indicating whether
            a given timestep should be masked. An individual `True` entry
            indicates that the corresponding timestep should be utilized,
            while a `False` entry indicates that the corresponding timestep
            should be ignored.
        training: Python boolean indicating whether the layer should behave in
            training mode or in inference mode.
            This argument is passed to the cell when calling it.
            This is only relevant if `dropout` or `recurrent_dropout` is used.
        initial_state: List of initial state tensors to be passed to the first
            call of the cell.

    Example:

    ```python
    inputs = np.random.random((32, 10, 8))
    simple_rnn = keras.layers.SimpleRNN(4)
    output = simple_rnn(inputs)  # The output has shape `(32, 4)`.
    simple_rnn = keras.layers.SimpleRNN(
        4, return_sequences=True, return_state=True
    )
    # whole_sequence_output has shape `(32, 10, 4)`.
    # final_state has shape `(32, 4)`.
    whole_sequence_output, final_state = simple_rnn(inputs)
    ```
    )-"


# keras_core.src.layers.rnn.simple_rnn.SimpleRNN
#' Fully-connected RNN where the output is to be fed back as the new input.
#'
#' @description
#'
#' # Call Arguments
#' - `sequence`: A 3D tensor, with shape `[batch, timesteps, feature]`.
#' - `mask`: Binary tensor of shape `[batch, timesteps]` indicating whether
#'     a given timestep should be masked. An individual `True` entry
#'     indicates that the corresponding timestep should be utilized,
#'     while a `False` entry indicates that the corresponding timestep
#'     should be ignored.
#' - `training`: Python boolean indicating whether the layer should behave in
#'     training mode or in inference mode.
#'     This argument is passed to the cell when calling it.
#'     This is only relevant if `dropout` or `recurrent_dropout` is used.
#' - `initial_state`: List of initial state tensors to be passed to the first
#'     call of the cell.
#'
#' # Examples
#' ```python
#' inputs = np.random.random((32, 10, 8))
#' simple_rnn = keras.layers.SimpleRNN(4)
#' output = simple_rnn(inputs)  # The output has shape `(32, 4)`.
#' simple_rnn = keras.layers.SimpleRNN(
#'     4, return_sequences=True, return_state=True
#' )
#' # whole_sequence_output has shape `(32, 10, 4)`.
#' # final_state has shape `(32, 4)`.
#' whole_sequence_output, final_state = simple_rnn(inputs)
#' ```
#'
#' @param units Positive integer, dimensionality of the output space.
#' @param activation Activation function to use.
#'     Default: hyperbolic tangent (`tanh`).
#'     If you pass None, no activation is applied
#'     (ie. "linear" activation: `a(x) = x`).
#' @param use_bias Boolean, (default `True`), whether the layer uses
#'     a bias vector.
#' @param kernel_initializer Initializer for the `kernel` weights matrix,
#'     used for the linear transformation of the inputs. Default:
#'     `"glorot_uniform"`.
#' @param recurrent_initializer Initializer for the `recurrent_kernel`
#'     weights matrix, used for the linear transformation of the recurrent
#'     state.  Default: `"orthogonal"`.
#' @param bias_initializer Initializer for the bias vector. Default: `"zeros"`.
#' @param kernel_regularizer Regularizer function applied to the `kernel` weights
#'     matrix. Default: `None`.
#' @param recurrent_regularizer Regularizer function applied to the
#'     `recurrent_kernel` weights matrix. Default: `None`.
#' @param bias_regularizer Regularizer function applied to the bias vector.
#'     Default: `None`.
#' @param activity_regularizer Regularizer function applied to the output of the
#'     layer (its "activation"). Default: `None`.
#' @param kernel_constraint Constraint function applied to the `kernel` weights
#'     matrix. Default: `None`.
#' @param recurrent_constraint Constraint function applied to the
#'     `recurrent_kernel` weights matrix.  Default: `None`.
#' @param bias_constraint Constraint function applied to the bias vector.
#'     Default: `None`.
#' @param dropout Float between 0 and 1.
#'     Fraction of the units to drop for the linear transformation
#'     of the inputs. Default: 0.
#' @param recurrent_dropout Float between 0 and 1.
#'     Fraction of the units to drop for the linear transformation of the
#'     recurrent state. Default: 0.
#' @param return_sequences Boolean. Whether to return the last output
#'     in the output sequence, or the full sequence. Default: `False`.
#' @param return_state Boolean. Whether to return the last state
#'     in addition to the output. Default: `False`.
#' @param go_backwards Boolean (default: `False`).
#'     If `True`, process the input sequence backwards and return the
#'     reversed sequence.
#' @param stateful Boolean (default: `False`). If `True`, the last state
#'     for each sample at index i in a batch will be used as initial
#'     state for the sample of index i in the following batch.
#' @param unroll Boolean (default: `False`).
#'     If `True`, the network will be unrolled,
#'     else a symbolic loop will be used.
#'     Unrolling can speed-up a RNN,
#'     although it tends to be more memory-intensive.
#'     Unrolling is only suitable for short sequences.
#'
#' @export
#' @family recurrent layers
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN>
layer_simple_rnn <-
function (object, units, activation = "tanh", use_bias = TRUE,
    kernel_initializer = "glorot_uniform", recurrent_initializer = "orthogonal",
    bias_initializer = "zeros", kernel_regularizer = NULL, recurrent_regularizer = NULL,
    bias_regularizer = NULL, activity_regularizer = NULL, kernel_constraint = NULL,
    recurrent_constraint = NULL, bias_constraint = NULL, dropout = 0,
    recurrent_dropout = 0, return_sequences = FALSE, return_state = FALSE,
    go_backwards = FALSE, stateful = FALSE, unroll = FALSE, seed = NULL,
    ...)
{
    args <- capture_args2(list(units = as_integer, input_shape = normalize_shape,
        batch_size = as_integer, batch_input_shape = normalize_shape),
        ignore = "object")
    create_layer(keras$layers$SimpleRNN, object, args)
}


# keras$layers$SimpleRNNCell
# keras_core.src.layers.rnn.simple_rnn.SimpleRNNCell
r"-(Cell class for SimpleRNN.

    This class processes one step within the whole time sequence input, whereas
    `keras.layer.SimpleRNN` processes the whole sequence.

    Args:
        units: Positive integer, dimensionality of the output space.
        activation: Activation function to use.
            Default: hyperbolic tangent (`tanh`).
            If you pass `None`, no activation is applied
            (ie. "linear" activation: `a(x) = x`).
        use_bias: Boolean, (default `True`), whether the layer
            should use a bias vector.
        kernel_initializer: Initializer for the `kernel` weights matrix,
            used for the linear transformation of the inputs. Default:
            `"glorot_uniform"`.
        recurrent_initializer: Initializer for the `recurrent_kernel`
            weights matrix, used for the linear transformation
            of the recurrent state. Default: `"orthogonal"`.
        bias_initializer: Initializer for the bias vector. Default: `"zeros"`.
        kernel_regularizer: Regularizer function applied to the `kernel` weights
            matrix. Default: `None`.
        recurrent_regularizer: Regularizer function applied to the
            `recurrent_kernel` weights matrix. Default: `None`.
        bias_regularizer: Regularizer function applied to the bias vector.
            Default: `None`.
        kernel_constraint: Constraint function applied to the `kernel` weights
            matrix. Default: `None`.
        recurrent_constraint: Constraint function applied to the
            `recurrent_kernel` weights matrix. Default: `None`.
        bias_constraint: Constraint function applied to the bias vector.
            Default: `None`.
        dropout: Float between 0 and 1. Fraction of the units to drop for the
            linear transformation of the inputs. Default: 0.
        recurrent_dropout: Float between 0 and 1. Fraction of the units to drop
            for the linear transformation of the recurrent state. Default: 0.
        seed: Random seed for dropout.

    Call arguments:
        sequence: A 2D tensor, with shape `(batch, features)`.
        states: A 2D tensor with shape `(batch, units)`, which is the state
            from the previous time step.
        training: Python boolean indicating whether the layer should behave in
            training mode or in inference mode. Only relevant when `dropout` or
            `recurrent_dropout` is used.

    Example:

    ```python
    inputs = np.random.random([32, 10, 8]).astype(np.float32)
    rnn = keras.layers.RNN(keras.layers.SimpleRNNCell(4))
    output = rnn(inputs)  # The output has shape `(32, 4)`.
    rnn = keras.layers.RNN(
        keras.layers.SimpleRNNCell(4),
        return_sequences=True,
        return_state=True
    )
    # whole_sequence_output has shape `(32, 10, 4)`.
    # final_state has shape `(32, 4)`.
    whole_sequence_output, final_state = rnn(inputs)
    ```
    )-"


# keras_core.src.layers.rnn.simple_rnn.SimpleRNNCell
#' Cell class for SimpleRNN.
#'
#' @description
#' This class processes one step within the whole time sequence input, whereas
#' `keras.layer.SimpleRNN` processes the whole sequence.
#'
#' # Call Arguments
#' - `sequence`: A 2D tensor, with shape `(batch, features)`.
#' - `states`: A 2D tensor with shape `(batch, units)`, which is the state
#'     from the previous time step.
#' - `training`: Python boolean indicating whether the layer should behave in
#'     training mode or in inference mode. Only relevant when `dropout` or
#'     `recurrent_dropout` is used.
#'
#' # Examples
#' ```python
#' inputs = np.random.random([32, 10, 8]).astype(np.float32)
#' rnn = keras.layers.RNN(keras.layers.SimpleRNNCell(4))
#' output = rnn(inputs)  # The output has shape `(32, 4)`.
#' rnn = keras.layers.RNN(
#'     keras.layers.SimpleRNNCell(4),
#'     return_sequences=True,
#'     return_state=True
#' )
#' # whole_sequence_output has shape `(32, 10, 4)`.
#' # final_state has shape `(32, 4)`.
#' whole_sequence_output, final_state = rnn(inputs)
#' ```
#'
#' @param units Positive integer, dimensionality of the output space.
#' @param activation Activation function to use.
#'     Default: hyperbolic tangent (`tanh`).
#'     If you pass `None`, no activation is applied
#'     (ie. "linear" activation: `a(x) = x`).
#' @param use_bias Boolean, (default `True`), whether the layer
#'     should use a bias vector.
#' @param kernel_initializer Initializer for the `kernel` weights matrix,
#'     used for the linear transformation of the inputs. Default:
#'     `"glorot_uniform"`.
#' @param recurrent_initializer Initializer for the `recurrent_kernel`
#'     weights matrix, used for the linear transformation
#'     of the recurrent state. Default: `"orthogonal"`.
#' @param bias_initializer Initializer for the bias vector. Default: `"zeros"`.
#' @param kernel_regularizer Regularizer function applied to the `kernel` weights
#'     matrix. Default: `None`.
#' @param recurrent_regularizer Regularizer function applied to the
#'     `recurrent_kernel` weights matrix. Default: `None`.
#' @param bias_regularizer Regularizer function applied to the bias vector.
#'     Default: `None`.
#' @param kernel_constraint Constraint function applied to the `kernel` weights
#'     matrix. Default: `None`.
#' @param recurrent_constraint Constraint function applied to the
#'     `recurrent_kernel` weights matrix. Default: `None`.
#' @param bias_constraint Constraint function applied to the bias vector.
#'     Default: `None`.
#' @param dropout Float between 0 and 1. Fraction of the units to drop for the
#'     linear transformation of the inputs. Default: 0.
#' @param recurrent_dropout Float between 0 and 1. Fraction of the units to drop
#'     for the linear transformation of the recurrent state. Default: 0.
#' @param seed Random seed for dropout.
#'
#' @export
#' @family recurrent layers
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNNCell>
layer_simple_rnn_cell <-
function (units, activation = "tanh", use_bias = TRUE, kernel_initializer = "glorot_uniform",
    recurrent_initializer = "orthogonal", bias_initializer = "zeros",
    kernel_regularizer = NULL, recurrent_regularizer = NULL,
    bias_regularizer = NULL, kernel_constraint = NULL, recurrent_constraint = NULL,
    bias_constraint = NULL, dropout = 0, recurrent_dropout = 0,
    seed = NULL, ...)
{
    args <- capture_args2(list(units = as_integer, input_shape = normalize_shape,
        batch_size = as_integer, batch_input_shape = normalize_shape))
    do.call(keras$layers$SimpleRNNCell, args)
}


# keras$layers$StackedRNNCells
# keras_core.src.layers.rnn.stacked_rnn_cells.StackedRNNCells
r"-(Wrapper allowing a stack of RNN cells to behave as a single cell.

    Used to implement efficient stacked RNNs.

    Args:
      cells: List of RNN cell instances.

    Examples:

    ```python
    batch_size = 3
    sentence_length = 5
    num_features = 2
    new_shape = (batch_size, sentence_length, num_features)
    x = np.reshape(np.arange(30), new_shape)

    rnn_cells = [keras.layers.LSTMCell(128) for _ in range(2)]
    stacked_lstm = keras.layers.StackedRNNCells(rnn_cells)
    lstm_layer = keras.layers.RNN(stacked_lstm)

    result = lstm_layer(x)
    ```
    )-"


# keras_core.src.layers.rnn.stacked_rnn_cells.StackedRNNCells
#' Wrapper allowing a stack of RNN cells to behave as a single cell.
#'
#' @description
#' Used to implement efficient stacked RNNs.
#'
#' # Examples
#' ```python
#' batch_size = 3
#' sentence_length = 5
#' num_features = 2
#' new_shape = (batch_size, sentence_length, num_features)
#' x = np.reshape(np.arange(30), new_shape)
#'
#' rnn_cells = [keras.layers.LSTMCell(128) for _ in range(2)]
#' stacked_lstm = keras.layers.StackedRNNCells(rnn_cells)
#' lstm_layer = keras.layers.RNN(stacked_lstm)
#'
#' result = lstm_layer(x)
#' ```
#'
#' @param cells List of RNN cell instances.
#'
#' @export
#' @family recurrent layers
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/layers/StackedRNNCells>
layer_stacked_rnn_cells <-
function (cells, ...)
{
    args <- capture_args2(list(input_shape = normalize_shape,
        batch_size = as_integer, batch_input_shape = normalize_shape))
    do.call(keras$layers$StackedRNNCells, args)
}


# keras$layers$TimeDistributed
# keras_core.src.layers.rnn.time_distributed.TimeDistributed
r"-(This wrapper allows to apply a layer to every temporal slice of an input.

    Every input should be at least 3D, and the dimension of index one of the
    first input will be considered to be the temporal dimension.

    Consider a batch of 32 video samples, where each sample is a 128x128 RGB
    image with `channels_last` data format, across 10 timesteps.
    The batch input shape is `(32, 10, 128, 128, 3)`.

    You can then use `TimeDistributed` to apply the same `Conv2D` layer to each
    of the 10 timesteps, independently:

    >>> inputs = layers.Input(shape=(10, 128, 128, 3), batch_size=32)
    >>> conv_2d_layer = layers.Conv2D(64, (3, 3))
    >>> outputs = layers.TimeDistributed(conv_2d_layer)(inputs)
    >>> outputs.shape
    (32, 10, 126, 126, 64)

    Because `TimeDistributed` applies the same instance of `Conv2D` to each of
    the timestamps, the same set of weights are used at each timestamp.

    Args:
        layer: a `keras.layers.Layer` instance.

    Call arguments:
        inputs: Input tensor of shape (batch, time, ...) or nested tensors,
            and each of which has shape (batch, time, ...).
        training: Python boolean indicating whether the layer should behave in
            training mode or in inference mode. This argument is passed to the
            wrapped layer (only if the layer supports this argument).
        mask: Binary tensor of shape `(samples, timesteps)` indicating whether
            a given timestep should be masked. This argument is passed to the
            wrapped layer (only if the layer supports this argument).
    )-"


# keras_core.src.layers.rnn.time_distributed.TimeDistributed
#' This wrapper allows to apply a layer to every temporal slice of an input.
#'
#' @description
#' Every input should be at least 3D, and the dimension of index one of the
#' first input will be considered to be the temporal dimension.
#'
#' Consider a batch of 32 video samples, where each sample is a 128x128 RGB
#' image with `channels_last` data format, across 10 timesteps.
#' The batch input shape is `(32, 10, 128, 128, 3)`.
#'
#' You can then use `TimeDistributed` to apply the same `Conv2D` layer to each
#' of the 10 timesteps, independently:
#'
#' ```python
#' inputs = layers.Input(shape=(10, 128, 128, 3), batch_size=32)
#' conv_2d_layer = layers.Conv2D(64, (3, 3))
#' outputs = layers.TimeDistributed(conv_2d_layer)(inputs)
#' outputs.shape
#' # (32, 10, 126, 126, 64)
#' ```
#'
#' Because `TimeDistributed` applies the same instance of `Conv2D` to each of
#' the timestamps, the same set of weights are used at each timestamp.
#'
#' # Call Arguments
#' - `inputs`: Input tensor of shape (batch, time, ...) or nested tensors,
#'     and each of which has shape (batch, time, ...).
#' - `training`: Python boolean indicating whether the layer should behave in
#'     training mode or in inference mode. This argument is passed to the
#'     wrapped layer (only if the layer supports this argument).
#' - `mask`: Binary tensor of shape `(samples, timesteps)` indicating whether
#'     a given timestep should be masked. This argument is passed to the
#'     wrapped layer (only if the layer supports this argument).
#'
#' @param layer a `keras.layers.Layer` instance.
#'
#' @export
#' @family recurrent layers
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed>
layer_time_distributed <-
function (object, layer, ...)
{
    args <- capture_args2(list(input_shape = normalize_shape,
        batch_size = as_integer, batch_input_shape = normalize_shape),
        ignore = "object")
    create_layer(keras$layers$TimeDistributed, object, args)
}
