## Autogenerated. Do not modify manually.


# keras$layers$BinaryCrossentropy
# keras_core.src.losses.losses.BinaryCrossentropy
r"-(Computes the cross-entropy loss between true labels and predicted labels.

    Use this cross-entropy loss for binary (0 or 1) classification applications.
    The loss function requires the following inputs:

    - `y_true` (true label): This is either 0 or 1.
    - `y_pred` (predicted value): This is the model's prediction, i.e, a single
        floating-point value which either represents a
        [logit](https://en.wikipedia.org/wiki/Logit), (i.e, value in [-inf, inf]
        when `from_logits=True`) or a probability (i.e, value in [0., 1.] when
        `from_logits=False`).

    Args:
        from_logits: Whether to interpret `y_pred` as a tensor of
            [logit](https://en.wikipedia.org/wiki/Logit) values. By default, we
            assume that `y_pred` is probabilities (i.e., values in [0, 1]).
        label_smoothing: Float in range [0, 1]. When 0, no smoothing occurs.
            When > 0, we compute the loss between the predicted labels
            and a smoothed version of the true labels, where the smoothing
            squeezes the labels towards 0.5. Larger values of
            `label_smoothing` correspond to heavier smoothing.
        axis: The axis along which to compute crossentropy (the features axis).
            Defaults to `-1`.
        reduction: Type of reduction to apply to the loss. In almost all cases
            this should be `"sum_over_batch_size"`.
            Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
        name: Optional name for the loss instance.

    Examples:

    **Recommended Usage:** (set `from_logits=True`)

    With `compile()` API:

    ```python
    model.compile(
        loss=keras.losses.BinaryCrossentropy(from_logits=True),
        ...
    )
    ```

    As a standalone function:

    >>> # Example 1: (batch_size = 1, number of samples = 4)
    >>> y_true = [0, 1, 0, 0]
    >>> y_pred = [-18.6, 0.51, 2.94, -12.8]
    >>> bce = keras.losses.BinaryCrossentropy(from_logits=True)
    >>> bce(y_true, y_pred)
    0.865

    >>> # Example 2: (batch_size = 2, number of samples = 4)
    >>> y_true = [[0, 1], [0, 0]]
    >>> y_pred = [[-18.6, 0.51], [2.94, -12.8]]
    >>> # Using default 'auto'/'sum_over_batch_size' reduction type.
    >>> bce = keras.losses.BinaryCrossentropy(from_logits=True)
    >>> bce(y_true, y_pred)
    0.865
    >>> # Using 'sample_weight' attribute
    >>> bce(y_true, y_pred, sample_weight=[0.8, 0.2])
    0.243
    >>> # Using 'sum' reduction` type.
    >>> bce = keras.losses.BinaryCrossentropy(from_logits=True,
    ...     reduction="sum")
    >>> bce(y_true, y_pred)
    1.730
    >>> # Using 'none' reduction type.
    >>> bce = keras.losses.BinaryCrossentropy(from_logits=True,
    ...     reduction=None)
    >>> bce(y_true, y_pred)
    array([0.235, 1.496], dtype=float32)

    **Default Usage:** (set `from_logits=False`)

    >>> # Make the following updates to the above "Recommended Usage" section
    >>> # 1. Set `from_logits=False`
    >>> keras.losses.BinaryCrossentropy() # OR ...('from_logits=False')
    >>> # 2. Update `y_pred` to use probabilities instead of logits
    >>> y_pred = [0.6, 0.3, 0.2, 0.8] # OR [[0.6, 0.3], [0.2, 0.8]]
    )-"


# keras_core.src.losses.losses.BinaryCrossentropy
#' Computes the cross-entropy loss between true labels and predicted labels.
#'
#' @description
#' Use this cross-entropy loss for binary (0 or 1) classification applications.
#' The loss function requires the following inputs:
#'
#' - `y_true` (true label): This is either 0 or 1.
#' - `y_pred` (predicted value): This is the model's prediction, i.e, a single
#'     floating-point value which either represents a
#'     `[logit](https://en.wikipedia.org/wiki/Logit), (i.e, value in [-inf, inf]`
#'     when `from_logits=True`) or a probability (i.e, value in `[0., 1.]` when
#'     `from_logits=False`).
#'
#' # Examples
#' ```python
#' y_true = [[0, 1], [0, 0]]
#' y_pred = [[0.6, 0.4], [0.4, 0.6]]
#' loss = keras.losses.binary_crossentropy(y_true, y_pred)
#' assert loss.shape == (2,)
#' loss
#' # array([0.916 , 0.714], dtype=float32)
#' ```
#' **Recommended Usage:** (set `from_logits=True`)
#'
#' With `compile()` API:
#'
#' ```python
#' model.compile(
#'     loss=keras.losses.BinaryCrossentropy(from_logits=True),
#'     ...
#' )
#' ```
#'
#' As a standalone function:
#'
#' ```python
#' # Example 1: (batch_size = 1, number of samples = 4)
#' y_true = [0, 1, 0, 0]
#' y_pred = [-18.6, 0.51, 2.94, -12.8]
#' bce = keras.losses.BinaryCrossentropy(from_logits=True)
#' bce(y_true, y_pred)
#' # 0.865
#' ```
#'
#' ```python
#' # Example 2: (batch_size = 2, number of samples = 4)
#' y_true = [[0, 1], [0, 0]]
#' y_pred = [[-18.6, 0.51], [2.94, -12.8]]
#' # Using default 'auto'/'sum_over_batch_size' reduction type.
#' bce = keras.losses.BinaryCrossentropy(from_logits=True)
#' bce(y_true, y_pred)
#' # 0.865
#' # Using 'sample_weight' attribute
#' bce(y_true, y_pred, sample_weight=[0.8, 0.2])
#' # 0.243
#' # Using 'sum' reduction` type.
#' bce = keras.losses.BinaryCrossentropy(from_logits=True,
#'     reduction="sum")
#' bce(y_true, y_pred)
#' # 1.730
#' # Using 'none' reduction type.
#' bce = keras.losses.BinaryCrossentropy(from_logits=True,
#'     reduction=None)
#' bce(y_true, y_pred)
#' # array([0.235, 1.496], dtype=float32)
#' ```
#'
#' **Default Usage:** (set `from_logits=False`)
#'
#' ```python
#' # Make the following updates to the above "Recommended Usage" section
#' # 1. Set `from_logits=False`
#' keras.losses.BinaryCrossentropy() # OR ...('from_logits=False')
#' # 2. Update `y_pred` to use probabilities instead of logits
#' y_pred = [0.6, 0.3, 0.2, 0.8] # OR [[0.6, 0.3], [0.2, 0.8]]
#' ```
#'
#' # Returns
#' Binary crossentropy loss value. shape = `[batch_size, d0, .. dN-1]`.
#'
#' @param from_logits Whether `y_pred` is expected to be a logits tensor. By
#'     default, we assume that `y_pred` encodes a probability distribution.
#' @param label_smoothing Float in `[0, 1]`. If > `0` then smooth the labels by
#'     squeezing them towards 0.5, that is,
#'     using `1. - 0.5 * label_smoothing` for the target class
#'     and `0.5 * label_smoothing` for the non-target class.
#' @param axis The axis along which the mean is computed. Defaults to `-1`.
#' @param reduction Type of reduction to apply to the loss. In almost all cases
#'     this should be `"sum_over_batch_size"`.
#'     Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
#' @param name Optional name for the loss instance.
#' @param y_true Ground truth values. shape = `[batch_size, d0, .. dN]`.
#' @param y_pred The predicted values. shape = `[batch_size, d0, .. dN]`.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family loss
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy>
loss_binary_crossentropy <-
function (y_true, y_pred, from_logits = FALSE, label_smoothing = 0,
    axis = -1L, ..., reduction = "sum_over_batch_size", name = "binary_crossentropy")
{
    args <- capture_args2(list(axis = as_axis))
    callable <- if (missing(y_true) && missing(y_pred))
        keras$losses$BinaryCrossentropy
    else keras$losses$binary_crossentropy
    do.call(callable, args)
}


# keras$layers$BinaryFocalCrossentropy
# keras_core.src.losses.losses.BinaryFocalCrossentropy
r"-(Computes focal cross-entropy loss between true labels and predictions.

    Binary cross-entropy loss is often used for binary (0 or 1) classification
    tasks. The loss function requires the following inputs:

    - `y_true` (true label): This is either 0 or 1.
    - `y_pred` (predicted value): This is the model's prediction, i.e, a single
        floating-point value which either represents a
        [logit](https://en.wikipedia.org/wiki/Logit), (i.e, value in [-inf, inf]
        when `from_logits=True`) or a probability (i.e, value in `[0., 1.]` when
        `from_logits=False`).

    According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it
    helps to apply a "focal factor" to down-weight easy examples and focus more
    on hard examples. By default, the focal tensor is computed as follows:

    `focal_factor = (1 - output) ** gamma` for class 1
    `focal_factor = output ** gamma` for class 0
    where `gamma` is a focusing parameter. When `gamma=0`, this function is
    equivalent to the binary crossentropy loss.

    Args:
        apply_class_balancing: A bool, whether to apply weight balancing on the
            binary classes 0 and 1.
        alpha: A weight balancing factor for class 1, default is `0.25` as
            mentioned in reference [Lin et al., 2018](
            https://arxiv.org/pdf/1708.02002.pdf).  The weight for class 0 is
            `1.0 - alpha`.
        gamma: A focusing parameter used to compute the focal factor, default is
            `2.0` as mentioned in the reference
            [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf).
        from_logits: Whether to interpret `y_pred` as a tensor of
            [logit](https://en.wikipedia.org/wiki/Logit) values. By default, we
            assume that `y_pred` are probabilities (i.e., values in `[0, 1]`).
        label_smoothing: Float in `[0, 1]`. When `0`, no smoothing occurs.
            When > `0`, we compute the loss between the predicted labels
            and a smoothed version of the true labels, where the smoothing
            squeezes the labels towards `0.5`.
            Larger values of `label_smoothing` correspond to heavier smoothing.
        axis: The axis along which to compute crossentropy (the features axis).
            Defaults to `-1`.
        reduction: Type of reduction to apply to the loss. In almost all cases
            this should be `"sum_over_batch_size"`.
            Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
        name: Optional name for the loss instance.

    Examples:

    With the `compile()` API:

    ```python
    model.compile(
        loss=keras.losses.BinaryFocalCrossentropy(
            gamma=2.0, from_logits=True),
        ...
    )
    ```

    As a standalone function:

    >>> # Example 1: (batch_size = 1, number of samples = 4)
    >>> y_true = [0, 1, 0, 0]
    >>> y_pred = [-18.6, 0.51, 2.94, -12.8]
    >>> loss = keras.losses.BinaryFocalCrossentropy(
    ...    gamma=2, from_logits=True)
    >>> loss(y_true, y_pred)
    0.691

    >>> # Apply class weight
    >>> loss = keras.losses.BinaryFocalCrossentropy(
    ...     apply_class_balancing=True, gamma=2, from_logits=True)
    >>> loss(y_true, y_pred)
    0.51

    >>> # Example 2: (batch_size = 2, number of samples = 4)
    >>> y_true = [[0, 1], [0, 0]]
    >>> y_pred = [[-18.6, 0.51], [2.94, -12.8]]
    >>> # Using default 'auto'/'sum_over_batch_size' reduction type.
    >>> loss = keras.losses.BinaryFocalCrossentropy(
    ...     gamma=3, from_logits=True)
    >>> loss(y_true, y_pred)
    0.647

    >>> # Apply class weight
    >>> loss = keras.losses.BinaryFocalCrossentropy(
    ...      apply_class_balancing=True, gamma=3, from_logits=True)
    >>> loss(y_true, y_pred)
    0.482

    >>> # Using 'sample_weight' attribute with focal effect
    >>> loss = keras.losses.BinaryFocalCrossentropy(
    ...     gamma=3, from_logits=True)
    >>> loss(y_true, y_pred, sample_weight=[0.8, 0.2])
    0.133

    >>> # Apply class weight
    >>> loss = keras.losses.BinaryFocalCrossentropy(
    ...      apply_class_balancing=True, gamma=3, from_logits=True)
    >>> loss(y_true, y_pred, sample_weight=[0.8, 0.2])
    0.097

    >>> # Using 'sum' reduction` type.
    >>> loss = keras.losses.BinaryFocalCrossentropy(
    ...     gamma=4, from_logits=True,
    ...     reduction="sum")
    >>> loss(y_true, y_pred)
    1.222

    >>> # Apply class weight
    >>> loss = keras.losses.BinaryFocalCrossentropy(
    ...     apply_class_balancing=True, gamma=4, from_logits=True,
    ...     reduction="sum")
    >>> loss(y_true, y_pred)
    0.914

    >>> # Using 'none' reduction type.
    >>> loss = keras.losses.BinaryFocalCrossentropy(
    ...     gamma=5, from_logits=True,
    ...     reduction=None)
    >>> loss(y_true, y_pred)
    array([0.0017 1.1561], dtype=float32)

    >>> # Apply class weight
    >>> loss = keras.losses.BinaryFocalCrossentropy(
    ...     apply_class_balancing=True, gamma=5, from_logits=True,
    ...     reduction=None)
    >>> loss(y_true, y_pred)
    array([0.0004 0.8670], dtype=float32)
    )-"


# keras_core.src.losses.losses.BinaryFocalCrossentropy
#' Computes focal cross-entropy loss between true labels and predictions.
#'
#' @description
#' According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it
#' helps to apply a focal factor to down-weight easy examples and focus more on
#' hard examples. By default, the focal tensor is computed as follows:
#'
#' `focal_factor = (1 - output) ** gamma` for class 1
#' `focal_factor = output ** gamma` for class 0
#' where `gamma` is a focusing parameter. When `gamma` = 0, there is no focal
#' effect on the binary crossentropy loss.
#'
#' If `apply_class_balancing == True`, this function also takes into account a
#' weight balancing factor for the binary classes 0 and 1 as follows:
#'
#' `weight = alpha` for class 1 (`target == 1`)
#' `weight = 1 - alpha` for class 0
#' where `alpha` is a float in the range of `[0, 1]`.
#' Binary cross-entropy loss is often used for binary (0 or 1) classification
#' tasks. The loss function requires the following inputs:
#'
#' - `y_true` (true label): This is either 0 or 1.
#' - `y_pred` (predicted value): This is the model's prediction, i.e, a single
#'     floating-point value which either represents a
#'     `[logit](https://en.wikipedia.org/wiki/Logit), (i.e, value in [-inf, inf]`
#'     when `from_logits=True`) or a probability (i.e, value in `[0., 1.]` when
#'     `from_logits=False`).
#'
#' According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it
#' helps to apply a "focal factor" to down-weight easy examples and focus more
#' on hard examples. By default, the focal tensor is computed as follows:
#'
#' `focal_factor = (1 - output) ** gamma` for class 1
#' `focal_factor = output ** gamma` for class 0
#' where `gamma` is a focusing parameter. When `gamma=0`, this function is
#' equivalent to the binary crossentropy loss.
#'
#' # Examples
#' ```python
#' y_true = [[0, 1], [0, 0]]
#' y_pred = [[0.6, 0.4], [0.4, 0.6]]
#' loss = keras.losses.binary_focal_crossentropy(
#'        y_true, y_pred, gamma=2)
#' assert loss.shape == (2,)
#' loss
#' # array([0.330, 0.206], dtype=float32)
#' ```
#' With the `compile()` API:
#'
#' ```python
#' model.compile(
#'     loss=keras.losses.BinaryFocalCrossentropy(
#'         gamma=2.0, from_logits=True),
#'     ...
#' )
#' ```
#'
#' As a standalone function:
#'
#' ```python
#' # Example 1: (batch_size = 1, number of samples = 4)
#' y_true = [0, 1, 0, 0]
#' y_pred = [-18.6, 0.51, 2.94, -12.8]
#' loss = keras.losses.BinaryFocalCrossentropy(
#'    gamma=2, from_logits=True)
#' loss(y_true, y_pred)
#' # 0.691
#' ```
#'
#' ```python
#' # Apply class weight
#' loss = keras.losses.BinaryFocalCrossentropy(
#'     apply_class_balancing=True, gamma=2, from_logits=True)
#' loss(y_true, y_pred)
#' # 0.51
#' ```
#'
#' ```python
#' # Example 2: (batch_size = 2, number of samples = 4)
#' y_true = [[0, 1], [0, 0]]
#' y_pred = [[-18.6, 0.51], [2.94, -12.8]]
#' # Using default 'auto'/'sum_over_batch_size' reduction type.
#' loss = keras.losses.BinaryFocalCrossentropy(
#'     gamma=3, from_logits=True)
#' loss(y_true, y_pred)
#' # 0.647
#' ```
#'
#' ```python
#' # Apply class weight
#' loss = keras.losses.BinaryFocalCrossentropy(
#'      apply_class_balancing=True, gamma=3, from_logits=True)
#' loss(y_true, y_pred)
#' # 0.482
#' ```
#'
#' ```python
#' # Using 'sample_weight' attribute with focal effect
#' loss = keras.losses.BinaryFocalCrossentropy(
#'     gamma=3, from_logits=True)
#' loss(y_true, y_pred, sample_weight=[0.8, 0.2])
#' # 0.133
#' ```
#'
#' ```python
#' # Apply class weight
#' loss = keras.losses.BinaryFocalCrossentropy(
#'      apply_class_balancing=True, gamma=3, from_logits=True)
#' loss(y_true, y_pred, sample_weight=[0.8, 0.2])
#' # 0.097
#' ```
#'
#' ```python
#' # Using 'sum' reduction` type.
#' loss = keras.losses.BinaryFocalCrossentropy(
#'     gamma=4, from_logits=True,
#'     reduction="sum")
#' loss(y_true, y_pred)
#' # 1.222
#' ```
#'
#' ```python
#' # Apply class weight
#' loss = keras.losses.BinaryFocalCrossentropy(
#'     apply_class_balancing=True, gamma=4, from_logits=True,
#'     reduction="sum")
#' loss(y_true, y_pred)
#' # 0.914
#' ```
#'
#' ```python
#' # Using 'none' reduction type.
#' loss = keras.losses.BinaryFocalCrossentropy(
#'     gamma=5, from_logits=True,
#'     reduction=None)
#' loss(y_true, y_pred)
#' # array([0.0017 1.1561], dtype=float32)
#' ```
#'
#' ```python
#' # Apply class weight
#' loss = keras.losses.BinaryFocalCrossentropy(
#'     apply_class_balancing=True, gamma=5, from_logits=True,
#'     reduction=None)
#' loss(y_true, y_pred)
#' # array([0.0004 0.8670], dtype=float32)
#' ```
#'
#' # Returns
#' Binary focal crossentropy loss value
#' with shape = `[batch_size, d0, .. dN-1]`.
#'
#' @param apply_class_balancing A bool, whether to apply weight balancing on the
#'     binary classes 0 and 1.
#' @param alpha A weight balancing factor for class 1, default is `0.25` as
#'     mentioned in the reference. The weight for class 0 is `1.0 - alpha`.
#' @param gamma A focusing parameter, default is `2.0` as mentioned in the
#'     reference.
#' @param from_logits Whether `y_pred` is expected to be a logits tensor. By
#'     default, we assume that `y_pred` encodes a probability distribution.
#' @param label_smoothing Float in `[0, 1]`. If > `0` then smooth the labels by
#'     squeezing them towards 0.5, that is,
#'     using `1. - 0.5 * label_smoothing` for the target class
#'     and `0.5 * label_smoothing` for the non-target class.
#' @param axis The axis along which the mean is computed. Defaults to `-1`.
#' @param reduction Type of reduction to apply to the loss. In almost all cases
#'     this should be `"sum_over_batch_size"`.
#'     Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
#' @param name Optional name for the loss instance.
#' @param y_true Ground truth values, of shape `(batch_size, d0, .. dN)`.
#' @param y_pred The predicted values, of shape `(batch_size, d0, .. dN)`.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family loss
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryFocalCrossentropy>
loss_binary_focal_crossentropy <-
function (y_true, y_pred, apply_class_balancing = FALSE, alpha = 0.25,
    gamma = 2, from_logits = FALSE, label_smoothing = 0, axis = -1L,
    ..., reduction = "sum_over_batch_size", name = "binary_focal_crossentropy")
{
    args <- capture_args2(list(axis = as_axis))
    callable <- if (missing(y_true) && missing(y_pred))
        keras$losses$BinaryFocalCrossentropy
    else keras$losses$binary_focal_crossentropy
    do.call(callable, args)
}


# keras$layers$CategoricalCrossentropy
# keras_core.src.losses.losses.CategoricalCrossentropy
r"-(Computes the crossentropy loss between the labels and predictions.

    Use this crossentropy loss function when there are two or more label
    classes. We expect labels to be provided in a `one_hot` representation. If
    you want to provide labels as integers, please use
    `SparseCategoricalCrossentropy` loss. There should be `num_classes` floating
    point values per feature, i.e., the shape of both `y_pred` and `y_true` are
    `[batch_size, num_classes]`.

    Args:
        from_logits: Whether `y_pred` is expected to be a logits tensor. By
            default, we assume that `y_pred` encodes a probability distribution.
        label_smoothing: Float in [0, 1]. When > 0, label values are smoothed,
            meaning the confidence on label values are relaxed. For example, if
            `0.1`, use `0.1 / num_classes` for non-target labels and
            `0.9 + 0.1 / num_classes` for target labels.
        axis: The axis along which to compute crossentropy (the features
            axis). Defaults to `-1`.
        reduction: Type of reduction to apply to the loss. In almost all cases
            this should be `"sum_over_batch_size"`.
            Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
        name: Optional name for the loss instance.

    Examples:

    Standalone usage:

    >>> y_true = [[0, 1, 0], [0, 0, 1]]
    >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
    >>> # Using 'auto'/'sum_over_batch_size' reduction type.
    >>> cce = keras.losses.CategoricalCrossentropy()
    >>> cce(y_true, y_pred)
    1.177

    >>> # Calling with 'sample_weight'.
    >>> cce(y_true, y_pred, sample_weight=np.array([0.3, 0.7]))
    0.814

    >>> # Using 'sum' reduction type.
    >>> cce = keras.losses.CategoricalCrossentropy(
    ...     reduction="sum")
    >>> cce(y_true, y_pred)
    2.354

    >>> # Using 'none' reduction type.
    >>> cce = keras.losses.CategoricalCrossentropy(
    ...     reduction=None)
    >>> cce(y_true, y_pred)
    array([0.0513, 2.303], dtype=float32)

    Usage with the `compile()` API:

    ```python
    model.compile(optimizer='sgd',
                  loss=keras.losses.CategoricalCrossentropy())
    ```
    )-"


# keras_core.src.losses.losses.CategoricalCrossentropy
#' Computes the crossentropy loss between the labels and predictions.
#'
#' @description
#' Use this crossentropy loss function when there are two or more label
#' classes. We expect labels to be provided in a `one_hot` representation. If
#' you want to provide labels as integers, please use
#' `SparseCategoricalCrossentropy` loss. There should be `num_classes` floating
#' point values per feature, i.e., the shape of both `y_pred` and `y_true` are
#' `[batch_size, num_classes]`.
#'
#' # Examples
#' ```python
#' y_true = [[0, 1, 0], [0, 0, 1]]
#' y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
#' loss = keras.losses.categorical_crossentropy(y_true, y_pred)
#' assert loss.shape == (2,)
#' loss
#' # array([0.0513, 2.303], dtype=float32)
#' ```
#' Standalone usage:
#'
#' ```python
#' y_true = [[0, 1, 0], [0, 0, 1]]
#' y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
#' # Using 'auto'/'sum_over_batch_size' reduction type.
#' cce = keras.losses.CategoricalCrossentropy()
#' cce(y_true, y_pred)
#' # 1.177
#' ```
#'
#' ```python
#' # Calling with 'sample_weight'.
#' cce(y_true, y_pred, sample_weight=np.array([0.3, 0.7]))
#' # 0.814
#' ```
#'
#' ```python
#' # Using 'sum' reduction type.
#' cce = keras.losses.CategoricalCrossentropy(
#'     reduction="sum")
#' cce(y_true, y_pred)
#' # 2.354
#' ```
#'
#' ```python
#' # Using 'none' reduction type.
#' cce = keras.losses.CategoricalCrossentropy(
#'     reduction=None)
#' cce(y_true, y_pred)
#' # array([0.0513, 2.303], dtype=float32)
#' ```
#'
#' Usage with the `compile()` API:
#'
#' ```python
#' model.compile(optimizer='sgd',
#'               loss=keras.losses.CategoricalCrossentropy())
#' ```
#'
#' # Returns
#' Categorical crossentropy loss value.
#'
#' @param from_logits Whether `y_pred` is expected to be a logits tensor. By
#'     default, we assume that `y_pred` encodes a probability distribution.
#' @param label_smoothing Float in `[0, 1].` If > `0` then smooth the labels. For
#'     example, if `0.1`, use `0.1 / num_classes` for non-target labels
#'     and `0.9 + 0.1 / num_classes` for target labels.
#' @param axis Defaults to `-1`. The dimension along which the entropy is
#'     computed.
#' @param reduction Type of reduction to apply to the loss. In almost all cases
#'     this should be `"sum_over_batch_size"`.
#'     Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
#' @param name Optional name for the loss instance.
#' @param y_true Tensor of one-hot true targets.
#' @param y_pred Tensor of predicted targets.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family loss
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy>
loss_categorical_crossentropy <-
function (y_true, y_pred, from_logits = FALSE, label_smoothing = 0,
    axis = -1L, ..., reduction = "sum_over_batch_size", name = "categorical_crossentropy")
{
    args <- capture_args2(list(axis = as_axis))
    callable <- if (missing(y_true) && missing(y_pred))
        keras$losses$CategoricalCrossentropy
    else keras$losses$categorical_crossentropy
    do.call(callable, args)
}


# keras$layers$CategoricalFocalCrossentropy
# keras_core.src.losses.losses.CategoricalFocalCrossentropy
r"-(Computes the alpha balanced focal crossentropy loss.

    Use this crossentropy loss function when there are two or more label
    classes and if you want to handle class imbalance without using
    `class_weights`. We expect labels to be provided in a `one_hot`
    representation.

    According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it
    helps to apply a focal factor to down-weight easy examples and focus more on
    hard examples. The general formula for the focal loss (FL)
    is as follows:

    `FL(p_t) = (1 - p_t) ** gamma * log(p_t)`

    where `p_t` is defined as follows:
    `p_t = output if y_true == 1, else 1 - output`

    `(1 - p_t) ** gamma` is the `modulating_factor`, where `gamma` is a focusing
    parameter. When `gamma` = 0, there is no focal effect on the cross entropy.
    `gamma` reduces the importance given to simple examples in a smooth manner.

    The authors use alpha-balanced variant of focal loss (FL) in the paper:
    `FL(p_t) = -alpha * (1 - p_t) ** gamma * log(p_t)`

    where `alpha` is the weight factor for the classes. If `alpha` = 1, the
    loss won't be able to handle class imbalance properly as all
    classes will have the same weight. This can be a constant or a list of
    constants. If alpha is a list, it must have the same length as the number
    of classes.

    The formula above can be generalized to:
    `FL(p_t) = alpha * (1 - p_t) ** gamma * CrossEntropy(y_true, y_pred)`

    where minus comes from `CrossEntropy(y_true, y_pred)` (CE).

    Extending this to multi-class case is straightforward:
    `FL(p_t) = alpha * (1 - p_t) ** gamma * CategoricalCE(y_true, y_pred)`

    In the snippet below, there is `num_classes` floating pointing values per
    example. The shape of both `y_pred` and `y_true` are
    `(batch_size, num_classes)`.

    Args:
        alpha: A weight balancing factor for all classes, default is `0.25` as
            mentioned in the reference. It can be a list of floats or a scalar.
            In the multi-class case, alpha may be set by inverse class
            frequency by using `compute_class_weight` from `sklearn.utils`.
        gamma: A focusing parameter, default is `2.0` as mentioned in the
            reference. It helps to gradually reduce the importance given to
            simple (easy) examples in a smooth manner.
        from_logits: Whether `output` is expected to be a logits tensor. By
            default, we consider that `output` encodes a probability
            distribution.
        label_smoothing: Float in [0, 1]. When > 0, label values are smoothed,
            meaning the confidence on label values are relaxed. For example, if
            `0.1`, use `0.1 / num_classes` for non-target labels and
            `0.9 + 0.1 / num_classes` for target labels.
        axis: The axis along which to compute crossentropy (the features
            axis). Defaults to `-1`.
        reduction: Type of reduction to apply to the loss. In almost all cases
            this should be `"sum_over_batch_size"`.
            Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
        name: Optional name for the loss instance.

    Examples:

    Standalone usage:

    >>> y_true = [[0., 1., 0.], [0., 0., 1.]]
    >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
    >>> # Using 'auto'/'sum_over_batch_size' reduction type.
    >>> cce = keras.losses.CategoricalFocalCrossentropy()
    >>> cce(y_true, y_pred)
    0.23315276

    >>> # Calling with 'sample_weight'.
    >>> cce(y_true, y_pred, sample_weight=np.array([0.3, 0.7]))
    0.1632

    >>> # Using 'sum' reduction type.
    >>> cce = keras.losses.CategoricalFocalCrossentropy(
    ...     reduction="sum")
    >>> cce(y_true, y_pred)
    0.46631

    >>> # Using 'none' reduction type.
    >>> cce = keras.losses.CategoricalFocalCrossentropy(
    ...     reduction=None)
    >>> cce(y_true, y_pred)
    array([3.2058331e-05, 4.6627346e-01], dtype=float32)

    Usage with the `compile()` API:

    ```python
    model.compile(optimizer='adam',
                  loss=keras.losses.CategoricalFocalCrossentropy())
    ```
    )-"


# keras_core.src.losses.losses.CategoricalFocalCrossentropy
#' Computes the alpha balanced focal crossentropy loss.
#'
#' @description
#' Use this crossentropy loss function when there are two or more label
#' classes and if you want to handle class imbalance without using
#' `class_weights`. We expect labels to be provided in a `one_hot`
#' representation.
#'
#' According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it
#' helps to apply a focal factor to down-weight easy examples and focus more on
#' hard examples. The general formula for the focal loss (FL)
#' is as follows:
#'
#' `FL(p_t) = (1 - p_t) ** gamma * log(p_t)`
#'
#' where `p_t` is defined as follows:
#' `p_t = output if y_true == 1, else 1 - output`
#'
#' `(1 - p_t) ** gamma` is the `modulating_factor`, where `gamma` is a focusing
#' parameter. When `gamma` = 0, there is no focal effect on the cross entropy.
#' `gamma` reduces the importance given to simple examples in a smooth manner.
#'
#' The authors use alpha-balanced variant of focal loss (FL) in the paper:
#' `FL(p_t) = -alpha * (1 - p_t) ** gamma * log(p_t)`
#'
#' where `alpha` is the weight factor for the classes. If `alpha` = 1, the
#' loss won't be able to handle class imbalance properly as all
#' classes will have the same weight. This can be a constant or a list of
#' constants. If alpha is a list, it must have the same length as the number
#' of classes.
#'
#' The formula above can be generalized to:
#' `FL(p_t) = alpha * (1 - p_t) ** gamma * CrossEntropy(y_true, y_pred)`
#'
#' where minus comes from `CrossEntropy(y_true, y_pred)` (CE).
#'
#' Extending this to multi-class case is straightforward:
#' `FL(p_t) = alpha * (1 - p_t) ** gamma * CategoricalCE(y_true, y_pred)`
#'
#' In the snippet below, there is `num_classes` floating pointing values per
#' example. The shape of both `y_pred` and `y_true` are
#' `(batch_size, num_classes)`.
#'
#' # Examples
#' ```python
#' y_true = [[0, 1, 0], [0, 0, 1]]
#' y_pred = [[0.05, 0.9, 0.05], [0.1, 0.85, 0.05]]
#' loss = keras.losses.categorical_focal_crossentropy(y_true, y_pred)
#' assert loss.shape == (2,)
#' loss
#' # array([2.63401289e-04, 6.75912094e-01], dtype=float32)
#' ```
#' Standalone usage:
#'
#' ```python
#' y_true = [[0., 1., 0.], [0., 0., 1.]]
#' y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
#' # Using 'auto'/'sum_over_batch_size' reduction type.
#' cce = keras.losses.CategoricalFocalCrossentropy()
#' cce(y_true, y_pred)
#' # 0.23315276
#' ```
#'
#' ```python
#' # Calling with 'sample_weight'.
#' cce(y_true, y_pred, sample_weight=np.array([0.3, 0.7]))
#' # 0.1632
#' ```
#'
#' ```python
#' # Using 'sum' reduction type.
#' cce = keras.losses.CategoricalFocalCrossentropy(
#'     reduction="sum")
#' cce(y_true, y_pred)
#' # 0.46631
#' ```
#'
#' ```python
#' # Using 'none' reduction type.
#' cce = keras.losses.CategoricalFocalCrossentropy(
#'     reduction=None)
#' cce(y_true, y_pred)
#' # array([3.2058331e-05, 4.6627346e-01], dtype=float32)
#' ```
#'
#' Usage with the `compile()` API:
#'
#' ```python
#' model.compile(optimizer='adam',
#'               loss=keras.losses.CategoricalFocalCrossentropy())
#' ```
#'
#' # Returns
#' Categorical focal crossentropy loss value.
#'
#' @param alpha A weight balancing factor for all classes, default is `0.25` as
#'     mentioned in the reference. It can be a list of floats or a scalar.
#'     In the multi-class case, alpha may be set by inverse class
#'     frequency by using `compute_class_weight` from `sklearn.utils`.
#' @param gamma A focusing parameter, default is `2.0` as mentioned in the
#'     reference. It helps to gradually reduce the importance given to
#'     simple examples in a smooth manner. When `gamma` = 0, there is
#'     no focal effect on the categorical crossentropy.
#' @param from_logits Whether `y_pred` is expected to be a logits tensor. By
#'     default, we assume that `y_pred` encodes a probability
#'     distribution.
#' @param label_smoothing Float in `[0, 1].` If > `0` then smooth the labels. For
#'     example, if `0.1`, use `0.1 / num_classes` for non-target labels
#'     and `0.9 + 0.1 / num_classes` for target labels.
#' @param axis Defaults to `-1`. The dimension along which the entropy is
#'     computed.
#' @param reduction Type of reduction to apply to the loss. In almost all cases
#'     this should be `"sum_over_batch_size"`.
#'     Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
#' @param name Optional name for the loss instance.
#' @param y_true Tensor of one-hot true targets.
#' @param y_pred Tensor of predicted targets.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family loss
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalFocalCrossentropy>
loss_categorical_focal_crossentropy <-
function (y_true, y_pred, alpha = 0.25, gamma = 2, from_logits = FALSE,
    label_smoothing = 0, axis = -1L, ..., reduction = "sum_over_batch_size",
    name = "categorical_focal_crossentropy")
{
    args <- capture_args2(list(axis = as_axis))
    callable <- if (missing(y_true) && missing(y_pred))
        keras$losses$CategoricalFocalCrossentropy
    else keras$losses$categorical_focal_crossentropy
    do.call(callable, args)
}


# keras$layers$CategoricalHinge
# keras_core.src.losses.losses.CategoricalHinge
r"-(Computes the categorical hinge loss between `y_true` & `y_pred`.

    Formula:

    ```python
    loss = maximum(neg - pos + 1, 0)
    ```

    where `neg=maximum((1-y_true)*y_pred)` and `pos=sum(y_true*y_pred)`

    Args:
        reduction: Type of reduction to apply to the loss. In almost all cases
            this should be `"sum_over_batch_size"`.
            Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
        name: Optional name for the loss instance.
    )-"


# keras_core.src.losses.losses.CategoricalHinge
#' Computes the categorical hinge loss between `y_true` & `y_pred`.
#'
#' @description
#' Formula:
#'
#' ```python
#' loss = maximum(neg - pos + 1, 0)
#' ```
#'
#' where `neg=maximum((1-y_true)*y_pred)` and `pos=sum(y_true*y_pred)`
#' Formula:
#'
#' ```python
#' loss = maximum(neg - pos + 1, 0)
#' ```
#'
#' where `neg=maximum((1-y_true)*y_pred)` and `pos=sum(y_true*y_pred)`
#'
#' # Returns
#' Categorical hinge loss values with shape = `[batch_size, d0, .. dN-1]`.
#'
#' # Examples
#' ```python
#' y_true = np.random.randint(0, 3, size=(2,))
#' y_true = np.eye(np.max(y_true) + 1)[y_true]
#' y_pred = np.random.random(size=(2, 3))
#' loss = keras.losses.categorical_hinge(y_true, y_pred)
#' ```
#'
#' @param reduction Type of reduction to apply to the loss. In almost all cases
#'     this should be `"sum_over_batch_size"`.
#'     Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
#' @param name Optional name for the loss instance.
#' @param y_true The ground truth values. `y_true` values are expected to be
#'     either `{-1, +1}` or `{0, 1}` (i.e. a one-hot-encoded tensor) with
#'     shape = `[batch_size, d0, .. dN]`.
#' @param y_pred The predicted values with shape = `[batch_size, d0, .. dN]`.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family loss
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalHinge>
loss_categorical_hinge <-
function (y_true, y_pred, ..., reduction = "sum_over_batch_size",
    name = "categorical_hinge")
{
    args <- capture_args2(NULL)
    callable <- if (missing(y_true) && missing(y_pred))
        keras$losses$CategoricalHinge
    else keras$losses$categorical_hinge
    do.call(callable, args)
}


# keras$layers$CosineSimilarity
# keras_core.src.losses.losses.CosineSimilarity
r"-(Computes the cosine similarity between `y_true` & `y_pred`.

    Note that it is a number between -1 and 1. When it is a negative number
    between -1 and 0, 0 indicates orthogonality and values closer to -1
    indicate greater similarity. This makes it usable as a loss function in a
    setting where you try to maximize the proximity between predictions and
    targets. If either `y_true` or `y_pred` is a zero vector, cosine similarity
    will be 0 regardless of the proximity between predictions and targets.

    Formula:

    ```python
    loss = -sum(l2_norm(y_true) * l2_norm(y_pred))
    ```

    Args:
        axis: The axis along which the cosine similarity is computed
            (the features axis). Defaults to `-1`.
        reduction: Type of reduction to apply to the loss. In almost all cases
            this should be `"sum_over_batch_size"`.
            Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
        name: Optional name for the loss instance.
    )-"


# keras_core.src.losses.losses.CosineSimilarity
#' Computes the cosine similarity between `y_true` & `y_pred`.
#'
#' @description
#' Formula:
#' ```python
#' loss = -sum(l2_norm(y_true) * l2_norm(y_pred))
#' ```
#'
#' Note that it is a number between -1 and 1. When it is a negative number
#' between -1 and 0, 0 indicates orthogonality and values closer to -1
#' indicate greater similarity. This makes it usable as a loss function in a
#' setting where you try to maximize the proximity between predictions and
#' targets. If either `y_true` or `y_pred` is a zero vector, cosine
#' similarity will be 0 regardless of the proximity between predictions
#' and targets.
#' Note that it is a number between -1 and 1. When it is a negative number
#' between -1 and 0, 0 indicates orthogonality and values closer to -1
#' indicate greater similarity. This makes it usable as a loss function in a
#' setting where you try to maximize the proximity between predictions and
#' targets. If either `y_true` or `y_pred` is a zero vector, cosine similarity
#' will be 0 regardless of the proximity between predictions and targets.
#'
#' Formula:
#'
#' ```python
#' loss = -sum(l2_norm(y_true) * l2_norm(y_pred))
#' ```
#'
#' # Returns
#' Cosine similarity tensor.
#'
#' # Examples
#' ```python
#' y_true = [[0., 1.], [1., 1.], [1., 1.]]
#' y_pred = [[1., 0.], [1., 1.], [-1., -1.]]
#' loss = keras.losses.cosine_similarity(y_true, y_pred, axis=-1)
#' # [-0., -0.99999994, 0.99999994]
#' ```
#'
#' @param axis Axis along which to determine similarity. Defaults to `-1`.
#' @param reduction Type of reduction to apply to the loss. In almost all cases
#'     this should be `"sum_over_batch_size"`.
#'     Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
#' @param name Optional name for the loss instance.
#' @param y_true Tensor of true targets.
#' @param y_pred Tensor of predicted targets.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family loss
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/losses/CosineSimilarity>
loss_cosine_similarity <-
function (y_true, y_pred, axis = -1L, ..., reduction = "sum_over_batch_size",
    name = "cosine_similarity")
{
    args <- capture_args2(list(axis = as_axis))
    callable <- if (missing(y_true) && missing(y_pred))
        keras$losses$CosineSimilarity
    else keras$losses$cosine_similarity
    do.call(callable, args)
}


# keras$layers$Hinge
# keras_core.src.losses.losses.Hinge
r"-(Computes the hinge loss between `y_true` & `y_pred`.

    Formula:

    ```python
    loss = maximum(1 - y_true * y_pred, 0)
    ```

    `y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are
    provided we will convert them to -1 or 1.

    Args:
        reduction: Type of reduction to apply to the loss. In almost all cases
            this should be `"sum_over_batch_size"`.
            Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
        name: Optional name for the loss instance.
    )-"


# keras_core.src.losses.losses.Hinge
#' Computes the hinge loss between `y_true` & `y_pred`.
#'
#' @description
#' Formula:
#'
#' ```python
#' loss = mean(maximum(1 - y_true * y_pred, 0), axis=-1)
#' ```
#' Formula:
#'
#' ```python
#' loss = maximum(1 - y_true * y_pred, 0)
#' ```
#'
#' `y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are
#' provided we will convert them to -1 or 1.
#'
#' # Returns
#' Hinge loss values with shape = `[batch_size, d0, .. dN-1]`.
#'
#' # Examples
#' ```python
#' y_true = np.random.choice([-1, 1], size=(2, 3))
#' y_pred = np.random.random(size=(2, 3))
#' loss = keras.losses.hinge(y_true, y_pred)
#' ```
#'
#' @param reduction Type of reduction to apply to the loss. In almost all cases
#'     this should be `"sum_over_batch_size"`.
#'     Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
#' @param name Optional name for the loss instance.
#' @param y_true The ground truth values. `y_true` values are expected to be -1
#'     or 1. If binary (0 or 1) labels are provided they will be converted
#'     to -1 or 1 with shape = `[batch_size, d0, .. dN]`.
#' @param y_pred The predicted values with shape = `[batch_size, d0, .. dN]`.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family loss
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/losses/Hinge>
loss_hinge <-
function (y_true, y_pred, ..., reduction = "sum_over_batch_size",
    name = "hinge")
{
    args <- capture_args2(NULL)
    callable <- if (missing(y_true) && missing(y_pred))
        keras$losses$Hinge
    else keras$losses$hinge
    do.call(callable, args)
}


# keras$layers$Huber
# keras_core.src.losses.losses.Huber
r"-(Computes the Huber loss between `y_true` & `y_pred`.

    Formula:

    ```python
    for x in error:
        if abs(x) <= delta:
            loss.append(0.5 * x^2)
        elif abs(x) > delta:
            loss.append(delta * abs(x) - 0.5 * delta^2)

    loss = mean(loss, axis=-1)
    ```
    See: [Huber loss](https://en.wikipedia.org/wiki/Huber_loss).

    Args:
        delta: A float, the point where the Huber loss function changes from a
            quadratic to linear.
        reduction: Type of reduction to apply to loss. Options are `"sum"`,
            `"sum_over_batch_size"` or `None`. Defaults to
            `"sum_over_batch_size"`.
        name: Optional name for the instance.
    )-"


# keras_core.src.losses.losses.Huber
#' Computes the Huber loss between `y_true` & `y_pred`.
#'
#' @description
#' Formula:
#' ```python
#' for x in error:
#'     if abs(x) <= delta:
#'         loss.append(0.5 * x^2)
#'     elif abs(x) > delta:
#'         loss.append(delta * abs(x) - 0.5 * delta^2)
#'
#' loss = mean(loss, axis=-1)
#' ```
#' See: [Huber loss](https://en.wikipedia.org/wiki/Huber_loss).
#' Formula:
#'
#' ```python
#' for x in error:
#'     if abs(x) <= delta:
#'         loss.append(0.5 * x^2)
#'     elif abs(x) > delta:
#'         loss.append(delta * abs(x) - 0.5 * delta^2)
#'
#' loss = mean(loss, axis=-1)
#' ```
#' See: [Huber loss](https://en.wikipedia.org/wiki/Huber_loss).
#'
#' # Examples
#' ```python
#' y_true = [[0, 1], [0, 0]]
#' y_pred = [[0.6, 0.4], [0.4, 0.6]]
#' loss = keras.losses.huber(y_true, y_pred)
#' # 0.155
#' ```
#'
#' # Returns
#'     Tensor with one scalar loss entry per sample.
#'
#' @param delta A float, the point where the Huber loss function changes from a
#'     quadratic to linear. Defaults to `1.0`.
#' @param reduction Type of reduction to apply to loss. Options are `"sum"`,
#'     `"sum_over_batch_size"` or `None`. Defaults to
#'     `"sum_over_batch_size"`.
#' @param name Optional name for the instance.
#' @param y_true tensor of true targets.
#' @param y_pred tensor of predicted targets.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family loss
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/losses/Huber>
loss_huber <-
function (y_true, y_pred, delta = 1, ..., reduction = "sum_over_batch_size",
    name = "huber_loss")
{
    args <- capture_args2(NULL)
    callable <- if (missing(y_true) && missing(y_pred))
        keras$losses$Huber
    else keras$losses$huber
    do.call(callable, args)
}


# keras$layers$KLDivergence
# keras_core.src.losses.losses.KLDivergence
r"-(Computes Kullback-Leibler divergence loss between `y_true` & `y_pred`.

    Formula:

    ```python
    loss = y_true * log(y_true / y_pred)
    ```

    Args:
        reduction: Type of reduction to apply to the loss. In almost all cases
            this should be `"sum_over_batch_size"`.
            Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
        name: Optional name for the loss instance.
    )-"


# keras_core.src.losses.losses.KLDivergence
#' Computes Kullback-Leibler divergence loss between `y_true` & `y_pred`.
#'
#' @description
#' Formula:
#'
#' ```python
#' loss = y_true * log(y_true / y_pred)
#' ```
#' Formula:
#'
#' ```python
#' loss = y_true * log(y_true / y_pred)
#' ```
#'
#' # Returns
#' KL Divergence loss values with shape = `[batch_size, d0, .. dN-1]`.
#'
#' # Examples
#' ```python
#' y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float32)
#' y_pred = np.random.random(size=(2, 3))
#' loss = keras.losses.kl_divergence(y_true, y_pred)
#' assert loss.shape == (2,)
#' y_true = ops.clip(y_true, 1e-7, 1)
#' y_pred = ops.clip(y_pred, 1e-7, 1)
#' assert np.array_equal(
#'     loss, np.sum(y_true * np.log(y_true / y_pred), axis=-1))
#' ```
#'
#' @param reduction Type of reduction to apply to the loss. In almost all cases
#'     this should be `"sum_over_batch_size"`.
#'     Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
#' @param name Optional name for the loss instance.
#' @param y_true Tensor of true targets.
#' @param y_pred Tensor of predicted targets.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family loss
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/losses/KLDivergence>
loss_kl_divergence <-
function (y_true, y_pred, ..., reduction = "sum_over_batch_size",
    name = "kl_divergence")
{
    args <- capture_args2(NULL)
    callable <- if (missing(y_true) && missing(y_pred))
        keras$losses$KLDivergence
    else keras$losses$kl_divergence
    do.call(callable, args)
}


# keras$layers$LogCosh
# keras_core.src.losses.losses.LogCosh
r"-(Computes the logarithm of the hyperbolic cosine of the prediction error.

    Formula:

    ```python
    error = y_pred - y_true
    logcosh = mean(log((exp(error) + exp(-error))/2), axis=-1)`
    ```
    where x is the error `y_pred - y_true`.

    Args:
        reduction: Type of reduction to apply to loss. Options are `"sum"`,
            `"sum_over_batch_size"` or `None`. Defaults to
            `"sum_over_batch_size"`.
        name: Optional name for the instance.
    )-"


# keras_core.src.losses.losses.LogCosh
#' Computes the logarithm of the hyperbolic cosine of the prediction error.
#'
#' @description
#' Formula:
#' ```python
#' loss = mean(log(cosh(y_pred - y_true)), axis=-1)
#' ```
#'
#' Note that `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small
#' `x` and to `abs(x) - log(2)` for large `x`. This means that 'logcosh' works
#' mostly like the mean squared error, but will not be so strongly affected by
#' the occasional wildly incorrect prediction.
#' Formula:
#'
#' ```python
#' error = y_pred - y_true
#' logcosh = mean(log((exp(error) + exp(-error))/2), axis=-1)`
#' ```
#' where x is the error `y_pred - y_true`.
#'
#' # Examples
#' ```python
#' y_true = [[0., 1.], [0., 0.]]
#' y_pred = [[1., 1.], [0., 0.]]
#' loss = keras.losses.log_cosh(y_true, y_pred)
#' # 0.108
#' ```
#'
#' # Returns
#'     Logcosh error values with shape = `[batch_size, d0, .. dN-1]`.
#'
#' @param reduction Type of reduction to apply to loss. Options are `"sum"`,
#'     `"sum_over_batch_size"` or `None`. Defaults to
#'     `"sum_over_batch_size"`.
#' @param name Optional name for the instance.
#' @param y_true Ground truth values with shape = `[batch_size, d0, .. dN]`.
#' @param y_pred The predicted values with shape = `[batch_size, d0, .. dN]`.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family loss
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/losses/LogCosh>
loss_logcosh <-
function (y_true, y_pred, ..., reduction = "sum_over_batch_size",
    name = "log_cosh")
{
    args <- capture_args2(NULL)
    callable <- if (missing(y_true) && missing(y_pred))
        keras$losses$LogCosh
    else keras$losses$log_cosh
    do.call(callable, args)
}


# keras$layers$MeanAbsoluteError
# keras_core.src.losses.losses.MeanAbsoluteError
r"-(Computes the mean of absolute difference between labels and predictions.

    Formula:

    ```python
    loss = mean(abs(y_true - y_pred))
    ```

    Args:
        reduction: Type of reduction to apply to the loss. In almost all cases
            this should be `"sum_over_batch_size"`.
            Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
        name: Optional name for the loss instance.
    )-"


# keras_core.src.losses.losses.MeanAbsoluteError
#' Computes the mean of absolute difference between labels and predictions.
#'
#' @description
#' ```python
#' loss = mean(abs(y_true - y_pred), axis=-1)
#' ```
#' Formula:
#'
#' ```python
#' loss = mean(abs(y_true - y_pred))
#' ```
#'
#' # Returns
#' Mean absolute error values with shape = `[batch_size, d0, .. dN-1]`.
#'
#' # Examples
#' ```python
#' y_true = np.random.randint(0, 2, size=(2, 3))
#' y_pred = np.random.random(size=(2, 3))
#' loss = keras.losses.mean_absolute_error(y_true, y_pred)
#' ```
#'
#' @param reduction Type of reduction to apply to the loss. In almost all cases
#'     this should be `"sum_over_batch_size"`.
#'     Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
#' @param name Optional name for the loss instance.
#' @param y_true Ground truth values with shape = `[batch_size, d0, .. dN]`.
#' @param y_pred The predicted values with shape = `[batch_size, d0, .. dN]`.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family loss
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanAbsoluteError>
loss_mean_absolute_error <-
function (y_true, y_pred, ..., reduction = "sum_over_batch_size",
    name = "mean_absolute_error")
{
    args <- capture_args2(NULL)
    callable <- if (missing(y_true) && missing(y_pred))
        keras$losses$MeanAbsoluteError
    else keras$losses$mean_absolute_error
    do.call(callable, args)
}


# keras$layers$MeanAbsolutePercentageError
# keras_core.src.losses.losses.MeanAbsolutePercentageError
r"-(Computes the mean absolute percentage error between `y_true` & `y_pred`.

    Formula:

    ```python
    loss = 100 * mean(abs((y_true - y_pred) / y_true))
    ```

    Args:
        reduction: Type of reduction to apply to the loss. In almost all cases
            this should be `"sum_over_batch_size"`.
            Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
        name: Optional name for the loss instance.
    )-"


# keras_core.src.losses.losses.MeanAbsolutePercentageError
#' Computes the mean absolute percentage error between `y_true` & `y_pred`.
#'
#' @description
#' Formula:
#'
#' ```python
#' loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1)
#' ```
#'
#' Division by zero is prevented by dividing by `maximum(y_true, epsilon)`
#' where `epsilon = keras.backend.epsilon()`
#' (default to `1e-7`).
#' Formula:
#'
#' ```python
#' loss = 100 * mean(abs((y_true - y_pred) / y_true))
#' ```
#'
#' # Returns
#' Mean absolute percentage error values with shape = `[batch_size, d0, ..
#' dN-1]`.
#'
#' # Examples
#' ```python
#' y_true = np.random.random(size=(2, 3))
#' y_pred = np.random.random(size=(2, 3))
#' loss = keras.losses.mean_absolute_percentage_error(y_true, y_pred)
#' ```
#'
#' @param reduction Type of reduction to apply to the loss. In almost all cases
#'     this should be `"sum_over_batch_size"`.
#'     Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
#' @param name Optional name for the loss instance.
#' @param y_true Ground truth values with shape = `[batch_size, d0, .. dN]`.
#' @param y_pred The predicted values with shape = `[batch_size, d0, .. dN]`.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family loss
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanAbsolutePercentageError>
loss_mean_absolute_percentage_error <-
function (y_true, y_pred, ..., reduction = "sum_over_batch_size",
    name = "mean_absolute_percentage_error")
{
    args <- capture_args2(NULL)
    callable <- if (missing(y_true) && missing(y_pred))
        keras$losses$MeanAbsolutePercentageError
    else keras$losses$mean_absolute_percentage_error
    do.call(callable, args)
}


# keras$layers$MeanSquaredError
# keras_core.src.losses.losses.MeanSquaredError
r"-(Computes the mean of squares of errors between labels and predictions.

    Formula:

    ```python
    loss = mean(square(y_true - y_pred))
    ```

    Args:
        reduction: Type of reduction to apply to the loss. In almost all cases
            this should be `"sum_over_batch_size"`.
            Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
        name: Optional name for the loss instance.
    )-"


# keras_core.src.losses.losses.MeanSquaredError
#' Computes the mean of squares of errors between labels and predictions.
#'
#' @description
#' Formula:
#'
#' ```python
#' loss = mean(square(y_true - y_pred), axis=-1)
#' ```
#' Formula:
#'
#' ```python
#' loss = mean(square(y_true - y_pred))
#' ```
#'
#' # Examples
#' ```python
#' y_true = np.random.randint(0, 2, size=(2, 3))
#' y_pred = np.random.random(size=(2, 3))
#' loss = keras.losses.mean_squared_error(y_true, y_pred)
#' ```
#'
#' # Returns
#'     Mean squared error values with shape = `[batch_size, d0, .. dN-1]`.
#'
#' @param reduction Type of reduction to apply to the loss. In almost all cases
#'     this should be `"sum_over_batch_size"`.
#'     Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
#' @param name Optional name for the loss instance.
#' @param y_true Ground truth values with shape = `[batch_size, d0, .. dN]`.
#' @param y_pred The predicted values with shape = `[batch_size, d0, .. dN]`.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family loss
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError>
loss_mean_squared_error <-
function (y_true, y_pred, ..., reduction = "sum_over_batch_size",
    name = "mean_squared_error")
{
    args <- capture_args2(NULL)
    callable <- if (missing(y_true) && missing(y_pred))
        keras$losses$MeanSquaredError
    else keras$losses$mean_squared_error
    do.call(callable, args)
}


# keras$layers$MeanSquaredLogarithmicError
# keras_core.src.losses.losses.MeanSquaredLogarithmicError
r"-(Computes the mean squared logarithmic error between `y_true` & `y_pred`.

    Formula:

    ```python
    loss = mean(square(log(y_true + 1) - log(y_pred + 1)))
    ```

    Args:
        reduction: Type of reduction to apply to the loss. In almost all cases
            this should be `"sum_over_batch_size"`.
            Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
        name: Optional name for the loss instance.
    )-"


# keras_core.src.losses.losses.MeanSquaredLogarithmicError
#' Computes the mean squared logarithmic error between `y_true` & `y_pred`.
#'
#' @description
#' Formula:
#'
#' ```python
#' loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1)
#' ```
#'
#' Note that `y_pred` and `y_true` cannot be less or equal to 0. Negative
#' values and 0 values will be replaced with `keras.backend.epsilon()`
#' (default to `1e-7`).
#' Formula:
#'
#' ```python
#' loss = mean(square(log(y_true + 1) - log(y_pred + 1)))
#' ```
#'
#' # Returns
#' Mean squared logarithmic error values with shape = `[batch_size, d0, ..
#' dN-1]`.
#'
#' # Examples
#' ```python
#' y_true = np.random.randint(0, 2, size=(2, 3))
#' y_pred = np.random.random(size=(2, 3))
#' loss = keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
#' ```
#'
#' @param reduction Type of reduction to apply to the loss. In almost all cases
#'     this should be `"sum_over_batch_size"`.
#'     Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
#' @param name Optional name for the loss instance.
#' @param y_true Ground truth values with shape = `[batch_size, d0, .. dN]`.
#' @param y_pred The predicted values with shape = `[batch_size, d0, .. dN]`.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family loss
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredLogarithmicError>
loss_mean_squared_logarithmic_error <-
function (y_true, y_pred, ..., reduction = "sum_over_batch_size",
    name = "mean_squared_logarithmic_error")
{
    args <- capture_args2(NULL)
    callable <- if (missing(y_true) && missing(y_pred))
        keras$losses$MeanSquaredLogarithmicError
    else keras$losses$mean_squared_logarithmic_error
    do.call(callable, args)
}


# keras$layers$Poisson
# keras_core.src.losses.losses.Poisson
r"-(Computes the Poisson loss between `y_true` & `y_pred`.

    Formula:

    ```python
    loss = y_pred - y_true * log(y_pred)
    ```

    Args:
        reduction: Type of reduction to apply to the loss. In almost all cases
            this should be `"sum_over_batch_size"`.
            Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
        name: Optional name for the loss instance.
    )-"


# keras_core.src.losses.losses.Poisson
#' Computes the Poisson loss between `y_true` & `y_pred`.
#'
#' @description
#' Formula:
#'
#' ```python
#' loss = y_pred - y_true * log(y_pred)
#' ```
#' Formula:
#'
#' ```python
#' loss = y_pred - y_true * log(y_pred)
#' ```
#'
#' # Returns
#' Poisson loss values with shape = `[batch_size, d0, .. dN-1]`.
#'
#' # Examples
#' ```python
#' y_true = np.random.randint(0, 2, size=(2, 3))
#' y_pred = np.random.random(size=(2, 3))
#' loss = keras.losses.poisson(y_true, y_pred)
#' assert loss.shape == (2,)
#' y_pred = y_pred + 1e-7
#' assert np.allclose(
#'     loss, np.mean(y_pred - y_true * np.log(y_pred), axis=-1),
#'     atol=1e-5)
#' ```
#'
#' @param reduction Type of reduction to apply to the loss. In almost all cases
#'     this should be `"sum_over_batch_size"`.
#'     Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
#' @param name Optional name for the loss instance.
#' @param y_true Ground truth values. shape = `[batch_size, d0, .. dN]`.
#' @param y_pred The predicted values. shape = `[batch_size, d0, .. dN]`.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family loss
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/losses/Poisson>
loss_poisson <-
function (y_true, y_pred, ..., reduction = "sum_over_batch_size",
    name = "poisson")
{
    args <- capture_args2(NULL)
    callable <- if (missing(y_true) && missing(y_pred))
        keras$losses$Poisson
    else keras$losses$poisson
    do.call(callable, args)
}


# keras$layers$SparseCategoricalCrossentropy
# keras_core.src.losses.losses.SparseCategoricalCrossentropy
r"-(Computes the crossentropy loss between the labels and predictions.

    Use this crossentropy loss function when there are two or more label
    classes.  We expect labels to be provided as integers. If you want to
    provide labels using `one-hot` representation, please use
    `CategoricalCrossentropy` loss.  There should be `# classes` floating point
    values per feature for `y_pred` and a single floating point value per
    feature for `y_true`.

    In the snippet below, there is a single floating point value per example for
    `y_true` and `num_classes` floating pointing values per example for
    `y_pred`. The shape of `y_true` is `[batch_size]` and the shape of `y_pred`
    is `[batch_size, num_classes]`.

    Args:
        from_logits: Whether `y_pred` is expected to be a logits tensor. By
            default, we assume that `y_pred` encodes a probability distribution.
        reduction: Type of reduction to apply to the loss. In almost all cases
            this should be `"sum_over_batch_size"`.
            Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
        name: Optional name for the loss instance.

    Examples:

    >>> y_true = [1, 2]
    >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
    >>> # Using 'auto'/'sum_over_batch_size' reduction type.
    >>> scce = keras.losses.SparseCategoricalCrossentropy()
    >>> scce(y_true, y_pred)
    1.177

    >>> # Calling with 'sample_weight'.
    >>> scce(y_true, y_pred, sample_weight=np.array([0.3, 0.7]))
    0.814

    >>> # Using 'sum' reduction type.
    >>> scce = keras.losses.SparseCategoricalCrossentropy(
    ...     reduction="sum")
    >>> scce(y_true, y_pred)
    2.354

    >>> # Using 'none' reduction type.
    >>> scce = keras.losses.SparseCategoricalCrossentropy(
    ...     reduction=None)
    >>> scce(y_true, y_pred)
    array([0.0513, 2.303], dtype=float32)

    Usage with the `compile()` API:

    ```python
    model.compile(optimizer='sgd',
                  loss=keras.losses.SparseCategoricalCrossentropy())
    ```
    )-"


# keras_core.src.losses.losses.SparseCategoricalCrossentropy
#' Computes the crossentropy loss between the labels and predictions.
#'
#' @description
#' Use this crossentropy loss function when there are two or more label
#' classes.  We expect labels to be provided as integers. If you want to
#' provide labels using `one-hot` representation, please use
#' `CategoricalCrossentropy` loss.  There should be `# classes` floating point
#' values per feature for `y_pred` and a single floating point value per
#' feature for `y_true`.
#'
#' In the snippet below, there is a single floating point value per example for
#' `y_true` and `num_classes` floating pointing values per example for
#' `y_pred`. The shape of `y_true` is `[batch_size]` and the shape of `y_pred`
#' is `[batch_size, num_classes]`.
#'
#' # Examples
#' ```python
#' y_true = [1, 2]
#' y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
#' loss = keras.losses.sparse_categorical_crossentropy(y_true, y_pred)
#' assert loss.shape == (2,)
#' loss
#' # array([0.0513, 2.303], dtype=float32)
#' ```
#' ```python
#' y_true = [1, 2]
#' y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
#' # Using 'auto'/'sum_over_batch_size' reduction type.
#' scce = keras.losses.SparseCategoricalCrossentropy()
#' scce(y_true, y_pred)
#' # 1.177
#' ```
#'
#' ```python
#' # Calling with 'sample_weight'.
#' scce(y_true, y_pred, sample_weight=np.array([0.3, 0.7]))
#' # 0.814
#' ```
#'
#' ```python
#' # Using 'sum' reduction type.
#' scce = keras.losses.SparseCategoricalCrossentropy(
#'     reduction="sum")
#' scce(y_true, y_pred)
#' # 2.354
#' ```
#'
#' ```python
#' # Using 'none' reduction type.
#' scce = keras.losses.SparseCategoricalCrossentropy(
#'     reduction=None)
#' scce(y_true, y_pred)
#' # array([0.0513, 2.303], dtype=float32)
#' ```
#'
#' Usage with the `compile()` API:
#'
#' ```python
#' model.compile(optimizer='sgd',
#'               loss=keras.losses.SparseCategoricalCrossentropy())
#' ```
#'
#' # Returns
#' Sparse categorical crossentropy loss value.
#'
#' @param from_logits Whether `y_pred` is expected to be a logits tensor. By
#'     default, we assume that `y_pred` encodes a probability distribution.
#' @param reduction Type of reduction to apply to the loss. In almost all cases
#'     this should be `"sum_over_batch_size"`.
#'     Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
#' @param name Optional name for the loss instance.
#' @param y_true Ground truth values.
#' @param y_pred The predicted values.
#' @param ignore_class Optional integer. The ID of a class to be ignored during
#'     loss computation. This is useful, for example, in segmentation
#'     problems featuring a "void" class (commonly -1 or 255) in
#'     segmentation maps. By default (`ignore_class=None`), all classes are
#'     considered.
#' @param axis Defaults to `-1`. The dimension along which the entropy is
#'     computed.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family loss
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy>
loss_sparse_categorical_crossentropy <-
function (y_true, y_pred, from_logits = FALSE, ignore_class = NULL,
    axis = -1L, ..., reduction = "sum_over_batch_size", name = "sparse_categorical_crossentropy")
{
    args <- capture_args2(list(ignore_class = as_integer, axis = as_axis))
    callable <- if (missing(y_true) && missing(y_pred))
        keras$losses$SparseCategoricalCrossentropy
    else keras$losses$sparse_categorical_crossentropy
    do.call(callable, args)
}


# keras$layers$SquaredHinge
# keras_core.src.losses.losses.SquaredHinge
r"-(Computes the squared hinge loss between `y_true` & `y_pred`.

    Formula:

    ```python
    loss = square(maximum(1 - y_true * y_pred, 0))
    ```

    `y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are
    provided we will convert them to -1 or 1.

    Args:
        reduction: Type of reduction to apply to the loss. In almost all cases
            this should be `"sum_over_batch_size"`.
            Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
        name: Optional name for the loss instance.
    )-"


# keras_core.src.losses.losses.SquaredHinge
#' Computes the squared hinge loss between `y_true` & `y_pred`.
#'
#' @description
#' Formula:
#'
#' ```python
#' loss = mean(square(maximum(1 - y_true * y_pred, 0)), axis=-1)
#' ```
#' Formula:
#'
#' ```python
#' loss = square(maximum(1 - y_true * y_pred, 0))
#' ```
#'
#' `y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are
#' provided we will convert them to -1 or 1.
#'
#' # Returns
#' Squared hinge loss values with shape = `[batch_size, d0, .. dN-1]`.
#'
#' # Examples
#' ```python
#' y_true = np.random.choice([-1, 1], size=(2, 3))
#' y_pred = np.random.random(size=(2, 3))
#' loss = keras.losses.squared_hinge(y_true, y_pred)
#' ```
#'
#' @param reduction Type of reduction to apply to the loss. In almost all cases
#'     this should be `"sum_over_batch_size"`.
#'     Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
#' @param name Optional name for the loss instance.
#' @param y_true The ground truth values. `y_true` values are expected to be -1
#'     or 1. If binary (0 or 1) labels are provided we will convert them
#'     to -1 or 1 with shape = `[batch_size, d0, .. dN]`.
#' @param y_pred The predicted values with shape = `[batch_size, d0, .. dN]`.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family loss
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/losses/SquaredHinge>
loss_squared_hinge <-
function (y_true, y_pred, ..., reduction = "sum_over_batch_size",
    name = "squared_hinge")
{
    args <- capture_args2(NULL)
    callable <- if (missing(y_true) && missing(y_pred))
        keras$losses$SquaredHinge
    else keras$losses$squared_hinge
    do.call(callable, args)
}
