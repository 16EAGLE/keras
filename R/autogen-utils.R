## Autogenerated. Do not modify manually.


# keras.utils.clear_session
# keras.src.backend.common.global_state.clear_session
r"-(Resets all state generated by Keras.

    Keras manages a global state, which it uses to implement the Functional
    model-building API and to uniquify autogenerated layer names.

    If you are creating many models in a loop, this global state will consume
    an increasing amount of memory over time, and you may want to clear it.
    Calling `clear_session()` releases the global state: this helps avoid
    clutter from old models and layers, especially when memory is limited.

    Example 1: calling `clear_session()` when creating models in a loop

    ```python
    for _ in range(100):
      # Without `clear_session()`, each iteration of this loop will
      # slightly increase the size of the global state managed by Keras
      model = keras.Sequential([
          keras.layers.Dense(10) for _ in range(10)])

    for _ in range(100):
      # With `clear_session()` called at the beginning,
      # Keras starts with a blank state at each iteration
      # and memory consumption is constant over time.
      keras.backend.clear_session()
      model = keras.Sequential([
          keras.layers.Dense(10) for _ in range(10)])
    ```

    Example 2: resetting the layer name generation counter

    >>> layers = [keras.layers.Dense(10) for _ in range(10)]
    >>> new_layer = keras.layers.Dense(10)
    >>> print(new_layer.name)
    dense_10
    >>> keras.backend.clear_session()
    >>> new_layer = keras.layers.Dense(10)
    >>> print(new_layer.name)
    dense
    )-"
#' Resets all state generated by Keras.
#'
#' @description
#' Keras manages a global state, which it uses to implement the Functional
#' model-building API and to uniquify autogenerated layer names.
#'
#' If you are creating many models in a loop, this global state will consume
#' an increasing amount of memory over time, and you may want to clear it.
#' Calling `clear_session()` releases the global state: this helps avoid
#' clutter from old models and layers, especially when memory is limited.
#'
#' Example 1: calling `clear_session()` when creating models in a loop
#'
#' ```python
#' for _ in range(100):
#'   # Without `clear_session()`, each iteration of this loop will
#'   # slightly increase the size of the global state managed by Keras
#'   model = keras.Sequential([
#'       keras.layers.Dense(10) for _ in range(10)])
#'
#' for _ in range(100):
#'   # With `clear_session()` called at the beginning,
#'   # Keras starts with a blank state at each iteration
#'   # and memory consumption is constant over time.
#'   keras.backend.clear_session()
#'   model = keras.Sequential([
#'       keras.layers.Dense(10) for _ in range(10)])
#' ```
#'
#' Example 2: resetting the layer name generation counter
#'
#' ```python
#' layers = [keras.layers.Dense(10) for _ in range(10)]
#' new_layer = keras.layers.Dense(10)
#' print(new_layer.name)
#' # dense_10
#' keras.backend.clear_session()
#' new_layer = keras.layers.Dense(10)
#' print(new_layer.name)
#' # dense
#' ```
#'
#' @export
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/clear_session>
clear_session <-
function ()
{
    args <- capture_args2(NULL)
    do.call(keras$utils$clear_session, args)
}


# keras.utils.FeatureSpace
# keras.src.layers.preprocessing.feature_space.FeatureSpace
r"-(One-stop utility for preprocessing and encoding structured data.

    Arguments:
        feature_names: Dict mapping the names of your features to their
            type specification, e.g. `{"my_feature": "integer_categorical"}`
            or `{"my_feature": FeatureSpace.integer_categorical()}`.
            For a complete list of all supported types, see
            "Available feature types" paragraph below.
        output_mode: One of `"concat"` or `"dict"`. In concat mode, all
            features get concatenated together into a single vector.
            In dict mode, the FeatureSpace returns a dict of individually
            encoded features (with the same keys as the input dict keys).
        crosses: List of features to be crossed together, e.g.
            `crosses=[("feature_1", "feature_2")]`. The features will be
            "crossed" by hashing their combined value into
            a fixed-length vector.
        crossing_dim: Default vector size for hashing crossed features.
            Defaults to `32`.
        hashing_dim: Default vector size for hashing features of type
            `"integer_hashed"` and `"string_hashed"`. Defaults to `32`.
        num_discretization_bins: Default number of bins to be used for
            discretizing features of type `"float_discretized"`.
            Defaults to `32`.

    **Available feature types:**

    Note that all features can be referred to by their string name,
    e.g. `"integer_categorical"`. When using the string name, the default
    argument values are used.

    ```python
    # Plain float values.
    FeatureSpace.float(name=None)

    # Float values to be preprocessed via featurewise standardization
    # (i.e. via a `keras.layers.Normalization` layer).
    FeatureSpace.float_normalized(name=None)

    # Float values to be preprocessed via linear rescaling
    # (i.e. via a `keras.layers.Rescaling` layer).
    FeatureSpace.float_rescaled(scale=1., offset=0., name=None)

    # Float values to be discretized. By default, the discrete
    # representation will then be one-hot encoded.
    FeatureSpace.float_discretized(
        num_bins, bin_boundaries=None, output_mode="one_hot", name=None)

    # Integer values to be indexed. By default, the discrete
    # representation will then be one-hot encoded.
    FeatureSpace.integer_categorical(
        max_tokens=None, num_oov_indices=1, output_mode="one_hot", name=None)

    # String values to be indexed. By default, the discrete
    # representation will then be one-hot encoded.
    FeatureSpace.string_categorical(
        max_tokens=None, num_oov_indices=1, output_mode="one_hot", name=None)

    # Integer values to be hashed into a fixed number of bins.
    # By default, the discrete representation will then be one-hot encoded.
    FeatureSpace.integer_hashed(num_bins, output_mode="one_hot", name=None)

    # String values to be hashed into a fixed number of bins.
    # By default, the discrete representation will then be one-hot encoded.
    FeatureSpace.string_hashed(num_bins, output_mode="one_hot", name=None)
    ```

    Examples:

    **Basic usage with a dict of input data:**

    ```python
    raw_data = {
        "float_values": [0.0, 0.1, 0.2, 0.3],
        "string_values": ["zero", "one", "two", "three"],
        "int_values": [0, 1, 2, 3],
    }
    dataset = tf.data.Dataset.from_tensor_slices(raw_data)

    feature_space = FeatureSpace(
        features={
            "float_values": "float_normalized",
            "string_values": "string_categorical",
            "int_values": "integer_categorical",
        },
        crosses=[("string_values", "int_values")],
        output_mode="concat",
    )
    # Before you start using the FeatureSpace,
    # you must `adapt()` it on some data.
    feature_space.adapt(dataset)

    # You can call the FeatureSpace on a dict of data (batched or unbatched).
    output_vector = feature_space(raw_data)
    ```

    **Basic usage with `tf.data`:**

    ```python
    # Unlabeled data
    preprocessed_ds = unlabeled_dataset.map(feature_space)

    # Labeled data
    preprocessed_ds = labeled_dataset.map(lambda x, y: (feature_space(x), y))
    ```

    **Basic usage with the Keras Functional API:**

    ```python
    # Retrieve a dict Keras Input objects
    inputs = feature_space.get_inputs()
    # Retrieve the corresponding encoded Keras tensors
    encoded_features = feature_space.get_encoded_features()
    # Build a Functional model
    outputs = keras.layers.Dense(1, activation="sigmoid")(encoded_features)
    model = keras.Model(inputs, outputs)
    ```

    **Customizing each feature or feature cross:**

    ```python
    feature_space = FeatureSpace(
        features={
            "float_values": FeatureSpace.float_normalized(),
            "string_values": FeatureSpace.string_categorical(max_tokens=10),
            "int_values": FeatureSpace.integer_categorical(max_tokens=10),
        },
        crosses=[
            FeatureSpace.cross(("string_values", "int_values"), crossing_dim=32)
        ],
        output_mode="concat",
    )
    ```

    **Returning a dict of integer-encoded features:**

    ```python
    feature_space = FeatureSpace(
        features={
            "string_values": FeatureSpace.string_categorical(output_mode="int"),
            "int_values": FeatureSpace.integer_categorical(output_mode="int"),
        },
        crosses=[
            FeatureSpace.cross(
                feature_names=("string_values", "int_values"),
                crossing_dim=32,
                output_mode="int",
            )
        ],
        output_mode="dict",
    )
    ```

    **Specifying your own Keras preprocessing layer:**

    ```python
    # Let's say that one of the features is a short text paragraph that
    # we want to encode as a vector (one vector per paragraph) via TF-IDF.
    data = {
        "text": ["1st string", "2nd string", "3rd string"],
    }

    # There's a Keras layer for this: TextVectorization.
    custom_layer = layers.TextVectorization(output_mode="tf_idf")

    # We can use FeatureSpace.feature to create a custom feature
    # that will use our preprocessing layer.
    feature_space = FeatureSpace(
        features={
            "text": FeatureSpace.feature(
                preprocessor=custom_layer, dtype="string", output_mode="float"
            ),
        },
        output_mode="concat",
    )
    feature_space.adapt(tf.data.Dataset.from_tensor_slices(data))
    output_vector = feature_space(data)
    ```

    **Retrieving the underlying Keras preprocessing layers:**

    ```python
    # The preprocessing layer of each feature is available in `.preprocessors`.
    preprocessing_layer = feature_space.preprocessors["feature1"]

    # The crossing layer of each feature cross is available in `.crossers`.
    # It's an instance of keras.layers.HashedCrossing.
    crossing_layer = feature_space.crossers["feature1_X_feature2"]
    ```

    **Saving and reloading a FeatureSpace:**

    ```python
    feature_space.save("featurespace.keras")
    reloaded_feature_space = keras.models.load_model("featurespace.keras")
    ```
    )-"
#' One-stop utility for preprocessing and encoding structured data.
#'
#' @description
#'
#' # Examples
#' **Basic usage with a dict of input data:**
#'
#' ```python
#' raw_data = {
#'     "float_values": [0.0, 0.1, 0.2, 0.3],
#'     "string_values": ["zero", "one", "two", "three"],
#'     "int_values": [0, 1, 2, 3],
#' }
#' dataset = tf.data.Dataset.from_tensor_slices(raw_data)
#'
#' feature_space = FeatureSpace(
#'     features={
#'         "float_values": "float_normalized",
#'         "string_values": "string_categorical",
#'         "int_values": "integer_categorical",
#'     },
#'     crosses=[("string_values", "int_values")],
#'     output_mode="concat",
#' )
#' # Before you start using the FeatureSpace,
#' # you must `adapt()` it on some data.
#' feature_space.adapt(dataset)
#'
#' # You can call the FeatureSpace on a dict of data (batched or unbatched).
#' output_vector = feature_space(raw_data)
#' ```
#'
#' **Basic usage with `tf.data`:**
#'
#' ```python
#' # Unlabeled data
#' preprocessed_ds = unlabeled_dataset.map(feature_space)
#'
#' # Labeled data
#' preprocessed_ds = labeled_dataset.map(lambda x, y: (feature_space(x), y))
#' ```
#'
#' **Basic usage with the Keras Functional API:**
#'
#' ```python
#' # Retrieve a dict Keras Input objects
#' inputs = feature_space.get_inputs()
#' # Retrieve the corresponding encoded Keras tensors
#' encoded_features = feature_space.get_encoded_features()
#' # Build a Functional model
#' outputs = keras.layers.Dense(1, activation="sigmoid")(encoded_features)
#' model = keras.Model(inputs, outputs)
#' ```
#'
#' **Customizing each feature or feature cross:**
#'
#' ```python
#' feature_space = FeatureSpace(
#'     features={
#'         "float_values": FeatureSpace.float_normalized(),
#'         "string_values": FeatureSpace.string_categorical(max_tokens=10),
#'         "int_values": FeatureSpace.integer_categorical(max_tokens=10),
#'     },
#'     crosses=[
#'         FeatureSpace.cross(("string_values", "int_values"), crossing_dim=32)
#'     ],
#'     output_mode="concat",
#' )
#' ```
#'
#' **Returning a dict of integer-encoded features:**
#'
#' ```python
#' feature_space = FeatureSpace(
#'     features={
#'         "string_values": FeatureSpace.string_categorical(output_mode="int"),
#'         "int_values": FeatureSpace.integer_categorical(output_mode="int"),
#'     },
#'     crosses=[
#'         FeatureSpace.cross(
#'             feature_names=("string_values", "int_values"),
#'             crossing_dim=32,
#'             output_mode="int",
#'         )
#'     ],
#'     output_mode="dict",
#' )
#' ```
#'
#' **Specifying your own Keras preprocessing layer:**
#'
#' ```python
#' # Let's say that one of the features is a short text paragraph that
#' # we want to encode as a vector (one vector per paragraph) via TF-IDF.
#' data = {
#'     "text": ["1st string", "2nd string", "3rd string"],
#' }
#'
#' # There's a Keras layer for this: TextVectorization.
#' custom_layer = layers.TextVectorization(output_mode="tf_idf")
#'
#' # We can use FeatureSpace.feature to create a custom feature
#' # that will use our preprocessing layer.
#' feature_space = FeatureSpace(
#'     features={
#'         "text": FeatureSpace.feature(
#'             preprocessor=custom_layer, dtype="string", output_mode="float"
#'         ),
#'     },
#'     output_mode="concat",
#' )
#' feature_space.adapt(tf.data.Dataset.from_tensor_slices(data))
#' output_vector = feature_space(data)
#' ```
#'
#' **Retrieving the underlying Keras preprocessing layers:**
#'
#' ```python
#' # The preprocessing layer of each feature is available in `.preprocessors`.
#' preprocessing_layer = feature_space.preprocessors["feature1"]
#'
#' # The crossing layer of each feature cross is available in `.crossers`.
#' # It's an instance of keras.layers.HashedCrossing.
#' crossing_layer = feature_space.crossers["feature1_X_feature2"]
#' ```
#'
#' **Saving and reloading a FeatureSpace:**
#'
#' ```python
#' feature_space.save("featurespace.keras")
#' reloaded_feature_space = keras.models.load_model("featurespace.keras")
#' ```
#'
#' @param feature_names Dict mapping the names of your features to their
#'         type specification, e.g. `{"my_feature": "integer_categorical"}`
#'         or `{"my_feature": FeatureSpace.integer_categorical()}`.
#'         For a complete list of all supported types, see
#'         "Available feature types" paragraph below.
#'     output_mode: One of `"concat"` or `"dict"`. In concat mode, all
#'         features get concatenated together into a single vector.
#'         In dict mode, the FeatureSpace returns a dict of individually
#'         encoded features (with the same keys as the input dict keys).
#'     crosses: List of features to be crossed together, e.g.
#'         `crosses=[("feature_1", "feature_2")]`. The features will be
#'         "crossed" by hashing their combined value into
#'         a fixed-length vector.
#'     crossing_dim: Default vector size for hashing crossed features.
#'         Defaults to `32`.
#'     hashing_dim: Default vector size for hashing features of type
#'         `"integer_hashed"` and `"string_hashed"`. Defaults to `32`.
#'     num_discretization_bins: Default number of bins to be used for
#'         discretizing features of type `"float_discretized"`.
#'         Defaults to `32`.
#'
#' **Available feature types:**
#'
#' Note that all features can be referred to by their string name,
#' e.g. `"integer_categorical"`. When using the string name, the default
#' argument values are used.
#'
#' ```python
#' # Plain float values.
#' FeatureSpace.float(name=None)
#'
#' # Float values to be preprocessed via featurewise standardization
#' # (i.e. via a `keras.layers.Normalization` layer).
#' FeatureSpace.float_normalized(name=None)
#'
#' # Float values to be preprocessed via linear rescaling
#' # (i.e. via a `keras.layers.Rescaling` layer).
#' FeatureSpace.float_rescaled(scale=1., offset=0., name=None)
#'
#' # Float values to be discretized. By default, the discrete
#' # representation will then be one-hot encoded.
#' FeatureSpace.float_discretized(
#'     num_bins, bin_boundaries=None, output_mode="one_hot", name=None)
#'
#' # Integer values to be indexed. By default, the discrete
#' # representation will then be one-hot encoded.
#' FeatureSpace.integer_categorical(
#'     max_tokens=None, num_oov_indices=1, output_mode="one_hot", name=None)
#'
#' # String values to be indexed. By default, the discrete
#' # representation will then be one-hot encoded.
#' FeatureSpace.string_categorical(
#'     max_tokens=None, num_oov_indices=1, output_mode="one_hot", name=None)
#'
#' # Integer values to be hashed into a fixed number of bins.
#' # By default, the discrete representation will then be one-hot encoded.
#' FeatureSpace.integer_hashed(num_bins, output_mode="one_hot", name=None)
#'
#' # String values to be hashed into a fixed number of bins.
#' # By default, the discrete representation will then be one-hot encoded.
#' FeatureSpace.string_hashed(num_bins, output_mode="one_hot", name=None)
#' ```
#' @param name String, name for the object
#' @param object see description
#' @param features see description
#' @param output_mode see description
#' @param crosses see description
#' @param crossing_dim see description
#' @param hashing_dim see description
#' @param num_discretization_bins Integer
#'
#' @export
#' @family preprocessing layers
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/FeatureSpace>
layer_feature_space <-
function (object, features, output_mode = "concat", crosses = NULL,
    crossing_dim = 32L, hashing_dim = 32L, num_discretization_bins = 32L,
    name = NULL, feature_names = NULL)
{
    args <- capture_args2(list(crossing_dim = as_integer, hashing_dim = as_integer,
        num_discretization_bins = as_integer, feature_names = as_integer),
        ignore = "object")
    create_layer(keras$utils$FeatureSpace, object, args)
}


# keras.utils.get_source_inputs
# keras.src.ops.operation_utils.get_source_inputs
r"-(Returns the list of input tensors necessary to compute `tensor`.

    Output will always be a list of tensors
    (potentially with 1 element).

    Args:
        tensor: The tensor to start from.

    Returns:
        List of input tensors.
    )-"
#' Returns the list of input tensors necessary to compute `tensor`.
#'
#' @description
#' Output will always be a list of tensors
#' (potentially with 1 element).
#'
#' # Returns
#'     List of input tensors.
#'
#' @param tensor The tensor to start from.
#'
#' @export
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_source_inputs>
get_source_inputs <-
function (tensor)
{
    args <- capture_args2(NULL)
    do.call(keras$utils$get_source_inputs, args)
}


# keras.utils.get_custom_objects
# keras.src.saving.object_registration.get_custom_objects
r"-(Retrieves a live reference to the global dictionary of custom objects.

    Custom objects set using using `custom_object_scope()` are not added to the
    global dictionary of custom objects, and will not appear in the returned
    dictionary.

    Example:

    ```python
    get_custom_objects().clear()
    get_custom_objects()['MyObject'] = MyObject
    ```

    Returns:
        Global dictionary mapping registered class names to classes.
    )-"
#' Retrieves a live reference to the global dictionary of custom objects.
#'
#' @description
#' Custom objects set using using `custom_object_scope()` are not added to the
#' global dictionary of custom objects, and will not appear in the returned
#' dictionary.
#'
#' # Examples
#' ```python
#' get_custom_objects().clear()
#' get_custom_objects()['MyObject'] = MyObject
#' ```
#'
#' # Returns
#'     Global dictionary mapping registered class names to classes.
#'
#' @export
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_custom_objects>
get_custom_objects <-
function ()
{
    args <- capture_args2(NULL)
    do.call(keras$utils$get_custom_objects, args)
}


# keras.utils.get_registered_name
# keras.src.saving.object_registration.get_registered_name
r"-(Returns the name registered to an object within the Keras framework.

    This function is part of the Keras serialization and deserialization
    framework. It maps objects to the string names associated with those objects
    for serialization/deserialization.

    Args:
        obj: The object to look up.

    Returns:
        The name associated with the object, or the default Python name if the
            object is not registered.
    )-"
#' Returns the name registered to an object within the Keras framework.
#'
#' @description
#' This function is part of the Keras serialization and deserialization
#' framework. It maps objects to the string names associated with those objects
#' for serialization/deserialization.
#'
#' # Returns
#' The name associated with the object, or the default Python name if the
#' object is not registered.
#'
#' @param obj The object to look up.
#'
#' @export
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_registered_name>
get_registered_name <-
function (obj)
{
    args <- capture_args2(NULL)
    do.call(keras$utils$get_registered_name, args)
}


# keras.utils.get_registered_object
# keras.src.saving.object_registration.get_registered_object
r"-(Returns the class associated with `name` if it is registered with Keras.

    This function is part of the Keras serialization and deserialization
    framework. It maps strings to the objects associated with them for
    serialization/deserialization.

    Example:

    ```python
    def from_config(cls, config, custom_objects=None):
        if 'my_custom_object_name' in config:
            config['hidden_cls'] = tf.keras.saving.get_registered_object(
                config['my_custom_object_name'], custom_objects=custom_objects)
    ```

    Args:
        name: The name to look up.
        custom_objects: A dictionary of custom objects to look the name up in.
            Generally, custom_objects is provided by the user.
        module_objects: A dictionary of custom objects to look the name up in.
            Generally, module_objects is provided by midlevel library
            implementers.

    Returns:
        An instantiable class associated with `name`, or `None` if no such class
            exists.
    )-"
#' Returns the class associated with `name` if it is registered with Keras.
#'
#' @description
#' This function is part of the Keras serialization and deserialization
#' framework. It maps strings to the objects associated with them for
#' serialization/deserialization.
#'
#' # Examples
#' ```python
#' def from_config(cls, config, custom_objects=None):
#'     if 'my_custom_object_name' in config:
#'         config['hidden_cls'] = tf.keras.saving.get_registered_object(
#'             config['my_custom_object_name'], custom_objects=custom_objects)
#' ```
#'
#' # Returns
#' An instantiable class associated with `name`, or `None` if no such class
#' exists.
#'
#' @param name The name to look up.
#' @param custom_objects A dictionary of custom objects to look the name up in.
#'     Generally, custom_objects is provided by the user.
#' @param module_objects A dictionary of custom objects to look the name up in.
#'     Generally, module_objects is provided by midlevel library
#'     implementers.
#'
#' @export
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_registered_object>
get_registered_object <-
function (name, custom_objects = NULL, module_objects = NULL)
{
    args <- capture_args2(NULL)
    do.call(keras$utils$get_registered_object, args)
}


# keras.utils.pack_x_y_sample_weight
# keras.src.trainers.data_adapters.data_adapter_utils.pack_x_y_sample_weight
r"-(Packs user-provided data into a tuple.

    This is a convenience utility for packing data into the tuple formats
    that `Model.fit()` uses.

    Standalone usage:

    >>> x = ops.ones((10, 1))
    >>> data = pack_x_y_sample_weight(x)
    >>> isinstance(data, ops.Tensor)
    True
    >>> y = ops.ones((10, 1))
    >>> data = pack_x_y_sample_weight(x, y)
    >>> isinstance(data, tuple)
    True
    >>> x, y = data

    Args:
        x: Features to pass to `Model`.
        y: Ground-truth targets to pass to `Model`.
        sample_weight: Sample weight for each element.

    Returns:
        Tuple in the format used in `Model.fit()`.
    )-"
#' Packs user-provided data into a tuple.
#'
#' @description
#' This is a convenience utility for packing data into the tuple formats
#' that `Model.fit()` uses.
#'
#' # Usage
#' Standalone usage:
#'
#' ```python
#' x = ops.ones((10, 1))
#' data = pack_x_y_sample_weight(x)
#' isinstance(data, ops.Tensor)
#' # True
#' y = ops.ones((10, 1))
#' data = pack_x_y_sample_weight(x, y)
#' isinstance(data, tuple)
#' # True
#' x, y = data
#' ```
#'
#' # Returns
#'     Tuple in the format used in `Model.fit()`.
#'
#' @param x Features to pass to `Model`.
#' @param y Ground-truth targets to pass to `Model`.
#' @param sample_weight Sample weight for each element.
#'
#' @export
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/pack_x_y_sample_weight>
pack_x_y_sample_weight <-
function (x, y = NULL, sample_weight = NULL)
{
    args <- capture_args2(NULL)
    do.call(keras$utils$pack_x_y_sample_weight, args)
}


# keras.utils.unpack_x_y_sample_weight
# keras.src.trainers.data_adapters.data_adapter_utils.unpack_x_y_sample_weight
r"-(Unpacks user-provided data tuple.

    This is a convenience utility to be used when overriding
    `Model.train_step`, `Model.test_step`, or `Model.predict_step`.
    This utility makes it easy to support data of the form `(x,)`,
    `(x, y)`, or `(x, y, sample_weight)`.

    Standalone usage:

    >>> features_batch = ops.ones((10, 5))
    >>> labels_batch = ops.zeros((10, 5))
    >>> data = (features_batch, labels_batch)
    >>> # `y` and `sample_weight` will default to `None` if not provided.
    >>> x, y, sample_weight = unpack_x_y_sample_weight(data)
    >>> sample_weight is None
    True

    Args:
        data: A tuple of the form `(x,)`, `(x, y)`, or `(x, y, sample_weight)`.

    Returns:
        The unpacked tuple, with `None`s for `y` and `sample_weight` if they are
        not provided.
    )-"
#' Unpacks user-provided data tuple.
#'
#' @description
#' This is a convenience utility to be used when overriding
#' `Model.train_step`, `Model.test_step`, or `Model.predict_step`.
#' This utility makes it easy to support data of the form `(x,)`,
#' `(x, y)`, or `(x, y, sample_weight)`.
#'
#' # Usage
#' Standalone usage:
#'
#' ```python
#' features_batch = ops.ones((10, 5))
#' labels_batch = ops.zeros((10, 5))
#' data = (features_batch, labels_batch)
#' # `y` and `sample_weight` will default to `None` if not provided.
#' x, y, sample_weight = unpack_x_y_sample_weight(data)
#' sample_weight is None
#' # True
#' ```
#'
#' # Returns
#' The unpacked tuple, with `None`s for `y` and `sample_weight` if they are
#' not provided.
#'
#' @param data A tuple of the form `(x,)`, `(x, y)`, or `(x, y, sample_weight)`.
#'
#' @export
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/unpack_x_y_sample_weight>
unpack_x_y_sample_weight <-
function (data)
{
    args <- capture_args2(NULL)
    do.call(keras$utils$unpack_x_y_sample_weight, args)
}


# keras.utils.audio_dataset_from_directory
# keras.src.utils.audio_dataset_utils.audio_dataset_from_directory
r"-(Generates a `tf.data.Dataset` from audio files in a directory.

    If your directory structure is:

    ```
    main_directory/
    ...class_a/
    ......a_audio_1.wav
    ......a_audio_2.wav
    ...class_b/
    ......b_audio_1.wav
    ......b_audio_2.wav
    ```

    Then calling `audio_dataset_from_directory(main_directory,
    labels='inferred')`
    will return a `tf.data.Dataset` that yields batches of audio files from
    the subdirectories `class_a` and `class_b`, together with labels
    0 and 1 (0 corresponding to `class_a` and 1 corresponding to `class_b`).

    Only `.wav` files are supported at this time.

    Args:
        directory: Directory where the data is located.
            If `labels` is `"inferred"`, it should contain subdirectories,
            each containing audio files for a class. Otherwise, the directory
            structure is ignored.
        labels: Either "inferred" (labels are generated from the directory
            structure), `None` (no labels), or a list/tuple of integer labels
            of the same size as the number of audio files found in
            the directory. Labels should be sorted according to the
            alphanumeric order of the audio file paths
            (obtained via `os.walk(directory)` in Python).
        label_mode: String describing the encoding of `labels`. Options are:
            - `"int"`: means that the labels are encoded as integers (e.g. for
              `sparse_categorical_crossentropy` loss).
            - `"categorical"` means that the labels are encoded as a categorical
              vector (e.g. for `categorical_crossentropy` loss)
            - `"binary"` means that the labels (there can be only 2)
              are encoded as `float32` scalars with values 0
              or 1 (e.g. for `binary_crossentropy`).
            - `None` (no labels).
        class_names: Only valid if "labels" is `"inferred"`.
            This is the explicit list of class names
            (must match names of subdirectories). Used to control the order
            of the classes (otherwise alphanumerical order is used).
        batch_size: Size of the batches of data. Default: 32. If `None`,
            the data will not be batched
            (the dataset will yield individual samples).
        sampling_rate: Audio sampling rate (in samples per second).
        output_sequence_length: Maximum length of an audio sequence. Audio files
            longer than this will be truncated to `output_sequence_length`.
            If set to `None`, then all sequences in the same batch will
            be padded to the
            length of the longest sequence in the batch.
        ragged: Whether to return a Ragged dataset (where each sequence has its
            own length). Defaults to `False`.
        shuffle: Whether to shuffle the data. Defaults to `True`.
            If set to `False`, sorts the data in alphanumeric order.
        seed: Optional random seed for shuffling and transformations.
        validation_split: Optional float between 0 and 1, fraction of data to
            reserve for validation.
        subset: Subset of the data to return. One of `"training"`,
            `"validation"` or `"both"`. Only used if `validation_split` is set.
        follow_links: Whether to visits subdirectories pointed to by symlinks.
            Defaults to `False`.

    Returns:

    A `tf.data.Dataset` object.

    - If `label_mode` is `None`, it yields `string` tensors of shape
      `(batch_size,)`, containing the contents of a batch of audio files.
    - Otherwise, it yields a tuple `(audio, labels)`, where `audio`
      has shape `(batch_size, sequence_length, num_channels)` and `labels`
      follows the format described
      below.

    Rules regarding labels format:

    - if `label_mode` is `int`, the labels are an `int32` tensor of shape
      `(batch_size,)`.
    - if `label_mode` is `binary`, the labels are a `float32` tensor of
      1s and 0s of shape `(batch_size, 1)`.
    - if `label_mode` is `categorical`, the labels are a `float32` tensor
      of shape `(batch_size, num_classes)`, representing a one-hot
      encoding of the class index.
    )-"
#' Generates a `tf.data.Dataset` from audio files in a directory.
#'
#' @description
#' If your directory structure is:
#'
#' ```
#' main_directory/
#' ...class_a/
#' ......a_audio_1.wav
#' ......a_audio_2.wav
#' ...class_b/
#' ......b_audio_1.wav
#' ......b_audio_2.wav
#' ```
#'
#' Then calling `audio_dataset_from_directory(main_directory,
#' labels='inferred')`
#' will return a `tf.data.Dataset` that yields batches of audio files from
#' the subdirectories `class_a` and `class_b`, together with labels
#' 0 and 1 (0 corresponding to `class_a` and 1 corresponding to `class_b`).
#'
#' Only `.wav` files are supported at this time.
#'
#' # Returns
#' A `tf.data.Dataset` object.
#'
#' - If `label_mode` is `None`, it yields `string` tensors of shape
#'   `(batch_size,)`, containing the contents of a batch of audio files.
#' - Otherwise, it yields a tuple `(audio, labels)`, where `audio`
#'   has shape `(batch_size, sequence_length, num_channels)` and `labels`
#'   follows the format described
#'   below.
#'
#' Rules regarding labels format:
#'
#' - if `label_mode` is `int`, the labels are an `int32` tensor of shape
#'   `(batch_size,)`.
#' - if `label_mode` is `binary`, the labels are a `float32` tensor of
#'   1s and 0s of shape `(batch_size, 1)`.
#' - if `label_mode` is `categorical`, the labels are a `float32` tensor
#'   of shape `(batch_size, num_classes)`, representing a one-hot
#'   encoding of the class index.
#'
#' @param directory Directory where the data is located.
#'     If `labels` is `"inferred"`, it should contain subdirectories,
#'     each containing audio files for a class. Otherwise, the directory
#'     structure is ignored.
#' @param labels Either "inferred" (labels are generated from the directory
#'     structure), `None` (no labels), or a list/tuple of integer labels
#'     of the same size as the number of audio files found in
#'     the directory. Labels should be sorted according to the
#'     alphanumeric order of the audio file paths
#'     (obtained via `os.walk(directory)` in Python).
#' @param label_mode String describing the encoding of `labels`. Options are:
#'     - `"int"`: means that the labels are encoded as integers (e.g. for
#'       `sparse_categorical_crossentropy` loss).
#'     - `"categorical"` means that the labels are encoded as a categorical
#'       vector (e.g. for `categorical_crossentropy` loss)
#'     - `"binary"` means that the labels (there can be only 2)
#'       are encoded as `float32` scalars with values 0
#'       or 1 (e.g. for `binary_crossentropy`).
#'     - `None` (no labels).
#' @param class_names Only valid if "labels" is `"inferred"`.
#'     This is the explicit list of class names
#'     (must match names of subdirectories). Used to control the order
#'     of the classes (otherwise alphanumerical order is used).
#' @param batch_size Size of the batches of data. Default: 32. If `None`,
#'     the data will not be batched
#'     (the dataset will yield individual samples).
#' @param sampling_rate Audio sampling rate (in samples per second).
#' @param output_sequence_length Maximum length of an audio sequence. Audio files
#'     longer than this will be truncated to `output_sequence_length`.
#'     If set to `None`, then all sequences in the same batch will
#'     be padded to the
#'     length of the longest sequence in the batch.
#' @param ragged Whether to return a Ragged dataset (where each sequence has its
#'     own length). Defaults to `False`.
#' @param shuffle Whether to shuffle the data. Defaults to `True`.
#'     If set to `False`, sorts the data in alphanumeric order.
#' @param seed Optional random seed for shuffling and transformations.
#' @param validation_split Optional float between 0 and 1, fraction of data to
#'     reserve for validation.
#' @param subset Subset of the data to return. One of `"training"`,
#'     `"validation"` or `"both"`. Only used if `validation_split` is set.
#' @param follow_links Whether to visits subdirectories pointed to by symlinks.
#'     Defaults to `False`.
#'
#' @export
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/audio_dataset_from_directory>
audio_dataset_from_directory <-
function (directory, labels = "inferred", label_mode = "int",
    class_names = NULL, batch_size = 32L, sampling_rate = NULL,
    output_sequence_length = NULL, ragged = FALSE, shuffle = TRUE,
    seed = NULL, validation_split = NULL, subset = NULL, follow_links = FALSE)
{
    args <- capture_args2(list(labels = as_integer, label_mode = as_integer,
        batch_size = as_integer, seed = as_integer))
    do.call(keras$utils$audio_dataset_from_directory, args)
}


# keras.utils.split_dataset
# keras.src.utils.dataset_utils.split_dataset
r"-(Splits a dataset into a left half and a right half (e.g. train / test).

    Args:
        dataset:
            A `tf.data.Dataset`, a `torch.utils.data.Dataset` object,
            or a list/tuple of arrays with the same length.
        left_size: If float (in the range `[0, 1]`), it signifies
            the fraction of the data to pack in the left dataset. If integer, it
            signifies the number of samples to pack in the left dataset. If
            `None`, defaults to the complement to `right_size`.
            Defaults to `None`.
        right_size: If float (in the range `[0, 1]`), it signifies
            the fraction of the data to pack in the right dataset.
            If integer, it signifies the number of samples to pack
            in the right dataset.
            If `None`, defaults to the complement to `left_size`.
            Defaults to `None`.
        shuffle: Boolean, whether to shuffle the data before splitting it.
        seed: A random seed for shuffling.

    Returns:
        A tuple of two `tf.data.Dataset` objects:
        the left and right splits.

    Example:

    >>> data = np.random.random(size=(1000, 4))
    >>> left_ds, right_ds = keras.utils.split_dataset(data, left_size=0.8)
    >>> int(left_ds.cardinality())
    800
    >>> int(right_ds.cardinality())
    200
    )-"
#' Splits a dataset into a left half and a right half (e.g. train / test).
#'
#' @description
#'
#' # Returns
#' A tuple of two `tf.data.Dataset` objects:
#' the left and right splits.
#'
#' # Examples
#' ```python
#' data = np.random.random(size=(1000, 4))
#' left_ds, right_ds = keras.utils.split_dataset(data, left_size=0.8)
#' int(left_ds.cardinality())
#' # 800
#' int(right_ds.cardinality())
#' # 200
#' ```
#'
#' @param dataset
#'     A `tf.data.Dataset`, a `torch.utils.data.Dataset` object,
#'     or a list/tuple of arrays with the same length.
#' @param left_size If float (in the range `[0, 1]`), it signifies
#'     the fraction of the data to pack in the left dataset. If integer, it
#'     signifies the number of samples to pack in the left dataset. If
#'     `None`, defaults to the complement to `right_size`.
#'     Defaults to `None`.
#' @param right_size If float (in the range `[0, 1]`), it signifies
#'     the fraction of the data to pack in the right dataset.
#'     If integer, it signifies the number of samples to pack
#'     in the right dataset.
#'     If `None`, defaults to the complement to `left_size`.
#'     Defaults to `None`.
#' @param shuffle Boolean, whether to shuffle the data before splitting it.
#' @param seed A random seed for shuffling.
#'
#' @export
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/split_dataset>
split_dataset <-
function (dataset, left_size = NULL, right_size = NULL, shuffle = FALSE,
    seed = NULL)
{
    args <- capture_args2(list(left_size = as_integer, right_size = as_integer,
        seed = as_integer))
    do.call(keras$utils$split_dataset, args)
}


# keras.utils.get_file
# keras.src.utils.file_utils.get_file
r"-(Downloads a file from a URL if it not already in the cache.

    By default the file at the url `origin` is downloaded to the
    cache_dir `~/.keras`, placed in the cache_subdir `datasets`,
    and given the filename `fname`. The final location of a file
    `example.txt` would therefore be `~/.keras/datasets/example.txt`.
    Files in `.tar`, `.tar.gz`, `.tar.bz`, and `.zip` formats can
    also be extracted.

    Passing a hash will verify the file after download. The command line
    programs `shasum` and `sha256sum` can compute the hash.

    Example:

    ```python
    path_to_downloaded_file = get_file(
        origin="https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz",
        extract=True,
    )
    ```

    Args:
        fname: Name of the file. If an absolute path, e.g. `"/path/to/file.txt"`
            is specified, the file will be saved at that location.
            If `None`, the name of the file at `origin` will be used.
        origin: Original URL of the file.
        untar: Deprecated in favor of `extract` argument.
            boolean, whether the file should be decompressed
        md5_hash: Deprecated in favor of `file_hash` argument.
            md5 hash of the file for verification
        file_hash: The expected hash string of the file after download.
            The sha256 and md5 hash algorithms are both supported.
        cache_subdir: Subdirectory under the Keras cache dir where the file is
            saved. If an absolute path, e.g. `"/path/to/folder"` is
            specified, the file will be saved at that location.
        hash_algorithm: Select the hash algorithm to verify the file.
            options are `"md5'`, `"sha256'`, and `"auto'`.
            The default 'auto' detects the hash algorithm in use.
        extract: True tries extracting the file as an Archive, like tar or zip.
        archive_format: Archive format to try for extracting the file.
            Options are `"auto'`, `"tar'`, `"zip'`, and `None`.
            `"tar"` includes tar, tar.gz, and tar.bz files.
            The default `"auto"` corresponds to `["tar", "zip"]`.
            None or an empty list will return no matches found.
        cache_dir: Location to store cached files, when None it
            defaults ether `$KERAS_HOME` if the `KERAS_HOME` environment
            variable is set or `~/.keras/`.

    Returns:
        Path to the downloaded file.

    ** Warning on malicious downloads **

    Downloading something from the Internet carries a risk.
    NEVER download a file/archive if you do not trust the source.
    We recommend that you specify the `file_hash` argument
    (if the hash of the source file is known) to make sure that the file you
    are getting is the one you expect.
    )-"
#' Downloads a file from a URL if it not already in the cache.
#'
#' @description
#' By default the file at the url `origin` is downloaded to the
#' cache_dir `~/.keras`, placed in the cache_subdir `datasets`,
#' and given the filename `fname`. The final location of a file
#' `example.txt` would therefore be `~/.keras/datasets/example.txt`.
#' Files in `.tar`, `.tar.gz`, `.tar.bz`, and `.zip` formats can
#' also be extracted.
#'
#' Passing a hash will verify the file after download. The command line
#' programs `shasum` and `sha256sum` can compute the hash.
#'
#' # Examples
#' ```python
#' path_to_downloaded_file = get_file(
#'     origin="https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz",
#'     extract=True,
#' )
#' ```
#'
#' # Returns
#' Path to the downloaded file.
#'
#' ** Warning on malicious downloads **
#'
#' Downloading something from the Internet carries a risk.
#' NEVER download a file/archive if you do not trust the source.
#' We recommend that you specify the `file_hash` argument
#' (if the hash of the source file is known) to make sure that the file you
#' are getting is the one you expect.
#'
#' @param fname Name of the file. If an absolute path, e.g. `"/path/to/file.txt"`
#'     is specified, the file will be saved at that location.
#'     If `None`, the name of the file at `origin` will be used.
#' @param origin Original URL of the file.
#' @param untar Deprecated in favor of `extract` argument.
#'     boolean, whether the file should be decompressed
#' @param md5_hash Deprecated in favor of `file_hash` argument.
#'     md5 hash of the file for verification
#' @param file_hash The expected hash string of the file after download.
#'     The sha256 and md5 hash algorithms are both supported.
#' @param cache_subdir Subdirectory under the Keras cache dir where the file is
#'     saved. If an absolute path, e.g. `"/path/to/folder"` is
#'     specified, the file will be saved at that location.
#' @param hash_algorithm Select the hash algorithm to verify the file.
#'     options are `"md5'`, `"sha256'`, and `"auto'`.
#'     The default 'auto' detects the hash algorithm in use.
#' @param extract True tries extracting the file as an Archive, like tar or zip.
#' @param archive_format Archive format to try for extracting the file.
#'     Options are `"auto'`, `"tar'`, `"zip'`, and `None`.
#'     `"tar"` includes tar, tar.gz, and tar.bz files.
#'     The default `"auto"` corresponds to `["tar", "zip"]`.
#'     None or an empty list will return no matches found.
#' @param cache_dir Location to store cached files, when None it
#'     defaults ether `$KERAS_HOME` if the `KERAS_HOME` environment
#'     variable is set or `~/.keras/`.
#'
#' @export
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file>
get_file <-
function (fname = NULL, origin = NULL, untar = FALSE, md5_hash = NULL,
    file_hash = NULL, cache_subdir = "datasets", hash_algorithm = "auto",
    extract = FALSE, archive_format = "auto", cache_dir = NULL)
{
    args <- capture_args2(NULL)
    do.call(keras$utils$get_file, args)
}


# keras.utils.image_dataset_from_directory
# keras.src.utils.image_dataset_utils.image_dataset_from_directory
r"-(Generates a `tf.data.Dataset` from image files in a directory.

    If your directory structure is:

    ```
    main_directory/
    ...class_a/
    ......a_image_1.jpg
    ......a_image_2.jpg
    ...class_b/
    ......b_image_1.jpg
    ......b_image_2.jpg
    ```

    Then calling `image_dataset_from_directory(main_directory,
    labels='inferred')` will return a `tf.data.Dataset` that yields batches of
    images from the subdirectories `class_a` and `class_b`, together with labels
    0 and 1 (0 corresponding to `class_a` and 1 corresponding to `class_b`).

    Supported image formats: `.jpeg`, `.jpg`, `.png`, `.bmp`, `.gif`.
    Animated gifs are truncated to the first frame.

    Args:
        directory: Directory where the data is located.
            If `labels` is `"inferred"`, it should contain
            subdirectories, each containing images for a class.
            Otherwise, the directory structure is ignored.
        labels: Either `"inferred"`
            (labels are generated from the directory structure),
            `None` (no labels),
            or a list/tuple of integer labels of the same size as the number of
            image files found in the directory. Labels should be sorted
            according to the alphanumeric order of the image file paths
            (obtained via `os.walk(directory)` in Python).
        label_mode: String describing the encoding of `labels`. Options are:
            - `"int"`: means that the labels are encoded as integers
                (e.g. for `sparse_categorical_crossentropy` loss).
            - `"categorical"` means that the labels are
                encoded as a categorical vector
                (e.g. for `categorical_crossentropy` loss).
            - `"binary"` means that the labels (there can be only 2)
                are encoded as `float32` scalars with values 0 or 1
                (e.g. for `binary_crossentropy`).
            - `None` (no labels).
        class_names: Only valid if `labels` is `"inferred"`.
            This is the explicit list of class names
            (must match names of subdirectories). Used to control the order
            of the classes (otherwise alphanumerical order is used).
        color_mode: One of `"grayscale"`, `"rgb"`, `"rgba"`.
            Defaults to `"rgb"`. Whether the images will be converted to
            have 1, 3, or 4 channels.
        batch_size: Size of the batches of data. Defaults to 32.
            If `None`, the data will not be batched
            (the dataset will yield individual samples).
        image_size: Size to resize images to after they are read from disk,
            specified as `(height, width)`. Defaults to `(256, 256)`.
            Since the pipeline processes batches of images that must all have
            the same size, this must be provided.
        shuffle: Whether to shuffle the data. Defaults to `True`.
            If set to `False`, sorts the data in alphanumeric order.
        seed: Optional random seed for shuffling and transformations.
        validation_split: Optional float between 0 and 1,
            fraction of data to reserve for validation.
        subset: Subset of the data to return.
            One of `"training"`, `"validation"`, or `"both"`.
            Only used if `validation_split` is set.
            When `subset="both"`, the utility returns a tuple of two datasets
            (the training and validation datasets respectively).
        interpolation: String, the interpolation method used when
            resizing images. Defaults to `"bilinear"`.
            Supports `"bilinear"`, `"nearest"`, `"bicubic"`, `"area"`,
            `"lanczos3"`, `"lanczos5"`, `"gaussian"`, `"mitchellcubic"`.
        follow_links: Whether to visit subdirectories pointed to by symlinks.
            Defaults to `False`.
        crop_to_aspect_ratio: If `True`, resize the images without aspect
            ratio distortion. When the original aspect ratio differs from the
            target aspect ratio, the output image will be cropped so as to
            return the largest possible window in the image
            (of size `image_size`) that matches the target aspect ratio. By
            default (`crop_to_aspect_ratio=False`), aspect ratio may not be
            preserved.
        data_format: If None uses keras.config.image_data_format()
            otherwise either 'channel_last' or 'channel_first'.

    Returns:

    A `tf.data.Dataset` object.

    - If `label_mode` is `None`, it yields `float32` tensors of shape
        `(batch_size, image_size[0], image_size[1], num_channels)`,
        encoding images (see below for rules regarding `num_channels`).
    - Otherwise, it yields a tuple `(images, labels)`, where `images` has
        shape `(batch_size, image_size[0], image_size[1], num_channels)`,
        and `labels` follows the format described below.

    Rules regarding labels format:

    - if `label_mode` is `"int"`, the labels are an `int32` tensor of shape
        `(batch_size,)`.
    - if `label_mode` is `"binary"`, the labels are a `float32` tensor of
        1s and 0s of shape `(batch_size, 1)`.
    - if `label_mode` is `"categorical"`, the labels are a `float32` tensor
        of shape `(batch_size, num_classes)`, representing a one-hot
        encoding of the class index.

    Rules regarding number of channels in the yielded images:

    - if `color_mode` is `"grayscale"`,
        there's 1 channel in the image tensors.
    - if `color_mode` is `"rgb"`,
        there are 3 channels in the image tensors.
    - if `color_mode` is `"rgba"`,
        there are 4 channels in the image tensors.
    )-"
#' Generates a `tf.data.Dataset` from image files in a directory.
#'
#' @description
#' If your directory structure is:
#'
#' ```
#' main_directory/
#' ...class_a/
#' ......a_image_1.jpg
#' ......a_image_2.jpg
#' ...class_b/
#' ......b_image_1.jpg
#' ......b_image_2.jpg
#' ```
#'
#' Then calling `image_dataset_from_directory(main_directory,
#' labels='inferred')` will return a `tf.data.Dataset` that yields batches of
#' images from the subdirectories `class_a` and `class_b`, together with labels
#' 0 and 1 (0 corresponding to `class_a` and 1 corresponding to `class_b`).
#'
#' Supported image formats: `.jpeg`, `.jpg`, `.png`, `.bmp`, `.gif`.
#' Animated gifs are truncated to the first frame.
#'
#' # Returns
#' A `tf.data.Dataset` object.
#'
#' - If `label_mode` is `None`, it yields `float32` tensors of shape
#'     `(batch_size, image_size[0], image_size[1], num_channels)`,
#'     encoding images (see below for rules regarding `num_channels`).
#' - Otherwise, it yields a tuple `(images, labels)`, where `images` has
#'     shape `(batch_size, image_size[0], image_size[1], num_channels)`,
#'     and `labels` follows the format described below.
#'
#' Rules regarding labels format:
#'
#' - if `label_mode` is `"int"`, the labels are an `int32` tensor of shape
#'     `(batch_size,)`.
#' - if `label_mode` is `"binary"`, the labels are a `float32` tensor of
#'     1s and 0s of shape `(batch_size, 1)`.
#' - if `label_mode` is `"categorical"`, the labels are a `float32` tensor
#'     of shape `(batch_size, num_classes)`, representing a one-hot
#'     encoding of the class index.
#'
#' Rules regarding number of channels in the yielded images:
#'
#' - if `color_mode` is `"grayscale"`,
#'     there's 1 channel in the image tensors.
#' - if `color_mode` is `"rgb"`,
#'     there are 3 channels in the image tensors.
#' - if `color_mode` is `"rgba"`,
#'     there are 4 channels in the image tensors.
#'
#' @param directory Directory where the data is located.
#'     If `labels` is `"inferred"`, it should contain
#'     subdirectories, each containing images for a class.
#'     Otherwise, the directory structure is ignored.
#' @param labels Either `"inferred"`
#'     (labels are generated from the directory structure),
#'     `None` (no labels),
#'     or a list/tuple of integer labels of the same size as the number of
#'     image files found in the directory. Labels should be sorted
#'     according to the alphanumeric order of the image file paths
#'     (obtained via `os.walk(directory)` in Python).
#' @param label_mode String describing the encoding of `labels`. Options are:
#'     - `"int"`: means that the labels are encoded as integers
#'         (e.g. for `sparse_categorical_crossentropy` loss).
#'     - `"categorical"` means that the labels are
#'         encoded as a categorical vector
#'         (e.g. for `categorical_crossentropy` loss).
#'     - `"binary"` means that the labels (there can be only 2)
#'         are encoded as `float32` scalars with values 0 or 1
#'         (e.g. for `binary_crossentropy`).
#'     - `None` (no labels).
#' @param class_names Only valid if `labels` is `"inferred"`.
#'     This is the explicit list of class names
#'     (must match names of subdirectories). Used to control the order
#'     of the classes (otherwise alphanumerical order is used).
#' @param color_mode One of `"grayscale"`, `"rgb"`, `"rgba"`.
#'     Defaults to `"rgb"`. Whether the images will be converted to
#'     have 1, 3, or 4 channels.
#' @param batch_size Size of the batches of data. Defaults to 32.
#'     If `None`, the data will not be batched
#'     (the dataset will yield individual samples).
#' @param image_size Size to resize images to after they are read from disk,
#'     specified as `(height, width)`. Defaults to `(256, 256)`.
#'     Since the pipeline processes batches of images that must all have
#'     the same size, this must be provided.
#' @param shuffle Whether to shuffle the data. Defaults to `True`.
#'     If set to `False`, sorts the data in alphanumeric order.
#' @param seed Optional random seed for shuffling and transformations.
#' @param validation_split Optional float between 0 and 1,
#'     fraction of data to reserve for validation.
#' @param subset Subset of the data to return.
#'     One of `"training"`, `"validation"`, or `"both"`.
#'     Only used if `validation_split` is set.
#'     When `subset="both"`, the utility returns a tuple of two datasets
#'     (the training and validation datasets respectively).
#' @param interpolation String, the interpolation method used when
#'     resizing images. Defaults to `"bilinear"`.
#'     Supports `"bilinear"`, `"nearest"`, `"bicubic"`, `"area"`,
#'     `"lanczos3"`, `"lanczos5"`, `"gaussian"`, `"mitchellcubic"`.
#' @param follow_links Whether to visit subdirectories pointed to by symlinks.
#'     Defaults to `False`.
#' @param crop_to_aspect_ratio If `True`, resize the images without aspect
#'     ratio distortion. When the original aspect ratio differs from the
#'     target aspect ratio, the output image will be cropped so as to
#'     return the largest possible window in the image
#'     (of size `image_size`) that matches the target aspect ratio. By
#'     default (`crop_to_aspect_ratio=False`), aspect ratio may not be
#'     preserved.
#' @param data_format If None uses keras.config.image_data_format()
#'     otherwise either 'channel_last' or 'channel_first'.
#'
#' @export
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory>
image_dataset_from_directory <-
function (directory, labels = "inferred", label_mode = "int",
    class_names = NULL, color_mode = "rgb", batch_size = 32L,
    image_size = list(256L, 256L), shuffle = TRUE, seed = NULL,
    validation_split = NULL, subset = NULL, interpolation = "bilinear",
    follow_links = FALSE, crop_to_aspect_ratio = FALSE, data_format = NULL)
{
    args <- capture_args2(list(labels = as_integer, label_mode = as_integer,
        batch_size = as_integer, seed = as_integer))
    do.call(keras$utils$image_dataset_from_directory, args)
}


# keras.utils.save_img
# keras.src.utils.image_utils.save_img
r"-(Saves an image stored as a NumPy array to a path or file object.

    Args:
        path: Path or file object.
        x: NumPy array.
        data_format: Image data format, either `"channels_first"` or
            `"channels_last"`.
        file_format: Optional file format override. If omitted, the format to
            use is determined from the filename extension. If a file object was
            used instead of a filename, this parameter should always be used.
        scale: Whether to rescale image values to be within `[0, 255]`.
        **kwargs: Additional keyword arguments passed to `PIL.Image.save()`.
    )-"
#' Saves an image stored as a NumPy array to a path or file object.
#'
#' @param path Path or file object.
#' @param x NumPy array.
#' @param data_format Image data format, either `"channels_first"` or
#'     `"channels_last"`.
#' @param file_format Optional file format override. If omitted, the format to
#'     use is determined from the filename extension. If a file object was
#'     used instead of a filename, this parameter should always be used.
#' @param scale Whether to rescale image values to be within `[0, 255]`.
#' @param ... Additional keyword arguments passed to `PIL.Image.save()`.
#'
#' @export
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/save_img>
image_array_save <-
function (x, path, data_format = NULL, file_format = NULL, scale = TRUE,
    ...)
{
    args <- capture_args2(NULL)
    do.call(keras$utils$save_img, args)
}


# keras.utils.array_to_img
# keras.src.utils.image_utils.array_to_img
r"-(Converts a 3D NumPy array to a PIL Image instance.

    Usage:

    ```python
    from PIL import Image
    img = np.random.random(size=(100, 100, 3))
    pil_img = keras.utils.array_to_img(img)
    ```

    Args:
        x: Input data, in any form that can be converted to a NumPy array.
        data_format: Image data format, can be either `"channels_first"` or
            `"channels_last"`. Defaults to `None`, in which case the global
            setting `keras.backend.image_data_format()` is used (unless you
            changed it, it defaults to `"channels_last"`).
        scale: Whether to rescale the image such that minimum and maximum values
            are 0 and 255 respectively. Defaults to `True`.
        dtype: Dtype to use. `None` means the global setting
            `keras.backend.floatx()` is used (unless you changed it, it
            defaults to `"float32"`). Defaults to `None`.

    Returns:
        A PIL Image instance.
    )-"
#' Converts a 3D NumPy array to a PIL Image instance.
#'
#' @description
#'
#' # Usage
#' ```python
#' from PIL import Image
#' img = np.random.random(size=(100, 100, 3))
#' pil_img = keras.utils.array_to_img(img)
#' ```
#'
#' # Returns
#'     A PIL Image instance.
#'
#' @param x Input data, in any form that can be converted to a NumPy array.
#' @param data_format Image data format, can be either `"channels_first"` or
#'     `"channels_last"`. Defaults to `None`, in which case the global
#'     setting `keras.backend.image_data_format()` is used (unless you
#'     changed it, it defaults to `"channels_last"`).
#' @param scale Whether to rescale the image such that minimum and maximum values
#'     are 0 and 255 respectively. Defaults to `True`.
#' @param dtype Dtype to use. `None` means the global setting
#'     `keras.backend.floatx()` is used (unless you changed it, it
#'     defaults to `"float32"`). Defaults to `None`.
#'
#' @export
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/array_to_img>
image_from_array <-
function (x, data_format = NULL, scale = TRUE, dtype = NULL)
{
    args <- capture_args2(NULL)
    do.call(keras$utils$array_to_img, args)
}


# keras.utils.load_img
# keras.src.utils.image_utils.load_img
r"-(Loads an image into PIL format.

    Usage:

    ```python
    image = keras.utils.load_img(image_path)
    input_arr = keras.utils.img_to_array(image)
    input_arr = np.array([input_arr])  # Convert single image to a batch.
    predictions = model.predict(input_arr)
    ```

    Args:
        path: Path to image file.
        color_mode: One of `"grayscale"`, `"rgb"`, `"rgba"`. Default: `"rgb"`.
            The desired image format.
        target_size: Either `None` (default to original size) or tuple of ints
            `(img_height, img_width)`.
        interpolation: Interpolation method used to resample the image if the
            target size is different from that of the loaded image. Supported
            methods are `"nearest"`, `"bilinear"`, and `"bicubic"`.
            If PIL version 1.1.3 or newer is installed, `"lanczos"`
            is also supported. If PIL version 3.4.0 or newer is installed,
            `"box"` and `"hamming"` are also
            supported. By default, `"nearest"` is used.
        keep_aspect_ratio: Boolean, whether to resize images to a target
            size without aspect ratio distortion. The image is cropped in
            the center with target aspect ratio before resizing.

    Returns:
        A PIL Image instance.
    )-"
#' Loads an image into PIL format.
#'
#' @description
#'
#' # Usage
#' ```python
#' image = keras.utils.load_img(image_path)
#' input_arr = keras.utils.img_to_array(image)
#' input_arr = np.array([input_arr])  # Convert single image to a batch.
#' predictions = model.predict(input_arr)
#' ```
#'
#' # Returns
#'     A PIL Image instance.
#'
#' @param path Path to image file.
#' @param color_mode One of `"grayscale"`, `"rgb"`, `"rgba"`. Default: `"rgb"`.
#'     The desired image format.
#' @param target_size Either `None` (default to original size) or tuple of ints
#'     `(img_height, img_width)`.
#' @param interpolation Interpolation method used to resample the image if the
#'     target size is different from that of the loaded image. Supported
#'     methods are `"nearest"`, `"bilinear"`, and `"bicubic"`.
#'     If PIL version 1.1.3 or newer is installed, `"lanczos"`
#'     is also supported. If PIL version 3.4.0 or newer is installed,
#'     `"box"` and `"hamming"` are also
#'     supported. By default, `"nearest"` is used.
#' @param keep_aspect_ratio Boolean, whether to resize images to a target
#'     size without aspect ratio distortion. The image is cropped in
#'     the center with target aspect ratio before resizing.
#'
#' @export
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/load_img>
image_load <-
function (path, color_mode = "rgb", target_size = NULL, interpolation = "nearest",
    keep_aspect_ratio = FALSE)
{
    args <- capture_args2(NULL)
    do.call(keras$utils$load_img, args)
}


# keras.utils.img_to_array
# keras.src.utils.image_utils.img_to_array
r"-(Converts a PIL Image instance to a NumPy array.

    Usage:

    ```python
    from PIL import Image
    img_data = np.random.random(size=(100, 100, 3))
    img = keras.utils.array_to_img(img_data)
    array = keras.utils.image.img_to_array(img)
    ```

    Args:
        img: Input PIL Image instance.
        data_format: Image data format, can be either `"channels_first"` or
            `"channels_last"`. Defaults to `None`, in which case the global
            setting `keras.backend.image_data_format()` is used (unless you
            changed it, it defaults to `"channels_last"`).
        dtype: Dtype to use. `None` means the global setting
            `keras.backend.floatx()` is used (unless you changed it, it
            defaults to `"float32"`).

    Returns:
        A 3D NumPy array.
    )-"
#' Converts a PIL Image instance to a NumPy array.
#'
#' @description
#'
#' # Usage
#' ```python
#' from PIL import Image
#' img_data = np.random.random(size=(100, 100, 3))
#' img = keras.utils.array_to_img(img_data)
#' array = keras.utils.image.img_to_array(img)
#' ```
#'
#' # Returns
#'     A 3D NumPy array.
#'
#' @param img Input PIL Image instance.
#' @param data_format Image data format, can be either `"channels_first"` or
#'     `"channels_last"`. Defaults to `None`, in which case the global
#'     setting `keras.backend.image_data_format()` is used (unless you
#'     changed it, it defaults to `"channels_last"`).
#' @param dtype Dtype to use. `None` means the global setting
#'     `keras.backend.floatx()` is used (unless you changed it, it
#'     defaults to `"float32"`).
#'
#' @export
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/img_to_array>
image_to_array <-
function (img, data_format = NULL, dtype = NULL)
{
    args <- capture_args2(NULL)
    do.call(keras$utils$img_to_array, args)
}


# keras.utils.model_to_dot
# keras.src.utils.model_visualization.model_to_dot
r"-(Convert a Keras model to dot format.

    Args:
        model: A Keras model instance.
        show_shapes: whether to display shape information.
        show_dtype: whether to display layer dtypes.
        show_layer_names: whether to display layer names.
        rankdir: `rankdir` argument passed to PyDot,
            a string specifying the format of the plot: `"TB"`
            creates a vertical plot; `"LR"` creates a horizontal plot.
        expand_nested: whether to expand nested Functional models
            into clusters.
        dpi: Image resolution in dots per inch.
        subgraph: whether to return a `pydot.Cluster` instance.
        show_layer_activations: Display layer activations (only for layers that
            have an `activation` property).
        show_trainable: whether to display if a layer is trainable.

    Returns:
        A `pydot.Dot` instance representing the Keras model or
        a `pydot.Cluster` instance representing nested model if
        `subgraph=True`.
    )-"
#' Convert a Keras model to dot format.
#'
#' @description
#'
#' # Returns
#' A `pydot.Dot` instance representing the Keras model or
#' a `pydot.Cluster` instance representing nested model if
#' `subgraph=True`.
#'
#' @param model A Keras model instance.
#' @param show_shapes whether to display shape information.
#' @param show_dtype whether to display layer dtypes.
#' @param show_layer_names whether to display layer names.
#' @param rankdir `rankdir` argument passed to PyDot,
#'     a string specifying the format of the plot: `"TB"`
#'     creates a vertical plot; `"LR"` creates a horizontal plot.
#' @param expand_nested whether to expand nested Functional models
#'     into clusters.
#' @param dpi Image resolution in dots per inch.
#' @param subgraph whether to return a `pydot.Cluster` instance.
#' @param show_layer_activations Display layer activations (only for layers that
#'     have an `activation` property).
#' @param show_trainable whether to display if a layer is trainable.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/model_to_dot>
model_to_dot <-
function (model, show_shapes = FALSE, show_dtype = FALSE, show_layer_names = TRUE,
    rankdir = "TB", expand_nested = FALSE, dpi = 200L, subgraph = FALSE,
    show_layer_activations = FALSE, show_trainable = FALSE, ...)
{
    args <- capture_args2(list(dpi = as_integer))
    do.call(keras$utils$model_to_dot, args)
}


# keras.utils.normalize
# keras.src.utils.numerical_utils.normalize
r"-(Normalizes an array.

    If the input is a NumPy array, a NumPy array will be returned.
    If it's a backend tensor, a backend tensor will be returned.

    Args:
        x: Array to normalize.
        axis: axis along which to normalize.
        order: Normalization order (e.g. `order=2` for L2 norm).

    Returns:
        A normalized copy of the array.
    )-"
#' Normalizes an array.
#'
#' @description
#' If the input is a NumPy array, a NumPy array will be returned.
#' If it's a backend tensor, a backend tensor will be returned.
#'
#' # Returns
#'     A normalized copy of the array.
#'
#' @param x Array to normalize.
#' @param axis axis along which to normalize.
#' @param order Normalization order (e.g. `order=2` for L2 norm).
#'
#' @export
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/normalize>
normalize <-
function (x, axis = -1L, order = 2L)
{
    args <- capture_args2(list(axis = as_axis, order = as_integer))
    do.call(keras$utils$normalize, args)
}


# keras.utils.to_categorical
# keras.src.utils.numerical_utils.to_categorical
r"-(Converts a class vector (integers) to binary class matrix.

    E.g. for use with `categorical_crossentropy`.

    Args:
        x: Array-like with class values to be converted into a matrix
            (integers from 0 to `num_classes - 1`).
        num_classes: Total number of classes. If `None`, this would be inferred
            as `max(x) + 1`. Defaults to `None`.

    Returns:
        A binary matrix representation of the input as a NumPy array. The class
        axis is placed last.

    Example:

    >>> a = keras.utils.to_categorical([0, 1, 2, 3], num_classes=4)
    >>> print(a)
    [[1. 0. 0. 0.]
     [0. 1. 0. 0.]
     [0. 0. 1. 0.]
     [0. 0. 0. 1.]]

    >>> b = np.array([.9, .04, .03, .03,
    ...               .3, .45, .15, .13,
    ...               .04, .01, .94, .05,
    ...               .12, .21, .5, .17],
    ...               shape=[4, 4])
    >>> loss = keras.backend.categorical_crossentropy(a, b)
    >>> print(np.around(loss, 5))
    [0.10536 0.82807 0.1011  1.77196]

    >>> loss = keras.backend.categorical_crossentropy(a, a)
    >>> print(np.around(loss, 5))
    [0. 0. 0. 0.]
    )-"
#' Converts a class vector (integers) to binary class matrix.
#'
#' @description
#' E.g. for use with `categorical_crossentropy`.
#'
#' # Returns
#' A binary matrix representation of the input as a NumPy array. The class
#' axis is placed last.
#'
#' # Examples
#' ```python
#' a = keras.utils.to_categorical([0, 1, 2, 3], num_classes=4)
#' print(a)
#' # [[1. 0. 0. 0.]
#' #  [0. 1. 0. 0.]
#' #  [0. 0. 1. 0.]
#' #  [0. 0. 0. 1.]]
#' ```
#'
#' ```python
#' b = np.array([.9, .04, .03, .03,
#'               .3, .45, .15, .13,
#'               .04, .01, .94, .05,
#'               .12, .21, .5, .17],
#'               shape=[4, 4])
#' loss = keras.backend.categorical_crossentropy(a, b)
#' print(np.around(loss, 5))
#' # [0.10536 0.82807 0.1011  1.77196]
#' ```
#'
#' ```python
#' loss = keras.backend.categorical_crossentropy(a, a)
#' print(np.around(loss, 5))
#' # [0. 0. 0. 0.]
#' ```
#'
#' @param x Array-like with class values to be converted into a matrix
#'     (integers from 0 to `num_classes - 1`).
#' @param num_classes Total number of classes. If `None`, this would be inferred
#'     as `max(x) + 1`. Defaults to `None`.
#'
#' @export
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical>
to_categorical <-
function (x, num_classes = NULL)
{
    args <- capture_args2(list(x = as_integer_array, num_classes = as_integer))
    do.call(keras$utils$to_categorical, args)
}


# keras.utils.set_random_seed
# keras.src.utils.rng_utils.set_random_seed
r"-(Sets all random seeds (Python, NumPy, and backend framework, e.g. TF).

    You can use this utility to make almost any Keras program fully
    deterministic. Some limitations apply in cases where network communications
    are involved (e.g. parameter server distribution), which creates additional
    sources of randomness, or when certain non-deterministic cuDNN ops are
    involved.

    Calling this utility is equivalent to the following:

    ```python
    import random
    import numpy as np
    from keras.utils.module_utils import tensorflow as tf
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
    ```

    Note that the TensorFlow seed is set even if you're not using TensorFlow
    as your backend framework, since many workflows leverage `tf.data`
    pipelines (which feature random shuffling). Likewise many workflows
    might leverage NumPy APIs.

    Arguments:
        seed: Integer, the random seed to use.
    )-"
#' Sets all random seeds (Python, NumPy, and backend framework, e.g. TF).
#'
#' @description
#' You can use this utility to make almost any Keras program fully
#' deterministic. Some limitations apply in cases where network communications
#' are involved (e.g. parameter server distribution), which creates additional
#' sources of randomness, or when certain non-deterministic cuDNN ops are
#' involved.
#'
#' Calling this utility is equivalent to the following:
#'
#' ```python
#' import random
#' import numpy as np
#' from keras.utils.module_utils import tensorflow as tf
#' random.seed(seed)
#' np.random.seed(seed)
#' tf.random.set_seed(seed)
#' ```
#'
#' Note that the TensorFlow seed is set even if you're not using TensorFlow
#' as your backend framework, since many workflows leverage `tf.data`
#' pipelines (which feature random shuffling). Likewise many workflows
#' might leverage NumPy APIs.
#'
#' @param seed Integer, the random seed to use.
#'
#' @export
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/set_random_seed>
set_random_seed <-
function (seed)
{
    args <- capture_args2(list(seed = as_integer))
    do.call(keras$utils$set_random_seed, args)
}


# keras.utils.pad_sequences
# keras.src.utils.sequence_utils.pad_sequences
r"-(Pads sequences to the same length.

    This function transforms a list (of length `num_samples`)
    of sequences (lists of integers)
    into a 2D NumPy array of shape `(num_samples, num_timesteps)`.
    `num_timesteps` is either the `maxlen` argument if provided,
    or the length of the longest sequence in the list.

    Sequences that are shorter than `num_timesteps`
    are padded with `value` until they are `num_timesteps` long.

    Sequences longer than `num_timesteps` are truncated
    so that they fit the desired length.

    The position where padding or truncation happens is determined by
    the arguments `padding` and `truncating`, respectively.
    Pre-padding or removing values from the beginning of the sequence is the
    default.

    >>> sequence = [[1], [2, 3], [4, 5, 6]]
    >>> keras.utils.pad_sequences(sequence)
    array([[0, 0, 1],
           [0, 2, 3],
           [4, 5, 6]], dtype=int32)

    >>> keras.utils.pad_sequences(sequence, value=-1)
    array([[-1, -1,  1],
           [-1,  2,  3],
           [ 4,  5,  6]], dtype=int32)

    >>> keras.utils.pad_sequences(sequence, padding='post')
    array([[1, 0, 0],
           [2, 3, 0],
           [4, 5, 6]], dtype=int32)

    >>> keras.utils.pad_sequences(sequence, maxlen=2)
    array([[0, 1],
           [2, 3],
           [5, 6]], dtype=int32)

    Args:
        sequences: List of sequences (each sequence is a list of integers).
        maxlen: Optional Int, maximum length of all sequences. If not provided,
            sequences will be padded to the length of the longest individual
            sequence.
        dtype: (Optional, defaults to `"int32"`). Type of the output sequences.
            To pad sequences with variable length strings, you can use `object`.
        padding: String, "pre" or "post" (optional, defaults to `"pre"`):
            pad either before or after each sequence.
        truncating: String, "pre" or "post" (optional, defaults to `"pre"`):
            remove values from sequences larger than
            `maxlen`, either at the beginning or at the end of the sequences.
        value: Float or String, padding value. (Optional, defaults to 0.)

    Returns:
        NumPy array with shape `(len(sequences), maxlen)`
    )-"
#' Pads sequences to the same length.
#'
#' @description
#' This function transforms a list (of length `num_samples`)
#' of sequences (lists of integers)
#' into a 2D NumPy array of shape `(num_samples, num_timesteps)`.
#' `num_timesteps` is either the `maxlen` argument if provided,
#' or the length of the longest sequence in the list.
#'
#' Sequences that are shorter than `num_timesteps`
#' are padded with `value` until they are `num_timesteps` long.
#'
#' Sequences longer than `num_timesteps` are truncated
#' so that they fit the desired length.
#'
#' The position where padding or truncation happens is determined by
#' the arguments `padding` and `truncating`, respectively.
#' Pre-padding or removing values from the beginning of the sequence is the
#' default.
#'
#' ```python
#' sequence = [[1], [2, 3], [4, 5, 6]]
#' keras.utils.pad_sequences(sequence)
#' # array([[0, 0, 1],
#' #        [0, 2, 3],
#' #        [4, 5, 6]], dtype=int32)
#' ```
#'
#' ```python
#' keras.utils.pad_sequences(sequence, value=-1)
#' # array([[-1, -1,  1],
#' #        [-1,  2,  3],
#' #        [ 4,  5,  6]], dtype=int32)
#' ```
#'
#' ```python
#' keras.utils.pad_sequences(sequence, padding='post')
#' # array([[1, 0, 0],
#' #        [2, 3, 0],
#' #        [4, 5, 6]], dtype=int32)
#' ```
#'
#' ```python
#' keras.utils.pad_sequences(sequence, maxlen=2)
#' # array([[0, 1],
#' #        [2, 3],
#' #        [5, 6]], dtype=int32)
#' ```
#'
#' # Returns
#'     NumPy array with shape `(len(sequences), maxlen)`
#'
#' @param sequences List of sequences (each sequence is a list of integers).
#' @param maxlen Optional Int, maximum length of all sequences. If not provided,
#'     sequences will be padded to the length of the longest individual
#'     sequence.
#' @param dtype (Optional, defaults to `"int32"`). Type of the output sequences.
#'     To pad sequences with variable length strings, you can use `object`.
#' @param padding String, "pre" or "post" (optional, defaults to `"pre"`):
#'     pad either before or after each sequence.
#' @param truncating String, "pre" or "post" (optional, defaults to `"pre"`):
#'     remove values from sequences larger than
#'     `maxlen`, either at the beginning or at the end of the sequences.
#' @param value Float or String, padding value. (Optional, defaults to 0.)
#'
#' @export
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences>
pad_sequences <-
function (sequences, maxlen = NULL, dtype = "int32", padding = "pre",
    truncating = "pre", value = 0)
{
    args <- capture_args2(list(maxlen = as_integer))
    do.call(keras$utils$pad_sequences, args)
}


# keras.utils.text_dataset_from_directory
# keras.src.utils.text_dataset_utils.text_dataset_from_directory
r"-(Generates a `tf.data.Dataset` from text files in a directory.

    If your directory structure is:

    ```
    main_directory/
    ...class_a/
    ......a_text_1.txt
    ......a_text_2.txt
    ...class_b/
    ......b_text_1.txt
    ......b_text_2.txt
    ```

    Then calling `text_dataset_from_directory(main_directory,
    labels='inferred')` will return a `tf.data.Dataset` that yields batches of
    texts from the subdirectories `class_a` and `class_b`, together with labels
    0 and 1 (0 corresponding to `class_a` and 1 corresponding to `class_b`).

    Only `.txt` files are supported at this time.

    Args:
        directory: Directory where the data is located.
            If `labels` is `"inferred"`, it should contain
            subdirectories, each containing text files for a class.
            Otherwise, the directory structure is ignored.
        labels: Either `"inferred"`
            (labels are generated from the directory structure),
            `None` (no labels),
            or a list/tuple of integer labels of the same size as the number of
            text files found in the directory. Labels should be sorted according
            to the alphanumeric order of the text file paths
            (obtained via `os.walk(directory)` in Python).
        label_mode: String describing the encoding of `labels`. Options are:
            - `"int"`: means that the labels are encoded as integers
                (e.g. for `sparse_categorical_crossentropy` loss).
            - `"categorical"` means that the labels are
                encoded as a categorical vector
                (e.g. for `categorical_crossentropy` loss).
            - `"binary"` means that the labels (there can be only 2)
                are encoded as `float32` scalars with values 0 or 1
                (e.g. for `binary_crossentropy`).
            - `None` (no labels).
        class_names: Only valid if `"labels"` is `"inferred"`.
            This is the explicit list of class names
            (must match names of subdirectories). Used to control the order
            of the classes (otherwise alphanumerical order is used).
        batch_size: Size of the batches of data. Defaults to 32.
            If `None`, the data will not be batched
            (the dataset will yield individual samples).
        max_length: Maximum size of a text string. Texts longer than this will
            be truncated to `max_length`.
        shuffle: Whether to shuffle the data. Defaults to `True`.
            If set to `False`, sorts the data in alphanumeric order.
        seed: Optional random seed for shuffling and transformations.
        validation_split: Optional float between 0 and 1,
            fraction of data to reserve for validation.
        subset: Subset of the data to return.
            One of `"training"`, `"validation"` or `"both"`.
            Only used if `validation_split` is set.
            When `subset="both"`, the utility returns a tuple of two datasets
            (the training and validation datasets respectively).
        follow_links: Whether to visits subdirectories pointed to by symlinks.
            Defaults to `False`.

    Returns:

    A `tf.data.Dataset` object.

    - If `label_mode` is `None`, it yields `string` tensors of shape
        `(batch_size,)`, containing the contents of a batch of text files.
    - Otherwise, it yields a tuple `(texts, labels)`, where `texts`
        has shape `(batch_size,)` and `labels` follows the format described
        below.

    Rules regarding labels format:

    - if `label_mode` is `int`, the labels are an `int32` tensor of shape
        `(batch_size,)`.
    - if `label_mode` is `binary`, the labels are a `float32` tensor of
        1s and 0s of shape `(batch_size, 1)`.
    - if `label_mode` is `categorical`, the labels are a `float32` tensor
        of shape `(batch_size, num_classes)`, representing a one-hot
        encoding of the class index.
    )-"
#' Generates a `tf.data.Dataset` from text files in a directory.
#'
#' @description
#' If your directory structure is:
#'
#' ```
#' main_directory/
#' ...class_a/
#' ......a_text_1.txt
#' ......a_text_2.txt
#' ...class_b/
#' ......b_text_1.txt
#' ......b_text_2.txt
#' ```
#'
#' Then calling `text_dataset_from_directory(main_directory,
#' labels='inferred')` will return a `tf.data.Dataset` that yields batches of
#' texts from the subdirectories `class_a` and `class_b`, together with labels
#' 0 and 1 (0 corresponding to `class_a` and 1 corresponding to `class_b`).
#'
#' Only `.txt` files are supported at this time.
#'
#' # Returns
#' A `tf.data.Dataset` object.
#'
#' - If `label_mode` is `None`, it yields `string` tensors of shape
#'     `(batch_size,)`, containing the contents of a batch of text files.
#' - Otherwise, it yields a tuple `(texts, labels)`, where `texts`
#'     has shape `(batch_size,)` and `labels` follows the format described
#'     below.
#'
#' Rules regarding labels format:
#'
#' - if `label_mode` is `int`, the labels are an `int32` tensor of shape
#'     `(batch_size,)`.
#' - if `label_mode` is `binary`, the labels are a `float32` tensor of
#'     1s and 0s of shape `(batch_size, 1)`.
#' - if `label_mode` is `categorical`, the labels are a `float32` tensor
#'     of shape `(batch_size, num_classes)`, representing a one-hot
#'     encoding of the class index.
#'
#' @param directory Directory where the data is located.
#'     If `labels` is `"inferred"`, it should contain
#'     subdirectories, each containing text files for a class.
#'     Otherwise, the directory structure is ignored.
#' @param labels Either `"inferred"`
#'     (labels are generated from the directory structure),
#'     `None` (no labels),
#'     or a list/tuple of integer labels of the same size as the number of
#'     text files found in the directory. Labels should be sorted according
#'     to the alphanumeric order of the text file paths
#'     (obtained via `os.walk(directory)` in Python).
#' @param label_mode String describing the encoding of `labels`. Options are:
#'     - `"int"`: means that the labels are encoded as integers
#'         (e.g. for `sparse_categorical_crossentropy` loss).
#'     - `"categorical"` means that the labels are
#'         encoded as a categorical vector
#'         (e.g. for `categorical_crossentropy` loss).
#'     - `"binary"` means that the labels (there can be only 2)
#'         are encoded as `float32` scalars with values 0 or 1
#'         (e.g. for `binary_crossentropy`).
#'     - `None` (no labels).
#' @param class_names Only valid if `"labels"` is `"inferred"`.
#'     This is the explicit list of class names
#'     (must match names of subdirectories). Used to control the order
#'     of the classes (otherwise alphanumerical order is used).
#' @param batch_size Size of the batches of data. Defaults to 32.
#'     If `None`, the data will not be batched
#'     (the dataset will yield individual samples).
#' @param max_length Maximum size of a text string. Texts longer than this will
#'     be truncated to `max_length`.
#' @param shuffle Whether to shuffle the data. Defaults to `True`.
#'     If set to `False`, sorts the data in alphanumeric order.
#' @param seed Optional random seed for shuffling and transformations.
#' @param validation_split Optional float between 0 and 1,
#'     fraction of data to reserve for validation.
#' @param subset Subset of the data to return.
#'     One of `"training"`, `"validation"` or `"both"`.
#'     Only used if `validation_split` is set.
#'     When `subset="both"`, the utility returns a tuple of two datasets
#'     (the training and validation datasets respectively).
#' @param follow_links Whether to visits subdirectories pointed to by symlinks.
#'     Defaults to `False`.
#'
#' @export
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/text_dataset_from_directory>
text_dataset_from_directory <-
function (directory, labels = "inferred", label_mode = "int",
    class_names = NULL, batch_size = 32L, max_length = NULL,
    shuffle = TRUE, seed = NULL, validation_split = NULL, subset = NULL,
    follow_links = FALSE)
{
    args <- capture_args2(list(labels = as_integer, label_mode = as_integer,
        batch_size = as_integer, seed = as_integer))
    do.call(keras$utils$text_dataset_from_directory, args)
}


# keras.utils.timeseries_dataset_from_array
# keras.src.utils.timeseries_dataset_utils.timeseries_dataset_from_array
r"-(Creates a dataset of sliding windows over a timeseries provided as array.

    This function takes in a sequence of data-points gathered at
    equal intervals, along with time series parameters such as
    length of the sequences/windows, spacing between two sequence/windows, etc.,
    to produce batches of timeseries inputs and targets.

    Args:
        data: Numpy array or eager tensor
            containing consecutive data points (timesteps).
            Axis 0 is expected to be the time dimension.
        targets: Targets corresponding to timesteps in `data`.
            `targets[i]` should be the target
            corresponding to the window that starts at index `i`
            (see example 2 below).
            Pass `None` if you don't have target data (in this case the dataset
            will only yield the input data).
        sequence_length: Length of the output sequences
            (in number of timesteps).
        sequence_stride: Period between successive output sequences.
            For stride `s`, output samples would
            start at index `data[i]`, `data[i + s]`, `data[i + 2 * s]`, etc.
        sampling_rate: Period between successive individual timesteps
            within sequences. For rate `r`, timesteps
            `data[i], data[i + r], ... data[i + sequence_length]`
            are used for creating a sample sequence.
        batch_size: Number of timeseries samples in each batch
            (except maybe the last one). If `None`, the data will not be batched
            (the dataset will yield individual samples).
        shuffle: Whether to shuffle output samples,
            or instead draw them in chronological order.
        seed: Optional int; random seed for shuffling.
        start_index: Optional int; data points earlier (exclusive)
            than `start_index` will not be used
            in the output sequences. This is useful to reserve part of the
            data for test or validation.
        end_index: Optional int; data points later (exclusive) than `end_index`
            will not be used in the output sequences.
            This is useful to reserve part of the data for test or validation.

    Returns:

    A `tf.data.Dataset` instance. If `targets` was passed, the dataset yields
    tuple `(batch_of_sequences, batch_of_targets)`. If not, the dataset yields
    only `batch_of_sequences`.

    Example 1:

    Consider indices `[0, 1, ... 98]`.
    With `sequence_length=10,  sampling_rate=2, sequence_stride=3`,
    `shuffle=False`, the dataset will yield batches of sequences
    composed of the following indices:

    ```
    First sequence:  [0  2  4  6  8 10 12 14 16 18]
    Second sequence: [3  5  7  9 11 13 15 17 19 21]
    Third sequence:  [6  8 10 12 14 16 18 20 22 24]
    ...
    Last sequence:   [78 80 82 84 86 88 90 92 94 96]
    ```

    In this case the last 2 data points are discarded since no full sequence
    can be generated to include them (the next sequence would have started
    at index 81, and thus its last step would have gone over 98).

    Example 2: Temporal regression.

    Consider an array `data` of scalar values, of shape `(steps,)`.
    To generate a dataset that uses the past 10
    timesteps to predict the next timestep, you would use:

    ```python
    input_data = data[:-10]
    targets = data[10:]
    dataset = timeseries_dataset_from_array(
        input_data, targets, sequence_length=10)
    for batch in dataset:
      inputs, targets = batch
      assert np.array_equal(inputs[0], data[:10])  # First sequence: steps [0-9]
      # Corresponding target: step 10
      assert np.array_equal(targets[0], data[10])
      break
    ```

    Example 3: Temporal regression for many-to-many architectures.

    Consider two arrays of scalar values `X` and `Y`,
    both of shape `(100,)`. The resulting dataset should consist samples with
    20 timestamps each. The samples should not overlap.
    To generate a dataset that uses the current timestamp
    to predict the corresponding target timestep, you would use:

    ```python
    X = np.arange(100)
    Y = X*2

    sample_length = 20
    input_dataset = timeseries_dataset_from_array(
        X, None, sequence_length=sample_length, sequence_stride=sample_length)
    target_dataset = timeseries_dataset_from_array(
        Y, None, sequence_length=sample_length, sequence_stride=sample_length)

    for batch in zip(input_dataset, target_dataset):
        inputs, targets = batch
        assert np.array_equal(inputs[0], X[:sample_length])

        # second sample equals output timestamps 20-40
        assert np.array_equal(targets[1], Y[sample_length:2*sample_length])
        break
    ```
    )-"
#' Creates a dataset of sliding windows over a timeseries provided as array.
#'
#' @description
#' This function takes in a sequence of data-points gathered at
#' equal intervals, along with time series parameters such as
#' length of the sequences/windows, spacing between two sequence/windows, etc.,
#' to produce batches of timeseries inputs and targets.
#'
#' # Returns
#' A `tf.data.Dataset` instance. If `targets` was passed, the dataset yields
#' tuple `(batch_of_sequences, batch_of_targets)`. If not, the dataset yields
#' only `batch_of_sequences`.
#'
#' Example 1:
#'
#' Consider indices `[0, 1, ... 98]`.
#' With `sequence_length=10,  sampling_rate=2, sequence_stride=3`,
#' `shuffle=False`, the dataset will yield batches of sequences
#' composed of the following indices:
#'
#' ```
#' First sequence:  [0  2  4  6  8 10 12 14 16 18]
#' Second sequence: [3  5  7  9 11 13 15 17 19 21]
#' Third sequence:  [6  8 10 12 14 16 18 20 22 24]
#' ...
#' Last sequence:   [78 80 82 84 86 88 90 92 94 96]
#' ```
#'
#' In this case the last 2 data points are discarded since no full sequence
#' can be generated to include them (the next sequence would have started
#' at index 81, and thus its last step would have gone over 98).
#'
#' Example 2: Temporal regression.
#'
#' Consider an array `data` of scalar values, of shape `(steps,)`.
#' To generate a dataset that uses the past 10
#' timesteps to predict the next timestep, you would use:
#'
#' ```python
#' input_data = data[:-10]
#' targets = data[10:]
#' dataset = timeseries_dataset_from_array(
#'     input_data, targets, sequence_length=10)
#' for batch in dataset:
#'   inputs, targets = batch
#'   assert np.array_equal(inputs[0], data[:10])  # First sequence: steps [0-9]
#'   # Corresponding target: step 10
#'   assert np.array_equal(targets[0], data[10])
#'   break
#' ```
#'
#' Example 3: Temporal regression for many-to-many architectures.
#'
#' Consider two arrays of scalar values `X` and `Y`,
#' both of shape `(100,)`. The resulting dataset should consist samples with
#' 20 timestamps each. The samples should not overlap.
#' To generate a dataset that uses the current timestamp
#' to predict the corresponding target timestep, you would use:
#'
#' ```python
#' X = np.arange(100)
#' Y = X*2
#'
#' sample_length = 20
#' input_dataset = timeseries_dataset_from_array(
#'     X, None, sequence_length=sample_length, sequence_stride=sample_length)
#' target_dataset = timeseries_dataset_from_array(
#'     Y, None, sequence_length=sample_length, sequence_stride=sample_length)
#'
#' for batch in zip(input_dataset, target_dataset):
#'     inputs, targets = batch
#'     assert np.array_equal(inputs[0], X[:sample_length])
#'
#'     # second sample equals output timestamps 20-40
#'     assert np.array_equal(targets[1], Y[sample_length:2*sample_length])
#'     break
#' ```
#'
#' @param data Numpy array or eager tensor
#'     containing consecutive data points (timesteps).
#'     Axis 0 is expected to be the time dimension.
#' @param targets Targets corresponding to timesteps in `data`.
#'     `targets[i]` should be the target
#'     corresponding to the window that starts at index `i`
#'     (see example 2 below).
#'     Pass `None` if you don't have target data (in this case the dataset
#'     will only yield the input data).
#' @param sequence_length Length of the output sequences
#'     (in number of timesteps).
#' @param sequence_stride Period between successive output sequences.
#'     For stride `s`, output samples would
#'     start at index `data[i]`, `data[i + s]`, `data[i + 2 * s]`, etc.
#' @param sampling_rate Period between successive individual timesteps
#'     within sequences. For rate `r`, timesteps
#'     `data[i], data[i + r], ... data[i + sequence_length]`
#'     are used for creating a sample sequence.
#' @param batch_size Number of timeseries samples in each batch
#'     (except maybe the last one). If `None`, the data will not be batched
#'     (the dataset will yield individual samples).
#' @param shuffle Whether to shuffle output samples,
#'     or instead draw them in chronological order.
#' @param seed Optional int; random seed for shuffling.
#' @param start_index Optional int; data points earlier (exclusive)
#'     than `start_index` will not be used
#'     in the output sequences. This is useful to reserve part of the
#'     data for test or validation.
#' @param end_index Optional int; data points later (exclusive) than `end_index`
#'     will not be used in the output sequences.
#'     This is useful to reserve part of the data for test or validation.
#'
#' @export
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/timeseries_dataset_from_array>
timeseries_dataset_from_array <-
function (data, targets, sequence_length, sequence_stride = 1L,
    sampling_rate = 1L, batch_size = 128L, shuffle = FALSE, seed = NULL,
    start_index = NULL, end_index = NULL)
{
    args <- capture_args2(list(sequence_stride = as_integer,
        sampling_rate = as_integer, batch_size = as_integer,
        seed = as_integer, start_index = as_integer, end_index = as_integer))
    do.call(keras$utils$timeseries_dataset_from_array, args)
}
