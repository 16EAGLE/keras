


#' Resets all state generated by Keras.
#'
#' @description
#' Keras manages a global state, which it uses to implement the Functional
#' model-building API and to uniquify autogenerated layer names.
#'
#' If you are creating many models in a loop, this global state will consume
#' an increasing amount of memory over time, and you may want to clear it.
#' Calling `clear_session()` releases the global state: this helps avoid
#' clutter from old models and layers, especially when memory is limited.
#'
#' Example 1: calling `clear_session()` when creating models in a loop
#'
#' ```{r}
#' for (i in 1:100) {
#'   # Without `clear_session()`, each iteration of this loop will
#'   # slightly increase the size of the global state managed by Keras
#'   model <- keras_model_sequential()
#'   for (j in 1:10) {
#'     model <- model |> layer_dense(units = 10)
#'   }
#' }
#'
#' for (i in 1:100) {
#'   # With `clear_session()` called at the beginning,
#'   # Keras starts with a blank state at each iteration
#'   # and memory consumption is constant over time.
#'   clear_session()
#'   model <- keras_model_sequential()
#'   for (j in 1:10) {
#'     model <- model |> layer_dense(units = 10)
#'   }
#' }
#' ```
#'
#' Example 2: resetting the layer name generation counter
#'
#' ```{r, include = FALSE}
#' clear_session()
#' ```
#'
#'
#' ```{r}
#' layers <- lapply(1:10, \(i) layer_dense(units = 10))
#'
#' new_layer <- layer_dense(units = 10)
#' print(new_layer$name)
#'
#' clear_session()
#' new_layer <- layer_dense(units = 10)
#' print(new_layer$name)
#' ```
#'
#' @export
#' @family backend
#' @family utils
#' @seealso
#' + <https:/keras.io/keras_core/api/utils/config_utils#clearsession-function>
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/clear_session>
#' @tether keras.utils.clear_session
clear_session <-
function ()
{
    args <- capture_args2(NULL)
    do.call(keras$utils$clear_session, args)
}


#' One-stop utility for preprocessing and encoding structured data.
#'
#' @description
#' **Available feature types:**
#'
#' Note that all features can be referred to by their string name,
#' e.g. `"integer_categorical"`. When using the string name, the default
#' argument values are used.
#'
#' ```{r, eval = FALSE}
#' # Plain float values.
#' feature_float(name = NULL)
#'
#' # Float values to be preprocessed via featurewise standardization
#' # (i.e. via a `layer_normalization()` layer).
#' feature_float_normalized(name = NULL)
#'
#' # Float values to be preprocessed via linear rescaling
#' # (i.e. via a `layer_rescaling` layer).
#' feature_float_rescaled(scale = 1., offset = 0., name = NULL)
#'
#' # Float values to be discretized. By default, the discrete
#' # representation will then be one-hot encoded.
#' feature_float_discretized(
#'   num_bins,
#'   bin_boundaries = NULL,
#'   output_mode = "one_hot",
#'   name = NULL
#' )
#'
#' # Integer values to be indexed. By default, the discrete
#' # representation will then be one-hot encoded.
#' feature_integer_categorical(
#'   max_tokens = NULL,
#'   num_oov_indices = 1,
#'   output_mode = "one_hot",
#'   name = NULL
#' )
#'
#' # String values to be indexed. By default, the discrete
#' # representation will then be one-hot encoded.
#' feature_string_categorical(
#'   max_tokens = NULL,
#'   num_oov_indices = 1,
#'   output_mode = "one_hot",
#'   name = NULL
#' )
#'
#' # Integer values to be hashed into a fixed number of bins.
#' # By default, the discrete representation will then be one-hot encoded.
#' feature_integer_hashed(num_bins, output_mode = "one_hot", name = NULL)
#'
#' # String values to be hashed into a fixed number of bins.
#' # By default, the discrete representation will then be one-hot encoded.
#' feature_string_hashed(num_bins, output_mode = "one_hot", name = NULL)
#' ```
#' # Examples
#' **Basic usage with a dict of input data:**
#'
#' ```{r}
#' raw_data <- list(
#'   float_values = c(0.0, 0.1, 0.2, 0.3),
#'   string_values = c("zero", "one", "two", "three"),
#'   int_values = as.integer(c(0, 1, 2, 3))
#' )
#'
#' dataset <- tfdatasets::tensor_slices_dataset(raw_data)
#'
#' feature_space <- layer_feature_space(
#'   features = list(
#'     float_values = "float_normalized",
#'     string_values = "string_categorical",
#'     int_values = "integer_categorical"
#'   ),
#'   crosses = list(c("string_values", "int_values")),
#'   output_mode = "concat"
#' )
#'
#' # Before you start using the feature_space(),
#' # you must `adapt()` it on some data.
#' feature_space |> adapt(dataset)
#'
#' # You can call the feature_space() on a named list of
#' # data (batched or unbatched).
#' output_vector <- feature_space(raw_data)
#' ```
#'
#' **Basic usage with `tf.data`:**
#'
#' ```{r, eval = FALSE}
#' library(tfdatasets)
#' # Unlabeled data
#' preprocessed_ds <- unlabeled_dataset |>
#'   dataset_map(feature_space)
#'
#' # Labeled data
#' preprocessed_ds <- labeled_dataset |>
#'   dataset_map(function(x, y) tuple(feature_space(x), y))
#' ```
#'
#' **Basic usage with the Keras Functional API:**
#'
#' ```{r}
#' # Retrieve a named list of Keras layer_input() objects
#' (inputs <- feature_space$get_inputs())
#' # Retrieve the corresponding encoded Keras tensors
#' (encoded_features <- feature_space$get_encoded_features())
#' # Build a Functional model
#' outputs <- encoded_features |> layer_dense(1, activation = "sigmoid")
#' model <- keras_model(inputs, outputs)
#' ```
#'
#' **Customizing each feature or feature cross:**
#'
#' ```{r}
#' feature_space <- layer_feature_space(
#'   features = list(
#'     float_values = feature_float_normalized(),
#'     string_values = feature_string_categorical(max_tokens = 10),
#'     int_values = feature_integer_categorical(max_tokens = 10)
#'   ),
#'   crosses = list(
#'     feature_cross(c("string_values", "int_values"), crossing_dim = 32)
#'   ),
#'   output_mode = "concat"
#' )
#' ```
#'
#' **Returning a dict of integer-encoded features:**
#'
#' ```{r}
#' feature_space <- layer_feature_space(
#'   features = list(
#'     "string_values" = feature_string_categorical(output_mode = "int"),
#'     "int_values" = feature_integer_categorical(output_mode = "int")
#'   ),
#'   crosses = list(
#'     feature_cross(
#'       feature_names = c("string_values", "int_values"),
#'       crossing_dim = 32,
#'       output_mode = "int"
#'     )
#'   ),
#'   output_mode = "dict"
#' )
#' ```
#'
#' **Specifying your own Keras preprocessing layer:**
#'
#' ```{r}
#' # Let's say that one of the features is a short text paragraph that
#' # we want to encode as a vector (one vector per paragraph) via TF-IDF.
#' data <- list(text = c("1st string", "2nd string", "3rd string"))
#'
#' # There's a Keras layer for this: layer_text_vectorization()
#' custom_layer <- layer_text_vectorization(output_mode = "tf_idf")
#'
#' # We can use feature_custom() to create a custom feature
#' # that will use our preprocessing layer.
#' feature_space <- layer_feature_space(
#'   features = list(
#'     text = feature_custom(preprocessor = custom_layer,
#'                           dtype = "string",
#'                           output_mode = "float"
#'     )
#'   ),
#'   output_mode = "concat"
#' )
#' feature_space |> adapt(tfdatasets::tensor_slices_dataset(data))
#' output_vector <- feature_space(data)
#' ```
#'
#' **Retrieving the underlying Keras preprocessing layers:**
#'
#' ```{r, eval = FALSE}
#' # The preprocessing layer of each feature is available in `$preprocessors`.
#' preprocessing_layer <- feature_space$preprocessors$feature1
#'
#' # The crossing layer of each feature cross is available in `$crossers`.
#' # It's an instance of layer_hashed_crossing()
#' crossing_layer <- feature_space$crossers[["feature1_X_feature2"]]
#' ```
#'
#' **Saving and reloading a FeatureSpace:**
#'
#' ```{r, eval = FALSE}
#' feature_space$save("featurespace.keras")
#' reloaded_feature_space <- keras$models$load_model("featurespace.keras")
#' ```
#'
#' @param feature_names
#' Dict mapping the names of your features to their
#' type specification, e.g. `list(my_feature = "integer_categorical")`
#' or `list(my_feature = feature_integer_categorical())`.
#' For a complete list of all supported types, see
#' "Available feature types" paragraph below.
#'
#' @param output_mode
#' One of `"concat"` or `"dict"`. In concat mode, all
#' features get concatenated together into a single vector.
#' In dict mode, the FeatureSpace returns a dict of individually
#' encoded features (with the same keys as the input dict keys).
#'
#' @param crosses
#' List of features to be crossed together, e.g.
#' `crosses=list(c("feature_1", "feature_2"))`. The features will be
#' "crossed" by hashing their combined value into
#' a fixed-length vector.
#'
#' @param crossing_dim
#' Default vector size for hashing crossed features.
#' Defaults to `32`.
#'
#' @param hashing_dim
#' Default vector size for hashing features of type
#' `"integer_hashed"` and `"string_hashed"`. Defaults to `32`.
#'
#' @param num_discretization_bins
#' Default number of bins to be used for
#' discretizing features of type `"float_discretized"`.
#' Defaults to `32`.
#'
#' @param name
#' String, name for the object
#'
#' @param object
#' see description
#'
#' @param features
#' see description
#'
#' @export
#' @family preprocessing layers
#' @family layers
#' @family utils
#' @seealso
#' + <https:/keras.io/api/utils/feature_space#featurespace-class>
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/FeatureSpace>
#' @tether keras.utils.FeatureSpace
layer_feature_space <-
function (object, features, output_mode = "concat", crosses = NULL,
    crossing_dim = 32L, hashing_dim = 32L, num_discretization_bins = 32L,
    name = NULL, feature_names = NULL)
{
    args <- capture_args2(list(crossing_dim = as_integer, hashing_dim = as_integer,
        num_discretization_bins = as_integer, features = as.list),
        ignore = "object")
    create_layer(keras$utils$FeatureSpace, object, args)
}


#' Returns the list of input tensors necessary to compute `tensor`.
#'
#' @description
#' Output will always be a list of tensors
#' (potentially with 1 element).
#'
#' @returns
#'     List of input tensors.
#'
#' @param tensor
#' The tensor to start from.
#'
#' @export
#' @family utils
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_source_inputs>
#' @tether keras.utils.get_source_inputs
get_source_inputs <-
function (tensor)
{
    args <- capture_args2(NULL)
    do.call(keras$utils$get_source_inputs, args)
}


#' Retrieves a live reference to the global dictionary of custom objects.
#'
#' @description
#' Custom objects set using using `custom_object_scope()` are not added to the
#' global dictionary of custom objects, and will not appear in the returned
#' dictionary.
#'
#' # Examples
#' ```{r, eval = FALSE}
#' get_custom_objects()$clear()
#' get_custom_objects()$update(list(MyObject = MyObject))
#' ```
#'
#' @returns
#'     Global dictionary (named list) mapping registered class names to classes.
#'
#' @export
#' @family object registration saving
#' @family saving
#' @family utils
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_custom_objects>
#' @tether keras.utils.get_custom_objects
get_custom_objects <-
function ()
{
    args <- capture_args2(NULL)
    do.call(keras$utils$get_custom_objects, args)
}


#' Returns the name registered to an object within the Keras framework.
#'
#' @description
#' This function is part of the Keras serialization and deserialization
#' framework. It maps objects to the string names associated with those objects
#' for serialization/deserialization.
#'
#' @returns
#' The name associated with the object, or the default Python name if the
#' object is not registered.
#'
#' @param obj
#' The object to look up.
#'
#' @export
#' @family object registration saving
#' @family saving
#' @family utils
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_registered_name>
#' @tether keras.utils.get_registered_name
get_registered_name <-
function (obj)
{
    args <- capture_args2(NULL)
    do.call(keras$utils$get_registered_name, args)
}


#' Returns the class associated with `name` if it is registered with Keras.
#'
#' @description
#' This function is part of the Keras serialization and deserialization
#' framework. It maps strings to the objects associated with them for
#' serialization/deserialization.
#'
#' # Examples
#' ```{r, eval = FALSE}
#' from_config <- function(cls, config, custom_objects = NULL) {
#'   if ('my_custom_object_name' %in% names(config)) {
#'     config$hidden_cls <- get_registered_object(
#'       config$my_custom_object_name,
#'       custom_objects = custom_objects)
#'   }
#' }
#' ```
#'
#' @returns
#' An instantiable class associated with `name`, or `NULL` if no such class
#' exists.
#'
#' @param name
#' The name to look up.
#'
#' @param custom_objects
#' A named list of custom objects to look the name up in.
#' Generally, custom_objects is provided by the user.
#'
#' @param module_objects
#' A named list of custom objects to look the name up in.
#' Generally, module_objects is provided by midlevel library
#' implementers.
#'
#' @export
#' @family object registration saving
#' @family saving
#' @family utils
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_registered_object>
#' @tether keras.utils.get_registered_object
get_registered_object <-
function (name, custom_objects = NULL, module_objects = NULL)
{
    args <- capture_args2(NULL)
    do.call(keras$utils$get_registered_object, args)
}


#' Packs user-provided data into a list.
#'
#' @description
#' This is a convenience utility for packing data into the list formats
#' that `fit()` uses.
#'
#' # Usage
#' Standalone usage:
#'
#' ```{r}
#' x <- op_ones(c(10, 1))
#' data <- pack_x_y_sample_weight(x)
#'
#' # TRUE
#' y <- op_ones(c(10, 1))
#' data <- pack_x_y_sample_weight(x, y)
#' ```
#'
#' @returns
#'     List in the format used in `fit()`.
#'
#' @param x
#' Features to pass to `Model`.
#'
#' @param y
#' Ground-truth targets to pass to `Model`.
#'
#' @param sample_weight
#' Sample weight for each element.
#'
#' @export
#' @family datum util adapter trainers
#' @family datum adapter trainers
#' @family trainers
#' @family utils
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/pack_x_y_sample_weight>
#'
#' @tether keras.utils.pack_x_y_sample_weight
pack_x_y_sample_weight <-
function (x, y = NULL, sample_weight = NULL)
{
    args <- capture_args2(NULL)
    do.call(keras$utils$pack_x_y_sample_weight, args)
}


#' Unpacks user-provided data list.
#'
#' @description
#' This is a convenience utility to be used when overriding
#' `$train_step`, `$test_step`, or `$predict_step`.
#' This utility makes it easy to support data of the form `(x,)`,
#' `(x, y)`, or `(x, y, sample_weight)`.
#'
#' # Usage
#' Standalone usage:
#'
#' ```{r}
#' features_batch <- op_ones(c(10, 5))
#' labels_batch <- op_zeros(c(10, 5))
#' data <- list(features_batch, labels_batch)
#' # `y` and `sample_weight` will default to `NULL` if not provided.
#' c(x, y, sample_weight) %<-% unpack_x_y_sample_weight(data)
#' ```
#'
#' @returns
#' The unpacked list, with `NULL`s for `y` and `sample_weight` if they are
#' not provided.
#'
#' @param data
#' A list of the form `(x,)`, `(x, y)`, or `(x, y, sample_weight)`.
#'
#' @export
#' @family datum util adapter trainers
#' @family datum adapter trainers
#' @family trainers
#' @family utils
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/unpack_x_y_sample_weight>
#'
#' @tether keras.utils.unpack_x_y_sample_weight
unpack_x_y_sample_weight <-
function (data)
{
    args <- capture_args2(NULL)
    do.call(keras$utils$unpack_x_y_sample_weight, args)
}


#' Generates a `tf.data.Dataset` from audio files in a directory.
#'
#' @description
#' If your directory structure is:
#'
#' ```
#' main_directory/
#' ...class_a/
#' ......a_audio_1.wav
#' ......a_audio_2.wav
#' ...class_b/
#' ......b_audio_1.wav
#' ......b_audio_2.wav
#' ```
#'
#' Then calling `audio_dataset_from_directory(main_directory,
#' labels = 'inferred')`
#' will return a `tf.data.Dataset` that yields batches of audio files from
#' the subdirectories `class_a` and `class_b`, together with labels
#' 0 and 1 (0 corresponding to `class_a` and 1 corresponding to `class_b`).
#'
#' Only `.wav` files are supported at this time.
#'
#' @returns
#' A `tf.data.Dataset` object.
#'
#' - If `label_mode` is `None`, it yields `string` tensors of shape
#'   `(batch_size,)`, containing the contents of a batch of audio files.
#' - Otherwise, it yields a tuple `(audio, labels)`, where `audio`
#'   has shape `(batch_size, sequence_length, num_channels)` and `labels`
#'   follows the format described
#'   below.
#'
#' Rules regarding labels format:
#'
#' - if `label_mode` is `int`, the labels are an `int32` tensor of shape
#'   `(batch_size,)`.
#' - if `label_mode` is `binary`, the labels are a `float32` tensor of
#'   1s and 0s of shape `(batch_size, 1)`.
#' - if `label_mode` is `categorical`, the labels are a `float32` tensor
#'   of shape `(batch_size, num_classes)`, representing a one-hot
#'   encoding of the class index.
#'
#' @param directory
#' Directory where the data is located.
#' If `labels` is `"inferred"`, it should contain subdirectories,
#' each containing audio files for a class. Otherwise, the directory
#' structure is ignored.
#'
#' @param labels
#' Either "inferred" (labels are generated from the directory
#' structure), `None` (no labels), or a list/tuple of integer labels
#' of the same size as the number of audio files found in
#' the directory. Labels should be sorted according to the
#' alphanumeric order of the audio file paths
#' (obtained via `os.walk(directory)` in Python).
#'
#' @param label_mode
#' String describing the encoding of `labels`. Options are:
#' - `"int"`: means that the labels are encoded as integers (e.g. for
#'   `sparse_categorical_crossentropy` loss).
#' - `"categorical"` means that the labels are encoded as a categorical
#'   vector (e.g. for `categorical_crossentropy` loss)
#' - `"binary"` means that the labels (there can be only 2)
#'   are encoded as `float32` scalars with values 0
#'   or 1 (e.g. for `binary_crossentropy`).
#' - `None` (no labels).
#'
#' @param class_names
#' Only valid if "labels" is `"inferred"`.
#' This is the explicit list of class names
#' (must match names of subdirectories). Used to control the order
#' of the classes (otherwise alphanumerical order is used).
#'
#' @param batch_size
#' Size of the batches of data. Default: 32. If `None`,
#' the data will not be batched
#' (the dataset will yield individual samples).
#'
#' @param sampling_rate
#' Audio sampling rate (in samples per second).
#'
#' @param output_sequence_length
#' Maximum length of an audio sequence. Audio files
#' longer than this will be truncated to `output_sequence_length`.
#' If set to `None`, then all sequences in the same batch will
#' be padded to the
#' length of the longest sequence in the batch.
#'
#' @param ragged
#' Whether to return a Ragged dataset (where each sequence has its
#' own length). Defaults to `False`.
#'
#' @param shuffle
#' Whether to shuffle the data. Defaults to `TRUE`.
#' If set to `False`, sorts the data in alphanumeric order.
#'
#' @param seed
#' Optional random seed for shuffling and transformations.
#'
#' @param validation_split
#' Optional float between 0 and 1, fraction of data to
#' reserve for validation.
#'
#' @param subset
#' Subset of the data to return. One of `"training"`,
#' `"validation"` or `"both"`. Only used if `validation_split` is set.
#'
#' @param follow_links
#' Whether to visits subdirectories pointed to by symlinks.
#' Defaults to `False`.
#'
#' @export
#' @family dataset utils
#' @family utils
#' @seealso
#' + <https:/keras.io/api/data_loading/audio#audiodatasetfromdirectory-function>
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/audio_dataset_from_directory>
#' @tether keras.utils.audio_dataset_from_directory
audio_dataset_from_directory <-
function (directory, labels = "inferred", label_mode = "int",
    class_names = NULL, batch_size = 32L, sampling_rate = NULL,
    output_sequence_length = NULL, ragged = FALSE, shuffle = TRUE,
    seed = NULL, validation_split = NULL, subset = NULL, follow_links = FALSE)
{
    args <- capture_args2(list(labels = as_integer, label_mode = as_integer,
        batch_size = as_integer, seed = as_integer))
    do.call(keras$utils$audio_dataset_from_directory, args)
}


#' Splits a dataset into a left half and a right half (e.g. train / test).
#'
#' @description
#'
#' # Examples
#' ```{r}
#' data <- random_uniform(c(1000, 4))
#' c(left_ds, right_ds) %<-% split_dataset(list(data$numpy()), left_size = 0.8)
#' left_ds$cardinality()
#' right_ds$cardinality()
#' ```
#'
#' @returns
#' A list of two `tf$data$Dataset` objects:
#' the left and right splits.
#'
#' @param dataset
#' A `tf$data$Dataset`, a `torch$utils$data$Dataset` object,
#' or a list of arrays with the same length.
#'
#' @param left_size
#' If float (in the range `[0, 1]`), it signifies
#' the fraction of the data to pack in the left dataset. If integer, it
#' signifies the number of samples to pack in the left dataset. If
#' `NULL`, defaults to the complement to `right_size`.
#' Defaults to `NULL`.
#'
#' @param right_size
#' If float (in the range `[0, 1]`), it signifies
#' the fraction of the data to pack in the right dataset.
#' If integer, it signifies the number of samples to pack
#' in the right dataset.
#' If `NULL`, defaults to the complement to `left_size`.
#' Defaults to `NULL`.
#'
#' @param shuffle
#' Boolean, whether to shuffle the data before splitting it.
#'
#' @param seed
#' A random seed for shuffling.
#'
#' @export
#' @family dataset utils
#' @family utils
#' @seealso
#' + <https:/keras.io/api/utils/python_utils#splitdataset-function>
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/split_dataset>
#'
#' @tether keras.utils.split_dataset
split_dataset <-
function (dataset, left_size = NULL, right_size = NULL, shuffle = FALSE,
    seed = NULL)
{
    args <- capture_args2(list(left_size = function (x)
    ifelse(x < 1, x, as_integer(x)), right_size = function (x)
    ifelse(x < 1, x, as_integer(x)), seed = as_integer))
    do.call(keras$utils$split_dataset, args)
}


#' Downloads a file from a URL if it not already in the cache.
#'
#' @description
#' By default the file at the url `origin` is downloaded to the
#' cache_dir `~/.keras`, placed in the cache_subdir `datasets`,
#' and given the filename `fname`. The final location of a file
#' `example.txt` would therefore be `~/.keras/datasets/example.txt`.
#' Files in `.tar`, `.tar.gz`, `.tar.bz`, and `.zip` formats can
#' also be extracted.
#'
#' Passing a hash will verify the file after download. The command line
#' programs `shasum` and `sha256sum` can compute the hash.
#'
#' # Examples
#' ```{r}
#' path_to_downloaded_file <- get_file(
#'     origin = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz",
#'     extract = TRUE
#' )
#' ```
#'
#' @returns
#' Path to the downloaded file.
#'
#' ** Warning on malicious downloads **
#'
#' Downloading something from the Internet carries a risk.
#' NEVER download a file/archive if you do not trust the source.
#' We recommend that you specify the `file_hash` argument
#' (if the hash of the source file is known) to make sure that the file you
#' are getting is the one you expect.
#'
#' @param fname
#' Name of the file. If an absolute path, e.g. `"/path/to/file.txt"`
#' is specified, the file will be saved at that location.
#' If `NULL`, the name of the file at `origin` will be used.
#'
#' @param origin
#' Original URL of the file.
#'
#' @param untar
#' Deprecated in favor of `extract` argument.
#' boolean, whether the file should be decompressed
#'
#' @param md5_hash
#' Deprecated in favor of `file_hash` argument.
#' md5 hash of the file for verification
#'
#' @param file_hash
#' The expected hash string of the file after download.
#' The sha256 and md5 hash algorithms are both supported.
#'
#' @param cache_subdir
#' Subdirectory under the Keras cache dir where the file is
#' saved. If an absolute path, e.g. `"/path/to/folder"` is
#' specified, the file will be saved at that location.
#'
#' @param hash_algorithm
#' Select the hash algorithm to verify the file.
#' options are `"md5'`, `"sha256'`, and `"auto'`.
#' The default 'auto' detects the hash algorithm in use.
#'
#' @param extract
#' `TRUE` tries extracting the file as an Archive, like tar or zip.
#'
#' @param archive_format
#' Archive format to try for extracting the file.
#' Options are `"auto'`, `"tar'`, `"zip'`, and `NULL`.
#' `"tar"` includes tar, tar.gz, and tar.bz files.
#' The default `"auto"` corresponds to `c("tar", "zip")`.
#' `NULL` or an empty list will return no matches found.
#'
#' @param cache_dir
#' Location to store cached files, when `NULL` it
#' defaults to `Sys.getenv("KERAS_HOME", '~/.keras/')`.
#'
#' @export
#' @family utils
#' @seealso
#' + <https:/keras.io/api/utils/python_utils#getfile-function>
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file>
#' @tether keras.utils.get_file
get_file <-
function (fname = NULL, origin = NULL, untar = FALSE, md5_hash = NULL,
    file_hash = NULL, cache_subdir = "datasets", hash_algorithm = "auto",
    extract = FALSE, archive_format = "auto", cache_dir = NULL)
{
    args <- capture_args2(NULL)
    do.call(keras$utils$get_file, args)
}


#' Generates a `tf.data.Dataset` from image files in a directory.
#'
#' @description
#' If your directory structure is:
#'
#' ```
#' main_directory/
#' ...class_a/
#' ......a_image_1.jpg
#' ......a_image_2.jpg
#' ...class_b/
#' ......b_image_1.jpg
#' ......b_image_2.jpg
#' ```
#'
#' Then calling `image_dataset_from_directory(main_directory,
#' labels = 'inferred')` will return a `tf.data.Dataset` that yields batches of
#' images from the subdirectories `class_a` and `class_b`, together with labels
#' 0 and 1 (0 corresponding to `class_a` and 1 corresponding to `class_b`).
#'
#' Supported image formats: `.jpeg`, `.jpg`, `.png`, `.bmp`, `.gif`.
#' Animated gifs are truncated to the first frame.
#'
#' @returns
#' A `tf.data.Dataset` object.
#'
#' - If `label_mode` is `None`, it yields `float32` tensors of shape
#'     `(batch_size, image_size[0], image_size[1], num_channels)`,
#'     encoding images (see below for rules regarding `num_channels`).
#' - Otherwise, it yields a tuple `(images, labels)`, where `images` has
#'     shape `(batch_size, image_size[0], image_size[1], num_channels)`,
#'     and `labels` follows the format described below.
#'
#' Rules regarding labels format:
#'
#' - if `label_mode` is `"int"`, the labels are an `int32` tensor of shape
#'     `(batch_size,)`.
#' - if `label_mode` is `"binary"`, the labels are a `float32` tensor of
#'     1s and 0s of shape `(batch_size, 1)`.
#' - if `label_mode` is `"categorical"`, the labels are a `float32` tensor
#'     of shape `(batch_size, num_classes)`, representing a one-hot
#'     encoding of the class index.
#'
#' Rules regarding number of channels in the yielded images:
#'
#' - if `color_mode` is `"grayscale"`,
#'     there's 1 channel in the image tensors.
#' - if `color_mode` is `"rgb"`,
#'     there are 3 channels in the image tensors.
#' - if `color_mode` is `"rgba"`,
#'     there are 4 channels in the image tensors.
#'
#' @param directory
#' Directory where the data is located.
#' If `labels` is `"inferred"`, it should contain
#' subdirectories, each containing images for a class.
#' Otherwise, the directory structure is ignored.
#'
#' @param labels
#' Either `"inferred"`
#' (labels are generated from the directory structure),
#' `None` (no labels),
#' or a list/tuple of integer labels of the same size as the number of
#' image files found in the directory. Labels should be sorted
#' according to the alphanumeric order of the image file paths
#' (obtained via `os.walk(directory)` in Python).
#'
#' @param label_mode
#' String describing the encoding of `labels`. Options are:
#' - `"int"`: means that the labels are encoded as integers
#'     (e.g. for `sparse_categorical_crossentropy` loss).
#' - `"categorical"` means that the labels are
#'     encoded as a categorical vector
#'     (e.g. for `categorical_crossentropy` loss).
#' - `"binary"` means that the labels (there can be only 2)
#'     are encoded as `float32` scalars with values 0 or 1
#'     (e.g. for `binary_crossentropy`).
#' - `None` (no labels).
#'
#' @param class_names
#' Only valid if `labels` is `"inferred"`.
#' This is the explicit list of class names
#' (must match names of subdirectories). Used to control the order
#' of the classes (otherwise alphanumerical order is used).
#'
#' @param color_mode
#' One of `"grayscale"`, `"rgb"`, `"rgba"`.
#' Defaults to `"rgb"`. Whether the images will be converted to
#' have 1, 3, or 4 channels.
#'
#' @param batch_size
#' Size of the batches of data. Defaults to 32.
#' If `None`, the data will not be batched
#' (the dataset will yield individual samples).
#'
#' @param image_size
#' Size to resize images to after they are read from disk,
#' specified as `(height, width)`. Defaults to `(256, 256)`.
#' Since the pipeline processes batches of images that must all have
#' the same size, this must be provided.
#'
#' @param shuffle
#' Whether to shuffle the data. Defaults to `TRUE`.
#' If set to `False`, sorts the data in alphanumeric order.
#'
#' @param seed
#' Optional random seed for shuffling and transformations.
#'
#' @param validation_split
#' Optional float between 0 and 1,
#' fraction of data to reserve for validation.
#'
#' @param subset
#' Subset of the data to return.
#' One of `"training"`, `"validation"`, or `"both"`.
#' Only used if `validation_split` is set.
#' When `subset = "both"`, the utility returns a tuple of two datasets
#' (the training and validation datasets respectively).
#'
#' @param interpolation
#' String, the interpolation method used when
#' resizing images. Defaults to `"bilinear"`.
#' Supports `"bilinear"`, `"nearest"`, `"bicubic"`, `"area"`,
#' `"lanczos3"`, `"lanczos5"`, `"gaussian"`, `"mitchellcubic"`.
#'
#' @param follow_links
#' Whether to visit subdirectories pointed to by symlinks.
#' Defaults to `False`.
#'
#' @param crop_to_aspect_ratio
#' If `TRUE`, resize the images without aspect
#' ratio distortion. When the original aspect ratio differs from the
#' target aspect ratio, the output image will be cropped so as to
#' return the largest possible window in the image
#' (of size `image_size`) that matches the target aspect ratio. By
#' default (`crop_to_aspect_ratio = False`), aspect ratio may not be
#' preserved.
#'
#' @param data_format
#' If None uses keras.config.image_data_format()
#' otherwise either 'channel_last' or 'channel_first'.
#'
#' @export
#' @family dataset utils
#' @family image dataset utils
#' @family utils
#' @family preprocessing
#' @seealso
#' + <https:/keras.io/api/data_loading/image#imagedatasetfromdirectory-function>
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory>
#' @tether keras.utils.image_dataset_from_directory
image_dataset_from_directory <-
function (directory, labels = "inferred", label_mode = "int",
    class_names = NULL, color_mode = "rgb", batch_size = 32L,
    image_size = list(256L, 256L), shuffle = TRUE, seed = NULL,
    validation_split = NULL, subset = NULL, interpolation = "bilinear",
    follow_links = FALSE, crop_to_aspect_ratio = FALSE, data_format = NULL)
{
    args <- capture_args2(list(labels = as_integer, label_mode = as_integer,
        batch_size = as_integer, seed = as_integer))
    do.call(keras$utils$image_dataset_from_directory, args)
}


#' Saves an image stored as a NumPy array to a path or file object.
#'
#' @param path
#' Path or file object.
#'
#' @param x
#' NumPy array.
#'
#' @param data_format
#' Image data format, either `"channels_first"` or
#' `"channels_last"`.
#'
#' @param file_format
#' Optional file format override. If omitted, the format to
#' use is determined from the filename extension. If a file object was
#' used instead of a filename, this parameter should always be used.
#'
#' @param scale
#' Whether to rescale image values to be within `[0, 255]`.
#'
#' @param ...
#' Additional keyword arguments passed to `PIL.Image.save()`.
#'
#' @export
#' @family image utils
#' @family utils
#' @seealso
#' + <https:/keras.io/api/data_loading/image#saveimg-function>
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/save_img>
#' @tether keras.utils.save_img
image_array_save <-
function (x, path, data_format = NULL, file_format = NULL, scale = TRUE,
    ...)
{
    args <- capture_args2(NULL)
    do.call(keras$utils$save_img, args)
}


#' Converts a 3D array to a PIL Image instance.
#'
#' @description
#'
#' # Usage
#' ```{r}
#' img <- array(runif(30000), dim = c(100, 100, 3))
#' pil_img <- image_from_array(img)
#' pil_img
#' ```
#'
#' @returns
#'     A PIL Image instance.
#'
#' @param x
#' Input data, in any form that can be converted to an array.
#'
#' @param data_format
#' Image data format, can be either `"channels_first"` or
#' `"channels_last"`. Defaults to `NULL`, in which case the global
#' setting `config_image_data_format()` is used (unless you
#' changed it, it defaults to `"channels_last"`).
#'
#' @param scale
#' Whether to rescale the image such that minimum and maximum values
#' are 0 and 255 respectively. Defaults to `TRUE`.
#'
#' @param dtype
#' Dtype to use. `NULL` means the global setting
#' `config_floatx()` is used (unless you changed it, it
#' defaults to `"float32"`). Defaults to `NULL`.
#'
#' @export
#' @family image utils
#' @family utils
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/array_to_img>
#' @tether keras.utils.array_to_img
image_from_array <-
function (x, data_format = NULL, scale = TRUE, dtype = NULL)
{
    args <- capture_args2(NULL)
    do.call(keras$utils$array_to_img, args)
}


#' Loads an image into PIL format.
#'
#' @description
#'
#' # Usage
#' ```{r}
#' image_path <- get_file(origin = "https://www.r-project.org/logo/Rlogo.png")
#' (image <- image_load(image_path))
#'
#' input_arr <- image_to_array(image)
#' str(input_arr)
#' input_arr %<>% array_reshape(dim = c(1, dim(input_arr))) # Convert single image to a batch.
#' ```
#'
#'
#' ```{r, eval = FALSE}
#' model |> predict(input_arr)
#' ```
#'
#' @returns
#'     A PIL Image instance.
#'
#' @param path
#' Path to image file.
#'
#' @param color_mode
#' One of `"grayscale"`, `"rgb"`, `"rgba"`. Default: `"rgb"`.
#' The desired image format.
#'
#' @param target_size
#' Either `NULL` (default to original size) or tuple of ints
#' `(img_height, img_width)`.
#'
#' @param interpolation
#' Interpolation method used to resample the image if the
#' target size is different from that of the loaded image. Supported
#' methods are `"nearest"`, `"bilinear"`, and `"bicubic"`.
#' If PIL version 1.1.3 or newer is installed, `"lanczos"`
#' is also supported. If PIL version 3.4.0 or newer is installed,
#' `"box"` and `"hamming"` are also
#' supported. By default, `"nearest"` is used.
#'
#' @param keep_aspect_ratio
#' Boolean, whether to resize images to a target
#' size without aspect ratio distortion. The image is cropped in
#' the center with target aspect ratio before resizing.
#'
#' @export
#' @family image utils
#' @family utils
#' @seealso
#' + <https:/keras.io/api/data_loading/image#loadimg-function>
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/load_img>
#' @tether keras.utils.load_img
image_load <-
function (path, color_mode = "rgb", target_size = NULL, interpolation = "nearest",
    keep_aspect_ratio = FALSE)
{
    args <- capture_args2(NULL)
    do.call(keras$utils$load_img, args)
}


#' Converts a PIL Image instance to a matrix.
#'
#' @description
#'
#' # Usage
#' ```{r}
#' image_path <- get_file(origin = "https://www.r-project.org/logo/Rlogo.png")
#' (img <- image_load(image_path))
#'
#' array <- image_to_array(img)
#' str(array)
#' ```
#'
#' @returns
#'     A 3D array.
#'
#' @param img
#' Input PIL Image instance.
#'
#' @param data_format
#' Image data format, can be either `"channels_first"` or
#' `"channels_last"`. Defaults to `NULL`, in which case the global
#' setting `config_image_data_format()` is used (unless you
#' changed it, it defaults to `"channels_last"`).
#'
#' @param dtype
#' Dtype to use. `NULL` means the global setting
#' `config_floatx()` is used (unless you changed it, it
#' defaults to `"float32"`).
#'
#' @export
#' @family image utils
#' @family utils
#' @seealso
#' + <https:/keras.io/api/data_loading/image#imgtoarray-function>
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/img_to_array>
#' @tether keras.utils.img_to_array
image_to_array <-
function (img, data_format = NULL, dtype = NULL)
{
    args <- capture_args2(NULL)
    do.call(keras$utils$img_to_array, args)
}


#' Convert a Keras model to dot format.
#'
#' @returns
#' A `pydot.Dot` instance representing the Keras model or
#' a `pydot.Cluster` instance representing nested model if
#' `subgraph=TRUE`.
#'
#' @param model
#' A Keras model instance.
#'
#' @param show_shapes
#' whether to display shape information.
#'
#' @param show_dtype
#' whether to display layer dtypes.
#'
#' @param show_layer_names
#' whether to display layer names.
#'
#' @param rankdir
#' `rankdir` argument passed to PyDot,
#' a string specifying the format of the plot: `"TB"`
#' creates a vertical plot; `"LR"` creates a horizontal plot.
#'
#' @param expand_nested
#' whether to expand nested Functional models
#' into clusters.
#'
#' @param dpi
#' Image resolution in dots per inch.
#'
#' @param subgraph
#' whether to return a `pydot.Cluster` instance.
#'
#' @param show_layer_activations
#' Display layer activations (only for layers that
#' have an `activation` property).
#'
#' @param show_trainable
#' whether to display if a layer is trainable.
#'
#' @param ...
#' For forward/backward compatability.
#'
#' @export
#' @family utils
#' @seealso
#' + <https:/keras.io/api/utils/model_plotting_utils#modeltodot-function>
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/model_to_dot>
#' @tether keras.utils.model_to_dot
model_to_dot <-
function (model, show_shapes = FALSE, show_dtype = FALSE, show_layer_names = TRUE,
    rankdir = "TB", expand_nested = FALSE, dpi = 200L, subgraph = FALSE,
    show_layer_activations = FALSE, show_trainable = FALSE, ...)
{
    args <- capture_args2(list(dpi = as_integer))
    do.call(keras$utils$model_to_dot, args)
}


#' Normalizes an array.
#'
#' @description
#' If the input is a NumPy array, a NumPy array will be returned.
#' If it's a backend tensor, a backend tensor will be returned.
#'
#' @returns
#'     A normalized copy of the array.
#'
#' @param x
#' Array to normalize.
#'
#' @param axis
#' axis along which to normalize.
#'
#' @param order
#' Normalization order (e.g. `order=2` for L2 norm).
#'
#' @export
#' @family numerical utils
#' @family utils
#' @seealso
#' + <https:/keras.io/api/utils/python_utils#normalize-function>
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/normalize>
#' @tether keras.utils.normalize
normalize <-
function (x, axis = -1L, order = 2L)
{
    args <- capture_args2(list(axis = as_axis, order = as_integer))
    do.call(keras$utils$normalize, args)
}


#' Converts a class vector (integers) to binary class matrix.
#'
#' @description
#' E.g. for use with `categorical_crossentropy`.
#'
#' # Examples
#' ```{r}
#' a <- to_categorical(c(0, 1, 2, 3), num_classes=4)
#' print(a)
#' ```
#'
#' ```{r}
#' b <- array(c(.9, .04, .03, .03,
#'               .3, .45, .15, .13,
#'               .04, .01, .94, .05,
#'               .12, .21, .5, .17),
#'               dim = c(4, 4))
#' loss <- op_categorical_crossentropy(a, b)
#' loss
#' ```
#'
#' ```{r}
#' loss <- op_categorical_crossentropy(a, a)
#' loss
#' ```
#'
#' @returns
#' A binary matrix representation of the input as a NumPy array. The class
#' axis is placed last.
#'
#' @param x
#' Array-like with class values to be converted into a matrix
#' (integers from 0 to `num_classes - 1`).
#'
#' @param num_classes
#' Total number of classes. If `NULL`, this would be inferred
#' as `max(x) + 1`. Defaults to `NULL`.
#'
#' @export
#' @family numerical utils
#' @family utils
#' @seealso
#' + <https:/keras.io/api/utils/python_utils#tocategorical-function>
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical>
#'
#' @tether keras.utils.to_categorical
to_categorical <-
function (x, num_classes = NULL)
{
    args <- capture_args2(list(x = as_integer_array, num_classes = as_integer))
    do.call(keras$utils$to_categorical, args)
}


#' Sets all random seeds (Python, NumPy, and backend framework, e.g. TF).
#'
#' @description
#' You can use this utility to make almost any Keras program fully
#' deterministic. Some limitations apply in cases where network communications
#' are involved (e.g. parameter server distribution), which creates additional
#' sources of randomness, or when certain non-deterministic cuDNN ops are
#' involved.
#'
#' Calling this utility is equivalent to the following in Python:
#'
#' ```
#' import random
#' import numpy as np
#' from keras.utils.module_utils import tensorflow as tf
#' random.seed(seed)
#' np.random.seed(seed)
#' tf.random.set_seed(seed)
#' ```
#'
#' Note that the TensorFlow seed is set even if you're not using TensorFlow
#' as your backend framework, since many workflows leverage `tf$data`
#' pipelines (which feature random shuffling). Likewise many workflows
#' might leverage NumPy APIs.
#'
#' @param seed
#' Integer, the random seed to use.
#'
#' @export
#' @family utils
#' @seealso
#' + <https:/keras.io/api/utils/python_utils#setrandomseed-function>
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/set_random_seed>
#'
#' @tether keras.utils.set_random_seed
set_random_seed <-
function (seed)
{
    args <- capture_args2(list(seed = as_integer))
    set.seed(args$seed)
    reticulate::py_set_seed(args$seed)
    do.call(keras$utils$set_random_seed, args)
}


#' Pads sequences to the same length.
#'
#' @description
#' This function transforms a list (of length `num_samples`)
#' of sequences (lists of integers)
#' into a 2D NumPy array of shape `(num_samples, num_timesteps)`.
#' `num_timesteps` is either the `maxlen` argument if provided,
#' or the length of the longest sequence in the list.
#'
#' Sequences that are shorter than `num_timesteps`
#' are padded with `value` until they are `num_timesteps` long.
#'
#' Sequences longer than `num_timesteps` are truncated
#' so that they fit the desired length.
#'
#' The position where padding or truncation happens is determined by
#' the arguments `padding` and `truncating`, respectively.
#' Pre-padding or removing values from the beginning of the sequence is the
#' default.
#'
#' ```{r}
#' sequence <- list(c(1), c(2, 3), c(4, 5, 6))
#' pad_sequences(sequence)
#' ```
#'
#' ```{r}
#' pad_sequences(sequence, value=-1)
#' ```
#'
#' ```{r}
#' pad_sequences(sequence, padding='post')
#' ```
#'
#' ```{r}
#' pad_sequences(sequence, maxlen=2)
#' ```
#'
#' @returns
#'     Array with shape `(len(sequences), maxlen)`
#'
#' @param sequences
#' List of sequences (each sequence is a list of integers).
#'
#' @param maxlen
#' Optional Int, maximum length of all sequences. If not provided,
#' sequences will be padded to the length of the longest individual
#' sequence.
#'
#' @param dtype
#' (Optional, defaults to `"int32"`). Type of the output sequences.
#' To pad sequences with variable length strings, you can use `object`.
#'
#' @param padding
#' String, "pre" or "post" (optional, defaults to `"pre"`):
#' pad either before or after each sequence.
#'
#' @param truncating
#' String, "pre" or "post" (optional, defaults to `"pre"`):
#' remove values from sequences larger than
#' `maxlen`, either at the beginning or at the end of the sequences.
#'
#' @param value
#' Float or String, padding value. (Optional, defaults to 0.)
#'
#' @export
#' @family utils
#' @seealso
#' + <https:/keras.io/keras_core/api/data_loading/timeseries#padsequences-function>
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences>
#'
#' @tether keras.utils.pad_sequences
pad_sequences <-
function (sequences, maxlen = NULL, dtype = "int32", padding = "pre",
    truncating = "pre", value = 0)
{
    args <- capture_args2(list(maxlen = as_integer, sequences = function (x)
    lapply(x, as.list)))
    do.call(keras$utils$pad_sequences, args)
}


#' Generates a `tf.data.Dataset` from text files in a directory.
#'
#' @description
#' If your directory structure is:
#'
#' ```
#' main_directory/
#' ...class_a/
#' ......a_text_1.txt
#' ......a_text_2.txt
#' ...class_b/
#' ......b_text_1.txt
#' ......b_text_2.txt
#' ```
#'
#' Then calling `text_dataset_from_directory(main_directory,
#' labels='inferred')` will return a `tf.data.Dataset` that yields batches of
#' texts from the subdirectories `class_a` and `class_b`, together with labels
#' 0 and 1 (0 corresponding to `class_a` and 1 corresponding to `class_b`).
#'
#' Only `.txt` files are supported at this time.
#'
#' @returns
#' A `tf.data.Dataset` object.
#'
#' - If `label_mode` is `None`, it yields `string` tensors of shape
#'     `(batch_size,)`, containing the contents of a batch of text files.
#' - Otherwise, it yields a tuple `(texts, labels)`, where `texts`
#'     has shape `(batch_size,)` and `labels` follows the format described
#'     below.
#'
#' Rules regarding labels format:
#'
#' - if `label_mode` is `int`, the labels are an `int32` tensor of shape
#'     `(batch_size,)`.
#' - if `label_mode` is `binary`, the labels are a `float32` tensor of
#'     1s and 0s of shape `(batch_size, 1)`.
#' - if `label_mode` is `categorical`, the labels are a `float32` tensor
#'     of shape `(batch_size, num_classes)`, representing a one-hot
#'     encoding of the class index.
#'
#' @param directory
#' Directory where the data is located.
#' If `labels` is `"inferred"`, it should contain
#' subdirectories, each containing text files for a class.
#' Otherwise, the directory structure is ignored.
#'
#' @param labels
#' Either `"inferred"`
#' (labels are generated from the directory structure),
#' `None` (no labels),
#' or a list/tuple of integer labels of the same size as the number of
#' text files found in the directory. Labels should be sorted according
#' to the alphanumeric order of the text file paths
#' (obtained via `os.walk(directory)` in Python).
#'
#' @param label_mode
#' String describing the encoding of `labels`. Options are:
#' - `"int"`: means that the labels are encoded as integers
#'     (e.g. for `sparse_categorical_crossentropy` loss).
#' - `"categorical"` means that the labels are
#'     encoded as a categorical vector
#'     (e.g. for `categorical_crossentropy` loss).
#' - `"binary"` means that the labels (there can be only 2)
#'     are encoded as `float32` scalars with values 0 or 1
#'     (e.g. for `binary_crossentropy`).
#' - `None` (no labels).
#'
#' @param class_names
#' Only valid if `"labels"` is `"inferred"`.
#' This is the explicit list of class names
#' (must match names of subdirectories). Used to control the order
#' of the classes (otherwise alphanumerical order is used).
#'
#' @param batch_size
#' Size of the batches of data. Defaults to 32.
#' If `None`, the data will not be batched
#' (the dataset will yield individual samples).
#'
#' @param max_length
#' Maximum size of a text string. Texts longer than this will
#' be truncated to `max_length`.
#'
#' @param shuffle
#' Whether to shuffle the data. Defaults to `TRUE`.
#' If set to `False`, sorts the data in alphanumeric order.
#'
#' @param seed
#' Optional random seed for shuffling and transformations.
#'
#' @param validation_split
#' Optional float between 0 and 1,
#' fraction of data to reserve for validation.
#'
#' @param subset
#' Subset of the data to return.
#' One of `"training"`, `"validation"` or `"both"`.
#' Only used if `validation_split` is set.
#' When `subset="both"`, the utility returns a tuple of two datasets
#' (the training and validation datasets respectively).
#'
#' @param follow_links
#' Whether to visits subdirectories pointed to by symlinks.
#' Defaults to `False`.
#'
#' @export
#' @family dataset utils
#' @family text dataset utils
#' @family utils
#' @family preprocessing
#' @seealso
#' + <https:/keras.io/api/data_loading/text#textdatasetfromdirectory-function>
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/text_dataset_from_directory>
#' @tether keras.utils.text_dataset_from_directory
text_dataset_from_directory <-
function (directory, labels = "inferred", label_mode = "int",
    class_names = NULL, batch_size = 32L, max_length = NULL,
    shuffle = TRUE, seed = NULL, validation_split = NULL, subset = NULL,
    follow_links = FALSE)
{
    args <- capture_args2(list(labels = as_integer, label_mode = as_integer,
        batch_size = as_integer, seed = as_integer))
    do.call(keras$utils$text_dataset_from_directory, args)
}


#' Creates a dataset of sliding windows over a timeseries provided as array.
#'
#' @description
#' This function takes in a sequence of data-points gathered at
#' equal intervals, along with time series parameters such as
#' length of the sequences/windows, spacing between two sequence/windows, etc.,
#' to produce batches of timeseries inputs and targets.
#'
#' @returns
#' A `tf$data$Dataset` instance. If `targets` was passed, the dataset yields
#' list `(batch_of_sequences, batch_of_targets)`. If not, the dataset yields
#' only `batch_of_sequences`.
#'
#' Example 1:
#'
#' Consider indices `[0, 1, ... 98]`.
#' With `sequence_length=10,  sampling_rate=2, sequence_stride=3`,
#' `shuffle=FALSE`, the dataset will yield batches of sequences
#' composed of the following indices:
#'
#' ```
#' First sequence:  [0  2  4  6  8 10 12 14 16 18]
#' Second sequence: [3  5  7  9 11 13 15 17 19 21]
#' Third sequence:  [6  8 10 12 14 16 18 20 22 24]
#' ...
#' Last sequence:   [78 80 82 84 86 88 90 92 94 96]
#' ```
#'
#' In this case the last 2 data points are discarded since no full sequence
#' can be generated to include them (the next sequence would have started
#' at index 81, and thus its last step would have gone over 98).
#'
#' Example 2: Temporal regression.
#'
#' Consider an array `data` of scalar values, of shape `(steps,)`.
#' To generate a dataset that uses the past 10
#' timesteps to predict the next timestep, you would use:
#'
#' ```{r}
#' data <- op_array(1:20)
#' input_data <- data[1:10]
#' targets <- data[11:20]
#' dataset <- timeseries_dataset_from_array(
#'   input_data, targets, sequence_length=10)
#' iter <- reticulate::as_iterator(dataset)
#' reticulate::iter_next(iter)
#' ```
#'
#' Example 3: Temporal regression for many-to-many architectures.
#'
#' Consider two arrays of scalar values `X` and `Y`,
#' both of shape `(100,)`. The resulting dataset should consist samples with
#' 20 timestamps each. The samples should not overlap.
#' To generate a dataset that uses the current timestamp
#' to predict the corresponding target timestep, you would use:
#'
#' ```{r}
#' X <- op_array(1:100)
#' Y <- X*2
#'
#' sample_length <- 20
#' input_dataset <- timeseries_dataset_from_array(
#'     X, NULL, sequence_length=sample_length, sequence_stride=sample_length)
#' target_dataset <- timeseries_dataset_from_array(
#'     Y, NULL, sequence_length=sample_length, sequence_stride=sample_length)
#'
#'
#' inputs <- reticulate::as_iterator(input_dataset) %>% reticulate::iter_next()
#' targets <- reticulate::as_iterator(target_dataset) %>% reticulate::iter_next()
#' ```
#'
#' @param data
#' array or eager tensor
#' containing consecutive data points (timesteps).
#' The first dimension is expected to be the time dimension.
#'
#' @param targets
#' Targets corresponding to timesteps in `data`.
#' `targets[i]` should be the target
#' corresponding to the window that starts at index `i`
#' (see example 2 below).
#' Pass `NULL` if you don't have target data (in this case the dataset
#' will only yield the input data).
#'
#' @param sequence_length
#' Length of the output sequences
#' (in number of timesteps).
#'
#' @param sequence_stride
#' Period between successive output sequences.
#' For stride `s`, output samples would
#' start at index `data[i]`, `data[i + s]`, `data[i + 2 * s]`, etc.
#'
#' @param sampling_rate
#' Period between successive individual timesteps
#' within sequences. For rate `r`, timesteps
#' `data[i], data[i + r], ... data[i + sequence_length]`
#' are used for creating a sample sequence.
#'
#' @param batch_size
#' Number of timeseries samples in each batch
#' (except maybe the last one). If `NULL`, the data will not be batched
#' (the dataset will yield individual samples).
#'
#' @param shuffle
#' Whether to shuffle output samples,
#' or instead draw them in chronological order.
#'
#' @param seed
#' Optional int; random seed for shuffling.
#'
#' @param start_index
#' Optional int; data points earlier (exclusive)
#' than `start_index` will not be used
#' in the output sequences. This is useful to reserve part of the
#' data for test or validation.
#'
#' @param end_index
#' Optional int; data points later (exclusive) than `end_index`
#' will not be used in the output sequences.
#' This is useful to reserve part of the data for test or validation.
#'
#' @export
#' @family dataset utils
#' @family timesery dataset utils
#' @family utils
#' @family preprocessing
#' @seealso
#' + <https:/keras.io/api/data_loading/timeseries#timeseriesdatasetfromarray-function>
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/utils/timeseries_dataset_from_array>
#'
#' @tether keras.utils.timeseries_dataset_from_array
timeseries_dataset_from_array <-
function (data, targets, sequence_length, sequence_stride = 1L,
    sampling_rate = 1L, batch_size = 128L, shuffle = FALSE, seed = NULL,
    start_index = NULL, end_index = NULL)
{
    args <- capture_args2(list(sequence_stride = as_integer,
        sampling_rate = as_integer, batch_size = as_integer,
        seed = as_integer, start_index = as_integer, end_index = as_integer))
    do.call(keras$utils$timeseries_dataset_from_array, args)
}
