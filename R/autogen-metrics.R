## Autogenerated. Do not modify manually.


# keras.metrics.binary_focal_crossentropy
# keras.src.losses.losses.binary_focal_crossentropy
r"-(Computes the binary focal crossentropy loss.

    According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it
    helps to apply a focal factor to down-weight easy examples and focus more on
    hard examples. By default, the focal tensor is computed as follows:

    `focal_factor = (1 - output) ** gamma` for class 1
    `focal_factor = output ** gamma` for class 0
    where `gamma` is a focusing parameter. When `gamma` = 0, there is no focal
    effect on the binary crossentropy loss.

    If `apply_class_balancing == True`, this function also takes into account a
    weight balancing factor for the binary classes 0 and 1 as follows:

    `weight = alpha` for class 1 (`target == 1`)
    `weight = 1 - alpha` for class 0
    where `alpha` is a float in the range of `[0, 1]`.

    Args:
        y_true: Ground truth values, of shape `(batch_size, d0, .. dN)`.
        y_pred: The predicted values, of shape `(batch_size, d0, .. dN)`.
        apply_class_balancing: A bool, whether to apply weight balancing on the
            binary classes 0 and 1.
        alpha: A weight balancing factor for class 1, default is `0.25` as
            mentioned in the reference. The weight for class 0 is `1.0 - alpha`.
        gamma: A focusing parameter, default is `2.0` as mentioned in the
            reference.
        from_logits: Whether `y_pred` is expected to be a logits tensor. By
            default, we assume that `y_pred` encodes a probability distribution.
        label_smoothing: Float in `[0, 1]`. If > `0` then smooth the labels by
            squeezing them towards 0.5, that is,
            using `1. - 0.5 * label_smoothing` for the target class
            and `0.5 * label_smoothing` for the non-target class.
        axis: The axis along which the mean is computed. Defaults to `-1`.

    Returns:
        Binary focal crossentropy loss value
        with shape = `[batch_size, d0, .. dN-1]`.

    Example:

    >>> y_true = [[0, 1], [0, 0]]
    >>> y_pred = [[0.6, 0.4], [0.4, 0.6]]
    >>> loss = keras.losses.binary_focal_crossentropy(
    ...        y_true, y_pred, gamma=2)
    >>> assert loss.shape == (2,)
    >>> loss
    array([0.330, 0.206], dtype=float32)
    )-"
#' Computes the binary focal crossentropy loss.
#'
#' @description
#' According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it
#' helps to apply a focal factor to down-weight easy examples and focus more on
#' hard examples. By default, the focal tensor is computed as follows:
#'
#' `focal_factor = (1 - output) ** gamma` for class 1
#' `focal_factor = output ** gamma` for class 0
#' where `gamma` is a focusing parameter. When `gamma` = 0, there is no focal
#' effect on the binary crossentropy loss.
#'
#' If `apply_class_balancing == True`, this function also takes into account a
#' weight balancing factor for the binary classes 0 and 1 as follows:
#'
#' `weight = alpha` for class 1 (`target == 1`)
#' `weight = 1 - alpha` for class 0
#' where `alpha` is a float in the range of `[0, 1]`.
#'
#' # Returns
#' Binary focal crossentropy loss value
#' with shape = `[batch_size, d0, .. dN-1]`.
#'
#' # Examples
#' ```python
#' y_true = [[0, 1], [0, 0]]
#' y_pred = [[0.6, 0.4], [0.4, 0.6]]
#' loss = keras.losses.binary_focal_crossentropy(
#'        y_true, y_pred, gamma=2)
#' assert loss.shape == (2,)
#' loss
#' # array([0.330, 0.206], dtype=float32)
#' ```
#'
#' @param y_true Ground truth values, of shape `(batch_size, d0, .. dN)`.
#' @param y_pred The predicted values, of shape `(batch_size, d0, .. dN)`.
#' @param apply_class_balancing A bool, whether to apply weight balancing on the
#'     binary classes 0 and 1.
#' @param alpha A weight balancing factor for class 1, default is `0.25` as
#'     mentioned in the reference. The weight for class 0 is `1.0 - alpha`.
#' @param gamma A focusing parameter, default is `2.0` as mentioned in the
#'     reference.
#' @param from_logits Whether `y_pred` is expected to be a logits tensor. By
#'     default, we assume that `y_pred` encodes a probability distribution.
#' @param label_smoothing Float in `[0, 1]`. If > `0` then smooth the labels by
#'     squeezing them towards 0.5, that is,
#'     using `1. - 0.5 * label_smoothing` for the target class
#'     and `0.5 * label_smoothing` for the non-target class.
#' @param axis The axis along which the mean is computed. Defaults to `-1`.
#'
#' @export
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/binary_focal_crossentropy>
metric_binary_focal_crossentropy <-
function (y_true, y_pred, apply_class_balancing = FALSE, alpha = 0.25,
    gamma = 2, from_logits = FALSE, label_smoothing = 0, axis = -1L)
{
    args <- capture_args2(list(axis = as_axis))
    do.call(keras$metrics$binary_focal_crossentropy, args)
}


# keras.metrics.categorical_focal_crossentropy
# keras.src.losses.losses.categorical_focal_crossentropy
r"-(Computes the categorical focal crossentropy loss.

    Args:
        y_true: Tensor of one-hot true targets.
        y_pred: Tensor of predicted targets.
        alpha: A weight balancing factor for all classes, default is `0.25` as
            mentioned in the reference. It can be a list of floats or a scalar.
            In the multi-class case, alpha may be set by inverse class
            frequency by using `compute_class_weight` from `sklearn.utils`.
        gamma: A focusing parameter, default is `2.0` as mentioned in the
            reference. It helps to gradually reduce the importance given to
            simple examples in a smooth manner. When `gamma` = 0, there is
            no focal effect on the categorical crossentropy.
        from_logits: Whether `y_pred` is expected to be a logits tensor. By
            default, we assume that `y_pred` encodes a probability
            distribution.
        label_smoothing: Float in [0, 1]. If > `0` then smooth the labels. For
            example, if `0.1`, use `0.1 / num_classes` for non-target labels
            and `0.9 + 0.1 / num_classes` for target labels.
        axis: Defaults to `-1`. The dimension along which the entropy is
            computed.

    Returns:
        Categorical focal crossentropy loss value.

    Example:

    >>> y_true = [[0, 1, 0], [0, 0, 1]]
    >>> y_pred = [[0.05, 0.9, 0.05], [0.1, 0.85, 0.05]]
    >>> loss = keras.losses.categorical_focal_crossentropy(y_true, y_pred)
    >>> assert loss.shape == (2,)
    >>> loss
    array([2.63401289e-04, 6.75912094e-01], dtype=float32)
    )-"
#' Computes the categorical focal crossentropy loss.
#'
#' @description
#'
#' # Returns
#' Categorical focal crossentropy loss value.
#'
#' # Examples
#' ```python
#' y_true = [[0, 1, 0], [0, 0, 1]]
#' y_pred = [[0.05, 0.9, 0.05], [0.1, 0.85, 0.05]]
#' loss = keras.losses.categorical_focal_crossentropy(y_true, y_pred)
#' assert loss.shape == (2,)
#' loss
#' # array([2.63401289e-04, 6.75912094e-01], dtype=float32)
#' ```
#'
#' @param y_true Tensor of one-hot true targets.
#' @param y_pred Tensor of predicted targets.
#' @param alpha A weight balancing factor for all classes, default is `0.25` as
#'     mentioned in the reference. It can be a list of floats or a scalar.
#'     In the multi-class case, alpha may be set by inverse class
#'     frequency by using `compute_class_weight` from `sklearn.utils`.
#' @param gamma A focusing parameter, default is `2.0` as mentioned in the
#'     reference. It helps to gradually reduce the importance given to
#'     simple examples in a smooth manner. When `gamma` = 0, there is
#'     no focal effect on the categorical crossentropy.
#' @param from_logits Whether `y_pred` is expected to be a logits tensor. By
#'     default, we assume that `y_pred` encodes a probability
#'     distribution.
#' @param label_smoothing Float in `[0, 1].` If > `0` then smooth the labels. For
#'     example, if `0.1`, use `0.1 / num_classes` for non-target labels
#'     and `0.9 + 0.1 / num_classes` for target labels.
#' @param axis Defaults to `-1`. The dimension along which the entropy is
#'     computed.
#'
#' @export
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/categorical_focal_crossentropy>
metric_categorical_focal_crossentropy <-
function (y_true, y_pred, alpha = 0.25, gamma = 2, from_logits = FALSE,
    label_smoothing = 0, axis = -1L)
{
    args <- capture_args2(list(axis = as_axis))
    do.call(keras$metrics$categorical_focal_crossentropy, args)
}


# keras.metrics.huber
# keras.src.losses.losses.huber
r"-(Computes Huber loss value.

    Formula:
    ```python
    for x in error:
        if abs(x) <= delta:
            loss.append(0.5 * x^2)
        elif abs(x) > delta:
            loss.append(delta * abs(x) - 0.5 * delta^2)

    loss = mean(loss, axis=-1)
    ```
    See: [Huber loss](https://en.wikipedia.org/wiki/Huber_loss).

    Example:

    >>> y_true = [[0, 1], [0, 0]]
    >>> y_pred = [[0.6, 0.4], [0.4, 0.6]]
    >>> loss = keras.losses.huber(y_true, y_pred)
    0.155


    Args:
        y_true: tensor of true targets.
        y_pred: tensor of predicted targets.
        delta: A float, the point where the Huber loss function changes from a
            quadratic to linear. Defaults to `1.0`.

    Returns:
        Tensor with one scalar loss entry per sample.
    )-"
#' Computes Huber loss value.
#'
#' @description
#' Formula:
#' ```python
#' for x in error:
#'     if abs(x) <= delta:
#'         loss.append(0.5 * x^2)
#'     elif abs(x) > delta:
#'         loss.append(delta * abs(x) - 0.5 * delta^2)
#'
#' loss = mean(loss, axis=-1)
#' ```
#' See: [Huber loss](https://en.wikipedia.org/wiki/Huber_loss).
#'
#' # Examples
#' ```python
#' y_true = [[0, 1], [0, 0]]
#' y_pred = [[0.6, 0.4], [0.4, 0.6]]
#' loss = keras.losses.huber(y_true, y_pred)
#' # 0.155
#' ```
#'
#' # Returns
#'     Tensor with one scalar loss entry per sample.
#'
#' @param y_true tensor of true targets.
#' @param y_pred tensor of predicted targets.
#' @param delta A float, the point where the Huber loss function changes from a
#'     quadratic to linear. Defaults to `1.0`.
#'
#' @export
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/huber>
metric_huber <-
function (y_true, y_pred, delta = 1)
{
    args <- capture_args2(NULL)
    do.call(keras$metrics$huber, args)
}


# keras.metrics.log_cosh
# keras.src.losses.losses.log_cosh
r"-(Logarithm of the hyperbolic cosine of the prediction error.

    Formula:
    ```python
    loss = mean(log(cosh(y_pred - y_true)), axis=-1)
    ```

    Note that `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small
    `x` and to `abs(x) - log(2)` for large `x`. This means that 'logcosh' works
    mostly like the mean squared error, but will not be so strongly affected by
    the occasional wildly incorrect prediction.

    Example:

    >>> y_true = [[0., 1.], [0., 0.]]
    >>> y_pred = [[1., 1.], [0., 0.]]
    >>> loss = keras.losses.log_cosh(y_true, y_pred)
    0.108

    Args:
        y_true: Ground truth values with shape = `[batch_size, d0, .. dN]`.
        y_pred: The predicted values with shape = `[batch_size, d0, .. dN]`.

    Returns:
        Logcosh error values with shape = `[batch_size, d0, .. dN-1]`.
    )-"
#' Logarithm of the hyperbolic cosine of the prediction error.
#'
#' @description
#' Formula:
#' ```python
#' loss = mean(log(cosh(y_pred - y_true)), axis=-1)
#' ```
#'
#' Note that `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small
#' `x` and to `abs(x) - log(2)` for large `x`. This means that 'logcosh' works
#' mostly like the mean squared error, but will not be so strongly affected by
#' the occasional wildly incorrect prediction.
#'
#' # Examples
#' ```python
#' y_true = [[0., 1.], [0., 0.]]
#' y_pred = [[1., 1.], [0., 0.]]
#' loss = keras.losses.log_cosh(y_true, y_pred)
#' # 0.108
#' ```
#'
#' # Returns
#'     Logcosh error values with shape = `[batch_size, d0, .. dN-1]`.
#'
#' @param y_true Ground truth values with shape = `[batch_size, d0, .. dN]`.
#' @param y_pred The predicted values with shape = `[batch_size, d0, .. dN]`.
#'
#' @export
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/log_cosh>
metric_log_cosh <-
function (y_true, y_pred)
{
    args <- capture_args2(NULL)
    do.call(keras$metrics$log_cosh, args)
}


# keras.metrics.BinaryAccuracy
# keras.src.metrics.accuracy_metrics.BinaryAccuracy
r"-(Calculates how often predictions match binary labels.

    This metric creates two local variables, `total` and `count` that are used
    to compute the frequency with which `y_pred` matches `y_true`. This
    frequency is ultimately returned as `binary accuracy`: an idempotent
    operation that simply divides `total` by `count`.

    If `sample_weight` is `None`, weights default to 1.
    Use `sample_weight` of 0 to mask values.

    Args:
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.
        threshold: (Optional) Float representing the threshold for deciding
        whether prediction values are 1 or 0.

    Standalone usage:

    >>> m = keras.metrics.BinaryAccuracy()
    >>> m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]])
    >>> m.result()
    0.75

    >>> m.reset_state()
    >>> m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]],
    ...                sample_weight=[1, 0, 0, 1])
    >>> m.result()
    0.5

    Usage with `compile()` API:

    ```python
    model.compile(optimizer='sgd',
                  loss='binary_crossentropy',
                  metrics=[keras.metrics.BinaryAccuracy()])
    ```
    )-"
#' Calculates how often predictions match binary labels.
#'
#' @description
#' This metric creates two local variables, `total` and `count` that are used
#' to compute the frequency with which `y_pred` matches `y_true`. This
#' frequency is ultimately returned as `binary accuracy`: an idempotent
#' operation that simply divides `total` by `count`.
#'
#' If `sample_weight` is `None`, weights default to 1.
#' Use `sample_weight` of 0 to mask values.
#'
#' # Usage
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.BinaryAccuracy()
#' m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]])
#' m.result()
#' # 0.75
#' ```
#'
#' ```python
#' m.reset_state()
#' m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]],
#'                sample_weight=[1, 0, 0, 1])
#' m.result()
#' # 0.5
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(optimizer='sgd',
#'               loss='binary_crossentropy',
#'               metrics=[keras.metrics.BinaryAccuracy()])
#' ```
#'
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param threshold (Optional) Float representing the threshold for deciding
#' whether prediction values are 1 or 0.
#' @param y_true Tensor of true targets.
#' @param y_pred Tensor of predicted targets.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryAccuracy>
metric_binary_accuracy <-
structure(function (y_true, y_pred, threshold = 0.5, ..., name = "binary_accuracy",
    dtype = NULL)
{
    args <- capture_args2(NULL)
    callable <- if (missing(y_true) && missing(y_pred))
        keras$metrics$BinaryAccuracy
    else keras$metrics$binary_accuracy
    do.call(callable, args)
}, py_function_name = "binary_accuracy")


# keras.metrics.CategoricalAccuracy
# keras.src.metrics.accuracy_metrics.CategoricalAccuracy
r"-(Calculates how often predictions match one-hot labels.

    You can provide logits of classes as `y_pred`, since argmax of
    logits and probabilities are same.

    This metric creates two local variables, `total` and `count` that are used
    to compute the frequency with which `y_pred` matches `y_true`. This
    frequency is ultimately returned as `categorical accuracy`: an idempotent
    operation that simply divides `total` by `count`.

    `y_pred` and `y_true` should be passed in as vectors of probabilities,
    rather than as labels. If necessary, use `ops.one_hot` to expand `y_true` as
    a vector.

    If `sample_weight` is `None`, weights default to 1.
    Use `sample_weight` of 0 to mask values.

    Args:
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Standalone usage:

    >>> m = keras.metrics.CategoricalAccuracy()
    >>> m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8],
    ...                 [0.05, 0.95, 0]])
    >>> m.result()
    0.5

    >>> m.reset_state()
    >>> m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8],
    ...                 [0.05, 0.95, 0]],
    ...                sample_weight=[0.7, 0.3])
    >>> m.result()
    0.3

    Usage with `compile()` API:

    ```python
    model.compile(optimizer='sgd',
                  loss='categorical_crossentropy',
                  metrics=[keras.metrics.CategoricalAccuracy()])
    ```
    )-"
#' Calculates how often predictions match one-hot labels.
#'
#' @description
#' You can provide logits of classes as `y_pred`, since argmax of
#' logits and probabilities are same.
#'
#' This metric creates two local variables, `total` and `count` that are used
#' to compute the frequency with which `y_pred` matches `y_true`. This
#' frequency is ultimately returned as `categorical accuracy`: an idempotent
#' operation that simply divides `total` by `count`.
#'
#' `y_pred` and `y_true` should be passed in as vectors of probabilities,
#' rather than as labels. If necessary, use `ops.one_hot` to expand `y_true` as
#' a vector.
#'
#' If `sample_weight` is `None`, weights default to 1.
#' Use `sample_weight` of 0 to mask values.
#'
#' # Usage
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.CategoricalAccuracy()
#' m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8],
#'                 [0.05, 0.95, 0]])
#' m.result()
#' # 0.5
#' ```
#'
#' ```python
#' m.reset_state()
#' m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8],
#'                 [0.05, 0.95, 0]],
#'                sample_weight=[0.7, 0.3])
#' m.result()
#' # 0.3
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(optimizer='sgd',
#'               loss='categorical_crossentropy',
#'               metrics=[keras.metrics.CategoricalAccuracy()])
#' ```
#'
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param y_true Tensor of true targets.
#' @param y_pred Tensor of predicted targets.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/CategoricalAccuracy>
metric_categorical_accuracy <-
structure(function (y_true, y_pred, ..., name = "categorical_accuracy",
    dtype = NULL)
{
    args <- capture_args2(NULL)
    callable <- if (missing(y_true) && missing(y_pred))
        keras$metrics$CategoricalAccuracy
    else keras$metrics$categorical_accuracy
    do.call(callable, args)
}, py_function_name = "categorical_accuracy")


# keras.metrics.SparseCategoricalAccuracy
# keras.src.metrics.accuracy_metrics.SparseCategoricalAccuracy
r"-(Calculates how often predictions match integer labels.

    ```python
    acc = np.dot(sample_weight, np.equal(y_true, np.argmax(y_pred, axis=1))
    ```

    You can provide logits of classes as `y_pred`, since argmax of
    logits and probabilities are same.

    This metric creates two local variables, `total` and `count` that are used
    to compute the frequency with which `y_pred` matches `y_true`. This
    frequency is ultimately returned as `sparse categorical accuracy`: an
    idempotent operation that simply divides `total` by `count`.

    If `sample_weight` is `None`, weights default to 1.
    Use `sample_weight` of 0 to mask values.

    Args:
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Standalone usage:

    >>> m = keras.metrics.SparseCategoricalAccuracy()
    >>> m.update_state([[2], [1]], [[0.1, 0.6, 0.3], [0.05, 0.95, 0]])
    >>> m.result()
    0.5

    >>> m.reset_state()
    >>> m.update_state([[2], [1]], [[0.1, 0.6, 0.3], [0.05, 0.95, 0]],
    ...                sample_weight=[0.7, 0.3])
    >>> m.result()
    0.3

    Usage with `compile()` API:

    ```python
    model.compile(optimizer='sgd',
                  loss='sparse_categorical_crossentropy',
                  metrics=[keras.metrics.SparseCategoricalAccuracy()])
    ```
    )-"
#' Calculates how often predictions match integer labels.
#'
#' @description
#' ```python
#' acc = np.dot(sample_weight, np.equal(y_true, np.argmax(y_pred, axis=1))
#' ```
#'
#' You can provide logits of classes as `y_pred`, since argmax of
#' logits and probabilities are same.
#'
#' This metric creates two local variables, `total` and `count` that are used
#' to compute the frequency with which `y_pred` matches `y_true`. This
#' frequency is ultimately returned as `sparse categorical accuracy`: an
#' idempotent operation that simply divides `total` by `count`.
#'
#' If `sample_weight` is `None`, weights default to 1.
#' Use `sample_weight` of 0 to mask values.
#'
#' # Usage
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.SparseCategoricalAccuracy()
#' m.update_state([[2], [1]], [[0.1, 0.6, 0.3], [0.05, 0.95, 0]])
#' m.result()
#' # 0.5
#' ```
#'
#' ```python
#' m.reset_state()
#' m.update_state([[2], [1]], [[0.1, 0.6, 0.3], [0.05, 0.95, 0]],
#'                sample_weight=[0.7, 0.3])
#' m.result()
#' # 0.3
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(optimizer='sgd',
#'               loss='sparse_categorical_crossentropy',
#'               metrics=[keras.metrics.SparseCategoricalAccuracy()])
#' ```
#'
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param y_true Tensor of true targets.
#' @param y_pred Tensor of predicted targets.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SparseCategoricalAccuracy>
metric_sparse_categorical_accuracy <-
structure(function (y_true, y_pred, ..., name = "sparse_categorical_accuracy",
    dtype = NULL)
{
    args <- capture_args2(NULL)
    callable <- if (missing(y_true) && missing(y_pred))
        keras$metrics$SparseCategoricalAccuracy
    else keras$metrics$sparse_categorical_accuracy
    do.call(callable, args)
}, py_function_name = "sparse_categorical_accuracy")


# keras.metrics.SparseTopKCategoricalAccuracy
# keras.src.metrics.accuracy_metrics.SparseTopKCategoricalAccuracy
r"-(Computes how often integer targets are in the top `K` predictions.

    Args:
        k: (Optional) Number of top elements to look at for computing accuracy.
            Defaults to `5`.
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Standalone usage:

    >>> m = keras.metrics.SparseTopKCategoricalAccuracy(k=1)
    >>> m.update_state([2, 1], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])
    >>> m.result()
    0.5

    >>> m.reset_state()
    >>> m.update_state([2, 1], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]],
    ...                sample_weight=[0.7, 0.3])
    >>> m.result()
    0.3

    Usage with `compile()` API:

    ```python
    model.compile(optimizer='sgd',
                  loss='sparse_categorical_crossentropy',
                  metrics=[keras.metrics.SparseTopKCategoricalAccuracy()])
    ```
    )-"
#' Computes how often integer targets are in the top `K` predictions.
#'
#' @description
#'
#' # Usage
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.SparseTopKCategoricalAccuracy(k=1)
#' m.update_state([2, 1], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])
#' m.result()
#' # 0.5
#' ```
#'
#' ```python
#' m.reset_state()
#' m.update_state([2, 1], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]],
#'                sample_weight=[0.7, 0.3])
#' m.result()
#' # 0.3
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(optimizer='sgd',
#'               loss='sparse_categorical_crossentropy',
#'               metrics=[keras.metrics.SparseTopKCategoricalAccuracy()])
#' ```
#'
#' @param k (Optional) Number of top elements to look at for computing accuracy.
#'     Defaults to `5`.
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param y_true Tensor of true targets.
#' @param y_pred Tensor of predicted targets.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SparseTopKCategoricalAccuracy>
metric_sparse_top_k_categorical_accuracy <-
structure(function (y_true, y_pred, k = 5L, ..., name = "sparse_top_k_categorical_accuracy",
    dtype = NULL)
{
    args <- capture_args2(list(k = as_integer))
    callable <- if (missing(y_true) && missing(y_pred))
        keras$metrics$SparseTopKCategoricalAccuracy
    else keras$metrics$sparse_top_k_categorical_accuracy
    do.call(callable, args)
}, py_function_name = "sparse_top_k_categorical_accuracy")


# keras.metrics.TopKCategoricalAccuracy
# keras.src.metrics.accuracy_metrics.TopKCategoricalAccuracy
r"-(Computes how often targets are in the top `K` predictions.

    Args:
        k: (Optional) Number of top elements to look at for computing accuracy.
            Defaults to `5`.
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Standalone usage:

    >>> m = keras.metrics.TopKCategoricalAccuracy(k=1)
    >>> m.update_state([[0, 0, 1], [0, 1, 0]],
    ...                [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])
    >>> m.result()
    0.5

    >>> m.reset_state()
    >>> m.update_state([[0, 0, 1], [0, 1, 0]],
    ...                [[0.1, 0.9, 0.8], [0.05, 0.95, 0]],
    ...                sample_weight=[0.7, 0.3])
    >>> m.result()
    0.3

    Usage with `compile()` API:

    ```python
    model.compile(optimizer='sgd',
                  loss='categorical_crossentropy',
                  metrics=[keras.metrics.TopKCategoricalAccuracy()])
    ```
    )-"
#' Computes how often targets are in the top `K` predictions.
#'
#' @description
#'
#' # Usage
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.TopKCategoricalAccuracy(k=1)
#' m.update_state([[0, 0, 1], [0, 1, 0]],
#'                [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])
#' m.result()
#' # 0.5
#' ```
#'
#' ```python
#' m.reset_state()
#' m.update_state([[0, 0, 1], [0, 1, 0]],
#'                [[0.1, 0.9, 0.8], [0.05, 0.95, 0]],
#'                sample_weight=[0.7, 0.3])
#' m.result()
#' # 0.3
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(optimizer='sgd',
#'               loss='categorical_crossentropy',
#'               metrics=[keras.metrics.TopKCategoricalAccuracy()])
#' ```
#'
#' @param k (Optional) Number of top elements to look at for computing accuracy.
#'     Defaults to `5`.
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param y_true Tensor of true targets.
#' @param y_pred Tensor of predicted targets.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/TopKCategoricalAccuracy>
metric_top_k_categorical_accuracy <-
structure(function (y_true, y_pred, k = 5L, ..., name = "top_k_categorical_accuracy",
    dtype = NULL)
{
    args <- capture_args2(list(k = as_integer))
    callable <- if (missing(y_true) && missing(y_pred))
        keras$metrics$TopKCategoricalAccuracy
    else keras$metrics$top_k_categorical_accuracy
    do.call(callable, args)
}, py_function_name = "top_k_categorical_accuracy")


# keras.metrics.AUC
# keras.src.metrics.confusion_metrics.AUC
r"-(Approximates the AUC (Area under the curve) of the ROC or PR curves.

    The AUC (Area under the curve) of the ROC (Receiver operating
    characteristic; default) or PR (Precision Recall) curves are quality
    measures of binary classifiers. Unlike the accuracy, and like cross-entropy
    losses, ROC-AUC and PR-AUC evaluate all the operational points of a model.

    This class approximates AUCs using a Riemann sum. During the metric
    accumulation phrase, predictions are accumulated within predefined buckets
    by value. The AUC is then computed by interpolating per-bucket averages.
    These buckets define the evaluated operational points.

    This metric creates four local variables, `true_positives`,
    `true_negatives`, `false_positives` and `false_negatives` that are used to
    compute the AUC.  To discretize the AUC curve, a linearly spaced set of
    thresholds is used to compute pairs of recall and precision values. The area
    under the ROC-curve is therefore computed using the height of the recall
    values by the false positive rate, while the area under the PR-curve is the
    computed using the height of the precision values by the recall.

    This value is ultimately returned as `auc`, an idempotent operation that
    computes the area under a discretized curve of precision versus recall
    values (computed using the aforementioned variables). The `num_thresholds`
    variable controls the degree of discretization with larger numbers of
    thresholds more closely approximating the true AUC. The quality of the
    approximation may vary dramatically depending on `num_thresholds`. The
    `thresholds` parameter can be used to manually specify thresholds which
    split the predictions more evenly.

    For a best approximation of the real AUC, `predictions` should be
    distributed approximately uniformly in the range `[0, 1]` (if
    `from_logits=False`). The quality of the AUC approximation may be poor if
    this is not the case. Setting `summation_method` to 'minoring' or 'majoring'
    can help quantify the error in the approximation by providing lower or upper
    bound estimate of the AUC.

    If `sample_weight` is `None`, weights default to 1.
    Use `sample_weight` of 0 to mask values.

    Args:
        num_thresholds: (Optional) The number of thresholds to
            use when discretizing the roc curve. Values must be > 1.
            Defaults to `200`.
        curve: (Optional) Specifies the name of the curve to be computed,
            `'ROC'` (default) or `'PR'` for the Precision-Recall-curve.
        summation_method: (Optional) Specifies the [Riemann summation method](
              https://en.wikipedia.org/wiki/Riemann_sum) used.
              'interpolation' (default) applies mid-point summation scheme for
              `ROC`.  For PR-AUC, interpolates (true/false) positives but not
              the ratio that is precision (see Davis & Goadrich 2006 for
              details); 'minoring' applies left summation for increasing
              intervals and right summation for decreasing intervals; 'majoring'
              does the opposite.
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.
        thresholds: (Optional) A list of floating point values to use as the
            thresholds for discretizing the curve. If set, the `num_thresholds`
            parameter is ignored. Values should be in `[0, 1]`. Endpoint
            thresholds equal to {`-epsilon`, `1+epsilon`} for a small positive
            epsilon value will be automatically included with these to correctly
            handle predictions equal to exactly 0 or 1.
        multi_label: boolean indicating whether multilabel data should be
            treated as such, wherein AUC is computed separately for each label
            and then averaged across labels, or (when `False`) if the data
            should be flattened into a single label before AUC computation. In
            the latter case, when multilabel data is passed to AUC, each
            label-prediction pair is treated as an individual data point. Should
            be set to False for multi-class data.
        num_labels: (Optional) The number of labels, used when `multi_label` is
            True. If `num_labels` is not specified, then state variables get
            created on the first call to `update_state`.
        label_weights: (Optional) list, array, or tensor of non-negative weights
            used to compute AUCs for multilabel data. When `multi_label` is
            True, the weights are applied to the individual label AUCs when they
            are averaged to produce the multi-label AUC. When it's False, they
            are used to weight the individual label predictions in computing the
            confusion matrix on the flattened data. Note that this is unlike
            `class_weights` in that `class_weights` weights the example
            depending on the value of its label, whereas `label_weights` depends
            only on the index of that label before flattening; therefore
            `label_weights` should not be used for multi-class data.
        from_logits: boolean indicating whether the predictions (`y_pred` in
        `update_state`) are probabilities or sigmoid logits. As a rule of thumb,
        when using a keras loss, the `from_logits` constructor argument of the
        loss should match the AUC `from_logits` constructor argument.

    Standalone usage:

    >>> m = keras.metrics.AUC(num_thresholds=3)
    >>> m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])
    >>> # threshold values are [0 - 1e-7, 0.5, 1 + 1e-7]
    >>> # tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]
    >>> # tp_rate = recall = [1, 0.5, 0], fp_rate = [1, 0, 0]
    >>> # auc = ((((1 + 0.5) / 2) * (1 - 0)) + (((0.5 + 0) / 2) * (0 - 0)))
    >>> #     = 0.75
    >>> m.result()
    0.75

    >>> m.reset_state()
    >>> m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9],
    ...                sample_weight=[1, 0, 0, 1])
    >>> m.result()
    1.0

    Usage with `compile()` API:

    ```python
    # Reports the AUC of a model outputting a probability.
    model.compile(optimizer='sgd',
                  loss=keras.losses.BinaryCrossentropy(),
                  metrics=[keras.metrics.AUC()])

    # Reports the AUC of a model outputting a logit.
    model.compile(optimizer='sgd',
                  loss=keras.losses.BinaryCrossentropy(from_logits=True),
                  metrics=[keras.metrics.AUC(from_logits=True)])
    ```
    )-"
#' Approximates the AUC (Area under the curve) of the ROC or PR curves.
#'
#' @description
#' The AUC (Area under the curve) of the ROC (Receiver operating
#' characteristic; default) or PR (Precision Recall) curves are quality
#' measures of binary classifiers. Unlike the accuracy, and like cross-entropy
#' losses, ROC-AUC and PR-AUC evaluate all the operational points of a model.
#'
#' This class approximates AUCs using a Riemann sum. During the metric
#' accumulation phrase, predictions are accumulated within predefined buckets
#' by value. The AUC is then computed by interpolating per-bucket averages.
#' These buckets define the evaluated operational points.
#'
#' This metric creates four local variables, `true_positives`,
#' `true_negatives`, `false_positives` and `false_negatives` that are used to
#' compute the AUC.  To discretize the AUC curve, a linearly spaced set of
#' thresholds is used to compute pairs of recall and precision values. The area
#' under the ROC-curve is therefore computed using the height of the recall
#' values by the false positive rate, while the area under the PR-curve is the
#' computed using the height of the precision values by the recall.
#'
#' This value is ultimately returned as `auc`, an idempotent operation that
#' computes the area under a discretized curve of precision versus recall
#' values (computed using the aforementioned variables). The `num_thresholds`
#' variable controls the degree of discretization with larger numbers of
#' thresholds more closely approximating the true AUC. The quality of the
#' approximation may vary dramatically depending on `num_thresholds`. The
#' `thresholds` parameter can be used to manually specify thresholds which
#' split the predictions more evenly.
#'
#' For a best approximation of the real AUC, `predictions` should be
#' distributed approximately uniformly in the range `[0, 1]` (if
#' `from_logits=False`). The quality of the AUC approximation may be poor if
#' this is not the case. Setting `summation_method` to 'minoring' or 'majoring'
#' can help quantify the error in the approximation by providing lower or upper
#' bound estimate of the AUC.
#'
#' If `sample_weight` is `None`, weights default to 1.
#' Use `sample_weight` of 0 to mask values.
#'
#' # Usage
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.AUC(num_thresholds=3)
#' m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])
#' # threshold values are [0 - 1e-7, 0.5, 1 + 1e-7]
#' # tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]
#' # tp_rate = recall = [1, 0.5, 0], fp_rate = [1, 0, 0]
#' # auc = ((((1 + 0.5) / 2) * (1 - 0)) + (((0.5 + 0) / 2) * (0 - 0)))
#' #     = 0.75
#' m.result()
#' # 0.75
#' ```
#'
#' ```python
#' m.reset_state()
#' m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9],
#'                sample_weight=[1, 0, 0, 1])
#' m.result()
#' # 1.0
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' # Reports the AUC of a model outputting a probability.
#' model.compile(optimizer='sgd',
#'               loss=keras.losses.BinaryCrossentropy(),
#'               metrics=[keras.metrics.AUC()])
#'
#' # Reports the AUC of a model outputting a logit.
#' model.compile(optimizer='sgd',
#'               loss=keras.losses.BinaryCrossentropy(from_logits=True),
#'               metrics=[keras.metrics.AUC(from_logits=True)])
#' ```
#'
#' @param num_thresholds (Optional) The number of thresholds to
#'     use when discretizing the roc curve. Values must be > 1.
#'     Defaults to `200`.
#' @param curve (Optional) Specifies the name of the curve to be computed,
#'     `'ROC'` (default) or `'PR'` for the Precision-Recall-curve.
#' @param summation_method (Optional) Specifies the [Riemann summation method](
#'       https://en.wikipedia.org/wiki/Riemann_sum) used.
#'       'interpolation' (default) applies mid-point summation scheme for
#'       `ROC`.  For PR-AUC, interpolates (true/false) positives but not
#'       the ratio that is precision (see Davis & Goadrich 2006 for
#'       details); 'minoring' applies left summation for increasing
#'       intervals and right summation for decreasing intervals; 'majoring'
#'       does the opposite.
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param thresholds (Optional) A list of floating point values to use as the
#'     thresholds for discretizing the curve. If set, the `num_thresholds`
#'     parameter is ignored. Values should be in `[0, 1]`. Endpoint
#'     thresholds equal to {`-epsilon`, `1+epsilon`} for a small positive
#'     epsilon value will be automatically included with these to correctly
#'     handle predictions equal to exactly 0 or 1.
#' @param multi_label boolean indicating whether multilabel data should be
#'     treated as such, wherein AUC is computed separately for each label
#'     and then averaged across labels, or (when `False`) if the data
#'     should be flattened into a single label before AUC computation. In
#'     the latter case, when multilabel data is passed to AUC, each
#'     label-prediction pair is treated as an individual data point. Should
#'     be set to False for multi-class data.
#' @param num_labels (Optional) The number of labels, used when `multi_label` is
#'     True. If `num_labels` is not specified, then state variables get
#'     created on the first call to `update_state`.
#' @param label_weights (Optional) list, array, or tensor of non-negative weights
#'     used to compute AUCs for multilabel data. When `multi_label` is
#'     True, the weights are applied to the individual label AUCs when they
#'     are averaged to produce the multi-label AUC. When it's False, they
#'     are used to weight the individual label predictions in computing the
#'     confusion matrix on the flattened data. Note that this is unlike
#'     `class_weights` in that `class_weights` weights the example
#'     depending on the value of its label, whereas `label_weights` depends
#'     only on the index of that label before flattening; therefore
#'     `label_weights` should not be used for multi-class data.
#' @param from_logits boolean indicating whether the predictions (`y_pred` in
#' `update_state`) are probabilities or sigmoid logits. As a rule of thumb,
#' when using a keras loss, the `from_logits` constructor argument of the
#' loss should match the AUC `from_logits` constructor argument.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/AUC>
metric_auc <-
function (..., num_thresholds = 200L, curve = "ROC", summation_method = "interpolation",
    name = NULL, dtype = NULL, thresholds = NULL, multi_label = FALSE,
    num_labels = NULL, label_weights = NULL, from_logits = FALSE)
{
    args <- capture_args2(list(num_thresholds = as_integer))
    do.call(keras$metrics$AUC, args)
}


# keras.metrics.FalseNegatives
# keras.src.metrics.confusion_metrics.FalseNegatives
r"-(Calculates the number of false negatives.

    If `sample_weight` is given, calculates the sum of the weights of
    false negatives. This metric creates one local variable, `accumulator`
    that is used to keep track of the number of false negatives.

    If `sample_weight` is `None`, weights default to 1.
    Use `sample_weight` of 0 to mask values.

    Args:
        thresholds: (Optional) Defaults to `0.5`. A float value, or a Python
            list/tuple of float threshold values in `[0, 1]`. A threshold is
            compared with prediction values to determine the truth value of
            predictions (i.e., above the threshold is `True`, below is `False`).
            If used with a loss function that sets `from_logits=True` (i.e. no
            sigmoid applied to predictions), `thresholds` should be set to 0.
            One metric value is generated for each threshold value.
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Standalone usage:

    >>> m = keras.metrics.FalseNegatives()
    >>> m.update_state([0, 1, 1, 1], [0, 1, 0, 0])
    >>> m.result()
    2.0

    >>> m.reset_state()
    >>> m.update_state([0, 1, 1, 1], [0, 1, 0, 0], sample_weight=[0, 0, 1, 0])
    >>> m.result()
    1.0
    )-"
#' Calculates the number of false negatives.
#'
#' @description
#' If `sample_weight` is given, calculates the sum of the weights of
#' false negatives. This metric creates one local variable, `accumulator`
#' that is used to keep track of the number of false negatives.
#'
#' If `sample_weight` is `None`, weights default to 1.
#' Use `sample_weight` of 0 to mask values.
#'
#' # Usage
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.FalseNegatives()
#' m.update_state([0, 1, 1, 1], [0, 1, 0, 0])
#' m.result()
#' # 2.0
#' ```
#'
#' ```python
#' m.reset_state()
#' m.update_state([0, 1, 1, 1], [0, 1, 0, 0], sample_weight=[0, 0, 1, 0])
#' m.result()
#' # 1.0
#' ```
#'
#' @param thresholds (Optional) Defaults to `0.5`. A float value, or a Python
#'     list/tuple of float threshold values in `[0, 1]`. A threshold is
#'     compared with prediction values to determine the truth value of
#'     predictions (i.e., above the threshold is `True`, below is `False`).
#'     If used with a loss function that sets `from_logits=True` (i.e. no
#'     sigmoid applied to predictions), `thresholds` should be set to 0.
#'     One metric value is generated for each threshold value.
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/FalseNegatives>
metric_false_negatives <-
function (..., thresholds = NULL, name = NULL, dtype = NULL)
{
    args <- capture_args2(NULL)
    do.call(keras$metrics$FalseNegatives, args)
}


# keras.metrics.FalsePositives
# keras.src.metrics.confusion_metrics.FalsePositives
r"-(Calculates the number of false positives.

    If `sample_weight` is given, calculates the sum of the weights of
    false positives. This metric creates one local variable, `accumulator`
    that is used to keep track of the number of false positives.

    If `sample_weight` is `None`, weights default to 1.
    Use `sample_weight` of 0 to mask values.

    Args:
        thresholds: (Optional) Defaults to `0.5`. A float value, or a Python
            list/tuple of float threshold values in `[0, 1]`. A threshold is
            compared with prediction values to determine the truth value of
            predictions (i.e., above the threshold is `True`, below is `False`).
            If used with a loss function that sets `from_logits=True` (i.e. no
            sigmoid applied to predictions), `thresholds` should be set to 0.
            One metric value is generated for each threshold value.
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Standalone usage:

    >>> m = keras.metrics.FalsePositives()
    >>> m.update_state([0, 1, 0, 0], [0, 0, 1, 1])
    >>> m.result()
    2.0

    >>> m.reset_state()
    >>> m.update_state([0, 1, 0, 0], [0, 0, 1, 1], sample_weight=[0, 0, 1, 0])
    >>> m.result()
    1.0
    )-"
#' Calculates the number of false positives.
#'
#' @description
#' If `sample_weight` is given, calculates the sum of the weights of
#' false positives. This metric creates one local variable, `accumulator`
#' that is used to keep track of the number of false positives.
#'
#' If `sample_weight` is `None`, weights default to 1.
#' Use `sample_weight` of 0 to mask values.
#'
#' # Usage
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.FalsePositives()
#' m.update_state([0, 1, 0, 0], [0, 0, 1, 1])
#' m.result()
#' # 2.0
#' ```
#'
#' ```python
#' m.reset_state()
#' m.update_state([0, 1, 0, 0], [0, 0, 1, 1], sample_weight=[0, 0, 1, 0])
#' m.result()
#' # 1.0
#' ```
#'
#' @param thresholds (Optional) Defaults to `0.5`. A float value, or a Python
#'     list/tuple of float threshold values in `[0, 1]`. A threshold is
#'     compared with prediction values to determine the truth value of
#'     predictions (i.e., above the threshold is `True`, below is `False`).
#'     If used with a loss function that sets `from_logits=True` (i.e. no
#'     sigmoid applied to predictions), `thresholds` should be set to 0.
#'     One metric value is generated for each threshold value.
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/FalsePositives>
metric_false_positives <-
function (..., thresholds = NULL, name = NULL, dtype = NULL)
{
    args <- capture_args2(NULL)
    do.call(keras$metrics$FalsePositives, args)
}


# keras.metrics.Precision
# keras.src.metrics.confusion_metrics.Precision
r"-(Computes the precision of the predictions with respect to the labels.

    The metric creates two local variables, `true_positives` and
    `false_positives` that are used to compute the precision. This value is
    ultimately returned as `precision`, an idempotent operation that simply
    divides `true_positives` by the sum of `true_positives` and
    `false_positives`.

    If `sample_weight` is `None`, weights default to 1.
    Use `sample_weight` of 0 to mask values.

    If `top_k` is set, we'll calculate precision as how often on average a class
    among the top-k classes with the highest predicted values of a batch entry
    is correct and can be found in the label for that entry.

    If `class_id` is specified, we calculate precision by considering only the
    entries in the batch for which `class_id` is above the threshold and/or in
    the top-k highest predictions, and computing the fraction of them for which
    `class_id` is indeed a correct label.

    Args:
        thresholds: (Optional) A float value, or a Python list/tuple of float
            threshold values in `[0, 1]`. A threshold is compared with
            prediction values to determine the truth value of predictions (i.e.,
            above the threshold is `True`, below is `False`). If used with a
            loss function that sets `from_logits=True` (i.e. no sigmoid applied
            to predictions), `thresholds` should be set to 0. One metric value
            is generated for each threshold value. If neither `thresholds` nor
            `top_k` are set, the default is to calculate precision with
            `thresholds=0.5`.
        top_k: (Optional) Unset by default. An int value specifying the top-k
            predictions to consider when calculating precision.
        class_id: (Optional) Integer class ID for which we want binary metrics.
            This must be in the half-open interval `[0, num_classes)`, where
            `num_classes` is the last dimension of predictions.
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Standalone usage:

    >>> m = keras.metrics.Precision()
    >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
    >>> m.result()
    0.6666667

    >>> m.reset_state()
    >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0])
    >>> m.result()
    1.0

    >>> # With top_k=2, it will calculate precision over y_true[:2]
    >>> # and y_pred[:2]
    >>> m = keras.metrics.Precision(top_k=2)
    >>> m.update_state([0, 0, 1, 1], [1, 1, 1, 1])
    >>> m.result()
    0.0

    >>> # With top_k=4, it will calculate precision over y_true[:4]
    >>> # and y_pred[:4]
    >>> m = keras.metrics.Precision(top_k=4)
    >>> m.update_state([0, 0, 1, 1], [1, 1, 1, 1])
    >>> m.result()
    0.5

    Usage with `compile()` API:

    ```python
    model.compile(optimizer='sgd',
                  loss='mse',
                  metrics=[keras.metrics.Precision()])
    ```

    Usage with a loss with `from_logits=True`:

    ```python
    model.compile(optimizer='adam',
                  loss=keras.losses.BinaryCrossentropy(from_logits=True),
                  metrics=[keras.metrics.Precision(thresholds=0)])
    ```
    )-"
#' Computes the precision of the predictions with respect to the labels.
#'
#' @description
#' The metric creates two local variables, `true_positives` and
#' `false_positives` that are used to compute the precision. This value is
#' ultimately returned as `precision`, an idempotent operation that simply
#' divides `true_positives` by the sum of `true_positives` and
#' `false_positives`.
#'
#' If `sample_weight` is `None`, weights default to 1.
#' Use `sample_weight` of 0 to mask values.
#'
#' If `top_k` is set, we'll calculate precision as how often on average a class
#' among the top-k classes with the highest predicted values of a batch entry
#' is correct and can be found in the label for that entry.
#'
#' If `class_id` is specified, we calculate precision by considering only the
#' entries in the batch for which `class_id` is above the threshold and/or in
#' the top-k highest predictions, and computing the fraction of them for which
#' `class_id` is indeed a correct label.
#'
#' # Usage
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.Precision()
#' m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
#' m.result()
#' # 0.6666667
#' ```
#'
#' ```python
#' m.reset_state()
#' m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0])
#' m.result()
#' # 1.0
#' ```
#'
#' ```python
#' # With top_k=2, it will calculate precision over y_true[:2]
#' # and y_pred[:2]
#' m = keras.metrics.Precision(top_k=2)
#' m.update_state([0, 0, 1, 1], [1, 1, 1, 1])
#' m.result()
#' # 0.0
#' ```
#'
#' ```python
#' # With top_k=4, it will calculate precision over y_true[:4]
#' # and y_pred[:4]
#' m = keras.metrics.Precision(top_k=4)
#' m.update_state([0, 0, 1, 1], [1, 1, 1, 1])
#' m.result()
#' # 0.5
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(optimizer='sgd',
#'               loss='mse',
#'               metrics=[keras.metrics.Precision()])
#' ```
#'
#' Usage with a loss with `from_logits=True`:
#'
#' ```python
#' model.compile(optimizer='adam',
#'               loss=keras.losses.BinaryCrossentropy(from_logits=True),
#'               metrics=[keras.metrics.Precision(thresholds=0)])
#' ```
#'
#' @param thresholds (Optional) A float value, or a Python list/tuple of float
#'     threshold values in `[0, 1]`. A threshold is compared with
#'     prediction values to determine the truth value of predictions (i.e.,
#'     above the threshold is `True`, below is `False`). If used with a
#'     loss function that sets `from_logits=True` (i.e. no sigmoid applied
#'     to predictions), `thresholds` should be set to 0. One metric value
#'     is generated for each threshold value. If neither `thresholds` nor
#'     `top_k` are set, the default is to calculate precision with
#'     `thresholds=0.5`.
#' @param top_k (Optional) Unset by default. An int value specifying the top-k
#'     predictions to consider when calculating precision.
#' @param class_id (Optional) Integer class ID for which we want binary metrics.
#'     This must be in the half-open interval `[0, num_classes)`, where
#'     `num_classes` is the last dimension of predictions.
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision>
metric_precision <-
function (..., thresholds = NULL, top_k = NULL, class_id = NULL,
    name = NULL, dtype = NULL)
{
    args <- capture_args2(list(top_k = as_integer, class_id = as_integer))
    do.call(keras$metrics$Precision, args)
}


# keras.metrics.PrecisionAtRecall
# keras.src.metrics.confusion_metrics.PrecisionAtRecall
r"-(Computes best precision where recall is >= specified value.

    This metric creates four local variables, `true_positives`,
    `true_negatives`, `false_positives` and `false_negatives` that are used to
    compute the precision at the given recall. The threshold for the given
    recall value is computed and used to evaluate the corresponding precision.

    If `sample_weight` is `None`, weights default to 1.
    Use `sample_weight` of 0 to mask values.

    If `class_id` is specified, we calculate precision by considering only the
    entries in the batch for which `class_id` is above the threshold
    predictions, and computing the fraction of them for which `class_id` is
    indeed a correct label.

    Args:
        recall: A scalar value in range `[0, 1]`.
        num_thresholds: (Optional) Defaults to 200. The number of thresholds to
            use for matching the given recall.
        class_id: (Optional) Integer class ID for which we want binary metrics.
            This must be in the half-open interval `[0, num_classes)`, where
            `num_classes` is the last dimension of predictions.
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Standalone usage:

    >>> m = keras.metrics.PrecisionAtRecall(0.5)
    >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
    >>> m.result()
    0.5

    >>> m.reset_state()
    >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8],
    ...                sample_weight=[2, 2, 2, 1, 1])
    >>> m.result()
    0.33333333

    Usage with `compile()` API:

    ```python
    model.compile(
        optimizer='sgd',
        loss='mse',
        metrics=[keras.metrics.PrecisionAtRecall(recall=0.8)])
    ```
    )-"
#' Computes best precision where recall is >= specified value.
#'
#' @description
#' This metric creates four local variables, `true_positives`,
#' `true_negatives`, `false_positives` and `false_negatives` that are used to
#' compute the precision at the given recall. The threshold for the given
#' recall value is computed and used to evaluate the corresponding precision.
#'
#' If `sample_weight` is `None`, weights default to 1.
#' Use `sample_weight` of 0 to mask values.
#'
#' If `class_id` is specified, we calculate precision by considering only the
#' entries in the batch for which `class_id` is above the threshold
#' predictions, and computing the fraction of them for which `class_id` is
#' indeed a correct label.
#'
#' # Usage
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.PrecisionAtRecall(0.5)
#' m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
#' m.result()
#' # 0.5
#' ```
#'
#' ```python
#' m.reset_state()
#' m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8],
#'                sample_weight=[2, 2, 2, 1, 1])
#' m.result()
#' # 0.33333333
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(
#'     optimizer='sgd',
#'     loss='mse',
#'     metrics=[keras.metrics.PrecisionAtRecall(recall=0.8)])
#' ```
#'
#' @param recall A scalar value in range `[0, 1]`.
#' @param num_thresholds (Optional) Defaults to 200. The number of thresholds to
#'     use for matching the given recall.
#' @param class_id (Optional) Integer class ID for which we want binary metrics.
#'     This must be in the half-open interval `[0, num_classes)`, where
#'     `num_classes` is the last dimension of predictions.
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/PrecisionAtRecall>
metric_precision_at_recall <-
function (..., recall, num_thresholds = 200L, class_id = NULL,
    name = NULL, dtype = NULL)
{
    args <- capture_args2(list(num_thresholds = as_integer, class_id = as_integer))
    do.call(keras$metrics$PrecisionAtRecall, args)
}


# keras.metrics.Recall
# keras.src.metrics.confusion_metrics.Recall
r"-(Computes the recall of the predictions with respect to the labels.

    This metric creates two local variables, `true_positives` and
    `false_negatives`, that are used to compute the recall. This value is
    ultimately returned as `recall`, an idempotent operation that simply divides
    `true_positives` by the sum of `true_positives` and `false_negatives`.

    If `sample_weight` is `None`, weights default to 1.
    Use `sample_weight` of 0 to mask values.

    If `top_k` is set, recall will be computed as how often on average a class
    among the labels of a batch entry is in the top-k predictions.

    If `class_id` is specified, we calculate recall by considering only the
    entries in the batch for which `class_id` is in the label, and computing the
    fraction of them for which `class_id` is above the threshold and/or in the
    top-k predictions.

    Args:
        thresholds: (Optional) A float value, or a Python list/tuple of float
            threshold values in `[0, 1]`. A threshold is compared with
            prediction values to determine the truth value of predictions (i.e.,
            above the threshold is `True`, below is `False`). If used with a
            loss function that sets `from_logits=True` (i.e. no sigmoid
            applied to predictions), `thresholds` should be set to 0.
            One metric value is generated for each threshold value.
            If neither `thresholds` nor `top_k` are set,
            the default is to calculate recall with `thresholds=0.5`.
        top_k: (Optional) Unset by default. An int value specifying the top-k
            predictions to consider when calculating recall.
        class_id: (Optional) Integer class ID for which we want binary metrics.
            This must be in the half-open interval `[0, num_classes)`, where
            `num_classes` is the last dimension of predictions.
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Standalone usage:

    >>> m = keras.metrics.Recall()
    >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
    >>> m.result()
    0.6666667

    >>> m.reset_state()
    >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0])
    >>> m.result()
    1.0

    Usage with `compile()` API:

    ```python
    model.compile(optimizer='sgd',
                  loss='mse',
                  metrics=[keras.metrics.Recall()])
    ```

    Usage with a loss with `from_logits=True`:

    ```python
    model.compile(optimizer='adam',
                  loss=keras.losses.BinaryCrossentropy(from_logits=True),
                  metrics=[keras.metrics.Recall(thresholds=0)])
    ```
    )-"
#' Computes the recall of the predictions with respect to the labels.
#'
#' @description
#' This metric creates two local variables, `true_positives` and
#' `false_negatives`, that are used to compute the recall. This value is
#' ultimately returned as `recall`, an idempotent operation that simply divides
#' `true_positives` by the sum of `true_positives` and `false_negatives`.
#'
#' If `sample_weight` is `None`, weights default to 1.
#' Use `sample_weight` of 0 to mask values.
#'
#' If `top_k` is set, recall will be computed as how often on average a class
#' among the labels of a batch entry is in the top-k predictions.
#'
#' If `class_id` is specified, we calculate recall by considering only the
#' entries in the batch for which `class_id` is in the label, and computing the
#' fraction of them for which `class_id` is above the threshold and/or in the
#' top-k predictions.
#'
#' # Usage
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.Recall()
#' m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
#' m.result()
#' # 0.6666667
#' ```
#'
#' ```python
#' m.reset_state()
#' m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0])
#' m.result()
#' # 1.0
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(optimizer='sgd',
#'               loss='mse',
#'               metrics=[keras.metrics.Recall()])
#' ```
#'
#' Usage with a loss with `from_logits=True`:
#'
#' ```python
#' model.compile(optimizer='adam',
#'               loss=keras.losses.BinaryCrossentropy(from_logits=True),
#'               metrics=[keras.metrics.Recall(thresholds=0)])
#' ```
#'
#' @param thresholds (Optional) A float value, or a Python list/tuple of float
#'     threshold values in `[0, 1]`. A threshold is compared with
#'     prediction values to determine the truth value of predictions (i.e.,
#'     above the threshold is `True`, below is `False`). If used with a
#'     loss function that sets `from_logits=True` (i.e. no sigmoid
#'     applied to predictions), `thresholds` should be set to 0.
#'     One metric value is generated for each threshold value.
#'     If neither `thresholds` nor `top_k` are set,
#'     the default is to calculate recall with `thresholds=0.5`.
#' @param top_k (Optional) Unset by default. An int value specifying the top-k
#'     predictions to consider when calculating recall.
#' @param class_id (Optional) Integer class ID for which we want binary metrics.
#'     This must be in the half-open interval `[0, num_classes)`, where
#'     `num_classes` is the last dimension of predictions.
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall>
metric_recall <-
function (..., thresholds = NULL, top_k = NULL, class_id = NULL,
    name = NULL, dtype = NULL)
{
    args <- capture_args2(list(top_k = as_integer, class_id = as_integer))
    do.call(keras$metrics$Recall, args)
}


# keras.metrics.RecallAtPrecision
# keras.src.metrics.confusion_metrics.RecallAtPrecision
r"-(Computes best recall where precision is >= specified value.

    For a given score-label-distribution the required precision might not
    be achievable, in this case 0.0 is returned as recall.

    This metric creates four local variables, `true_positives`,
    `true_negatives`, `false_positives` and `false_negatives` that are used to
    compute the recall at the given precision. The threshold for the given
    precision value is computed and used to evaluate the corresponding recall.

    If `sample_weight` is `None`, weights default to 1.
    Use `sample_weight` of 0 to mask values.

    If `class_id` is specified, we calculate precision by considering only the
    entries in the batch for which `class_id` is above the threshold
    predictions, and computing the fraction of them for which `class_id` is
    indeed a correct label.

    Args:
        precision: A scalar value in range `[0, 1]`.
            num_thresholds: (Optional) Defaults to 200. The number of thresholds
            to use for matching the given precision.
        class_id: (Optional) Integer class ID for which we want binary metrics.
            This must be in the half-open interval `[0, num_classes)`, where
            `num_classes` is the last dimension of predictions.
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Standalone usage:

    >>> m = keras.metrics.RecallAtPrecision(0.8)
    >>> m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])
    >>> m.result()
    0.5

    >>> m.reset_state()
    >>> m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9],
    ...                sample_weight=[1, 0, 0, 1])
    >>> m.result()
    1.0

    Usage with `compile()` API:

    ```python
    model.compile(
        optimizer='sgd',
        loss='mse',
        metrics=[keras.metrics.RecallAtPrecision(precision=0.8)])
    ```
    )-"
#' Computes best recall where precision is >= specified value.
#'
#' @description
#' For a given score-label-distribution the required precision might not
#' be achievable, in this case 0.0 is returned as recall.
#'
#' This metric creates four local variables, `true_positives`,
#' `true_negatives`, `false_positives` and `false_negatives` that are used to
#' compute the recall at the given precision. The threshold for the given
#' precision value is computed and used to evaluate the corresponding recall.
#'
#' If `sample_weight` is `None`, weights default to 1.
#' Use `sample_weight` of 0 to mask values.
#'
#' If `class_id` is specified, we calculate precision by considering only the
#' entries in the batch for which `class_id` is above the threshold
#' predictions, and computing the fraction of them for which `class_id` is
#' indeed a correct label.
#'
#' # Usage
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.RecallAtPrecision(0.8)
#' m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])
#' m.result()
#' # 0.5
#' ```
#'
#' ```python
#' m.reset_state()
#' m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9],
#'                sample_weight=[1, 0, 0, 1])
#' m.result()
#' # 1.0
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(
#'     optimizer='sgd',
#'     loss='mse',
#'     metrics=[keras.metrics.RecallAtPrecision(precision=0.8)])
#' ```
#'
#' @param precision A scalar value in range `[0, 1]`.
#' @param num_thresholds (Optional) Defaults to 200. The number of thresholds
#'     to use for matching the given precision.
#' @param class_id (Optional) Integer class ID for which we want binary metrics.
#'     This must be in the half-open interval `[0, num_classes)`, where
#'     `num_classes` is the last dimension of predictions.
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/RecallAtPrecision>
metric_recall_at_precision <-
function (..., precision, num_thresholds = 200L, class_id = NULL,
    name = NULL, dtype = NULL)
{
    args <- capture_args2(list(num_thresholds = as_integer, class_id = as_integer))
    do.call(keras$metrics$RecallAtPrecision, args)
}


# keras.metrics.SensitivityAtSpecificity
# keras.src.metrics.confusion_metrics.SensitivityAtSpecificity
r"-(Computes best sensitivity where specificity is >= specified value.

    `Sensitivity` measures the proportion of actual positives that are correctly
    identified as such `(tp / (tp + fn))`.
    `Specificity` measures the proportion of actual negatives that are correctly
    identified as such `(tn / (tn + fp))`.

    This metric creates four local variables, `true_positives`,
    `true_negatives`, `false_positives` and `false_negatives` that are used to
    compute the sensitivity at the given specificity. The threshold for the
    given specificity value is computed and used to evaluate the corresponding
    sensitivity.

    If `sample_weight` is `None`, weights default to 1.
    Use `sample_weight` of 0 to mask values.

    If `class_id` is specified, we calculate precision by considering only the
    entries in the batch for which `class_id` is above the threshold
    predictions, and computing the fraction of them for which `class_id` is
    indeed a correct label.

    For additional information about specificity and sensitivity, see
    [the following](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).

    Args:
        specificity: A scalar value in range `[0, 1]`.
        num_thresholds: (Optional) Defaults to 200. The number of thresholds to
            use for matching the given specificity.
        class_id: (Optional) Integer class ID for which we want binary metrics.
            This must be in the half-open interval `[0, num_classes)`, where
            `num_classes` is the last dimension of predictions.
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Standalone usage:

    >>> m = keras.metrics.SensitivityAtSpecificity(0.5)
    >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
    >>> m.result()
    0.5

    >>> m.reset_state()
    >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8],
    ...                sample_weight=[1, 1, 2, 2, 1])
    >>> m.result()
    0.333333

    Usage with `compile()` API:

    ```python
    model.compile(
        optimizer='sgd',
        loss='mse',
        metrics=[keras.metrics.SensitivityAtSpecificity()])
    ```
    )-"
#' Computes best sensitivity where specificity is >= specified value.
#'
#' @description
#' `Sensitivity` measures the proportion of actual positives that are correctly
#' identified as such `(tp / (tp + fn))`.
#' `Specificity` measures the proportion of actual negatives that are correctly
#' identified as such `(tn / (tn + fp))`.
#'
#' This metric creates four local variables, `true_positives`,
#' `true_negatives`, `false_positives` and `false_negatives` that are used to
#' compute the sensitivity at the given specificity. The threshold for the
#' given specificity value is computed and used to evaluate the corresponding
#' sensitivity.
#'
#' If `sample_weight` is `None`, weights default to 1.
#' Use `sample_weight` of 0 to mask values.
#'
#' If `class_id` is specified, we calculate precision by considering only the
#' entries in the batch for which `class_id` is above the threshold
#' predictions, and computing the fraction of them for which `class_id` is
#' indeed a correct label.
#'
#' For additional information about specificity and sensitivity, see
#' [the following](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).
#'
#' # Usage
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.SensitivityAtSpecificity(0.5)
#' m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
#' m.result()
#' # 0.5
#' ```
#'
#' ```python
#' m.reset_state()
#' m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8],
#'                sample_weight=[1, 1, 2, 2, 1])
#' m.result()
#' # 0.333333
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(
#'     optimizer='sgd',
#'     loss='mse',
#'     metrics=[keras.metrics.SensitivityAtSpecificity()])
#' ```
#'
#' @param specificity A scalar value in range `[0, 1]`.
#' @param num_thresholds (Optional) Defaults to 200. The number of thresholds to
#'     use for matching the given specificity.
#' @param class_id (Optional) Integer class ID for which we want binary metrics.
#'     This must be in the half-open interval `[0, num_classes)`, where
#'     `num_classes` is the last dimension of predictions.
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SensitivityAtSpecificity>
metric_sensitivity_at_specificity <-
function (..., specificity, num_thresholds = 200L, class_id = NULL,
    name = NULL, dtype = NULL)
{
    args <- capture_args2(list(num_thresholds = as_integer, class_id = as_integer))
    do.call(keras$metrics$SensitivityAtSpecificity, args)
}


# keras.metrics.SpecificityAtSensitivity
# keras.src.metrics.confusion_metrics.SpecificityAtSensitivity
r"-(Computes best specificity where sensitivity is >= specified value.

    `Sensitivity` measures the proportion of actual positives that are correctly
    identified as such `(tp / (tp + fn))`.
    `Specificity` measures the proportion of actual negatives that are correctly
    identified as such `(tn / (tn + fp))`.

    This metric creates four local variables, `true_positives`,
    `true_negatives`, `false_positives` and `false_negatives` that are used to
    compute the specificity at the given sensitivity. The threshold for the
    given sensitivity value is computed and used to evaluate the corresponding
    specificity.

    If `sample_weight` is `None`, weights default to 1.
    Use `sample_weight` of 0 to mask values.

    If `class_id` is specified, we calculate precision by considering only the
    entries in the batch for which `class_id` is above the threshold
    predictions, and computing the fraction of them for which `class_id` is
    indeed a correct label.

    For additional information about specificity and sensitivity, see
    [the following](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).

    Args:
        sensitivity: A scalar value in range `[0, 1]`.
        num_thresholds: (Optional) Defaults to 200. The number of thresholds to
            use for matching the given sensitivity.
        class_id: (Optional) Integer class ID for which we want binary metrics.
            This must be in the half-open interval `[0, num_classes)`, where
            `num_classes` is the last dimension of predictions.
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Standalone usage:

    >>> m = keras.metrics.SpecificityAtSensitivity(0.5)
    >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
    >>> m.result()
    0.66666667

    >>> m.reset_state()
    >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8],
    ...                sample_weight=[1, 1, 2, 2, 2])
    >>> m.result()
    0.5

    Usage with `compile()` API:

    ```python
    model.compile(
        optimizer='sgd',
        loss='mse',
        metrics=[keras.metrics.SpecificityAtSensitivity()])
    ```
    )-"
#' Computes best specificity where sensitivity is >= specified value.
#'
#' @description
#' `Sensitivity` measures the proportion of actual positives that are correctly
#' identified as such `(tp / (tp + fn))`.
#' `Specificity` measures the proportion of actual negatives that are correctly
#' identified as such `(tn / (tn + fp))`.
#'
#' This metric creates four local variables, `true_positives`,
#' `true_negatives`, `false_positives` and `false_negatives` that are used to
#' compute the specificity at the given sensitivity. The threshold for the
#' given sensitivity value is computed and used to evaluate the corresponding
#' specificity.
#'
#' If `sample_weight` is `None`, weights default to 1.
#' Use `sample_weight` of 0 to mask values.
#'
#' If `class_id` is specified, we calculate precision by considering only the
#' entries in the batch for which `class_id` is above the threshold
#' predictions, and computing the fraction of them for which `class_id` is
#' indeed a correct label.
#'
#' For additional information about specificity and sensitivity, see
#' [the following](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).
#'
#' # Usage
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.SpecificityAtSensitivity(0.5)
#' m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
#' m.result()
#' # 0.66666667
#' ```
#'
#' ```python
#' m.reset_state()
#' m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8],
#'                sample_weight=[1, 1, 2, 2, 2])
#' m.result()
#' # 0.5
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(
#'     optimizer='sgd',
#'     loss='mse',
#'     metrics=[keras.metrics.SpecificityAtSensitivity()])
#' ```
#'
#' @param sensitivity A scalar value in range `[0, 1]`.
#' @param num_thresholds (Optional) Defaults to 200. The number of thresholds to
#'     use for matching the given sensitivity.
#' @param class_id (Optional) Integer class ID for which we want binary metrics.
#'     This must be in the half-open interval `[0, num_classes)`, where
#'     `num_classes` is the last dimension of predictions.
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SpecificityAtSensitivity>
metric_specificity_at_sensitivity <-
function (..., sensitivity, num_thresholds = 200L, class_id = NULL,
    name = NULL, dtype = NULL)
{
    args <- capture_args2(list(num_thresholds = as_integer, class_id = as_integer))
    do.call(keras$metrics$SpecificityAtSensitivity, args)
}


# keras.metrics.TrueNegatives
# keras.src.metrics.confusion_metrics.TrueNegatives
r"-(Calculates the number of true negatives.

    If `sample_weight` is given, calculates the sum of the weights of
    true negatives. This metric creates one local variable, `accumulator`
    that is used to keep track of the number of true negatives.

    If `sample_weight` is `None`, weights default to 1.
    Use `sample_weight` of 0 to mask values.

    Args:
        thresholds: (Optional) Defaults to `0.5`. A float value, or a Python
            list/tuple of float threshold values in `[0, 1]`. A threshold is
            compared with prediction values to determine the truth value of
            predictions (i.e., above the threshold is `True`, below is `False`).
            If used with a loss function that sets `from_logits=True` (i.e. no
            sigmoid applied to predictions), `thresholds` should be set to 0.
            One metric value is generated for each threshold value.
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Standalone usage:

    >>> m = keras.metrics.TrueNegatives()
    >>> m.update_state([0, 1, 0, 0], [1, 1, 0, 0])
    >>> m.result()
    2.0

    >>> m.reset_state()
    >>> m.update_state([0, 1, 0, 0], [1, 1, 0, 0], sample_weight=[0, 0, 1, 0])
    >>> m.result()
    1.0
    )-"
#' Calculates the number of true negatives.
#'
#' @description
#' If `sample_weight` is given, calculates the sum of the weights of
#' true negatives. This metric creates one local variable, `accumulator`
#' that is used to keep track of the number of true negatives.
#'
#' If `sample_weight` is `None`, weights default to 1.
#' Use `sample_weight` of 0 to mask values.
#'
#' # Usage
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.TrueNegatives()
#' m.update_state([0, 1, 0, 0], [1, 1, 0, 0])
#' m.result()
#' # 2.0
#' ```
#'
#' ```python
#' m.reset_state()
#' m.update_state([0, 1, 0, 0], [1, 1, 0, 0], sample_weight=[0, 0, 1, 0])
#' m.result()
#' # 1.0
#' ```
#'
#' @param thresholds (Optional) Defaults to `0.5`. A float value, or a Python
#'     list/tuple of float threshold values in `[0, 1]`. A threshold is
#'     compared with prediction values to determine the truth value of
#'     predictions (i.e., above the threshold is `True`, below is `False`).
#'     If used with a loss function that sets `from_logits=True` (i.e. no
#'     sigmoid applied to predictions), `thresholds` should be set to 0.
#'     One metric value is generated for each threshold value.
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/TrueNegatives>
metric_true_negatives <-
function (..., thresholds = NULL, name = NULL, dtype = NULL)
{
    args <- capture_args2(NULL)
    do.call(keras$metrics$TrueNegatives, args)
}


# keras.metrics.TruePositives
# keras.src.metrics.confusion_metrics.TruePositives
r"-(Calculates the number of true positives.

    If `sample_weight` is given, calculates the sum of the weights of
    true positives. This metric creates one local variable, `true_positives`
    that is used to keep track of the number of true positives.

    If `sample_weight` is `None`, weights default to 1.
    Use `sample_weight` of 0 to mask values.

    Args:
        thresholds: (Optional) Defaults to `0.5`. A float value, or a Python
            list/tuple of float threshold values in `[0, 1]`. A threshold is
            compared with prediction values to determine the truth value of
            predictions (i.e., above the threshold is `True`, below is `False`).
            If used with a loss function that sets `from_logits=True` (i.e. no
            sigmoid applied to predictions), `thresholds` should be set to 0.
            One metric value is generated for each threshold value.
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Standalone usage:

    >>> m = keras.metrics.TruePositives()
    >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
    >>> m.result()
    2.0

    >>> m.reset_state()
    >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0])
    >>> m.result()
    1.0
    )-"
#' Calculates the number of true positives.
#'
#' @description
#' If `sample_weight` is given, calculates the sum of the weights of
#' true positives. This metric creates one local variable, `true_positives`
#' that is used to keep track of the number of true positives.
#'
#' If `sample_weight` is `None`, weights default to 1.
#' Use `sample_weight` of 0 to mask values.
#'
#' # Usage
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.TruePositives()
#' m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
#' m.result()
#' # 2.0
#' ```
#'
#' ```python
#' m.reset_state()
#' m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0])
#' m.result()
#' # 1.0
#' ```
#'
#' @param thresholds (Optional) Defaults to `0.5`. A float value, or a Python
#'     list/tuple of float threshold values in `[0, 1]`. A threshold is
#'     compared with prediction values to determine the truth value of
#'     predictions (i.e., above the threshold is `True`, below is `False`).
#'     If used with a loss function that sets `from_logits=True` (i.e. no
#'     sigmoid applied to predictions), `thresholds` should be set to 0.
#'     One metric value is generated for each threshold value.
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/TruePositives>
metric_true_positives <-
function (..., thresholds = NULL, name = NULL, dtype = NULL)
{
    args <- capture_args2(NULL)
    do.call(keras$metrics$TruePositives, args)
}


# keras.metrics.F1Score
# keras.src.metrics.f_score_metrics.F1Score
r"-(Computes F-1 Score.

    Formula:

    ```python
    f1_score = 2 * (precision * recall) / (precision + recall)
    ```
    This is the harmonic mean of precision and recall.
    Its output range is `[0, 1]`. It works for both multi-class
    and multi-label classification.

    Args:
        average: Type of averaging to be performed on data.
            Acceptable values are `None`, `"micro"`, `"macro"`
            and `"weighted"`. Defaults to `None`.
            If `None`, no averaging is performed and `result()` will return
            the score for each class.
            If `"micro"`, compute metrics globally by counting the total
            true positives, false negatives and false positives.
            If `"macro"`, compute metrics for each label,
            and return their unweighted mean.
            This does not take label imbalance into account.
            If `"weighted"`, compute metrics for each label,
            and return their average weighted by support
            (the number of true instances for each label).
            This alters `"macro"` to account for label imbalance.
            It can result in an score that is not between precision and recall.
        threshold: Elements of `y_pred` greater than `threshold` are
            converted to be 1, and the rest 0. If `threshold` is
            `None`, the argmax of `y_pred` is converted to 1, and the rest to 0.
        name: Optional. String name of the metric instance.
        dtype: Optional. Data type of the metric result.

    Returns:
        F-1 Score: float.

    Example:

    >>> metric = keras.metrics.F1Score(threshold=0.5)
    >>> y_true = np.array([[1, 1, 1],
    ...                    [1, 0, 0],
    ...                    [1, 1, 0]], np.int32)
    >>> y_pred = np.array([[0.2, 0.6, 0.7],
    ...                    [0.2, 0.6, 0.6],
    ...                    [0.6, 0.8, 0.0]], np.float32)
    >>> metric.update_state(y_true, y_pred)
    >>> result = metric.result()
    array([0.5      , 0.8      , 0.6666667], dtype=float32)
    )-"
#' Computes F-1 Score.
#'
#' @description
#' Formula:
#'
#' ```python
#' f1_score = 2 * (precision * recall) / (precision + recall)
#' ```
#' This is the harmonic mean of precision and recall.
#' Its output range is `[0, 1]`. It works for both multi-class
#' and multi-label classification.
#'
#' # Returns
#' F-1 Score: float.
#'
#' # Examples
#' ```python
#' metric = keras.metrics.F1Score(threshold=0.5)
#' y_true = np.array([[1, 1, 1],
#'                    [1, 0, 0],
#'                    [1, 1, 0]], np.int32)
#' y_pred = np.array([[0.2, 0.6, 0.7],
#'                    [0.2, 0.6, 0.6],
#'                    [0.6, 0.8, 0.0]], np.float32)
#' metric.update_state(y_true, y_pred)
#' result = metric.result()
#' # array([0.5      , 0.8      , 0.6666667], dtype=float32)
#' ```
#'
#' @param average Type of averaging to be performed on data.
#'     Acceptable values are `None`, `"micro"`, `"macro"`
#'     and `"weighted"`. Defaults to `None`.
#'     If `None`, no averaging is performed and `result()` will return
#'     the score for each class.
#'     If `"micro"`, compute metrics globally by counting the total
#'     true positives, false negatives and false positives.
#'     If `"macro"`, compute metrics for each label,
#'     and return their unweighted mean.
#'     This does not take label imbalance into account.
#'     If `"weighted"`, compute metrics for each label,
#'     and return their average weighted by support
#'     (the number of true instances for each label).
#'     This alters `"macro"` to account for label imbalance.
#'     It can result in an score that is not between precision and recall.
#' @param threshold Elements of `y_pred` greater than `threshold` are
#'     converted to be 1, and the rest 0. If `threshold` is
#'     `None`, the argmax of `y_pred` is converted to 1, and the rest to 0.
#' @param name Optional. String name of the metric instance.
#' @param dtype Optional. Data type of the metric result.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/F1Score>
metric_f1_score <-
function (..., average = NULL, threshold = NULL, name = "f1_score",
    dtype = NULL)
{
    args <- capture_args2(NULL)
    do.call(keras$metrics$F1Score, args)
}


# keras.metrics.FBetaScore
# keras.src.metrics.f_score_metrics.FBetaScore
r"-(Computes F-Beta score.

    Formula:

    ```python
    b2 = beta ** 2
    f_beta_score = (1 + b2) * (precision * recall) / (precision * b2 + recall)
    ```
    This is the weighted harmonic mean of precision and recall.
    Its output range is `[0, 1]`. It works for both multi-class
    and multi-label classification.

    Args:
        average: Type of averaging to be performed across per-class results
            in the multi-class case.
            Acceptable values are `None`, `"micro"`, `"macro"` and
            `"weighted"`. Defaults to `None`.
            If `None`, no averaging is performed and `result()` will return
            the score for each class.
            If `"micro"`, compute metrics globally by counting the total
            true positives, false negatives and false positives.
            If `"macro"`, compute metrics for each label,
            and return their unweighted mean.
            This does not take label imbalance into account.
            If `"weighted"`, compute metrics for each label,
            and return their average weighted by support
            (the number of true instances for each label).
            This alters `"macro"` to account for label imbalance.
            It can result in an score that is not between precision and recall.
        beta: Determines the weight of given to recall
            in the harmonic mean between precision and recall (see pseudocode
            equation above). Defaults to `1`.
        threshold: Elements of `y_pred` greater than `threshold` are
            converted to be 1, and the rest 0. If `threshold` is
            `None`, the argmax of `y_pred` is converted to 1, and the rest to 0.
        name: Optional. String name of the metric instance.
        dtype: Optional. Data type of the metric result.

    Returns:
        F-Beta Score: float.

    Example:

    >>> metric = keras.metrics.FBetaScore(beta=2.0, threshold=0.5)
    >>> y_true = np.array([[1, 1, 1],
    ...                    [1, 0, 0],
    ...                    [1, 1, 0]], np.int32)
    >>> y_pred = np.array([[0.2, 0.6, 0.7],
    ...                    [0.2, 0.6, 0.6],
    ...                    [0.6, 0.8, 0.0]], np.float32)
    >>> metric.update_state(y_true, y_pred)
    >>> result = metric.result()
    >>> result
    [0.3846154 , 0.90909094, 0.8333334 ]
    )-"
#' Computes F-Beta score.
#'
#' @description
#' Formula:
#'
#' ```python
#' b2 = beta ** 2
#' f_beta_score = (1 + b2) * (precision * recall) / (precision * b2 + recall)
#' ```
#' This is the weighted harmonic mean of precision and recall.
#' Its output range is `[0, 1]`. It works for both multi-class
#' and multi-label classification.
#'
#' # Returns
#' F-Beta Score: float.
#'
#' # Examples
#' ```python
#' metric = keras.metrics.FBetaScore(beta=2.0, threshold=0.5)
#' y_true = np.array([[1, 1, 1],
#'                    [1, 0, 0],
#'                    [1, 1, 0]], np.int32)
#' y_pred = np.array([[0.2, 0.6, 0.7],
#'                    [0.2, 0.6, 0.6],
#'                    [0.6, 0.8, 0.0]], np.float32)
#' metric.update_state(y_true, y_pred)
#' result = metric.result()
#' result
#' # [0.3846154 , 0.90909094, 0.8333334 ]
#' ```
#'
#' @param average Type of averaging to be performed across per-class results
#'     in the multi-class case.
#'     Acceptable values are `None`, `"micro"`, `"macro"` and
#'     `"weighted"`. Defaults to `None`.
#'     If `None`, no averaging is performed and `result()` will return
#'     the score for each class.
#'     If `"micro"`, compute metrics globally by counting the total
#'     true positives, false negatives and false positives.
#'     If `"macro"`, compute metrics for each label,
#'     and return their unweighted mean.
#'     This does not take label imbalance into account.
#'     If `"weighted"`, compute metrics for each label,
#'     and return their average weighted by support
#'     (the number of true instances for each label).
#'     This alters `"macro"` to account for label imbalance.
#'     It can result in an score that is not between precision and recall.
#' @param beta Determines the weight of given to recall
#'     in the harmonic mean between precision and recall (see pseudocode
#'     equation above). Defaults to `1`.
#' @param threshold Elements of `y_pred` greater than `threshold` are
#'     converted to be 1, and the rest 0. If `threshold` is
#'     `None`, the argmax of `y_pred` is converted to 1, and the rest to 0.
#' @param name Optional. String name of the metric instance.
#' @param dtype Optional. Data type of the metric result.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/FBetaScore>
metric_fbeta_score <-
function (..., average = NULL, beta = 1, threshold = NULL, name = "fbeta_score",
    dtype = NULL)
{
    args <- capture_args2(NULL)
    do.call(keras$metrics$FBetaScore, args)
}


# keras.metrics.CategoricalHinge
# keras.src.metrics.hinge_metrics.CategoricalHinge
r"-(Computes the categorical hinge metric between `y_true` and `y_pred`.

    Args:
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Standalone usage:
    >>> m = keras.metrics.CategoricalHinge()
    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
    >>> m.result().numpy()
    1.4000001
    >>> m.reset_state()
    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],
    ...                sample_weight=[1, 0])
    >>> m.result()
    1.2
    )-"
#' Computes the categorical hinge metric between `y_true` and `y_pred`.
#'
#' @description
#' Formula:
#'
#' ```python
#' loss = maximum(neg - pos + 1, 0)
#' ```
#'
#' where `neg=maximum((1-y_true)*y_pred)` and `pos=sum(y_true*y_pred)`
#'
#' # Usage
#' Standalone usage:
#' ```python
#' m = keras.metrics.CategoricalHinge()
#' m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
#' m.result().numpy()
#' # 1.4000001
#' m.reset_state()
#' m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],
#'                sample_weight=[1, 0])
#' m.result()
#' # 1.2
#' ```
#'
#' # Returns
#' Categorical hinge loss values with shape = `[batch_size, d0, .. dN-1]`.
#'
#' # Examples
#' ```python
#' y_true = np.random.randint(0, 3, size=(2,))
#' y_true = np.eye(np.max(y_true) + 1)[y_true]
#' y_pred = np.random.random(size=(2, 3))
#' loss = keras.losses.categorical_hinge(y_true, y_pred)
#' ```
#'
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param y_true The ground truth values. `y_true` values are expected to be
#'     either `{-1, +1}` or `{0, 1}` (i.e. a one-hot-encoded tensor) with
#'     shape = `[batch_size, d0, .. dN]`.
#' @param y_pred The predicted values with shape = `[batch_size, d0, .. dN]`.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/CategoricalHinge>
metric_categorical_hinge <-
structure(function (y_true, y_pred, ..., name = "categorical_hinge",
    dtype = NULL)
{
    args <- capture_args2(NULL)
    callable <- if (missing(y_true) && missing(y_pred))
        keras$metrics$CategoricalHinge
    else keras$metrics$categorical_hinge
    do.call(callable, args)
}, py_function_name = "categorical_hinge")


# keras.metrics.Hinge
# keras.src.metrics.hinge_metrics.Hinge
r"-(Computes the hinge metric between `y_true` and `y_pred`.

    `y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are
    provided we will convert them to -1 or 1.

    Args:
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Standalone usage:

    >>> m = keras.metrics.Hinge()
    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
    >>> m.result()
    1.3
    >>> m.reset_state()
    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],
    ...                sample_weight=[1, 0])
    >>> m.result()
    1.1
    )-"
#' Computes the hinge metric between `y_true` and `y_pred`.
#'
#' @description
#' Formula:
#'
#' ```python
#' loss = mean(maximum(1 - y_true * y_pred, 0), axis=-1)
#' ```
#' `y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are
#' provided we will convert them to -1 or 1.
#'
#' # Usage
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.Hinge()
#' m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
#' m.result()
#' # 1.3
#' m.reset_state()
#' m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],
#'                sample_weight=[1, 0])
#' m.result()
#' # 1.1
#' ```
#'
#' # Returns
#' Hinge loss values with shape = `[batch_size, d0, .. dN-1]`.
#'
#' # Examples
#' ```python
#' y_true = np.random.choice([-1, 1], size=(2, 3))
#' y_pred = np.random.random(size=(2, 3))
#' loss = keras.losses.hinge(y_true, y_pred)
#' ```
#'
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param y_true The ground truth values. `y_true` values are expected to be -1
#'     or 1. If binary (0 or 1) labels are provided they will be converted
#'     to -1 or 1 with shape = `[batch_size, d0, .. dN]`.
#' @param y_pred The predicted values with shape = `[batch_size, d0, .. dN]`.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Hinge>
metric_hinge <-
structure(function (y_true, y_pred, ..., name = "hinge", dtype = NULL)
{
    args <- capture_args2(NULL)
    callable <- if (missing(y_true) && missing(y_pred))
        keras$metrics$Hinge
    else keras$metrics$hinge
    do.call(callable, args)
}, py_function_name = "hinge")


# keras.metrics.SquaredHinge
# keras.src.metrics.hinge_metrics.SquaredHinge
r"-(Computes the hinge metric between `y_true` and `y_pred`.

    `y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are
    provided we will convert them to -1 or 1.

    Args:
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Standalone usage:

    >>> m = keras.metrics.SquaredHinge()
    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
    >>> m.result()
    1.86
    >>> m.reset_state()
    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],
    ...                sample_weight=[1, 0])
    >>> m.result()
    1.46
    )-"
#' Computes the hinge metric between `y_true` and `y_pred`.
#'
#' @description
#' Formula:
#'
#' ```python
#' loss = mean(square(maximum(1 - y_true * y_pred, 0)), axis=-1)
#' ```
#' `y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are
#' provided we will convert them to -1 or 1.
#'
#' # Usage
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.SquaredHinge()
#' m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
#' m.result()
#' # 1.86
#' m.reset_state()
#' m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],
#'                sample_weight=[1, 0])
#' m.result()
#' # 1.46
#' ```
#'
#' # Returns
#' Squared hinge loss values with shape = `[batch_size, d0, .. dN-1]`.
#'
#' # Examples
#' ```python
#' y_true = np.random.choice([-1, 1], size=(2, 3))
#' y_pred = np.random.random(size=(2, 3))
#' loss = keras.losses.squared_hinge(y_true, y_pred)
#' ```
#'
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param y_true The ground truth values. `y_true` values are expected to be -1
#'     or 1. If binary (0 or 1) labels are provided we will convert them
#'     to -1 or 1 with shape = `[batch_size, d0, .. dN]`.
#' @param y_pred The predicted values with shape = `[batch_size, d0, .. dN]`.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SquaredHinge>
metric_squared_hinge <-
structure(function (y_true, y_pred, ..., name = "squared_hinge",
    dtype = NULL)
{
    args <- capture_args2(NULL)
    callable <- if (missing(y_true) && missing(y_pred))
        keras$metrics$SquaredHinge
    else keras$metrics$squared_hinge
    do.call(callable, args)
}, py_function_name = "squared_hinge")


# keras.metrics.BinaryIoU
# keras.src.metrics.iou_metrics.BinaryIoU
r"-(Computes the Intersection-Over-Union metric for class 0 and/or 1.

    Formula:

    ```python
    iou = true_positives / (true_positives + false_positives + false_negatives)
    ```
    Intersection-Over-Union is a common evaluation metric for semantic image
    segmentation.

    To compute IoUs, the predictions are accumulated in a confusion matrix,
    weighted by `sample_weight` and the metric is then calculated from it.

    If `sample_weight` is `None`, weights default to 1.
    Use `sample_weight` of 0 to mask values.

    This class can be used to compute IoUs for a binary classification task
    where the predictions are provided as logits. First a `threshold` is applied
    to the predicted values such that those that are below the `threshold` are
    converted to class 0 and those that are above the `threshold` are converted
    to class 1.

    IoUs for classes 0 and 1 are then computed, the mean of IoUs for the classes
    that are specified by `target_class_ids` is returned.

    Note: with `threshold=0`, this metric has the same behavior as `IoU`.

    Args:
        target_class_ids: A tuple or list of target class ids for which the
            metric is returned. Options are `[0]`, `[1]`, or `[0, 1]`. With
            `[0]` (or `[1]`), the IoU metric for class 0 (or class 1,
            respectively) is returned. With `[0, 1]`, the mean of IoUs for the
            two classes is returned.
        threshold: A threshold that applies to the prediction logits to convert
            them to either predicted class 0 if the logit is below `threshold`
            or predicted class 1 if the logit is above `threshold`.
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Examples:

    Standalone usage:

    >>> m = keras.metrics.BinaryIoU(target_class_ids=[0, 1], threshold=0.3)
    >>> m.update_state([0, 1, 0, 1], [0.1, 0.2, 0.4, 0.7])
    >>> m.result()
    0.33333334

    >>> m.reset_state()
    >>> m.update_state([0, 1, 0, 1], [0.1, 0.2, 0.4, 0.7],
    ...                sample_weight=[0.2, 0.3, 0.4, 0.1])
    >>> # cm = [[0.2, 0.4],
    >>> #        [0.3, 0.1]]
    >>> # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5],
    >>> # true_positives = [0.2, 0.1]
    >>> # iou = [0.222, 0.125]
    >>> m.result()
    0.17361112

    Usage with `compile()` API:

    ```python
    model.compile(
        optimizer='sgd',
        loss='mse',
        metrics=[keras.metrics.BinaryIoU(
            target_class_ids=[0],
            threshold=0.5
        )]
    )
    ```
    )-"
#' Computes the Intersection-Over-Union metric for class 0 and/or 1.
#'
#' @description
#' Formula:
#'
#' ```python
#' iou = true_positives / (true_positives + false_positives + false_negatives)
#' ```
#' Intersection-Over-Union is a common evaluation metric for semantic image
#' segmentation.
#'
#' To compute IoUs, the predictions are accumulated in a confusion matrix,
#' weighted by `sample_weight` and the metric is then calculated from it.
#'
#' If `sample_weight` is `None`, weights default to 1.
#' Use `sample_weight` of 0 to mask values.
#'
#' This class can be used to compute IoUs for a binary classification task
#' where the predictions are provided as logits. First a `threshold` is applied
#' to the predicted values such that those that are below the `threshold` are
#' converted to class 0 and those that are above the `threshold` are converted
#' to class 1.
#'
#' IoUs for classes 0 and 1 are then computed, the mean of IoUs for the classes
#' that are specified by `target_class_ids` is returned.
#'
#' # Note
#' with `threshold=0`, this metric has the same behavior as `IoU`.
#'
#' # Examples
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.BinaryIoU(target_class_ids=[0, 1], threshold=0.3)
#' m.update_state([0, 1, 0, 1], [0.1, 0.2, 0.4, 0.7])
#' m.result()
#' # 0.33333334
#' ```
#'
#' ```python
#' m.reset_state()
#' m.update_state([0, 1, 0, 1], [0.1, 0.2, 0.4, 0.7],
#'                sample_weight=[0.2, 0.3, 0.4, 0.1])
#' # cm = [[0.2, 0.4],
#' #        [0.3, 0.1]]
#' # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5],
#' # true_positives = [0.2, 0.1]
#' # iou = [0.222, 0.125]
#' m.result()
#' # 0.17361112
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(
#'     optimizer='sgd',
#'     loss='mse',
#'     metrics=[keras.metrics.BinaryIoU(
#'         target_class_ids=[0],
#'         threshold=0.5
#'     )]
#' )
#' ```
#'
#' @param target_class_ids A tuple or list of target class ids for which the
#'     metric is returned. Options are `[0]`, `[1]`, or `[0, 1]`. With
#'     `[0]` (or `[1]`), the IoU metric for class 0 (or class 1,
#'     respectively) is returned. With `[0, 1]`, the mean of IoUs for the
#'     two classes is returned.
#' @param threshold A threshold that applies to the prediction logits to convert
#'     them to either predicted class 0 if the logit is below `threshold`
#'     or predicted class 1 if the logit is above `threshold`.
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryIoU>
metric_binary_iou <-
function (..., target_class_ids = list(0L, 1L), threshold = 0.5,
    name = NULL, dtype = NULL)
{
    args <- capture_args2(NULL)
    do.call(keras$metrics$BinaryIoU, args)
}


# keras.metrics.IoU
# keras.src.metrics.iou_metrics.IoU
r"-(Computes the Intersection-Over-Union metric for specific target classes.

    Formula:

    ```python
    iou = true_positives / (true_positives + false_positives + false_negatives)
    ```
    Intersection-Over-Union is a common evaluation metric for semantic image
    segmentation.

    To compute IoUs, the predictions are accumulated in a confusion matrix,
    weighted by `sample_weight` and the metric is then calculated from it.

    If `sample_weight` is `None`, weights default to 1.
    Use `sample_weight` of 0 to mask values.

    Note, this class first computes IoUs for all individual classes, then
    returns the mean of IoUs for the classes that are specified by
    `target_class_ids`. If `target_class_ids` has only one id value, the IoU of
    that specific class is returned.

    Args:
        num_classes: The possible number of labels the prediction task can have.
        target_class_ids: A tuple or list of target class ids for which the
            metric is returned. To compute IoU for a specific class, a list
            (or tuple) of a single id value should be provided.
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.
        ignore_class: Optional integer. The ID of a class to be ignored during
            metric computation. This is useful, for example, in segmentation
            problems featuring a "void" class (commonly -1 or 255) in
            segmentation maps. By default (`ignore_class=None`), all classes are
              considered.
        sparse_y_true: Whether labels are encoded using integers or
            dense floating point vectors. If `False`, the `argmax` function
            is used to determine each sample's most likely associated label.
        sparse_y_pred: Whether predictions are encoded using integers or
            dense floating point vectors. If `False`, the `argmax` function
            is used to determine each sample's most likely associated label.
        axis: (Optional) -1 is the dimension containing the logits.
            Defaults to `-1`.

    Examples:

    Standalone usage:

    >>> # cm = [[1, 1],
    >>> #        [1, 1]]
    >>> # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]
    >>> # iou = true_positives / (sum_row + sum_col - true_positives))
    >>> # iou = [0.33, 0.33]
    >>> m = keras.metrics.IoU(num_classes=2, target_class_ids=[0])
    >>> m.update_state([0, 0, 1, 1], [0, 1, 0, 1])
    >>> m.result()
    0.33333334

    >>> m.reset_state()
    >>> m.update_state([0, 0, 1, 1], [0, 1, 0, 1],
    ...                sample_weight=[0.3, 0.3, 0.3, 0.1])
    >>> # cm = [[0.3, 0.3],
    >>> #        [0.3, 0.1]]
    >>> # sum_row = [0.6, 0.4], sum_col = [0.6, 0.4],
    >>> # true_positives = [0.3, 0.1]
    >>> # iou = [0.33, 0.14]
    >>> m.result()
    0.33333334

    Usage with `compile()` API:

    ```python
    model.compile(
        optimizer='sgd',
        loss='mse',
        metrics=[keras.metrics.IoU(num_classes=2, target_class_ids=[0])])
    ```
    )-"
#' Computes the Intersection-Over-Union metric for specific target classes.
#'
#' @description
#' Formula:
#'
#' ```python
#' iou = true_positives / (true_positives + false_positives + false_negatives)
#' ```
#' Intersection-Over-Union is a common evaluation metric for semantic image
#' segmentation.
#'
#' To compute IoUs, the predictions are accumulated in a confusion matrix,
#' weighted by `sample_weight` and the metric is then calculated from it.
#'
#' If `sample_weight` is `None`, weights default to 1.
#' Use `sample_weight` of 0 to mask values.
#'
#' Note, this class first computes IoUs for all individual classes, then
#' returns the mean of IoUs for the classes that are specified by
#' `target_class_ids`. If `target_class_ids` has only one id value, the IoU of
#' that specific class is returned.
#'
#' # Examples
#' Standalone usage:
#'
#' ```python
#' # cm = [[1, 1],
#' #        [1, 1]]
#' # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]
#' # iou = true_positives / (sum_row + sum_col - true_positives))
#' # iou = [0.33, 0.33]
#' m = keras.metrics.IoU(num_classes=2, target_class_ids=[0])
#' m.update_state([0, 0, 1, 1], [0, 1, 0, 1])
#' m.result()
#' # 0.33333334
#' ```
#'
#' ```python
#' m.reset_state()
#' m.update_state([0, 0, 1, 1], [0, 1, 0, 1],
#'                sample_weight=[0.3, 0.3, 0.3, 0.1])
#' # cm = [[0.3, 0.3],
#' #        [0.3, 0.1]]
#' # sum_row = [0.6, 0.4], sum_col = [0.6, 0.4],
#' # true_positives = [0.3, 0.1]
#' # iou = [0.33, 0.14]
#' m.result()
#' # 0.33333334
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(
#'     optimizer='sgd',
#'     loss='mse',
#'     metrics=[keras.metrics.IoU(num_classes=2, target_class_ids=[0])])
#' ```
#'
#' @param num_classes The possible number of labels the prediction task can have.
#' @param target_class_ids A tuple or list of target class ids for which the
#'     metric is returned. To compute IoU for a specific class, a list
#'     (or tuple) of a single id value should be provided.
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param ignore_class Optional integer. The ID of a class to be ignored during
#'     metric computation. This is useful, for example, in segmentation
#'     problems featuring a "void" class (commonly -1 or 255) in
#'     segmentation maps. By default (`ignore_class=None`), all classes are
#'       considered.
#' @param sparse_y_true Whether labels are encoded using integers or
#'     dense floating point vectors. If `False`, the `argmax` function
#'     is used to determine each sample's most likely associated label.
#' @param sparse_y_pred Whether predictions are encoded using integers or
#'     dense floating point vectors. If `False`, the `argmax` function
#'     is used to determine each sample's most likely associated label.
#' @param axis (Optional) -1 is the dimension containing the logits.
#'     Defaults to `-1`.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/IoU>
metric_iou <-
function (..., num_classes, target_class_ids, name = NULL, dtype = NULL,
    ignore_class = NULL, sparse_y_true = TRUE, sparse_y_pred = TRUE,
    axis = -1L)
{
    args <- capture_args2(list(ignore_class = as_integer, axis = as_axis))
    do.call(keras$metrics$IoU, args)
}


# keras.metrics.MeanIoU
# keras.src.metrics.iou_metrics.MeanIoU
r"-(Computes the mean Intersection-Over-Union metric.

    Formula:

    ```python
    iou = true_positives / (true_positives + false_positives + false_negatives)
    ```
    Intersection-Over-Union is a common evaluation metric for semantic image
    segmentation.

    To compute IoUs, the predictions are accumulated in a confusion matrix,
    weighted by `sample_weight` and the metric is then calculated from it.

    If `sample_weight` is `None`, weights default to 1.
    Use `sample_weight` of 0 to mask values.

    Note that this class first computes IoUs for all individual classes, then
    returns the mean of these values.

    Args:
        num_classes: The possible number of labels the prediction task can have.
            This value must be provided, since a confusion matrix of dimension =
            [num_classes, num_classes] will be allocated.
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.
        ignore_class: Optional integer. The ID of a class to be ignored during
            metric computation. This is useful, for example, in segmentation
            problems featuring a "void" class (commonly -1 or 255) in
            segmentation maps. By default (`ignore_class=None`), all classes are
            considered.
        sparse_y_true: Whether labels are encoded using integers or
            dense floating point vectors. If `False`, the `argmax` function
            is used to determine each sample's most likely associated label.
        sparse_y_pred: Whether predictions are encoded using integers or
            dense floating point vectors. If `False`, the `argmax` function
            is used to determine each sample's most likely associated label.
        axis: (Optional) The dimension containing the logits. Defaults to `-1`.

    Examples:

    Standalone usage:

    >>> # cm = [[1, 1],
    >>> #        [1, 1]]
    >>> # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]
    >>> # iou = true_positives / (sum_row + sum_col - true_positives))
    >>> # result = (1 / (2 + 2 - 1) + 1 / (2 + 2 - 1)) / 2 = 0.33
    >>> m = keras.metrics.MeanIoU(num_classes=2)
    >>> m.update_state([0, 0, 1, 1], [0, 1, 0, 1])
    >>> m.result()
    0.33333334

    >>> m.reset_state()
    >>> m.update_state([0, 0, 1, 1], [0, 1, 0, 1],
    ...                sample_weight=[0.3, 0.3, 0.3, 0.1])
    >>> m.result().numpy()
    0.23809525

    Usage with `compile()` API:

    ```python
    model.compile(
        optimizer='sgd',
        loss='mse',
        metrics=[keras.metrics.MeanIoU(num_classes=2)])
    ```
    )-"
#' Computes the mean Intersection-Over-Union metric.
#'
#' @description
#' Formula:
#'
#' ```python
#' iou = true_positives / (true_positives + false_positives + false_negatives)
#' ```
#' Intersection-Over-Union is a common evaluation metric for semantic image
#' segmentation.
#'
#' To compute IoUs, the predictions are accumulated in a confusion matrix,
#' weighted by `sample_weight` and the metric is then calculated from it.
#'
#' If `sample_weight` is `None`, weights default to 1.
#' Use `sample_weight` of 0 to mask values.
#'
#' Note that this class first computes IoUs for all individual classes, then
#' returns the mean of these values.
#'
#' # Examples
#' Standalone usage:
#'
#' ```python
#' # cm = [[1, 1],
#' #        [1, 1]]
#' # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]
#' # iou = true_positives / (sum_row + sum_col - true_positives))
#' # result = (1 / (2 + 2 - 1) + 1 / (2 + 2 - 1)) / 2 = 0.33
#' m = keras.metrics.MeanIoU(num_classes=2)
#' m.update_state([0, 0, 1, 1], [0, 1, 0, 1])
#' m.result()
#' # 0.33333334
#' ```
#'
#' ```python
#' m.reset_state()
#' m.update_state([0, 0, 1, 1], [0, 1, 0, 1],
#'                sample_weight=[0.3, 0.3, 0.3, 0.1])
#' m.result().numpy()
#' # 0.23809525
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(
#'     optimizer='sgd',
#'     loss='mse',
#'     metrics=[keras.metrics.MeanIoU(num_classes=2)])
#' ```
#'
#' @param num_classes The possible number of labels the prediction task can have.
#'     This value must be provided, since a confusion matrix of dimension =
#'     `[num_classes, num_classes]` will be allocated.
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param ignore_class Optional integer. The ID of a class to be ignored during
#'     metric computation. This is useful, for example, in segmentation
#'     problems featuring a "void" class (commonly -1 or 255) in
#'     segmentation maps. By default (`ignore_class=None`), all classes are
#'     considered.
#' @param sparse_y_true Whether labels are encoded using integers or
#'     dense floating point vectors. If `False`, the `argmax` function
#'     is used to determine each sample's most likely associated label.
#' @param sparse_y_pred Whether predictions are encoded using integers or
#'     dense floating point vectors. If `False`, the `argmax` function
#'     is used to determine each sample's most likely associated label.
#' @param axis (Optional) The dimension containing the logits. Defaults to `-1`.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanIoU>
metric_mean_iou <-
function (..., num_classes, name = NULL, dtype = NULL, ignore_class = NULL,
    sparse_y_true = TRUE, sparse_y_pred = TRUE, axis = -1L)
{
    args <- capture_args2(list(ignore_class = as_integer, axis = as_axis))
    do.call(keras$metrics$MeanIoU, args)
}


# keras.metrics.OneHotIoU
# keras.src.metrics.iou_metrics.OneHotIoU
r"-(Computes the Intersection-Over-Union metric for one-hot encoded labels.

    Formula:

    ```python
    iou = true_positives / (true_positives + false_positives + false_negatives)
    ```
    Intersection-Over-Union is a common evaluation metric for semantic image
    segmentation.

    To compute IoUs, the predictions are accumulated in a confusion matrix,
    weighted by `sample_weight` and the metric is then calculated from it.

    If `sample_weight` is `None`, weights default to 1.
    Use `sample_weight` of 0 to mask values.

    This class can be used to compute IoU for multi-class classification tasks
    where the labels are one-hot encoded (the last axis should have one
    dimension per class). Note that the predictions should also have the same
    shape. To compute the IoU, first the labels and predictions are converted
    back into integer format by taking the argmax over the class axis. Then the
    same computation steps as for the base `IoU` class apply.

    Note, if there is only one channel in the labels and predictions, this class
    is the same as class `IoU`. In this case, use `IoU` instead.

    Also, make sure that `num_classes` is equal to the number of classes in the
    data, to avoid a "labels out of bound" error when the confusion matrix is
    computed.

    Args:
        num_classes: The possible number of labels the prediction task can have.
        target_class_ids: A tuple or list of target class ids for which the
            metric is returned. To compute IoU for a specific class, a list
            (or tuple) of a single id value should be provided.
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.
        ignore_class: Optional integer. The ID of a class to be ignored during
            metric computation. This is useful, for example, in segmentation
            problems featuring a "void" class (commonly -1 or 255) in
            segmentation maps. By default (`ignore_class=None`), all classes are
            considered.
        sparse_y_pred: Whether predictions are encoded using integers or
            dense floating point vectors. If `False`, the `argmax` function
            is used to determine each sample's most likely associated label.
        axis: (Optional) The dimension containing the logits. Defaults to `-1`.

    Examples:

    Standalone usage:

    >>> y_true = np.array([[0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0]])
    >>> y_pred = np.array([[0.2, 0.3, 0.5], [0.1, 0.2, 0.7], [0.5, 0.3, 0.1],
    ...                       [0.1, 0.4, 0.5]])
    >>> sample_weight = [0.1, 0.2, 0.3, 0.4]
    >>> m = keras.metrics.OneHotIoU(num_classes=3, target_class_ids=[0, 2])
    >>> m.update_state(
    ...     y_true=y_true, y_pred=y_pred, sample_weight=sample_weight)
    >>> # cm = [[0, 0, 0.2+0.4],
    >>> #       [0.3, 0, 0],
    >>> #       [0, 0, 0.1]]
    >>> # sum_row = [0.3, 0, 0.7], sum_col = [0.6, 0.3, 0.1]
    >>> # true_positives = [0, 0, 0.1]
    >>> # single_iou = true_positives / (sum_row + sum_col - true_positives))
    >>> # mean_iou = (0 / (0.3 + 0.6 - 0) + 0.1 / (0.7 + 0.1 - 0.1)) / 2
    >>> m.result()
    0.071

    Usage with `compile()` API:

    ```python
    model.compile(
        optimizer='sgd',
        loss='mse',
        metrics=[keras.metrics.OneHotIoU(
            num_classes=3,
            target_class_id=[1]
        )]
    )
    ```
    )-"
#' Computes the Intersection-Over-Union metric for one-hot encoded labels.
#'
#' @description
#' Formula:
#'
#' ```python
#' iou = true_positives / (true_positives + false_positives + false_negatives)
#' ```
#' Intersection-Over-Union is a common evaluation metric for semantic image
#' segmentation.
#'
#' To compute IoUs, the predictions are accumulated in a confusion matrix,
#' weighted by `sample_weight` and the metric is then calculated from it.
#'
#' If `sample_weight` is `None`, weights default to 1.
#' Use `sample_weight` of 0 to mask values.
#'
#' This class can be used to compute IoU for multi-class classification tasks
#' where the labels are one-hot encoded (the last axis should have one
#' dimension per class). Note that the predictions should also have the same
#' shape. To compute the IoU, first the labels and predictions are converted
#' back into integer format by taking the argmax over the class axis. Then the
#' same computation steps as for the base `IoU` class apply.
#'
#' Note, if there is only one channel in the labels and predictions, this class
#' is the same as class `IoU`. In this case, use `IoU` instead.
#'
#' Also, make sure that `num_classes` is equal to the number of classes in the
#' data, to avoid a "labels out of bound" error when the confusion matrix is
#' computed.
#'
#' # Examples
#' Standalone usage:
#'
#' ```python
#' y_true = np.array([[0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0]])
#' y_pred = np.array([[0.2, 0.3, 0.5], [0.1, 0.2, 0.7], [0.5, 0.3, 0.1],
#'                       [0.1, 0.4, 0.5]])
#' sample_weight = [0.1, 0.2, 0.3, 0.4]
#' m = keras.metrics.OneHotIoU(num_classes=3, target_class_ids=[0, 2])
#' m.update_state(
#'     y_true=y_true, y_pred=y_pred, sample_weight=sample_weight)
#' # cm = [[0, 0, 0.2+0.4],
#' #       [0.3, 0, 0],
#' #       [0, 0, 0.1]]
#' # sum_row = [0.3, 0, 0.7], sum_col = [0.6, 0.3, 0.1]
#' # true_positives = [0, 0, 0.1]
#' # single_iou = true_positives / (sum_row + sum_col - true_positives))
#' # mean_iou = (0 / (0.3 + 0.6 - 0) + 0.1 / (0.7 + 0.1 - 0.1)) / 2
#' m.result()
#' # 0.071
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(
#'     optimizer='sgd',
#'     loss='mse',
#'     metrics=[keras.metrics.OneHotIoU(
#'         num_classes=3,
#'         target_class_id=[1]
#'     )]
#' )
#' ```
#'
#' @param num_classes The possible number of labels the prediction task can have.
#' @param target_class_ids A tuple or list of target class ids for which the
#'     metric is returned. To compute IoU for a specific class, a list
#'     (or tuple) of a single id value should be provided.
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param ignore_class Optional integer. The ID of a class to be ignored during
#'     metric computation. This is useful, for example, in segmentation
#'     problems featuring a "void" class (commonly -1 or 255) in
#'     segmentation maps. By default (`ignore_class=None`), all classes are
#'     considered.
#' @param sparse_y_pred Whether predictions are encoded using integers or
#'     dense floating point vectors. If `False`, the `argmax` function
#'     is used to determine each sample's most likely associated label.
#' @param axis (Optional) The dimension containing the logits. Defaults to `-1`.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/OneHotIoU>
metric_one_hot_iou <-
function (..., num_classes, target_class_ids, name = NULL, dtype = NULL,
    ignore_class = NULL, sparse_y_pred = FALSE, axis = -1L)
{
    args <- capture_args2(list(ignore_class = as_integer, axis = as_axis))
    do.call(keras$metrics$OneHotIoU, args)
}


# keras.metrics.OneHotMeanIoU
# keras.src.metrics.iou_metrics.OneHotMeanIoU
r"-(Computes mean Intersection-Over-Union metric for one-hot encoded labels.

    Formula:

    ```python
    iou = true_positives / (true_positives + false_positives + false_negatives)
    ```
    Intersection-Over-Union is a common evaluation metric for semantic image
    segmentation.

    To compute IoUs, the predictions are accumulated in a confusion matrix,
    weighted by `sample_weight` and the metric is then calculated from it.

    If `sample_weight` is `None`, weights default to 1.
    Use `sample_weight` of 0 to mask values.

    This class can be used to compute the mean IoU for multi-class
    classification tasks where the labels are one-hot encoded (the last axis
    should have one dimension per class). Note that the predictions should also
    have the same shape. To compute the mean IoU, first the labels and
    predictions are converted back into integer format by taking the argmax over
    the class axis. Then the same computation steps as for the base `MeanIoU`
    class apply.

    Note, if there is only one channel in the labels and predictions, this class
    is the same as class `MeanIoU`. In this case, use `MeanIoU` instead.

    Also, make sure that `num_classes` is equal to the number of classes in the
    data, to avoid a "labels out of bound" error when the confusion matrix is
    computed.

    Args:
        num_classes: The possible number of labels the prediction task can have.
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.
        ignore_class: Optional integer. The ID of a class to be ignored during
            metric computation. This is useful, for example, in segmentation
            problems featuring a "void" class (commonly -1 or 255) in
            segmentation maps. By default (`ignore_class=None`), all classes are
            considered.
        sparse_y_pred: Whether predictions are encoded using natural numbers or
            probability distribution vectors. If `False`, the `argmax`
            function will be used to determine each sample's most likely
            associated label.
        axis: (Optional) The dimension containing the logits. Defaults to `-1`.

    Examples:

    Standalone usage:

    >>> y_true = np.array([[0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0]])
    >>> y_pred = np.array([[0.2, 0.3, 0.5], [0.1, 0.2, 0.7], [0.5, 0.3, 0.1],
    ...                       [0.1, 0.4, 0.5]])
    >>> sample_weight = [0.1, 0.2, 0.3, 0.4]
    >>> m = keras.metrics.OneHotMeanIoU(num_classes=3)
    >>> m.update_state(
    ...     y_true=y_true, y_pred=y_pred, sample_weight=sample_weight)
    >>> # cm = [[0, 0, 0.2+0.4],
    >>> #       [0.3, 0, 0],
    >>> #       [0, 0, 0.1]]
    >>> # sum_row = [0.3, 0, 0.7], sum_col = [0.6, 0.3, 0.1]
    >>> # true_positives = [0, 0, 0.1]
    >>> # single_iou = true_positives / (sum_row + sum_col - true_positives))
    >>> # mean_iou = (0 + 0 + 0.1 / (0.7 + 0.1 - 0.1)) / 3
    >>> m.result()
    0.048

    Usage with `compile()` API:

    ```python
    model.compile(
        optimizer='sgd',
        loss='mse',
        metrics=[keras.metrics.OneHotMeanIoU(num_classes=3)])
    ```
    )-"
#' Computes mean Intersection-Over-Union metric for one-hot encoded labels.
#'
#' @description
#' Formula:
#'
#' ```python
#' iou = true_positives / (true_positives + false_positives + false_negatives)
#' ```
#' Intersection-Over-Union is a common evaluation metric for semantic image
#' segmentation.
#'
#' To compute IoUs, the predictions are accumulated in a confusion matrix,
#' weighted by `sample_weight` and the metric is then calculated from it.
#'
#' If `sample_weight` is `None`, weights default to 1.
#' Use `sample_weight` of 0 to mask values.
#'
#' This class can be used to compute the mean IoU for multi-class
#' classification tasks where the labels are one-hot encoded (the last axis
#' should have one dimension per class). Note that the predictions should also
#' have the same shape. To compute the mean IoU, first the labels and
#' predictions are converted back into integer format by taking the argmax over
#' the class axis. Then the same computation steps as for the base `MeanIoU`
#' class apply.
#'
#' Note, if there is only one channel in the labels and predictions, this class
#' is the same as class `MeanIoU`. In this case, use `MeanIoU` instead.
#'
#' Also, make sure that `num_classes` is equal to the number of classes in the
#' data, to avoid a "labels out of bound" error when the confusion matrix is
#' computed.
#'
#' # Examples
#' Standalone usage:
#'
#' ```python
#' y_true = np.array([[0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0]])
#' y_pred = np.array([[0.2, 0.3, 0.5], [0.1, 0.2, 0.7], [0.5, 0.3, 0.1],
#'                       [0.1, 0.4, 0.5]])
#' sample_weight = [0.1, 0.2, 0.3, 0.4]
#' m = keras.metrics.OneHotMeanIoU(num_classes=3)
#' m.update_state(
#'     y_true=y_true, y_pred=y_pred, sample_weight=sample_weight)
#' # cm = [[0, 0, 0.2+0.4],
#' #       [0.3, 0, 0],
#' #       [0, 0, 0.1]]
#' # sum_row = [0.3, 0, 0.7], sum_col = [0.6, 0.3, 0.1]
#' # true_positives = [0, 0, 0.1]
#' # single_iou = true_positives / (sum_row + sum_col - true_positives))
#' # mean_iou = (0 + 0 + 0.1 / (0.7 + 0.1 - 0.1)) / 3
#' m.result()
#' # 0.048
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(
#'     optimizer='sgd',
#'     loss='mse',
#'     metrics=[keras.metrics.OneHotMeanIoU(num_classes=3)])
#' ```
#'
#' @param num_classes The possible number of labels the prediction task can have.
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param ignore_class Optional integer. The ID of a class to be ignored during
#'     metric computation. This is useful, for example, in segmentation
#'     problems featuring a "void" class (commonly -1 or 255) in
#'     segmentation maps. By default (`ignore_class=None`), all classes are
#'     considered.
#' @param sparse_y_pred Whether predictions are encoded using natural numbers or
#'     probability distribution vectors. If `False`, the `argmax`
#'     function will be used to determine each sample's most likely
#'     associated label.
#' @param axis (Optional) The dimension containing the logits. Defaults to `-1`.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/OneHotMeanIoU>
metric_one_hot_mean_iou <-
function (..., num_classes, name = NULL, dtype = NULL, ignore_class = NULL,
    sparse_y_pred = FALSE, axis = -1L)
{
    args <- capture_args2(list(ignore_class = as_integer, axis = as_axis))
    do.call(keras$metrics$OneHotMeanIoU, args)
}


# keras.metrics.BinaryCrossentropy
# keras.src.metrics.probabilistic_metrics.BinaryCrossentropy
r"-(Computes the crossentropy metric between the labels and predictions.

    This is the crossentropy metric class to be used when there are only two
    label classes (0 and 1).

    Args:
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.
        from_logits: (Optional) Whether output is expected
            to be a logits tensor. By default, we consider
            that output encodes a probability distribution.
        label_smoothing: (Optional) Float in `[0, 1]`.
            When > 0, label values are smoothed,
            meaning the confidence on label values are relaxed.
            e.g. `label_smoothing=0.2` means that we will use
            a value of 0.1 for label "0" and 0.9 for label "1".

    Examples:

    Standalone usage:

    >>> m = keras.metrics.BinaryCrossentropy()
    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
    >>> m.result()
    0.81492424

    >>> m.reset_state()
    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],
    ...                sample_weight=[1, 0])
    >>> m.result()
    0.9162905

    Usage with `compile()` API:

    ```python
    model.compile(
        optimizer='sgd',
        loss='mse',
        metrics=[keras.metrics.BinaryCrossentropy()])
    ```
    )-"
#' Computes the crossentropy metric between the labels and predictions.
#'
#' @description
#' This is the crossentropy metric class to be used when there are only two
#' label classes (0 and 1).
#'
#' # Examples
#' ```python
#' y_true = [[0, 1], [0, 0]]
#' y_pred = [[0.6, 0.4], [0.4, 0.6]]
#' loss = keras.losses.binary_crossentropy(y_true, y_pred)
#' assert loss.shape == (2,)
#' loss
#' # array([0.916 , 0.714], dtype=float32)
#' ```
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.BinaryCrossentropy()
#' m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
#' m.result()
#' # 0.81492424
#' ```
#'
#' ```python
#' m.reset_state()
#' m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],
#'                sample_weight=[1, 0])
#' m.result()
#' # 0.9162905
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(
#'     optimizer='sgd',
#'     loss='mse',
#'     metrics=[keras.metrics.BinaryCrossentropy()])
#' ```
#'
#' # Returns
#' Binary crossentropy loss value. shape = `[batch_size, d0, .. dN-1]`.
#'
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param from_logits Whether `y_pred` is expected to be a logits tensor. By
#'     default, we assume that `y_pred` encodes a probability distribution.
#' @param label_smoothing Float in `[0, 1]`. If > `0` then smooth the labels by
#'     squeezing them towards 0.5, that is,
#'     using `1. - 0.5 * label_smoothing` for the target class
#'     and `0.5 * label_smoothing` for the non-target class.
#' @param y_true Ground truth values. shape = `[batch_size, d0, .. dN]`.
#' @param y_pred The predicted values. shape = `[batch_size, d0, .. dN]`.
#' @param axis The axis along which the mean is computed. Defaults to `-1`.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryCrossentropy>
metric_binary_crossentropy <-
structure(function (y_true, y_pred, from_logits = FALSE, label_smoothing = 0,
    axis = -1L, ..., name = "binary_crossentropy", dtype = NULL)
{
    args <- capture_args2(list(label_smoothing = as_integer,
        axis = as_axis))
    callable <- if (missing(y_true) && missing(y_pred))
        keras$metrics$BinaryCrossentropy
    else keras$metrics$binary_crossentropy
    do.call(callable, args)
}, py_function_name = "binary_crossentropy")


# keras.metrics.CategoricalCrossentropy
# keras.src.metrics.probabilistic_metrics.CategoricalCrossentropy
r"-(Computes the crossentropy metric between the labels and predictions.

    This is the crossentropy metric class to be used when there are multiple
    label classes (2 or more). It assumes that labels are one-hot encoded,
    e.g., when labels values are `[2, 0, 1]`, then
    `y_true` is `[[0, 0, 1], [1, 0, 0], [0, 1, 0]]`.

    Args:
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.
        from_logits: (Optional) Whether output is expected to be
            a logits tensor. By default, we consider that output
            encodes a probability distribution.
        label_smoothing: (Optional) Float in `[0, 1]`.
            When > 0, label values are smoothed, meaning the confidence
            on label values are relaxed. e.g. `label_smoothing=0.2` means
            that we will use a value of 0.1 for label
            "0" and 0.9 for label "1".
        axis: (Optional) Defaults to `-1`.
            The dimension along which entropy is computed.

    Examples:

    Standalone usage:

    >>> # EPSILON = 1e-7, y = y_true, y` = y_pred
    >>> # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)
    >>> # y` = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]
    >>> # xent = -sum(y * log(y'), axis = -1)
    >>> #      = -((log 0.95), (log 0.1))
    >>> #      = [0.051, 2.302]
    >>> # Reduced xent = (0.051 + 2.302) / 2
    >>> m = keras.metrics.CategoricalCrossentropy()
    >>> m.update_state([[0, 1, 0], [0, 0, 1]],
    ...                [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])
    >>> m.result()
    1.1769392

    >>> m.reset_state()
    >>> m.update_state([[0, 1, 0], [0, 0, 1]],
    ...                [[0.05, 0.95, 0], [0.1, 0.8, 0.1]],
    ...                sample_weight=np.array([0.3, 0.7]))
    >>> m.result()
    1.6271976

    Usage with `compile()` API:

    ```python
    model.compile(
        optimizer='sgd',
        loss='mse',
        metrics=[keras.metrics.CategoricalCrossentropy()])
    ```
    )-"
#' Computes the crossentropy metric between the labels and predictions.
#'
#' @description
#' This is the crossentropy metric class to be used when there are multiple
#' label classes (2 or more). It assumes that labels are one-hot encoded,
#' e.g., when labels values are `[2, 0, 1]`, then
#' `y_true` is `[[0, 0, 1], [1, 0, 0], [0, 1, 0]]`.
#'
#' # Examples
#' ```python
#' y_true = [[0, 1, 0], [0, 0, 1]]
#' y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
#' loss = keras.losses.categorical_crossentropy(y_true, y_pred)
#' assert loss.shape == (2,)
#' loss
#' # array([0.0513, 2.303], dtype=float32)
#' ```
#' Standalone usage:
#'
#' ```python
#' # EPSILON = 1e-7, y = y_true, y` = y_pred
#' # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)
#' # y` = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]
#' # xent = -sum(y * log(y'), axis = -1)
#' #      = -((log 0.95), (log 0.1))
#' #      = [0.051, 2.302]
#' # Reduced xent = (0.051 + 2.302) / 2
#' m = keras.metrics.CategoricalCrossentropy()
#' m.update_state([[0, 1, 0], [0, 0, 1]],
#'                [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])
#' m.result()
#' # 1.1769392
#' ```
#'
#' ```python
#' m.reset_state()
#' m.update_state([[0, 1, 0], [0, 0, 1]],
#'                [[0.05, 0.95, 0], [0.1, 0.8, 0.1]],
#'                sample_weight=np.array([0.3, 0.7]))
#' m.result()
#' # 1.6271976
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(
#'     optimizer='sgd',
#'     loss='mse',
#'     metrics=[keras.metrics.CategoricalCrossentropy()])
#' ```
#'
#' # Returns
#' Categorical crossentropy loss value.
#'
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param from_logits Whether `y_pred` is expected to be a logits tensor. By
#'     default, we assume that `y_pred` encodes a probability distribution.
#' @param label_smoothing Float in `[0, 1].` If > `0` then smooth the labels. For
#'     example, if `0.1`, use `0.1 / num_classes` for non-target labels
#'     and `0.9 + 0.1 / num_classes` for target labels.
#' @param axis Defaults to `-1`. The dimension along which the entropy is
#'     computed.
#' @param y_true Tensor of one-hot true targets.
#' @param y_pred Tensor of predicted targets.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/CategoricalCrossentropy>
metric_categorical_crossentropy <-
structure(function (y_true, y_pred, from_logits = FALSE, label_smoothing = 0,
    axis = -1L, ..., name = "categorical_crossentropy", dtype = NULL)
{
    args <- capture_args2(list(label_smoothing = as_integer,
        axis = as_axis))
    callable <- if (missing(y_true) && missing(y_pred))
        keras$metrics$CategoricalCrossentropy
    else keras$metrics$categorical_crossentropy
    do.call(callable, args)
}, py_function_name = "categorical_crossentropy")


# keras.metrics.KLDivergence
# keras.src.metrics.probabilistic_metrics.KLDivergence
r"-(Computes Kullback-Leibler divergence metric between `y_true` and
    `y_pred`.

    Formula:

    ```python
    metric = y_true * log(y_true / y_pred)
    ```

    Args:
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Standalone usage:

    >>> m = keras.metrics.KLDivergence()
    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
    >>> m.result()
    0.45814306

    >>> m.reset_state()
    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],
    ...                sample_weight=[1, 0])
    >>> m.result()
    0.9162892

    Usage with `compile()` API:

    ```python
    model.compile(optimizer='sgd',
                  loss='mse',
                  metrics=[keras.metrics.KLDivergence()])
    ```
    )-"
#' Computes Kullback-Leibler divergence metric between `y_true` and
#'
#' @description
#' Formula:
#'
#' ```python
#' loss = y_true * log(y_true / y_pred)
#' ```
#' `y_pred`.
#'
#' Formula:
#'
#' ```python
#' metric = y_true * log(y_true / y_pred)
#' ```
#'
#' # Usage
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.KLDivergence()
#' m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
#' m.result()
#' # 0.45814306
#' ```
#'
#' ```python
#' m.reset_state()
#' m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],
#'                sample_weight=[1, 0])
#' m.result()
#' # 0.9162892
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(optimizer='sgd',
#'               loss='mse',
#'               metrics=[keras.metrics.KLDivergence()])
#' ```
#'
#' # Returns
#' KL Divergence loss values with shape = `[batch_size, d0, .. dN-1]`.
#'
#' # Examples
#' ```python
#' y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float32)
#' y_pred = np.random.random(size=(2, 3))
#' loss = keras.losses.kl_divergence(y_true, y_pred)
#' assert loss.shape == (2,)
#' y_true = ops.clip(y_true, 1e-7, 1)
#' y_pred = ops.clip(y_pred, 1e-7, 1)
#' assert np.array_equal(
#'     loss, np.sum(y_true * np.log(y_true / y_pred), axis=-1))
#' ```
#'
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param y_true Tensor of true targets.
#' @param y_pred Tensor of predicted targets.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/KLDivergence>
metric_kl_divergence <-
structure(function (y_true, y_pred, ..., name = "kl_divergence",
    dtype = NULL)
{
    args <- capture_args2(NULL)
    callable <- if (missing(y_true) && missing(y_pred))
        keras$metrics$KLDivergence
    else keras$metrics$kl_divergence
    do.call(callable, args)
}, py_function_name = "kl_divergence")


# keras.metrics.Poisson
# keras.src.metrics.probabilistic_metrics.Poisson
r"-(Computes the Poisson metric between `y_true` and `y_pred`.

    Formula:

    ```python
    metric = y_pred - y_true * log(y_pred)
    ```

    Args:
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Examples:

    Standalone usage:

    >>> m = keras.metrics.Poisson()
    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
    >>> m.result()
    0.49999997

    >>> m.reset_state()
    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],
    ...                sample_weight=[1, 0])
    >>> m.result()
    0.99999994

    Usage with `compile()` API:

    ```python
    model.compile(optimizer='sgd',
                  loss='mse',
                  metrics=[keras.metrics.Poisson()])
    ```
    )-"
#' Computes the Poisson metric between `y_true` and `y_pred`.
#'
#' @description
#' Formula:
#'
#' ```python
#' loss = y_pred - y_true * log(y_pred)
#' ```
#' Formula:
#'
#' ```python
#' metric = y_pred - y_true * log(y_pred)
#' ```
#'
#' # Examples
#' ```python
#' y_true = np.random.randint(0, 2, size=(2, 3))
#' y_pred = np.random.random(size=(2, 3))
#' loss = keras.losses.poisson(y_true, y_pred)
#' assert loss.shape == (2,)
#' y_pred = y_pred + 1e-7
#' assert np.allclose(
#'     loss, np.mean(y_pred - y_true * np.log(y_pred), axis=-1),
#'     atol=1e-5)
#' ```
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.Poisson()
#' m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
#' m.result()
#' # 0.49999997
#' ```
#'
#' ```python
#' m.reset_state()
#' m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],
#'                sample_weight=[1, 0])
#' m.result()
#' # 0.99999994
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(optimizer='sgd',
#'               loss='mse',
#'               metrics=[keras.metrics.Poisson()])
#' ```
#'
#' # Returns
#' Poisson loss values with shape = `[batch_size, d0, .. dN-1]`.
#'
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param y_true Ground truth values. shape = `[batch_size, d0, .. dN]`.
#' @param y_pred The predicted values. shape = `[batch_size, d0, .. dN]`.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Poisson>
metric_poisson <-
structure(function (y_true, y_pred, ..., name = "poisson", dtype = NULL)
{
    args <- capture_args2(NULL)
    callable <- if (missing(y_true) && missing(y_pred))
        keras$metrics$Poisson
    else keras$metrics$poisson
    do.call(callable, args)
}, py_function_name = "poisson")


# keras.metrics.SparseCategoricalCrossentropy
# keras.src.metrics.probabilistic_metrics.SparseCategoricalCrossentropy
r"-(Computes the crossentropy metric between the labels and predictions.

    Use this crossentropy metric when there are two or more label classes.
    It expects labels to be provided as integers. If you want to provide labels
    that are one-hot encoded, please use the `CategoricalCrossentropy`
    metric instead.

    There should be `num_classes` floating point values per feature for `y_pred`
    and a single floating point value per feature for `y_true`.

    Args:
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.
        from_logits: (Optional) Whether output is expected
            to be a logits tensor. By default, we consider that output
            encodes a probability distribution.
        axis: (Optional) Defaults to `-1`.
            The dimension along which entropy is computed.

    Examples:

    Standalone usage:

    >>> # y_true = one_hot(y_true) = [[0, 1, 0], [0, 0, 1]]
    >>> # logits = log(y_pred)
    >>> # softmax = exp(logits) / sum(exp(logits), axis=-1)
    >>> # softmax = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]
    >>> # xent = -sum(y * log(softmax), 1)
    >>> # log(softmax) = [[-2.9957, -0.0513, -16.1181],
    >>> #                [-2.3026, -0.2231, -2.3026]]
    >>> # y_true * log(softmax) = [[0, -0.0513, 0], [0, 0, -2.3026]]
    >>> # xent = [0.0513, 2.3026]
    >>> # Reduced xent = (0.0513 + 2.3026) / 2
    >>> m = keras.metrics.SparseCategoricalCrossentropy()
    >>> m.update_state([1, 2],
    ...                [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])
    >>> m.result()
    1.1769392

    >>> m.reset_state()
    >>> m.update_state([1, 2],
    ...                [[0.05, 0.95, 0], [0.1, 0.8, 0.1]],
    ...                sample_weight=np.array([0.3, 0.7]))
    >>> m.result()
    1.6271976

    Usage with `compile()` API:

    ```python
    model.compile(
        optimizer='sgd',
        loss='mse',
        metrics=[keras.metrics.SparseCategoricalCrossentropy()])
    ```
    )-"
#' Computes the crossentropy metric between the labels and predictions.
#'
#' @description
#' Use this crossentropy metric when there are two or more label classes.
#' It expects labels to be provided as integers. If you want to provide labels
#' that are one-hot encoded, please use the `CategoricalCrossentropy`
#' metric instead.
#'
#' There should be `num_classes` floating point values per feature for `y_pred`
#' and a single floating point value per feature for `y_true`.
#'
#' # Examples
#' ```python
#' y_true = [1, 2]
#' y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
#' loss = keras.losses.sparse_categorical_crossentropy(y_true, y_pred)
#' assert loss.shape == (2,)
#' loss
#' # array([0.0513, 2.303], dtype=float32)
#' ```
#' Standalone usage:
#'
#' ```python
#' # y_true = one_hot(y_true) = [[0, 1, 0], [0, 0, 1]]
#' # logits = log(y_pred)
#' # softmax = exp(logits) / sum(exp(logits), axis=-1)
#' # softmax = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]
#' # xent = -sum(y * log(softmax), 1)
#' # log(softmax) = [[-2.9957, -0.0513, -16.1181],
#' #                [-2.3026, -0.2231, -2.3026]]
#' # y_true * log(softmax) = [[0, -0.0513, 0], [0, 0, -2.3026]]
#' # xent = [0.0513, 2.3026]
#' # Reduced xent = (0.0513 + 2.3026) / 2
#' m = keras.metrics.SparseCategoricalCrossentropy()
#' m.update_state([1, 2],
#'                [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])
#' m.result()
#' # 1.1769392
#' ```
#'
#' ```python
#' m.reset_state()
#' m.update_state([1, 2],
#'                [[0.05, 0.95, 0], [0.1, 0.8, 0.1]],
#'                sample_weight=np.array([0.3, 0.7]))
#' m.result()
#' # 1.6271976
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(
#'     optimizer='sgd',
#'     loss='mse',
#'     metrics=[keras.metrics.SparseCategoricalCrossentropy()])
#' ```
#'
#' # Returns
#' Sparse categorical crossentropy loss value.
#'
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param from_logits Whether `y_pred` is expected to be a logits tensor. By
#'     default, we assume that `y_pred` encodes a probability distribution.
#' @param axis Defaults to `-1`. The dimension along which the entropy is
#'     computed.
#' @param y_true Ground truth values.
#' @param y_pred The predicted values.
#' @param ignore_class Optional integer. The ID of a class to be ignored during
#'     loss computation. This is useful, for example, in segmentation
#'     problems featuring a "void" class (commonly -1 or 255) in
#'     segmentation maps. By default (`ignore_class=None`), all classes are
#'     considered.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SparseCategoricalCrossentropy>
metric_sparse_categorical_crossentropy <-
structure(function (y_true, y_pred, from_logits = FALSE, ignore_class = NULL,
    axis = -1L, ..., name = "sparse_categorical_crossentropy",
    dtype = NULL)
{
    args <- capture_args2(list(axis = as_axis, ignore_class = as_integer))
    callable <- if (missing(y_true) && missing(y_pred))
        keras$metrics$SparseCategoricalCrossentropy
    else keras$metrics$sparse_categorical_crossentropy
    do.call(callable, args)
}, py_function_name = "sparse_categorical_crossentropy")


# keras.metrics.Mean
# keras.src.metrics.reduction_metrics.Mean
r"-(Compute the (weighted) mean of the given values.

    For example, if values is `[1, 3, 5, 7]` then the mean is 4.
    If `sample_weight` was specified as `[1, 1, 0, 0]` then the mean would be 2.

    This metric creates two variables, `total` and `count`.
    The mean value returned is simply `total` divided by `count`.

    Args:
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Example:

    >>> m = Mean()
    >>> m.update_state([1, 3, 5, 7])
    >>> m.result()
    4.0

    >>> m.reset_state()
    >>> m.update_state([1, 3, 5, 7], sample_weight=[1, 1, 0, 0])
    >>> m.result()
    2.0
    ```
    )-"
#' Compute the (weighted) mean of the given values.
#'
#' @description
#' For example, if values is `[1, 3, 5, 7]` then the mean is 4.
#' If `sample_weight` was specified as `[1, 1, 0, 0]` then the mean would be 2.
#'
#' This metric creates two variables, `total` and `count`.
#' The mean value returned is simply `total` divided by `count`.
#'
#' # Examples
#' ```python
#' m = Mean()
#' m.update_state([1, 3, 5, 7])
#' m.result()
#' # 4.0
#' ```
#'
#' ```python
#' m.reset_state()
#' m.update_state([1, 3, 5, 7], sample_weight=[1, 1, 0, 0])
#' m.result()
#' # 2.0
#' ```
#' ```
#'
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Mean>
metric_mean <-
function (..., name = "mean", dtype = NULL)
{
    args <- capture_args2(NULL)
    do.call(keras$metrics$Mean, args)
}


# keras.metrics.MeanMetricWrapper
# keras.src.metrics.reduction_metrics.MeanMetricWrapper
r"-(Wrap a stateless metric function with the Mean metric.

    You could use this class to quickly build a mean metric from a function. The
    function needs to have the signature `fn(y_true, y_pred)` and return a
    per-sample loss array. `MeanMetricWrapper.result()` will return
    the average metric value across all samples seen so far.

    For example:

    ```python
    def mse(y_true, y_pred):
        return (y_true - y_pred) ** 2

    mse_metric = MeanMetricWrapper(fn=mse)
    ```

    Args:
        fn: The metric function to wrap, with signature
            `fn(y_true, y_pred, **kwargs)`.
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.
        **kwargs: Keyword arguments to pass on to `fn`.
    )-"
#' Wrap a stateless metric function with the Mean metric.
#'
#' @description
#' You could use this class to quickly build a mean metric from a function. The
#' function needs to have the signature `fn(y_true, y_pred)` and return a
#' per-sample loss array. `MeanMetricWrapper.result()` will return
#' the average metric value across all samples seen so far.
#'
#' For example:
#'
#' ```python
#' def mse(y_true, y_pred):
#'     return (y_true - y_pred) ** 2
#'
#' mse_metric = MeanMetricWrapper(fn=mse)
#' ```
#'
#' @param fn The metric function to wrap, with signature
#'     `fn(y_true, y_pred, **kwargs)`.
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param ... Keyword arguments to pass on to `fn`.
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanMetricWrapper>
metric_mean_wrapper <-
function (..., fn, name = NULL, dtype = NULL)
{
    args <- capture_args2(NULL)
    do.call(keras$metrics$MeanMetricWrapper, args)
}


# keras.metrics.Sum
# keras.src.metrics.reduction_metrics.Sum
r"-(Compute the (weighted) sum of the given values.

    For example, if `values` is `[1, 3, 5, 7]` then their sum is 16.
    If `sample_weight` was specified as `[1, 1, 0, 0]` then the sum would be 4.

    This metric creates one variable, `total`.
    This is ultimately returned as the sum value.

    Args:
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Example:

    >>> m = metrics.Sum()
    >>> m.update_state([1, 3, 5, 7])
    >>> m.result()
    16.0

    >>> m = metrics.Sum()
    >>> m.update_state([1, 3, 5, 7], sample_weight=[1, 1, 0, 0])
    >>> m.result()
    4.0
    )-"
#' Compute the (weighted) sum of the given values.
#'
#' @description
#' For example, if `values` is `[1, 3, 5, 7]` then their sum is 16.
#' If `sample_weight` was specified as `[1, 1, 0, 0]` then the sum would be 4.
#'
#' This metric creates one variable, `total`.
#' This is ultimately returned as the sum value.
#'
#' # Examples
#' ```python
#' m = metrics.Sum()
#' m.update_state([1, 3, 5, 7])
#' m.result()
#' # 16.0
#' ```
#'
#' ```python
#' m = metrics.Sum()
#' m.update_state([1, 3, 5, 7], sample_weight=[1, 1, 0, 0])
#' m.result()
#' # 4.0
#' ```
#'
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Sum>
metric_sum <-
function (..., name = "sum", dtype = NULL)
{
    args <- capture_args2(NULL)
    do.call(keras$metrics$Sum, args)
}


# keras.metrics.CosineSimilarity
# keras.src.metrics.regression_metrics.CosineSimilarity
r"-(Computes the cosine similarity between the labels and predictions.

    Formula:

    ```python
    loss = sum(l2_norm(y_true) * l2_norm(y_pred))
    ```
    See: [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity).
    This metric keeps the average cosine similarity between `predictions` and
    `labels` over a stream of data.

    Args:
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.
        axis: (Optional) Defaults to `-1`. The dimension along which the cosine
            similarity is computed.

    Examples:

    Standalone usage:

    >>> # l2_norm(y_true) = [[0., 1.], [1./1.414, 1./1.414]]
    >>> # l2_norm(y_pred) = [[1., 0.], [1./1.414, 1./1.414]]
    >>> # l2_norm(y_true) . l2_norm(y_pred) = [[0., 0.], [0.5, 0.5]]
    >>> # result = mean(sum(l2_norm(y_true) . l2_norm(y_pred), axis=1))
    >>> #        = ((0. + 0.) +  (0.5 + 0.5)) / 2
    >>> m = keras.metrics.CosineSimilarity(axis=1)
    >>> m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]])
    >>> m.result()
    0.49999997
    >>> m.reset_state()
    >>> m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]],
    ...                sample_weight=[0.3, 0.7])
    >>> m.result()
    0.6999999

    Usage with `compile()` API:

    ```python
    model.compile(
        optimizer='sgd',
        loss='mse',
        metrics=[keras.metrics.CosineSimilarity(axis=1)])
    ```
    )-"
#' Computes the cosine similarity between the labels and predictions.
#'
#' @description
#' Formula:
#'
#' ```python
#' loss = sum(l2_norm(y_true) * l2_norm(y_pred))
#' ```
#' See: [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity).
#' This metric keeps the average cosine similarity between `predictions` and
#' `labels` over a stream of data.
#'
#' # Examples
#' Standalone usage:
#'
#' ```python
#' # l2_norm(y_true) = [[0., 1.], [1./1.414, 1./1.414]]
#' # l2_norm(y_pred) = [[1., 0.], [1./1.414, 1./1.414]]
#' # l2_norm(y_true) . l2_norm(y_pred) = [[0., 0.], [0.5, 0.5]]
#' # result = mean(sum(l2_norm(y_true) . l2_norm(y_pred), axis=1))
#' #        = ((0. + 0.) +  (0.5 + 0.5)) / 2
#' m = keras.metrics.CosineSimilarity(axis=1)
#' m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]])
#' m.result()
#' # 0.49999997
#' m.reset_state()
#' m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]],
#'                sample_weight=[0.3, 0.7])
#' m.result()
#' # 0.6999999
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(
#'     optimizer='sgd',
#'     loss='mse',
#'     metrics=[keras.metrics.CosineSimilarity(axis=1)])
#' ```
#'
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param axis (Optional) Defaults to `-1`. The dimension along which the cosine
#'     similarity is computed.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/CosineSimilarity>
metric_cosine_similarity <-
function (..., name = "cosine_similarity", dtype = NULL, axis = -1L)
{
    args <- capture_args2(list(axis = as_axis))
    do.call(keras$metrics$CosineSimilarity, args)
}


# keras.metrics.LogCoshError
# keras.src.metrics.regression_metrics.LogCoshError
r"-(Computes the logarithm of the hyperbolic cosine of the prediction error.

    Formula:

    ```python
    error = y_pred - y_true
    logcosh = mean(log((exp(error) + exp(-error))/2), axis=-1)
    ```

    Args:
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Examples:

    Standalone usage:

    >>> m = keras.metrics.LogCoshError()
    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
    >>> m.result()
    0.10844523
    >>> m.reset_state()
    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],
    ...                sample_weight=[1, 0])
    >>> m.result()
    0.21689045

    Usage with `compile()` API:

    ```python
    model.compile(optimizer='sgd',
                  loss='mse',
                  metrics=[keras.metrics.LogCoshError()])
    ```
    )-"
#' Computes the logarithm of the hyperbolic cosine of the prediction error.
#'
#' @description
#' Formula:
#'
#' ```python
#' error = y_pred - y_true
#' logcosh = mean(log((exp(error) + exp(-error))/2), axis=-1)
#' ```
#'
#' # Examples
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.LogCoshError()
#' m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
#' m.result()
#' # 0.10844523
#' m.reset_state()
#' m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],
#'                sample_weight=[1, 0])
#' m.result()
#' # 0.21689045
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(optimizer='sgd',
#'               loss='mse',
#'               metrics=[keras.metrics.LogCoshError()])
#' ```
#'
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/LogCoshError>
metric_log_cosh_error <-
function (..., name = "logcosh", dtype = NULL)
{
    args <- capture_args2(NULL)
    do.call(keras$metrics$LogCoshError, args)
}


# keras.metrics.MeanAbsoluteError
# keras.src.metrics.regression_metrics.MeanAbsoluteError
r"-(Computes the mean absolute error between the labels and predictions.

    Formula:

    ```python
    loss = mean(abs(y_true - y_pred))
    ```

    Args:
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Examples:

    Standalone usage:

    >>> m = keras.metrics.MeanAbsoluteError()
    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
    >>> m.result()
    0.25
    >>> m.reset_state()
    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],
    ...                sample_weight=[1, 0])
    >>> m.result()
    0.5

    Usage with `compile()` API:

    ```python
    model.compile(
        optimizer='sgd',
        loss='mse',
        metrics=[keras.metrics.MeanAbsoluteError()])
    ```
    )-"
#' Computes the mean absolute error between the labels and predictions.
#'
#' @description
#' ```python
#' loss = mean(abs(y_true - y_pred), axis=-1)
#' ```
#' Formula:
#'
#' ```python
#' loss = mean(abs(y_true - y_pred))
#' ```
#'
#' # Examples
#' ```python
#' y_true = np.random.randint(0, 2, size=(2, 3))
#' y_pred = np.random.random(size=(2, 3))
#' loss = keras.losses.mean_absolute_error(y_true, y_pred)
#' ```
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.MeanAbsoluteError()
#' m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
#' m.result()
#' # 0.25
#' m.reset_state()
#' m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],
#'                sample_weight=[1, 0])
#' m.result()
#' # 0.5
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(
#'     optimizer='sgd',
#'     loss='mse',
#'     metrics=[keras.metrics.MeanAbsoluteError()])
#' ```
#'
#' # Returns
#' Mean absolute error values with shape = `[batch_size, d0, .. dN-1]`.
#'
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param y_true Ground truth values with shape = `[batch_size, d0, .. dN]`.
#' @param y_pred The predicted values with shape = `[batch_size, d0, .. dN]`.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanAbsoluteError>
metric_mean_absolute_error <-
structure(function (y_true, y_pred, ..., name = "mean_absolute_error",
    dtype = NULL)
{
    args <- capture_args2(NULL)
    callable <- if (missing(y_true) && missing(y_pred))
        keras$metrics$MeanAbsoluteError
    else keras$metrics$mean_absolute_error
    do.call(callable, args)
}, py_function_name = "mean_absolute_error")


# keras.metrics.MeanAbsolutePercentageError
# keras.src.metrics.regression_metrics.MeanAbsolutePercentageError
r"-(Computes mean absolute percentage error between `y_true` and `y_pred`.

    Formula:

    ```python
    loss = 100 * mean(abs((y_true - y_pred) / y_true))
    ```

    Args:
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Examples:

    Standalone usage:

    >>> m = keras.metrics.MeanAbsolutePercentageError()
    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
    >>> m.result()
    250000000.0
    >>> m.reset_state()
    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],
    ...                sample_weight=[1, 0])
    >>> m.result()
    500000000.0

    Usage with `compile()` API:

    ```python
    model.compile(
        optimizer='sgd',
        loss='mse',
        metrics=[keras.metrics.MeanAbsolutePercentageError()])
    ```
    )-"
#' Computes mean absolute percentage error between `y_true` and `y_pred`.
#'
#' @description
#' Formula:
#'
#' ```python
#' loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1)
#' ```
#'
#' Division by zero is prevented by dividing by `maximum(y_true, epsilon)`
#' where `epsilon = keras.backend.epsilon()`
#' (default to `1e-7`).
#' Formula:
#'
#' ```python
#' loss = 100 * mean(abs((y_true - y_pred) / y_true))
#' ```
#'
#' # Examples
#' ```python
#' y_true = np.random.random(size=(2, 3))
#' y_pred = np.random.random(size=(2, 3))
#' loss = keras.losses.mean_absolute_percentage_error(y_true, y_pred)
#' ```
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.MeanAbsolutePercentageError()
#' m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
#' m.result()
#' # 250000000.0
#' m.reset_state()
#' m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],
#'                sample_weight=[1, 0])
#' m.result()
#' # 500000000.0
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(
#'     optimizer='sgd',
#'     loss='mse',
#'     metrics=[keras.metrics.MeanAbsolutePercentageError()])
#' ```
#'
#' # Returns
#' Mean absolute percentage error values with shape = `[batch_size, d0, ..
#' dN-1]`.
#'
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param y_true Ground truth values with shape = `[batch_size, d0, .. dN]`.
#' @param y_pred The predicted values with shape = `[batch_size, d0, .. dN]`.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanAbsolutePercentageError>
metric_mean_absolute_percentage_error <-
structure(function (y_true, y_pred, ..., name = "mean_absolute_percentage_error",
    dtype = NULL)
{
    args <- capture_args2(NULL)
    callable <- if (missing(y_true) && missing(y_pred))
        keras$metrics$MeanAbsolutePercentageError
    else keras$metrics$mean_absolute_percentage_error
    do.call(callable, args)
}, py_function_name = "mean_absolute_percentage_error")


# keras.metrics.MeanSquaredError
# keras.src.metrics.regression_metrics.MeanSquaredError
r"-(Computes the mean squared error between `y_true` and `y_pred`.

    Formula:

    ```python
    loss = mean(square(y_true - y_pred))
    ```

    Args:
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Example:

    >>> m = keras.metrics.MeanSquaredError()
    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
    >>> m.result()
    0.25
    )-"
#' Computes the mean squared error between `y_true` and `y_pred`.
#'
#' @description
#' Formula:
#'
#' ```python
#' loss = mean(square(y_true - y_pred), axis=-1)
#' ```
#' Formula:
#'
#' ```python
#' loss = mean(square(y_true - y_pred))
#' ```
#'
#' # Examples
#' ```python
#' y_true = np.random.randint(0, 2, size=(2, 3))
#' y_pred = np.random.random(size=(2, 3))
#' loss = keras.losses.mean_squared_error(y_true, y_pred)
#' ```
#' ```python
#' m = keras.metrics.MeanSquaredError()
#' m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
#' m.result()
#' # 0.25
#' ```
#'
#' # Returns
#'     Mean squared error values with shape = `[batch_size, d0, .. dN-1]`.
#'
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param y_true Ground truth values with shape = `[batch_size, d0, .. dN]`.
#' @param y_pred The predicted values with shape = `[batch_size, d0, .. dN]`.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanSquaredError>
metric_mean_squared_error <-
structure(function (y_true, y_pred, ..., name = "mean_squared_error",
    dtype = NULL)
{
    args <- capture_args2(NULL)
    callable <- if (missing(y_true) && missing(y_pred))
        keras$metrics$MeanSquaredError
    else keras$metrics$mean_squared_error
    do.call(callable, args)
}, py_function_name = "mean_squared_error")


# keras.metrics.MeanSquaredLogarithmicError
# keras.src.metrics.regression_metrics.MeanSquaredLogarithmicError
r"-(Computes mean squared logarithmic error between `y_true` and `y_pred`.

    Formula:

    ```python
    loss = mean(square(log(y_true + 1) - log(y_pred + 1)))
    ```

    Args:
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Examples:

    Standalone usage:

    >>> m = keras.metrics.MeanSquaredLogarithmicError()
    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
    >>> m.result()
    0.12011322
    >>> m.reset_state()
    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],
    ...                sample_weight=[1, 0])
    >>> m.result()
    0.24022643

    Usage with `compile()` API:

    ```python
    model.compile(
        optimizer='sgd',
        loss='mse',
        metrics=[keras.metrics.MeanSquaredLogarithmicError()])
    ```
    )-"
#' Computes mean squared logarithmic error between `y_true` and `y_pred`.
#'
#' @description
#' Formula:
#'
#' ```python
#' loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1)
#' ```
#'
#' Note that `y_pred` and `y_true` cannot be less or equal to 0. Negative
#' values and 0 values will be replaced with `keras.backend.epsilon()`
#' (default to `1e-7`).
#' Formula:
#'
#' ```python
#' loss = mean(square(log(y_true + 1) - log(y_pred + 1)))
#' ```
#'
#' # Examples
#' ```python
#' y_true = np.random.randint(0, 2, size=(2, 3))
#' y_pred = np.random.random(size=(2, 3))
#' loss = keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
#' ```
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.MeanSquaredLogarithmicError()
#' m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
#' m.result()
#' # 0.12011322
#' m.reset_state()
#' m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],
#'                sample_weight=[1, 0])
#' m.result()
#' # 0.24022643
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(
#'     optimizer='sgd',
#'     loss='mse',
#'     metrics=[keras.metrics.MeanSquaredLogarithmicError()])
#' ```
#'
#' # Returns
#' Mean squared logarithmic error values with shape = `[batch_size, d0, ..
#' dN-1]`.
#'
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param y_true Ground truth values with shape = `[batch_size, d0, .. dN]`.
#' @param y_pred The predicted values with shape = `[batch_size, d0, .. dN]`.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanSquaredLogarithmicError>
metric_mean_squared_logarithmic_error <-
structure(function (y_true, y_pred, ..., name = "mean_squared_logarithmic_error",
    dtype = NULL)
{
    args <- capture_args2(NULL)
    callable <- if (missing(y_true) && missing(y_pred))
        keras$metrics$MeanSquaredLogarithmicError
    else keras$metrics$mean_squared_logarithmic_error
    do.call(callable, args)
}, py_function_name = "mean_squared_logarithmic_error")


# keras.metrics.R2Score
# keras.src.metrics.regression_metrics.R2Score
r"-(Computes R2 score.

    Formula:

    ```python
    sum_squares_residuals = sum((y_true - y_pred) ** 2)
    sum_squares = sum((y_true - mean(y_true)) ** 2)
    R2 = 1 - sum_squares_residuals / sum_squares
    ```

    This is also called the
    [coefficient of determination](
    https://en.wikipedia.org/wiki/Coefficient_of_determination).

    It indicates how close the fitted regression line
    is to ground-truth data.

    - The highest score possible is 1.0. It indicates that the predictors
        perfectly accounts for variation in the target.
    - A score of 0.0 indicates that the predictors do not
        account for variation in the target.
    - It can also be negative if the model is worse than random.

    This metric can also compute the "Adjusted R2" score.

    Args:
        class_aggregation: Specifies how to aggregate scores corresponding to
            different output classes (or target dimensions),
            i.e. different dimensions on the last axis of the predictions.
            Equivalent to `multioutput` argument in Scikit-Learn.
            Should be one of
            `None` (no aggregation), `"uniform_average"`,
            `"variance_weighted_average"`.
        num_regressors: Number of independent regressors used
            ("Adjusted R2" score). 0 is the standard R2 score.
            Defaults to `0`.
        name: Optional. string name of the metric instance.
        dtype: Optional. data type of the metric result.

    Example:

    >>> y_true = np.array([[1], [4], [3]], dtype=np.float32)
    >>> y_pred = np.array([[2], [4], [4]], dtype=np.float32)
    >>> metric = keras.metrics.R2Score()
    >>> metric.update_state(y_true, y_pred)
    >>> result = metric.result()
    >>> result
    0.57142854
    )-"
#' Computes R2 score.
#'
#' @description
#' Formula:
#'
#' ```python
#' sum_squares_residuals = sum((y_true - y_pred) ** 2)
#' sum_squares = sum((y_true - mean(y_true)) ** 2)
#' R2 = 1 - sum_squares_residuals / sum_squares
#' ```
#'
#' This is also called the
#' [coefficient of determination](
#' https://en.wikipedia.org/wiki/Coefficient_of_determination).
#'
#' It indicates how close the fitted regression line
#' is to ground-truth data.
#'
#' - The highest score possible is 1.0. It indicates that the predictors
#'     perfectly accounts for variation in the target.
#' - A score of 0.0 indicates that the predictors do not
#'     account for variation in the target.
#' - It can also be negative if the model is worse than random.
#'
#' This metric can also compute the "Adjusted R2" score.
#'
#' # Examples
#' ```python
#' y_true = np.array([[1], [4], [3]], dtype=np.float32)
#' y_pred = np.array([[2], [4], [4]], dtype=np.float32)
#' metric = keras.metrics.R2Score()
#' metric.update_state(y_true, y_pred)
#' result = metric.result()
#' result
#' # 0.57142854
#' ```
#'
#' @param class_aggregation Specifies how to aggregate scores corresponding to
#'     different output classes (or target dimensions),
#'     i.e. different dimensions on the last axis of the predictions.
#'     Equivalent to `multioutput` argument in Scikit-Learn.
#'     Should be one of
#'     `None` (no aggregation), `"uniform_average"`,
#'     `"variance_weighted_average"`.
#' @param num_regressors Number of independent regressors used
#'     ("Adjusted R2" score). 0 is the standard R2 score.
#'     Defaults to `0`.
#' @param name Optional. string name of the metric instance.
#' @param dtype Optional. data type of the metric result.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/R2Score>
metric_r2_score <-
function (..., class_aggregation = "uniform_average", num_regressors = 0L,
    name = "r2_score", dtype = NULL)
{
    args <- capture_args2(list(num_regressors = as_integer))
    do.call(keras$metrics$R2Score, args)
}


# keras.metrics.RootMeanSquaredError
# keras.src.metrics.regression_metrics.RootMeanSquaredError
r"-(Computes root mean squared error metric between `y_true` and `y_pred`.

    Formula:

    ```python
    loss = sqrt(mean((y_pred - y_true) ** 2))
    ```

    Args:
        name: (Optional) string name of the metric instance.
        dtype: (Optional) data type of the metric result.

    Examples:

    Standalone usage:

    >>> m = keras.metrics.RootMeanSquaredError()
    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
    >>> m.result()
    0.5

    >>> m.reset_state()
    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],
    ...                sample_weight=[1, 0])
    >>> m.result()
    0.70710677

    Usage with `compile()` API:

    ```python
    model.compile(
        optimizer='sgd',
        loss='mse',
        metrics=[keras.metrics.RootMeanSquaredError()])
    ```
    )-"
#' Computes root mean squared error metric between `y_true` and `y_pred`.
#'
#' @description
#' Formula:
#'
#' ```python
#' loss = sqrt(mean((y_pred - y_true) ** 2))
#' ```
#'
#' # Examples
#' Standalone usage:
#'
#' ```python
#' m = keras.metrics.RootMeanSquaredError()
#' m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
#' m.result()
#' # 0.5
#' ```
#'
#' ```python
#' m.reset_state()
#' m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],
#'                sample_weight=[1, 0])
#' m.result()
#' # 0.70710677
#' ```
#'
#' Usage with `compile()` API:
#'
#' ```python
#' model.compile(
#'     optimizer='sgd',
#'     loss='mse',
#'     metrics=[keras.metrics.RootMeanSquaredError()])
#' ```
#'
#' @param name (Optional) string name of the metric instance.
#' @param dtype (Optional) data type of the metric result.
#' @param ... Passed on to the Python callable
#'
#' @export
#' @family metric
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/RootMeanSquaredError>
metric_root_mean_squared_error <-
function (..., name = "root_mean_squared_error", dtype = NULL)
{
    args <- capture_args2(NULL)
    do.call(keras$metrics$RootMeanSquaredError, args)
}
