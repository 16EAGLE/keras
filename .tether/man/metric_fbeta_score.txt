#' Computes F-Beta score.
#'
#' @description
#' Formula:
#'
#' ```python
#' b2 = beta ** 2
#' f_beta_score = (1 + b2) * (precision * recall) / (precision * b2 + recall)
#' ```
#' This is the weighted harmonic mean of precision and recall.
#' Its output range is `[0, 1]`. It works for both multi-class
#' and multi-label classification.
#'
#' # Examples
#' ```python
#' metric = keras.metrics.FBetaScore(beta=2.0, threshold=0.5)
#' y_true = np.array([[1, 1, 1],
#'                    [1, 0, 0],
#'                    [1, 1, 0]], np.int32)
#' y_pred = np.array([[0.2, 0.6, 0.7],
#'                    [0.2, 0.6, 0.6],
#'                    [0.6, 0.8, 0.0]], np.float32)
#' metric.update_state(y_true, y_pred)
#' result = metric.result()
#' result
#' # [0.3846154 , 0.90909094, 0.8333334 ]
#' ```
#'
#' @returns
#' F-Beta Score: float.
#'
#' @param average
#' Type of averaging to be performed across per-class results
#' in the multi-class case.
#' Acceptable values are `None`, `"micro"`, `"macro"` and
#' `"weighted"`. Defaults to `None`.
#' If `None`, no averaging is performed and `result()` will return
#' the score for each class.
#' If `"micro"`, compute metrics globally by counting the total
#' true positives, false negatives and false positives.
#' If `"macro"`, compute metrics for each label,
#' and return their unweighted mean.
#' This does not take label imbalance into account.
#' If `"weighted"`, compute metrics for each label,
#' and return their average weighted by support
#' (the number of true instances for each label).
#' This alters `"macro"` to account for label imbalance.
#' It can result in an score that is not between precision and recall.
#'
#' @param beta
#' Determines the weight of given to recall
#' in the harmonic mean between precision and recall (see pseudocode
#' equation above). Defaults to `1`.
#'
#' @param threshold
#' Elements of `y_pred` greater than `threshold` are
#' converted to be 1, and the rest 0. If `threshold` is
#' `None`, the argmax of `y_pred` is converted to 1, and the rest to 0.
#'
#' @param name
#' Optional. String name of the metric instance.
#'
#' @param dtype
#' Optional. Data type of the metric result.
#'
#' @param ...
#' For forward/backward compatability.
#'
#' @export
#' @family f score metrics
#' @family metrics
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/FBetaScore>
metric_fbeta_score <-
__signature__
keras.metrics.FBetaScore(
  average=None,
  beta=1.0,
  threshold=None,
  name='fbeta_score',
  dtype=None
)
