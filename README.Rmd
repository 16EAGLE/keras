---
title: "keras for R"
output: github_document
---

```{r setup, include=FALSE}
# generate dummy training data
x_train <- matrix(rexp(1000*100), nrow = 1000, ncol = 100)
y_train <- matrix(round(runif(1000*10, min = 0, max = 9)), nrow = 1000, ncol = 10)

# genereate dummy input data
x_test <- matrix(rexp(10*100), nrow = 10, ncol = 100)
y_test <- matrix(round(runif(10*10, min = 0, max = 9)), nrow = 10, ncol = 10)
```


# R interface to Keras 

[![Build Status](https://travis-ci.org/rstudio/keras.svg?branch=master)](https://travis-ci.org/rstudio/keras)
[![license](https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000)](https://github.com/rstudio/keras/blob/master/LICENSE)

Keras is a high-level neural networks API developed with a focus on enabling fast experimentation. *Being able to go from idea to result with the least possible delay is key to doing good research.*

Use Keras if you need a deep learning library that:

- Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).
- Supports both convolutional networks and recurrent networks, as well as combinations of the two.
- Runs seamlessly on CPU and GPU.

The R interface to Keras uses [TensorFlow](https://rstudio.github.io/tensorflow/) as it's underlying computation engine.

## Getting started: 30 seconds to Keras

The core data structure of Keras is a model, a way to organize layers. The simplest type of model is the Sequential model, a linear stack of layers. For more complex architectures, you should use the Keras functional API, which allows to build arbitrary graphs of layers.

Here is the Sequential model:

```{r}
library(keras)

model <- model_sequential() %>% 
  layer_dense(units = 64, input_shape = 100) %>% 
  layer_activation(activation = 'relu') %>% 
  layer_dense(units = 10) %>% 
  layer_activation(activation = 'softmax') %>% 
  compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizer_sgd(lr = 0.02),
    metrics = c('accuracy')
  )
```

You can now iterate on your training data in batches (`x_train` and `y_train` are R matrices):

```{r, results='hide'}
fit(model, x_train, y_train, epochs = 5, batch_size = 32)
```

Evaluate your performance in one line:

```{r, results = 'hide'}
loss_and_metrics <- evaluate(model, x_test, y_test, batch_size = 128)
```

Or generate predictions on new data:

```{r, results = 'hide'}
classes <- predict(model, x_test, batch_size = 128)
```

Building a question answering system, an image classification model, a Neural Turing Machine, or any other model is just as fast. The ideas behind deep learning are simple, so why should their implementation be painful?
