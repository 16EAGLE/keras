---
title: Writing your own callbacks
authors: Rick Chao, Francois Chollet
date-created: 2019/03/20
last-modified: 2023/06/25
description: Complete guide to writing new Keras callbacks.
output: rmarkdown::html_vignette
date: 'Last Modified: 2023-11-07; Last Rendered: 2023-11-07'
vignette: '%\VignetteIndexEntry{Writing your own callbacks} %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}'
---

Writing your own callbacks
================

## Introduction

A callback is a powerful tool to customize the behavior of a Keras model
during training, evaluation, or inference. Examples include
`keras.callbacks.TensorBoard` to visualize training progress and results
with TensorBoard, or `keras.callbacks.ModelCheckpoint` to periodically
save your model during training.

In this guide, you will learn what a Keras callback is, what it can do,
and how you can build your own. We provide a few demos of simple
callback applications to get you started.

## Setup

``` r
library(keras)
`add<-` <- `+`
envir::import_from(dplyr, last)
```

## Keras callbacks overview

All callbacks subclass the `keras.callbacks.Callback` class, and
override a set of methods called at various stages of training, testing,
and predicting. Callbacks are useful to get a view on internal states
and statistics of the model during training.

You can pass a list of callbacks (as the keyword argument `callbacks`)
to the following model methods:

- `fit()`
- `evaluate()`
- `predict()`

## An overview of callback methods

### Global methods

#### `on_(train|test|predict)_begin(logs = NULL)`

Called at the beginning of `fit`/`evaluate`/`predict`.

#### `on_(train|test|predict)_end(logs = NULL)`

Called at the end of `fit`/`evaluate`/`predict`.

### Batch-level methods for training/testing/predicting

#### `on_(train|test|predict)_batch_begin(batch, logs = NULL)`

Called right before processing a batch during
training/testing/predicting.

#### `on_(train|test|predict)_batch_end(batch, logs = NULL)`

Called at the end of training/testing/predicting a batch. Within this
method, `logs` is a named list containing the metrics results.

### Epoch-level methods (training only)

#### `on_epoch_begin(epoch, logs = NULL)`

Called at the beginning of an epoch during training.

#### `on_epoch_end(epoch, logs = NULL)`

Called at the end of an epoch during training.

## A basic example

Let’s take a look at a concrete example. To get started, let’s import
tensorflow and define a simple Sequential Keras model:

``` r
# Define the Keras model to add callbacks to
get_model <- function() {
  model <- keras_model_sequential()
  model |> layer_dense(units = 1)
  model |> compile(
    optimizer = optimizer_rmsprop(learning_rate = 0.1),
    loss = "mean_squared_error",
    metrics = "mean_absolute_error"
  )
  model
}
```

Then, load the MNIST data for training and testing from Keras datasets
API:

``` r
# Load example MNIST data and pre-process it
mnist <- dataset_mnist()

flatten_and_rescale <- function(x) {
  x <- array_reshape(x, c(-1, 784))
  x <- x / 255
  x
}

mnist$train$x <- flatten_and_rescale(mnist$train$x)
mnist$test$x  <- flatten_and_rescale(mnist$test$x)

# limit to 500 samples
n <- 200
mnist$train$x <- mnist$train$x[1:n,]
mnist$train$y <- mnist$train$y[1:n]
mnist$test$x  <- mnist$test$x[1:n,]
mnist$test$y  <- mnist$test$y[1:n]
```

Now, define a simple custom callback that logs:

- When `fit`/`evaluate`/`predict` starts & ends
- When each epoch starts & ends
- When each training batch starts & ends
- When each evaluation (test) batch starts & ends
- When each inference (prediction) batch starts & ends

``` r
show <- function(msg, logs) {
  cat(glue::glue(msg, .envir = parent.frame()),
      "got logs: ", sep = "; ")
  str(logs); cat("\n")
}

callback_custom <- new_callback_class(
  "CustomCallback",
  on_train_begin         = function(logs = NULL) show("Starting training", logs),
  on_epoch_begin         = function(epoch, logs = NULL) show("Start epoch {epoch} of training", logs),
  on_train_batch_begin   = function(batch, logs = NULL) show("...Training: start of batch {batch}", logs),
  on_train_batch_end     = function(batch, logs = NULL) show("...Training: end of batch {batch}",  logs),
  on_epoch_end           = function(epoch, logs = NULL) show("End epoch {epoch} of training", logs),
  on_train_end           = function(logs = NULL) show("Stop training", logs),


  on_test_begin          = function(logs = NULL) show("Start testing", logs),
  on_test_batch_begin    = function(batch, logs = NULL) show("...Evaluating: start of batch {batch}", logs),
  on_test_batch_end      = function(batch, logs = NULL) show("...Evaluating: end of batch {batch}", logs),
  on_test_end            = function(logs = NULL) show("Stop testing", logs),

  on_predict_begin       = function(logs = NULL) show("Start predicting", logs),
  on_predict_end         = function(logs = NULL) show("Stop predicting", logs),
  on_predict_batch_begin = function(batch, logs = NULL) show("...Predicting: start of batch {batch}", logs),
  on_predict_batch_end   = function(batch, logs = NULL) show("...Predicting: end of batch {batch}", logs),
)
```

Let’s try it out:

``` r
model <- get_model()
model |> fit(
  mnist$train$x, mnist$train$y,
  epochs = 2, verbose = 0, validation_split = 0.5,
  callbacks = list(callback_custom())
)
## Starting training; got logs:  Named list()
## 
## Start epoch 0 of training; got logs:  Named list()
## 
## ...Training: start of batch 0; got logs:  Named list()
## 
## ...Training: end of batch 0; got logs: List of 2
##  $ loss               : num 30.4
##  $ mean_absolute_error: num 4.64
## 
## ...Training: start of batch 1; got logs:  Named list()
## 
## ...Training: end of batch 1; got logs: List of 2
##  $ loss               : num 489
##  $ mean_absolute_error: num 16.5
## 
## ...Training: start of batch 2; got logs:  Named list()
## 
## ...Training: end of batch 2; got logs: List of 2
##  $ loss               : num 335
##  $ mean_absolute_error: num 12.4
## 
## ...Training: start of batch 3; got logs:  Named list()
## 
## ...Training: end of batch 3; got logs: List of 2
##  $ loss               : num 254
##  $ mean_absolute_error: num 10
## 
## Start testing; got logs:  Named list()
## 
## ...Evaluating: start of batch 0; got logs:  Named list()
## 
## ...Evaluating: end of batch 0; got logs: List of 2
##  $ loss               : num 13
##  $ mean_absolute_error: num 2.78
## 
## ...Evaluating: start of batch 1; got logs:  Named list()
## 
## ...Evaluating: end of batch 1; got logs: List of 2
##  $ loss               : num 18.1
##  $ mean_absolute_error: num 3.52
## 
## ...Evaluating: start of batch 2; got logs:  Named list()
## 
## ...Evaluating: end of batch 2; got logs: List of 2
##  $ loss               : num 18.8
##  $ mean_absolute_error: num 3.58
## 
## ...Evaluating: start of batch 3; got logs:  Named list()
## 
## ...Evaluating: end of batch 3; got logs: List of 2
##  $ loss               : num 19.2
##  $ mean_absolute_error: num 3.7
## 
## Stop testing; got logs: List of 2
##  $ loss               : num 19.2
##  $ mean_absolute_error: num 3.7
## 
## End epoch 0 of training; got logs: List of 4
##  $ loss                   : num 254
##  $ mean_absolute_error    : num 10
##  $ val_loss               : num 19.2
##  $ val_mean_absolute_error: num 3.7
## 
## Start epoch 1 of training; got logs:  Named list()
## 
## ...Training: start of batch 0; got logs:  Named list()
## 
## ...Training: end of batch 0; got logs: List of 2
##  $ loss               : num 18.5
##  $ mean_absolute_error: num 3.52
## 
## ...Training: start of batch 1; got logs:  Named list()
## 
## ...Training: end of batch 1; got logs: List of 2
##  $ loss               : num 13.2
##  $ mean_absolute_error: num 2.92
## 
## ...Training: start of batch 2; got logs:  Named list()
## 
## ...Training: end of batch 2; got logs: List of 2
##  $ loss               : num 10.5
##  $ mean_absolute_error: num 2.56
## 
## ...Training: start of batch 3; got logs:  Named list()
## 
## ...Training: end of batch 3; got logs: List of 2
##  $ loss               : num 8.4
##  $ mean_absolute_error: num 2.25
## 
## Start testing; got logs:  Named list()
## 
## ...Evaluating: start of batch 0; got logs:  Named list()
## 
## ...Evaluating: end of batch 0; got logs: List of 2
##  $ loss               : num 3.83
##  $ mean_absolute_error: num 1.64
## 
## ...Evaluating: start of batch 1; got logs:  Named list()
## 
## ...Evaluating: end of batch 1; got logs: List of 2
##  $ loss               : num 5.13
##  $ mean_absolute_error: num 1.86
## 
## ...Evaluating: start of batch 2; got logs:  Named list()
## 
## ...Evaluating: end of batch 2; got logs: List of 2
##  $ loss               : num 5.16
##  $ mean_absolute_error: num 1.88
## 
## ...Evaluating: start of batch 3; got logs:  Named list()
## 
## ...Evaluating: end of batch 3; got logs: List of 2
##  $ loss               : num 4.82
##  $ mean_absolute_error: num 1.85
## 
## Stop testing; got logs: List of 2
##  $ loss               : num 4.82
##  $ mean_absolute_error: num 1.85
## 
## End epoch 1 of training; got logs: List of 4
##  $ loss                   : num 8.4
##  $ mean_absolute_error    : num 2.25
##  $ val_loss               : num 4.82
##  $ val_mean_absolute_error: num 1.85
## 
## Stop training; got logs: List of 4
##  $ loss                   : num 8.4
##  $ mean_absolute_error    : num 2.25
##  $ val_loss               : num 4.82
##  $ val_mean_absolute_error: num 1.85

res <- model |> evaluate(
  mnist$test$x, mnist$test$y,
  verbose = 0,
  callbacks = list(callback_custom())
)
## Start testing; got logs:  Named list()
## 
## ...Evaluating: start of batch 0; got logs:  Named list()
## 
## ...Evaluating: end of batch 0; got logs: List of 2
##  $ loss               : num 6.83
##  $ mean_absolute_error: num 2.14
## 
## ...Evaluating: start of batch 1; got logs:  Named list()
## 
## ...Evaluating: end of batch 1; got logs: List of 2
##  $ loss               : num 6.15
##  $ mean_absolute_error: num 1.98
## 
## ...Evaluating: start of batch 2; got logs:  Named list()
## 
## ...Evaluating: end of batch 2; got logs: List of 2
##  $ loss               : num 5.93
##  $ mean_absolute_error: num 1.99
## 
## ...Evaluating: start of batch 3; got logs:  Named list()
## 
## ...Evaluating: end of batch 3; got logs: List of 2
##  $ loss               : num 6.34
##  $ mean_absolute_error: num 2.03
## 
## ...Evaluating: start of batch 4; got logs:  Named list()
## 
## ...Evaluating: end of batch 4; got logs: List of 2
##  $ loss               : num 6.24
##  $ mean_absolute_error: num 2.02
## 
## ...Evaluating: start of batch 5; got logs:  Named list()
## 
## ...Evaluating: end of batch 5; got logs: List of 2
##  $ loss               : num 6.02
##  $ mean_absolute_error: num 1.99
## 
## ...Evaluating: start of batch 6; got logs:  Named list()
## 
## ...Evaluating: end of batch 6; got logs: List of 2
##  $ loss               : num 6.15
##  $ mean_absolute_error: num 2.04
## 
## Stop testing; got logs: List of 2
##  $ loss               : num 6.15
##  $ mean_absolute_error: num 2.04

res <- model |> predict(
  mnist$test$x, 
  verbose = 0,
  callbacks = list(callback_custom())
)
## Start predicting; got logs:  Named list()
## 
## ...Predicting: start of batch 0; got logs:  Named list()
## 
## ...Predicting: end of batch 0; got logs: List of 1
##  $ outputs:<tf.Tensor: shape=(32, 1), dtype=float32, numpy=…>
## 
## ...Predicting: start of batch 1; got logs:  Named list()
## 
## ...Predicting: end of batch 1; got logs: List of 1
##  $ outputs:<tf.Tensor: shape=(32, 1), dtype=float32, numpy=…>
## 
## ...Predicting: start of batch 2; got logs:  Named list()
## 
## ...Predicting: end of batch 2; got logs: List of 1
##  $ outputs:<tf.Tensor: shape=(32, 1), dtype=float32, numpy=…>
## 
## ...Predicting: start of batch 3; got logs:  Named list()
## 
## ...Predicting: end of batch 3; got logs: List of 1
##  $ outputs:<tf.Tensor: shape=(32, 1), dtype=float32, numpy=…>
## 
## ...Predicting: start of batch 4; got logs:  Named list()
## 
## ...Predicting: end of batch 4; got logs: List of 1
##  $ outputs:<tf.Tensor: shape=(32, 1), dtype=float32, numpy=…>
## 
## ...Predicting: start of batch 5; got logs:  Named list()
## 
## ...Predicting: end of batch 5; got logs: List of 1
##  $ outputs:<tf.Tensor: shape=(32, 1), dtype=float32, numpy=…>
## 
## ...Predicting: start of batch 6; got logs:  Named list()
## 
## ...Predicting: end of batch 6; got logs: List of 1
##  $ outputs:<tf.Tensor: shape=(8, 1), dtype=float32, numpy=…>
## 
## Stop predicting; got logs:  Named list()
```

### Usage of `logs` dict

The `logs` named list contains the loss value, and all the metrics at
the end of a batch or epoch. Example includes the loss and mean absolute
error.

``` r
callback_print_loss_and_mae <- new_callback_class(
  "LossAndErrorPrintingCallback",

  on_train_batch_end = function(batch, logs = NULL)
    cat(sprintf("Up to batch %i, the average loss is %7.2f.\n",
                batch,  logs$loss)),

  on_test_batch_end = function(batch, logs = NULL)
    cat(sprintf("Up to batch %i, the average loss is %7.2f.\n",
                batch, logs$loss)),

  on_epoch_end = function(epoch, logs = NULL)
    cat(sprintf(
      "The average loss for epoch %2i is %9.2f and mean absolute error is %7.2f.\n",
      epoch, logs$loss, logs$mean_absolute_error
    ))
)


model <- get_model()
model |> fit(
  mnist$train$x, mnist$train$y,
  epochs = 2, verbose = 0,
  callbacks = list(callback_print_loss_and_mae())
)
## Up to batch 0, the average loss is   26.27.
## Up to batch 1, the average loss is  412.34.
## Up to batch 2, the average loss is  282.25.
## Up to batch 3, the average loss is  214.56.
## Up to batch 4, the average loss is  173.39.
## Up to batch 5, the average loss is  145.38.
## Up to batch 6, the average loss is  125.27.
## The average loss for epoch  0 is    125.27 and mean absolute error is    6.19.
## Up to batch 0, the average loss is    5.81.
## Up to batch 1, the average loss is    8.28.
## Up to batch 2, the average loss is    9.64.
## Up to batch 3, the average loss is   10.74.
## Up to batch 4, the average loss is   10.78.
## Up to batch 5, the average loss is    9.91.
## Up to batch 6, the average loss is    9.24.
## The average loss for epoch  1 is      9.24 and mean absolute error is    2.45.

res = model |> evaluate(
  mnist$test$x, mnist$test$y,
  verbose = 0,
  callbacks = list(callback_print_loss_and_mae())
)
## Up to batch 0, the average loss is    6.75.
## Up to batch 1, the average loss is    7.60.
## Up to batch 2, the average loss is    7.16.
## Up to batch 3, the average loss is    7.48.
## Up to batch 4, the average loss is    7.20.
## Up to batch 5, the average loss is    7.25.
## Up to batch 6, the average loss is    6.91.
```

For more information about callbacks, you can check out the [Keras
callback API documentation](https://keras.io/api/callbacks/).

## Usage of `self$model` attribute

In addition to receiving log information when one of their methods is
called, callbacks have access to the model associated with the current
round of training/evaluation/inference: `self$model`.

Here are of few of the things you can do with `self$model` in a
callback:

- Set `self$model$stop_training <- TRUE` to immediately interrupt
  training.
- Mutate hyperparameters of the optimizer (available as
  `self$model$optimizer`), such as `self$model$optimizer$learning_rate`.
- Save the model at period intervals.
- Record the output of `model |> predict()` on a few test samples at the
  end of each epoch, to use as a sanity check during training.
- Extract visualizations of intermediate features at the end of each
  epoch, to monitor what the model is learning over time.
- etc.

Let’s see this in action in a couple of examples.

## Examples of Keras callback applications

### Early stopping at minimum loss

This first example shows the creation of a `Callback` that stops
training when the minimum of loss has been reached, by setting the
attribute `self$model$stop_training` (boolean). Optionally, you can
provide an argument `patience` to specify how many epochs we should wait
before stopping after having reached a local minimum.

`callback_early_stopping()` provides a more complete and general
implementation.

``` r
callback_early_stopping_at_min_loss <- new_callback_class(
  "EarlyStoppingAtMinLoss",
  `__doc__` =
    "Stop training when the loss is at its min, i.e. the loss stops decreasing.

    Arguments:
        patience: Number of epochs to wait after min has been hit. After this
        number of no improvement, training stops.
    ",

  initialize = function(patience = 0) {
    super$initialize()
    self$patience <- patience
    # best_weights to store the weights at which the minimum loss occurs.
    self$best_weights <- NULL
  },

  on_train_begin = function(logs = NULL) {
    # The number of epoch it has waited when loss is no longer minimum.
    self$wait <- 0
    # The epoch the training stops at.
    self$stopped_epoch <- 0
    # Initialize the best as infinity.
    self$best <- Inf
  },

  on_epoch_end = function(epoch, logs = NULL) {
    current <- logs$loss
    if (current < self$best) {
      self$best <- current
      self$wait <- 0
      # Record the best weights if current results is better (less).
      self$best_weights <- self$model$get_weights()
    } else {
      add(self$wait) <- 1
      if (self$wait >= self$patience) {
        self$stopped_epoch <- epoch
        model <- self$model
        model$stop_training <- TRUE
        cat("Restoring model weights from the end of the best epoch.\n")
        model$set_weights(self$best_weights)
      }
    }
  },

  on_train_end = function(logs = NULL)
    if (self$stopped_epoch > 0)
      cat(sprintf("Epoch %05d: early stopping\n", self$stopped_epoch + 1))
)


model <- get_model()
model |> fit(
  mnist$train$x,
  mnist$train$y,
  batch_size = 64,
  steps_per_epoch = 5,
  epochs = 30,
  verbose = 0,
  callbacks = list(callback_print_loss_and_mae(),
                   callback_early_stopping_at_min_loss())
)
## Up to batch 0, the average loss is   29.74.
## Up to batch 1, the average loss is  467.31.
## Up to batch 2, the average loss is  319.13.
## Up to batch 3, the average loss is  240.82.
## The average loss for epoch  0 is    240.82 and mean absolute error is    9.66.
## Up to batch 0, the average loss is   10.09.
## Up to batch 1, the average loss is    9.81.
## Up to batch 2, the average loss is    8.59.
## Up to batch 3, the average loss is    7.00.
## The average loss for epoch  1 is      7.00 and mean absolute error is    2.14.
## Up to batch 0, the average loss is    4.71.
## Up to batch 1, the average loss is    5.41.
## Up to batch 2, the average loss is    5.07.
## Up to batch 3, the average loss is    4.09.
## The average loss for epoch  2 is      4.09 and mean absolute error is    1.60.
## Up to batch 0, the average loss is    3.31.
## Up to batch 1, the average loss is    3.99.
## Up to batch 2, the average loss is    3.80.
## Up to batch 3, the average loss is    3.05.
## The average loss for epoch  3 is      3.05 and mean absolute error is    1.34.
## Up to batch 0, the average loss is    2.89.
## Up to batch 1, the average loss is    3.50.
## Up to batch 2, the average loss is    3.39.
## Up to batch 3, the average loss is    2.87.
## The average loss for epoch  4 is      2.87 and mean absolute error is    1.33.
## Up to batch 0, the average loss is    3.64.
## Up to batch 1, the average loss is    5.16.
## Up to batch 2, the average loss is    8.02.
## Up to batch 3, the average loss is   12.52.
## The average loss for epoch  5 is     12.52 and mean absolute error is    2.91.
## Restoring model weights from the end of the best epoch.
## Epoch 00006: early stopping
```

### Learning rate scheduling

In this example, we show how a custom Callback can be used to
dynamically change the learning rate of the optimizer during the course
of training.

See `keras$callbacks$LearningRateScheduler` for a more general
implementations (in RStudio, press F1 while the cursor is over
`LearningRateScheduler` and a browser will open to [this
page](https://www.tensorflow.org/versions/r2.5/api_docs/python/tf/keras/callbacks/LearningRateScheduler)).

``` r
callback_custom_learning_rate_scheduler <- new_callback_class(
  "CustomLearningRateScheduler",
  `__doc__` =
  "Learning rate scheduler which sets the learning rate according to schedule.

    Arguments:
        schedule: a function that takes an epoch index
            (integer, indexed from 0) and current learning rate
            as inputs and returns a new learning rate as output (float).
    ",

  initialize = function(schedule) {
    super$initialize()
    self$schedule <- schedule
  },

  on_epoch_begin = function(epoch, logs = NULL) {
    ## When in doubt about what types of objects are in scope (e.g., self$model)
    ## use a debugger to interact with the actual objects at the console!
    # browser()

    if (!"learning_rate" %in% names(self$model$optimizer))
      stop('Optimizer must have a "learning_rate" attribute.')

    # # Get the current learning rate from model's optimizer.
    # use as.numeric() to convert the keras variablea to an R numeric
    lr <- as.numeric(self$model$optimizer$learning_rate)
    # # Call schedule function to get the scheduled learning rate.
    scheduled_lr <- self$schedule(epoch, lr)
    # # Set the value back to the optimizer before this epoch starts
    optimizer <- self$model$optimizer
    optimizer$learning_rate <- scheduled_lr
    cat(sprintf("\nEpoch %03d: Learning rate is %6.4f.\n", epoch, scheduled_lr))
  }
)

LR_SCHEDULE <- tibble::tribble(
  ~start_epoch, ~learning_rate,
             0,            0.1,
             3,           0.05,
             6,           0.01,
             9,          0.005,
            12,          0.001,
  )

lr_schedule <- function(epoch, learning_rate) {
  "Helper function to retrieve the scheduled learning rate based on epoch."
  with(LR_SCHEDULE, learning_rate[last(which(epoch >= start_epoch))])
}

model <- get_model()
model |> fit(
  mnist$train$x,
  mnist$train$y,
  batch_size = 64,
  steps_per_epoch = 5,
  epochs = 15,
  verbose = 0,
  callbacks = list(
    callback_print_loss_and_mae(),
    callback_custom_learning_rate_scheduler(lr_schedule)
  )
)
## 
## Epoch 000: Learning rate is 0.1000.
## Up to batch 0, the average loss is   27.86.
## Up to batch 1, the average loss is  464.16.
## Up to batch 2, the average loss is  317.03.
## Up to batch 3, the average loss is  239.11.
## The average loss for epoch  0 is    239.11 and mean absolute error is    9.61.
## 
## Epoch 001: Learning rate is 0.1000.
## Up to batch 0, the average loss is    9.90.
## Up to batch 1, the average loss is    9.56.
## Up to batch 2, the average loss is    8.41.
## Up to batch 3, the average loss is    6.93.
## The average loss for epoch  1 is      6.93 and mean absolute error is    2.15.
## 
## Epoch 002: Learning rate is 0.1000.
## Up to batch 0, the average loss is    5.27.
## Up to batch 1, the average loss is    5.88.
## Up to batch 2, the average loss is    5.46.
## Up to batch 3, the average loss is    4.42.
## The average loss for epoch  2 is      4.42 and mean absolute error is    1.68.
## 
## Epoch 003: Learning rate is 0.0500.
## Up to batch 0, the average loss is    3.43.
## Up to batch 1, the average loss is    4.08.
## Up to batch 2, the average loss is    3.86.
## Up to batch 3, the average loss is    3.11.
## The average loss for epoch  3 is      3.11 and mean absolute error is    1.38.
## 
## Epoch 004: Learning rate is 0.0500.
## Up to batch 0, the average loss is    2.96.
## Up to batch 1, the average loss is    3.60.
## Up to batch 2, the average loss is    3.47.
## Up to batch 3, the average loss is    2.76.
## The average loss for epoch  4 is      2.76 and mean absolute error is    1.27.
## 
## Epoch 005: Learning rate is 0.0500.
## Up to batch 0, the average loss is    2.56.
## Up to batch 1, the average loss is    3.19.
## Up to batch 2, the average loss is    3.13.
## Up to batch 3, the average loss is    2.47.
## The average loss for epoch  5 is      2.47 and mean absolute error is    1.19.
## 
## Epoch 006: Learning rate is 0.0100.
## Up to batch 0, the average loss is    2.25.
## Up to batch 1, the average loss is    2.84.
## Up to batch 2, the average loss is    2.83.
## Up to batch 3, the average loss is    2.21.
## The average loss for epoch  6 is      2.21 and mean absolute error is    1.09.
## 
## Epoch 007: Learning rate is 0.0100.
## Up to batch 0, the average loss is    2.20.
## Up to batch 1, the average loss is    2.76.
## Up to batch 2, the average loss is    2.75.
## Up to batch 3, the average loss is    2.15.
## The average loss for epoch  7 is      2.15 and mean absolute error is    1.08.
## 
## Epoch 008: Learning rate is 0.0100.
## Up to batch 0, the average loss is    2.12.
## Up to batch 1, the average loss is    2.67.
## Up to batch 2, the average loss is    2.68.
## Up to batch 3, the average loss is    2.09.
## The average loss for epoch  8 is      2.09 and mean absolute error is    1.07.
## 
## Epoch 009: Learning rate is 0.0050.
## Up to batch 0, the average loss is    2.04.
## Up to batch 1, the average loss is    2.57.
## Up to batch 2, the average loss is    2.60.
## Up to batch 3, the average loss is    2.02.
## The average loss for epoch  9 is      2.02 and mean absolute error is    1.05.
## 
## Epoch 010: Learning rate is 0.0050.
## Up to batch 0, the average loss is    2.00.
## Up to batch 1, the average loss is    2.52.
## Up to batch 2, the average loss is    2.55.
## Up to batch 3, the average loss is    1.98.
## The average loss for epoch 10 is      1.98 and mean absolute error is    1.04.
## 
## Epoch 011: Learning rate is 0.0050.
## Up to batch 0, the average loss is    1.95.
## Up to batch 1, the average loss is    2.47.
## Up to batch 2, the average loss is    2.50.
## Up to batch 3, the average loss is    1.94.
## The average loss for epoch 11 is      1.94 and mean absolute error is    1.03.
## 
## Epoch 012: Learning rate is 0.0010.
## Up to batch 0, the average loss is    1.90.
## Up to batch 1, the average loss is    2.40.
## Up to batch 2, the average loss is    2.44.
## Up to batch 3, the average loss is    1.89.
## The average loss for epoch 12 is      1.89 and mean absolute error is    1.01.
## 
## Epoch 013: Learning rate is 0.0010.
## Up to batch 0, the average loss is    1.89.
## Up to batch 1, the average loss is    2.38.
## Up to batch 2, the average loss is    2.43.
## Up to batch 3, the average loss is    1.88.
## The average loss for epoch 13 is      1.88 and mean absolute error is    1.01.
## 
## Epoch 014: Learning rate is 0.0010.
## Up to batch 0, the average loss is    1.88.
## Up to batch 1, the average loss is    2.36.
## Up to batch 2, the average loss is    2.41.
## Up to batch 3, the average loss is    1.87.
## The average loss for epoch 14 is      1.87 and mean absolute error is    1.01.
```

### Built-in Keras callbacks

Be sure to check out the existing Keras callbacks by reading the [API
docs](https://keras.io/api/callbacks/). Applications include logging to
CSV, saving the model, visualizing metrics in TensorBoard, and a lot
more!
