<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Guide to the Functional API • keras</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">Keras for R</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li class="dropdown-header">Getting Started</li>
    <li>
      <a href="../articles/sequential_model.html">Guide to the Sequential Model</a>
    </li>
    <li>
      <a href="../articles/functional_api.html">Guide to the Functional API</a>
    </li>
    <li>
      <a href="../faq.html">Frequently Asked Questions</a>
    </li>
  </ul>
</li>
<li>
  <a href="../articles/examples/index.html">Examples</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/rstudio/keras">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Guide to the Functional API</h1>
                        <h4 class="author">JJ Allaire</h4>
            
            <h4 class="date">2017-04-07</h4>
          </div>

    
    
<div class="contents">
<p>The Keras functional API is the way to go for defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.</p>
<p>This guide assumes that you are already familiar with the <a href="sequential_model.html">Sequential</a> model.</p>
<p>Let’s start with something simple.</p>
<div id="first-example-a-densely-connected-network" class="section level2">
<h2 class="hasAnchor">
<a href="#first-example-a-densely-connected-network" class="anchor"></a>First example: a densely-connected network</h2>
<p>The <a href="sequential_model.html">Sequential</a> model is probably a better choice to implement such a network, but it helps to start with something really simple.</p>
<p>To use the functional API, build your input and output layers and then pass them to the <code><a href="../reference/model.html">model()</a></code> function. This model can be trained just like Keras sequential models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(keras)

<span class="co"># input layer</span>
inputs &lt;-<span class="st"> </span><span class="kw"><a href="../reference/layer_input.html">layer_input</a></span>(<span class="dt">shape =</span> <span class="kw">c</span>(<span class="dv">784</span>))
 
<span class="co"># outputs compose input + dense layers</span>
predictions &lt;-<span class="st"> </span>inputs %&gt;%
<span class="st">  </span><span class="kw"><a href="../reference/layer_dense.html">layer_dense</a></span>(<span class="dt">units =</span> <span class="dv">64</span>, <span class="dt">activation =</span> <span class="st">'relu'</span>) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw"><a href="../reference/layer_dense.html">layer_dense</a></span>(<span class="dt">units =</span> <span class="dv">64</span>, <span class="dt">activation =</span> <span class="st">'relu'</span>) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw"><a href="../reference/layer_dense.html">layer_dense</a></span>(<span class="dt">units =</span> <span class="dv">10</span>, <span class="dt">activation =</span> <span class="st">'softmax'</span>)

<span class="co"># create and compile model</span>
model &lt;-<span class="st"> </span><span class="kw"><a href="../reference/model.html">model</a></span>(<span class="dt">inputs =</span> inputs, <span class="dt">outputs =</span> predictions) %&gt;%
<span class="st">  </span><span class="kw"><a href="../reference/compile.html">compile</a></span>(
    <span class="dt">optimizer =</span> <span class="st">'rmsprop'</span>,
    <span class="dt">loss =</span> <span class="st">'categorical_crossentropy'</span>,
    <span class="dt">metrics =</span> <span class="kw">c</span>(<span class="st">'accuracy'</span>)
  )</code></pre></div>
</div>
<div id="models-can-be-treated-like-layers" class="section level2">
<h2 class="hasAnchor">
<a href="#models-can-be-treated-like-layers" class="anchor"></a>Models can be treated like layers</h2>
<p>With the functional API, it is easy to re-use trained models: you can treat any model as if it were a layer. Note that you aren’t just re-using the architecture of the model, you are also re-using its weights.</p>
<p>This can allow you to for instance quickly create models that can process sequences of inputs. You could turn an image classification model into a video classification model, in just one line:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Input tensor for sequences of 20 timesteps,</span>
<span class="co"># each containing a 784-dimensional vector</span>
input_sequences &lt;-<span class="st"> </span><span class="kw"><a href="../reference/layer_input.html">layer_input</a></span>(<span class="dt">shape =</span> <span class="kw">c</span>(<span class="dv">20</span>, <span class="dv">784</span>))

<span class="co"># This applies our previous model to every timestep in the input sequences.</span>
<span class="co"># the output of the previous model was a 10-way softmax,</span>
<span class="co"># so the output of the layer below will be a sequence of 20 vectors of size 10.</span>
processed_sequences &lt;-<span class="st"> </span>input_sequences %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw"><a href="../reference/time_distributed.html">time_distributed</a></span>(model)</code></pre></div>
</div>
<div id="multi-input-and-multi-output-models" class="section level2">
<h2 class="hasAnchor">
<a href="#multi-input-and-multi-output-models" class="anchor"></a>Multi-input and multi-output models</h2>
<p>Here’s a good use case for the functional API: models with multiple inputs and outputs. The functional API makes it easy to manipulate a large number of intertwined datastreams.</p>
<p>Let’s consider the following model. We seek to predict how many retweets and likes a news headline will receive on Twitter. The main input to the model will be the headline itself, as a sequence of words, but to spice things up, our model will also have an auxiliary input, receiving extra data such as the time of day when the headline was posted, etc.</p>
<p>The model will also be supervised via two loss functions. Using the main loss function earlier in a model is a good regularization mechanism for deep models.</p>
<p>Here’s what our model looks like:</p>
<p><img src="https://s3.amazonaws.com/keras.io/img/multi-input-multi-output-graph.png" alt="multi-input-multi-output-graph" style="width: 400px;"></p>
<p>Let’s implement it with the functional API.</p>
<p>The main input will receive the headline, as a sequence of integers (each integer encodes a word). The integers will be between 1 and 10,000 (a vocabulary of 10,000 words) and the sequences will be 100 words long.</p>
<p>We’ll include an</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(keras)

main_input &lt;-<span class="st"> </span><span class="kw"><a href="../reference/layer_input.html">layer_input</a></span>(<span class="dt">shape =</span> <span class="kw">c</span>(<span class="dv">100</span>), <span class="dt">dtype =</span> <span class="st">'int32'</span>, <span class="dt">name =</span> <span class="st">'main_input'</span>)

lstm_out &lt;-<span class="st"> </span>main_input %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw"><a href="../reference/layer_embedding.html">layer_embedding</a></span>(<span class="dt">input_dim =</span> <span class="dv">10000</span>, <span class="dt">output_dim =</span> <span class="dv">512</span>, <span class="dt">input_length =</span> <span class="dv">100</span>) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw"><a href="../reference/layer_lstm.html">layer_lstm</a></span>(<span class="dt">units =</span> <span class="dv">32</span>)</code></pre></div>
<p>Here we insert the auxiliary loss, allowing the LSTM and Embedding layer to be trained smoothly even though the main loss will be much higher in the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">auxiliary_output &lt;-<span class="st"> </span>lstm_out %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw"><a href="../reference/layer_dense.html">layer_dense</a></span>(<span class="dt">units =</span> <span class="dv">1</span>, <span class="dt">activation =</span> <span class="st">'sigmoid'</span>, <span class="dt">name =</span> <span class="st">'aux_output'</span>)</code></pre></div>
<p>At this point, we feed into the model our auxiliary input data by concatenating it with the LSTM output, stacking a deep densely-connected network on top and adding the main logistic regression layer</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">auxiliary_input &lt;-<span class="st"> </span><span class="kw"><a href="../reference/layer_input.html">layer_input</a></span>(<span class="dt">shape =</span> <span class="kw">c</span>(<span class="dv">5</span>), <span class="dt">name =</span> <span class="st">'aux_input'</span>)

main_output &lt;-<span class="st"> </span><span class="kw"><a href="../reference/layer_concatenate.html">layer_concatenate</a></span>(<span class="kw">c</span>(lstm_out, auxiliary_input)) %&gt;%<span class="st">  </span>
<span class="st">  </span><span class="kw"><a href="../reference/layer_dense.html">layer_dense</a></span>(<span class="dt">units =</span> <span class="dv">64</span>, <span class="dt">activation =</span> <span class="st">'relu'</span>) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw"><a href="../reference/layer_dense.html">layer_dense</a></span>(<span class="dt">units =</span> <span class="dv">64</span>, <span class="dt">activation =</span> <span class="st">'relu'</span>) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw"><a href="../reference/layer_dense.html">layer_dense</a></span>(<span class="dt">units =</span> <span class="dv">64</span>, <span class="dt">activation =</span> <span class="st">'relu'</span>) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw"><a href="../reference/layer_dense.html">layer_dense</a></span>(<span class="dt">units =</span> <span class="dv">1</span>, <span class="dt">activation =</span> <span class="st">'sigmoid'</span>, <span class="dt">name =</span> <span class="st">'main_output'</span>)</code></pre></div>
<p>This defines a model with two inputs and two outputs:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model &lt;-<span class="st"> </span><span class="kw"><a href="../reference/model.html">model</a></span>(
  <span class="dt">inputs =</span> <span class="kw">c</span>(main_input, auxiliary_input), 
  <span class="dt">outputs =</span> <span class="kw">c</span>(main_output, auxiliary_output)
)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/summary.html">summary</a></span>(model)</code></pre></div>
<pre><code>Model
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
main_input (InputLayer)          (None, 100)           0                                            
____________________________________________________________________________________________________
embedding_1 (Embedding)          (None, 100, 512)      5120000                                      
____________________________________________________________________________________________________
lstm_1 (LSTM)                    (None, 32)            69760                                        
____________________________________________________________________________________________________
aux_input (InputLayer)           (None, 5)             0                                            
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 37)            0                                            
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 64)            2432                                         
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 64)            4160                                         
____________________________________________________________________________________________________
dense_3 (Dense)                  (None, 64)            4160                                         
____________________________________________________________________________________________________
main_output (Dense)              (None, 1)             65                                           
____________________________________________________________________________________________________
aux_output (Dense)               (None, 1)             33                                           
====================================================================================================
Total params: 5,200,610
Trainable params: 5,200,610
Non-trainable params: 0
____________________________________________________________________________________________________</code></pre>
<p>We compile the model and assign a weight of 0.2 to the auxiliary loss. To specify different <code>loss_weights</code> or <code>loss</code> for each different output, you can use a list or a dictionary. Here we pass a single loss as the <code>loss</code> argument, so the same loss will be used on all outputs.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/compile.html">compile</a></span>(model,
  <span class="dt">optimizer =</span> <span class="st">'rmsprop'</span>,
  <span class="dt">loss =</span> <span class="st">'binary_crossentropy'</span>,
  <span class="dt">loss_weights =</span> <span class="kw">c</span>(<span class="fl">1.0</span>, <span class="fl">0.2</span>)
)</code></pre></div>
<p>We can train the model by passing it lists of input arrays and target arrays:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/fit.html">fit</a></span>(model,
  <span class="dt">x =</span> <span class="kw">c</span>(headline_data, additional_data),
  <span class="dt">y =</span> <span class="kw">c</span>(labels, labels),
  <span class="dt">epochs =</span> <span class="dv">50</span>,
  <span class="dt">batch_size =</span> <span class="dv">32</span>
)</code></pre></div>
<p>Since our inputs and outputs are named (we passed them a “name” argument), We could also have compiled the model via:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/compile.html">compile</a></span>(model,
  <span class="dt">optimizer =</span> <span class="st">'rmsprop'</span>,
  <span class="dt">loss =</span> <span class="kw">list</span>(<span class="dt">main_output =</span> <span class="st">'binary_crossentropy'</span>, <span class="dt">aux_output =</span> <span class="st">'binary_crossentropy'</span>),
  <span class="dt">loss_weights =</span> <span class="kw">list</span>(<span class="dt">main_output =</span> <span class="fl">1.0</span>, <span class="dt">aux_output =</span> <span class="fl">0.2</span>)
)

<span class="co"># And trained it via:</span>
<span class="kw"><a href="../reference/fit.html">fit</a></span>(model,
  <span class="dt">x =</span> <span class="kw">list</span>(<span class="dt">main_input =</span> headline_data, <span class="dt">aux_input =</span> additional_data),
  <span class="dt">y =</span> <span class="kw">list</span>(<span class="dt">main_output =</span> labels, <span class="dt">aux_output =</span> labels),
  <span class="dt">epochs =</span> <span class="dv">50</span>,
  <span class="dt">batch_size =</span> <span class="dv">32</span>
)</code></pre></div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#first-example-a-densely-connected-network">First example: a densely-connected network</a></li>
      <li><a href="#models-can-be-treated-like-layers">Models can be treated like layers</a></li>
      <li><a href="#multi-input-and-multi-output-models">Multi-input and multi-output models</a></li>
      </ul>
</div>
      </div>

</div>


<style type="text/css">
  
h4.date,
h4.author {
  display: none;
}

</style>
<script type="text/javascript">
  
$(document).ready(function() {
   
});
  
</script><footer><div class="copyright">
  <p>Developed by François Chollet, JJ Allaire.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
