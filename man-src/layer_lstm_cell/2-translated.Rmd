---
knit: ({source(here::here("tools/knit.R")); knit_man_src})
---
Cell class for the LSTM layer.

@description
This class processes one step within the whole time sequence input, whereas
`keras.layer.LSTM` processes the whole sequence.

# Call Arguments
- `inputs`: A 2D tensor, with shape `(batch, features)`.
- `states`: A 2D tensor with shape `(batch, units)`, which is the state
    from the previous time step.
- `training`: Boolean indicating whether the layer should behave in
    training mode or in inference mode. Only relevant when `dropout` or
    `recurrent_dropout` is used.

# Examples
```{r}
inputs <- random_uniform(c(32, 10, 8))
output <- inputs |>
  layer_rnn(cell = layer_lstm_cell(4))
shape(output)

rnn <- layer_rnn(cell = layer_lstm_cell(4),
                 return_sequences = T,
                 return_state = T)
c(whole_sequence_output, ...final_state) %<-% rnn(inputs)
str(whole_sequence_output)
str(final_state)
```

@param units
Positive integer, dimensionality of the output space.

@param activation
Activation function to use. Default: hyperbolic tangent
(`tanh`). If you pass `NULL`, no activation is applied
(ie. "linear" activation: `a(x) = x`).

@param recurrent_activation
Activation function to use for the recurrent step.
Default: sigmoid (`sigmoid`). If you pass `NULL`, no activation is
applied (ie. "linear" activation: `a(x) = x`).

@param use_bias
Boolean, (default `TRUE`), whether the layer
should use a bias vector.

@param kernel_initializer
Initializer for the `kernel` weights matrix,
used for the linear transformation of the inputs. Default:
`"glorot_uniform"`.

@param recurrent_initializer
Initializer for the `recurrent_kernel`
weights matrix, used for the linear transformation
of the recurrent state. Default: `"orthogonal"`.

@param bias_initializer
Initializer for the bias vector. Default: `"zeros"`.

@param unit_forget_bias
Boolean (default `TRUE`). If `TRUE`,
add 1 to the bias of the forget gate at initialization.
Setting it to `TRUE` will also force `bias_initializer="zeros"`.
This is recommended in [Jozefowicz et al.](
https://github.com/mlresearch/v37/blob/gh-pages/jozefowicz15.pdf)

@param kernel_regularizer
Regularizer function applied to the `kernel` weights
matrix. Default: `NULL`.

@param recurrent_regularizer
Regularizer function applied to the
`recurrent_kernel` weights matrix. Default: `NULL`.

@param bias_regularizer
Regularizer function applied to the bias vector.
Default: `NULL`.

@param kernel_constraint
Constraint function applied to the `kernel` weights
matrix. Default: `NULL`.

@param recurrent_constraint
Constraint function applied to the
`recurrent_kernel` weights matrix. Default: `NULL`.

@param bias_constraint
Constraint function applied to the bias vector.
Default: `NULL`.

@param dropout
Float between 0 and 1. Fraction of the units to drop for the
linear transformation of the inputs. Default: 0.

@param recurrent_dropout
Float between 0 and 1. Fraction of the units to drop
for the linear transformation of the recurrent state. Default: 0.

@param seed
Random seed for dropout.

@param ...
Passed on to the Python callable

@export
@family rnn layers
@family layers
@seealso
+ <https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell>
