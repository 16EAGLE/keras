diff --git a/man-src/callback_tensorboard/2-translated.Rmd b/man-src/callback_tensorboard/2-translated.Rmd
index 41dc7964..6d41c711 100644
--- a/man-src/callback_tensorboard/2-translated.Rmd
+++ b/man-src/callback_tensorboard/2-translated.Rmd
@@ -13,9 +13,9 @@ This callback logs events for TensorBoard, including:
 
-When used in `model.evaluate()` or regular validation
+When used in `model |> evaluate()` or regular validation
 in addition to epoch summaries, there will be a summary that records
-evaluation metrics vs `model.optimizer.iterations` written. The metric names
-will be prepended with `evaluation`, with `model.optimizer.iterations` being
+evaluation metrics vs `model$optimizer$iterations` written. The metric names
+will be prepended with `evaluation`, with `model$optimizer$iterations` being
 the step in the visualized TensorBoard.
 
-If you have installed TensorFlow with pip, you should be able
+If you have installed TensorFlow with `pip` or `reticulate::py_install()`, you should be able
 to launch TensorBoard from the command line:
@@ -25,2 +25,3 @@ tensorboard --logdir=path_to_your_logs
 ```
+or from R with `tensorflow::tensorboard()`.
 
@@ -32,5 +33,5 @@ Basic usage:
 
-```python
-tensorboard_callback = keras.callbacks.TensorBoard(log_dir="./logs")
-model.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])
+```{r, eval = FALSE}
+tensorboard_callback <- callback_tensorboard(log_dir = "./logs")
+model %>% fit(x_train, y_train, epochs = 2, callbacks = list(tensorboard_callback))
 # Then run the tensorboard command to view the visualizations.
@@ -40,21 +41,22 @@ Custom batch-level summaries in a subclassed Model:
 
-```python
-class MyModel(keras.Model):
-
-    def build(self, _):
-        self.dense = keras.layers.Dense(10)
-
-    def call(self, x):
-        outputs = self.dense(x)
-        tf.summary.histogram('outputs', outputs)
-        return outputs
-
-model = MyModel()
-model.compile('sgd', 'mse')
-
-# Make sure to set `update_freq=N` to log a batch-level summary every N
-# batches.  In addition to any `tf.summary` contained in `model.call()`,
-# metrics added in `Model.compile` will be logged every N batches.
-tb_callback = keras.callbacks.TensorBoard('./logs', update_freq=1)
-model.fit(x_train, y_train, callbacks=[tb_callback])
+```{r, eval = FALSE}
+MyModel <- new_model_class("MyModel",
+  initialize = function() {
+    self$dense <- layer_dense(units = 10)
+  },
+  call = function(x) {
+    outputs <- x |> self$dense()
+    tf$summary$histogram('outputs', outputs)
+    outputs
+  }
+)
+
+model <- MyModel()
+model |> compile(optimizer = 'sgd', loss = 'mse')
+
+# Make sure to set `update_freq = N` to log a batch-level summary every N
+# batches. In addition to any `tf$summary` contained in `model$call()`,
+# metrics added in `model |>compile` will be logged every N batches.
+tb_callback <- callback_tensorboard(log_dir = './logs', update_freq = 1)
+model |> fit(x_train, y_train, callbacks = list(tb_callback))
 ```
@@ -63,12 +65,15 @@ Custom batch-level summaries in a Functional API Model:
 
-```python
-def my_summary(x):
-    tf.summary.histogram('x', x)
-    return x
+```{r, eval = FALSE}
+my_summary <- function(x) {
+  tf$summary$histogram('x', x)
+  x
+}
+
+inputs <- layer_input(10)
+outputs <- inputs |>
+  layer_dense(10) |>
+  layer_lambda(my_summary)
 
-inputs = keras.Input(10)
-x = keras.layers.Dense(10)(inputs)
-outputs = keras.layers.Lambda(my_summary)(x)
-model = keras.Model(inputs, outputs)
-model.compile('sgd', 'mse')
+model <- keras_model(inputs, outputs)
+model |> compile(optimizer = 'sgd', loss = 'mse')
 
@@ -77,4 +82,4 @@ model.compile('sgd', 'mse')
 # metrics added in `Model.compile` will be logged every N batches.
-tb_callback = keras.callbacks.TensorBoard('./logs', update_freq=1)
-model.fit(x_train, y_train, callbacks=[tb_callback])
+tb_callback <- callback_tensorboard(log_dir = './logs', update_freq = 1)
+model |> fit(x_train, y_train, callbacks = list(tb_callback))
 ```
@@ -83,12 +88,14 @@ Profiling:
 
-```python
+```{r, eval = FALSE}
 # Profile a single batch, e.g. the 5th batch.
-tensorboard_callback = keras.callbacks.TensorBoard(
-    log_dir='./logs', profile_batch=5)
-model.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])
+tensorboard_callback <- callback_tensorboard(
+  log_dir = './logs', profile_batch = 5)
+model |> fit(x_train, y_train, epochs = 2,
+             callbacks = list(tensorboard_callback))
 
 # Profile a range of batches, e.g. from 10 to 20.
-tensorboard_callback = keras.callbacks.TensorBoard(
-    log_dir='./logs', profile_batch=(10,20))
-model.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])
+tensorboard_callback <- callback_tensorboard(
+  log_dir = './logs', profile_batch = c(10, 20))
+model |> fit(x_train, y_train, epochs = 2,
+             callbacks = list(tensorboard_callback))
 ```
@@ -97,3 +104,3 @@ model.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])
     parsed by TensorBoard. e.g.,
-    `log_dir = os.path.join(working_dir, 'logs')`.
+    `log_dir = file.path(working_dir, 'logs')`.
     This directory should not be reused by any other callbacks.
@@ -106,3 +113,3 @@ model.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])
     Note that the log file can become quite large
-    when `write_graph` is set to `True`.
+    when `write_graph` is set to `TRUE`.
 @param write_images whether to write model weights to visualize as image in
