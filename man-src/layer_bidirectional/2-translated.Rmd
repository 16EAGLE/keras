---
knit: ({source(here::here("tools/knit.R")); knit_man_src})
---
Bidirectional wrapper for RNNs.

@description

Bidirectional wrapper for RNNs, allowing the same RNN to be used for both
forward and backward passes.

# Call Arguments
The call arguments for this layer are the same as those of the
wrapped RNN layer. Beware that when passing the `initial_state`
argument during the call of this layer, the first half in the
list of elements in the `initial_state` list will be passed to
the forward RNN call and the last half in the list of elements
will be passed to the backward RNN call.

# Note
Instantiating a `Bidirectional` layer from an existing RNN layer
instance will not reuse the weights state of the RNN layer instance -- the
`Bidirectional` layer will have freshly initialized weights.

```{r}
model <- keras_model_sequential(input_shape = c(5, 10)) |>
  bidirectional(layer_lstm(units = 10, return_sequences = TRUE)) |>
  bidirectional(layer_lstm(units = 10)) |>
  layer_dense(units = 5, activation = "softmax")

model |> compile(loss = 'categorical_crossentropy',
                 optimizer = 'rmsprop')

# With custom backward layer
forward_layer <- layer_lstm(units = 10, return_sequences = TRUE)
backward_layer <- layer_lstm(units = 10, return_sequences = TRUE, 
                             go_backwards = TRUE, activation = 'relu')

model <- keras_model_sequential(input_shape = c(5, 10)) |>
  bidirectional(forward_layer, backward_layer = backward_layer) |>
  layer_dense(units = 5, activation = "softmax")

model |> compile(
  loss = 'categorical_crossentropy',
  optimizer = 'rmsprop'
)
```

@param layer `layer_RNN` instance, such as
    `layer_lstm()` or `layer_gru()`.
    It could also be any `Layer` instance
    that meets the following criteria:
    1. Be a sequence-processing layer (accepts 3D+ inputs).
    2. Have a `go_backwards`, `return_sequences` and `return_state`
    attribute (with the same semantics as for the `layer_rnn()` class).
    3. Have an `input_spec` attribute.
    4. Implement serialization via `get_config()` and `from_config()`.
    Note that the recommended way to create new RNN layers is to write a
    custom RNN cell and use it with `layer_rnn()`, instead of
    creating a `new_layer_class()` directly.
    When `return_sequences` is `TRUE`, the output of the masked
    timestep will be zero regardless of the layer's original
    `zero_output_for_mask` value.
@param merge_mode Mode by which outputs of the forward and backward RNNs
    will be combined. One of `"sum"`, `"mul"`, `"concat"`, `"ave"`, or `NULL`.
    If `NULL`, the outputs will not be combined,
    they will be returned as a list. Defaults to `"concat"`.
@param backward_layer Optional `layer_rnn()`,
    or a `Layer` instance to be used to handle
    backwards input processing.
    If `backward_layer` is not provided, the layer instance passed
    as the `layer` argument will be used to generate the backward layer
    automatically.
    Note that the provided `backward_layer` layer should have properties
    matching those of the `layer` argument, in particular
    it should have the same values for `stateful`, `return_states`,
    `return_sequences`, etc. In addition, `backward_layer`
    and `layer` should have different `go_backwards` argument values.
    A `ValueError` will be raised if these requirements are not met.
@param object Object to compose the layer with. A tensor, array, or sequential model.
@param ... Passed on to the Python callable
@param weights see description

@export
@family recurrent layers
@seealso
+ <https:/keras.io/api/layers/recurrent_layers/bidirectional#bidirectional-class>
+ <https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional>
