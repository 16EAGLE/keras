diff --git a/man-src/callback_early_stopping/2-translated.Rmd b/man-src/callback_early_stopping/2-translated.Rmd
--- a/man-src/callback_early_stopping/2-translated.Rmd
+++ b/man-src/callback_early_stopping/2-translated.Rmd
@@ -5,23 +5,25 @@ Assuming the goal of a training is to minimize the loss. With this, the
 metric to be monitored would be `'loss'`, and mode would be `'min'`. A
-`model.fit()` training loop will check at end of every epoch whether
+`model$fit()` training loop will check at end of every epoch whether
 the loss is no longer decreasing, considering the `min_delta` and
 `patience` if applicable. Once it's found no longer decreasing,
-`model.stop_training` is marked True and the training terminates.
+`model$stop_training` is marked `TRUE` and the training terminates.
 
-The quantity to be monitored needs to be available in `logs` dict.
-To make it so, pass the loss or metrics at `model.compile()`.
+The quantity to be monitored needs to be available in `logs` list.
+To make it so, pass the loss or metrics at `model$compile()`.
 
 # Examples
-```python
-callback = keras.callbacks.EarlyStopping(monitor='loss',
-                                              patience=3)
+```{r}
+callback <- callback_early_stopping(monitor = 'loss',
+                                   patience = 3)
 # This callback will stop the training when there is no improvement in
 # the loss for three consecutive epochs.
-model = keras.models.Sequential([keras.layers.Dense(10)])
-model.compile(keras.optimizers.SGD(), loss='mse')
-history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),
-                    epochs=10, batch_size=1, callbacks=[callback],
-                    verbose=0)
-len(history.history['loss'])  # Only 4 epochs are run.
-# 4
+model <- keras_model_sequential() %>%
+  layer_dense(10)
+model %>% compile(optimizer = optimizer_sgd(), loss = 'mse')
+history <- model %>% fit(x = k_ones(c(5, 20)),
+                         y = k_zeros(5),
+                         epochs = 10, batch_size = 1,
+                         callbacks = list(callback),
+                         verbose = 0)
+nrow(as.data.frame(history))  # Only 4 epochs are run.
 ```
@@ -41,7 +43,7 @@ len(history.history['loss'])  # Only 4 epochs are run.
     of the monitored quantity. Defaults to `"auto"`.
-@param baseline Baseline value for the monitored quantity. If not `None`,
+@param baseline Baseline value for the monitored quantity. If not `NULL`,
     training will stop if the model doesn't show improvement over the
-    baseline. Defaults to `None`.
+    baseline. Defaults to `NULL`.
 @param restore_best_weights Whether to restore model weights from the epoch
-    with the best value of the monitored quantity. If `False`, the model
+    with the best value of the monitored quantity. If `FALSE`, the model
     weights obtained at the last step of training are used. An epoch
@@ -50,3 +52,3 @@ len(history.history['loss'])  # Only 4 epochs are run.
     for `patience` epochs and restore weights from the best epoch in
-    that set. Defaults to `False`.
+    that set. Defaults to `FALSE`.
 @param start_from_epoch Number of epochs to wait before starting to monitor
