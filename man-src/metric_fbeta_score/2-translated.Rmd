---
knit: ({source(here::here("tools/knit.R")); knit_man_src})
---
Computes F-Beta score.

@description
Formula:

```{r, eval = FALSE}
b2 <- beta^2
f_beta_score <- (1 + b2) * (precision * recall) / (precision * b2 + recall)
```
This is the weighted harmonic mean of precision and recall.
Its output range is `[0, 1]`. It works for both multi-class
and multi-label classification.

# Examples
```{r}
metric <- metric_fbeta_score(beta = 2.0, threshold = 0.5)
y_true <- rbind(c(1, 1, 1),
                c(1, 0, 0),
                c(1, 1, 0))
y_pred <- rbind(c(0.2, 0.6, 0.7),
                c(0.2, 0.6, 0.6),
                c(0.6, 0.8, 0.0))
metric$update_state(y_true, y_pred)
metric$result()
```

@returns
F-Beta Score: float.

@param average
Type of averaging to be performed across per-class results
in the multi-class case.
Acceptable values are `NULL`, `"micro"`, `"macro"` and
`"weighted"`. Defaults to `NULL`.
If `NULL`, no averaging is performed and `result()` will return
the score for each class.
If `"micro"`, compute metrics globally by counting the total
true positives, false negatives and false positives.
If `"macro"`, compute metrics for each label,
and return their unweighted mean.
This does not take label imbalance into account.
If `"weighted"`, compute metrics for each label,
and return their average weighted by support
(the number of true instances for each label).
This alters `"macro"` to account for label imbalance.
It can result in an score that is not between precision and recall.

@param beta
Determines the weight of given to recall
in the harmonic mean between precision and recall (see pseudocode
equation above). Defaults to `1`.

@param threshold
Elements of `y_pred` greater than `threshold` are
converted to be 1, and the rest 0. If `threshold` is
`NULL`, the argmax of `y_pred` is converted to 1, and the rest to 0.

@param name
Optional. String name of the metric instance.

@param dtype
Optional. Data type of the metric result.

@param ...
Passed on to the Python callable

@export
@family f score metrics
@family metrics
@seealso
+ <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/FBetaScore>

