---
knit: ({source(here::here("tools/knit.R")); knit_man_src})
---
Computes Kullback-Leibler divergence metric between `y_true` and

@description
Formula:

```{r, eval = FALSE}
loss <- y_true * log(y_true / y_pred)
```

# Usage
Standalone usage:

```{r}
m <- metric_kl_divergence()
m$update_state(rbind(c(0, 1), c(0, 0)), rbind(c(0.6, 0.4), c(0.4, 0.6)))
m$result()
```

```{r}
m$reset_state()
m$update_state(rbind(c(0, 1), c(0, 0)), rbind(c(0.6, 0.4), c(0.4, 0.6)),
               sample_weight = c(1, 0))
m$result()
```

Usage with `compile()` API:

```{r, eval = FALSE}
model %>% compile(optimizer = 'sgd',
                  loss = 'mse',
                  metrics = list(metric_kl_divergence()))
```

@param name
(Optional) string name of the metric instance.

@param dtype
(Optional) data type of the metric result.

@param y_true
Tensor of true targets.

@param y_pred
Tensor of predicted targets.

@param ...
Passed on to the Python callable

@export
@family losses
@family metrics
@family probabilistic metrics
@seealso
+ <https:/keras.io/api/metrics/probabilistic_metrics#kldivergence-class>
+ <https://www.tensorflow.org/api_docs/python/tf/keras/metrics/KLDivergence>

