---
knit: ({source(here::here("tools/knit.R")); knit_man_src})
---
Computes the alpha balanced focal crossentropy loss.

@description
Use this crossentropy loss function when there are two or more label
classes and if you want to handle class imbalance without using
`class_weights`. We expect labels to be provided in a `one_hot`
representation.

According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it
helps to apply a focal factor to down-weight easy examples and focus more on
hard examples. The general formula for the focal loss (FL)
is as follows:

`FL(p_t) = (1 - p_t)^gamma * log(p_t)`

where `p_t` is defined as follows:
`p_t = output if y_true == 1, else 1 - output`

`(1 - p_t)^gamma` is the `modulating_factor`, where `gamma` is a focusing
parameter. When `gamma` = 0, there is no focal effect on the cross entropy.
`gamma` reduces the importance given to simple examples in a smooth manner.

The authors use alpha-balanced variant of focal loss (FL) in the paper:
`FL(p_t) = -alpha * (1 - p_t)^gamma * log(p_t)`

where `alpha` is the weight factor for the classes. If `alpha` = 1, the
loss won't be able to handle class imbalance properly as all
classes will have the same weight. This can be a constant or a list of
constants. If alpha is a list, it must have the same length as the number
of classes.

The formula above can be generalized to:
`FL(p_t) = alpha * (1 - p_t)^gamma * CrossEntropy(y_true, y_pred)`

where minus comes from `CrossEntropy(y_true, y_pred)` (CE).

Extending this to multi-class case is straightforward:
`FL(p_t) = alpha * (1 - p_t) ** gamma * CategoricalCE(y_true, y_pred)`

In the snippet below, there is `num_classes` floating pointing values per
example. The shape of both `y_pred` and `y_true` are
`(batch_size, num_classes)`.

# Examples
```{r}
y_true <- rbind(c(0, 1, 0), c(0, 0, 1))
y_pred <- rbind(c(0.05, 0.95, 0), c(0.1, 0.8, 0.1))
loss <- loss_categorical_focal_crossentropy(y_true, y_pred)
loss
```
Standalone usage:

```{r}
y_true <- rbind(c(0, 1, 0), c(0, 0, 1))
y_pred <- rbind(c(0.05, 0.95, 0), c(0.1, 0.8, 0.1))
# Using 'auto'/'sum_over_batch_size' reduction type.
cce <- loss_categorical_focal_crossentropy()
cce(y_true, y_pred)
```

```{r}
# Calling with 'sample_weight'.
cce(y_true, y_pred, sample_weight = k_array(c(0.3, 0.7)))
```

```{r}
# Using 'sum' reduction type.
cce <- loss_categorical_focal_crossentropy(reduction = "sum")
cce(y_true, y_pred)
```

```{r}
# Using 'none' reduction type.
cce <- loss_categorical_focal_crossentropy(reduction = NULL)
cce(y_true, y_pred)
```

Usage with the `compile()` API:

```{r, eval = FALSE}
model %>% compile(
  optimizer = 'adam',
  loss = loss_categorical_focal_crossentropy())
```

@returns
Categorical focal crossentropy loss value.

@param alpha
A weight balancing factor for all classes, default is `0.25` as
mentioned in the reference. It can be a list of floats or a scalar.
In the multi-class case, alpha may be set by inverse class
frequency by using `compute_class_weight` from `sklearn.utils`.

@param gamma
A focusing parameter, default is `2.0` as mentioned in the
reference. It helps to gradually reduce the importance given to
simple examples in a smooth manner. When `gamma` = 0, there is
no focal effect on the categorical crossentropy.

@param from_logits
Whether `output` is expected to be a logits tensor. By
default, we consider that `output` encodes a probability
distribution.

@param label_smoothing
Float in `[0, 1].` When > 0, label values are smoothed,
meaning the confidence on label values are relaxed. For example, if
`0.1`, use `0.1 / num_classes` for non-target labels and
`0.9 + 0.1 / num_classes` for target labels.

@param axis
The axis along which to compute crossentropy (the features
axis). Defaults to `-1`.

@param reduction
Type of reduction to apply to the loss. In almost all cases
this should be `"sum_over_batch_size"`.
Supported options are `"sum"`, `"sum_over_batch_size"` or `NULL`.

@param name
Optional name for the loss instance.

@param y_true
Tensor of one-hot true targets.

@param y_pred
Tensor of predicted targets.

@param ...
Passed on to the Python callable

@export
@family losses
@seealso
+ <https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalFocalCrossentropy>

