diff --git a/man-src/callback_learning_rate_scheduler/2-translated.Rmd b/man-src/callback_learning_rate_scheduler/2-translated.Rmd
index 84b452ea..0368cf7e 100644
--- a/man-src/callback_learning_rate_scheduler/2-translated.Rmd
+++ b/man-src/callback_learning_rate_scheduler/2-translated.Rmd
@@ -9,23 +9,25 @@ the optimizer.
 # Examples
-```python
+```{r}
 # This function keeps the initial learning rate for the first ten epochs
 # and decreases it exponentially after that.
-def scheduler(epoch, lr):
-    if epoch < 10:
-        return lr
-    else:
-        return lr * ops.exp(-0.1)
-# >>>
-model = keras.models.Sequential([keras.layers.Dense(10)])
-model.compile(keras.optimizers.SGD(), loss='mse')
-round(model.optimizer.learning_rate, 5)
-# 0.01
+scheduler <- function(epoch, lr) {
+    if (epoch < 10) {
+        return(lr)
+    } else {
+        return(lr * exp(-0.1))
+    }
+}
+
+model <- keras_model_sequential() %>%
+  layer_dense(units = 10)
+model %>% compile(optimizer = optimizer_sgd(), loss = 'mse')
+round(model$optimizer$lr, 5)
 ```
 
-```python
-callback = keras.callbacks.LearningRateScheduler(scheduler)
-history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),
-                    epochs=15, callbacks=[callback], verbose=0)
-round(model.optimizer.learning_rate, 5)
-# 0.00607
+```{r}
+callback <- callback_learning_rate_scheduler(schedule = scheduler)
+history <- model %>% fit(x = matrix(runif(100), nrow = 5, ncol = 20),
+                         y = rep(0, 5),
+                         epochs = 15, callbacks = list(callback), verbose = 0)
+round(model$optimizer$lr, 5)
 ```
