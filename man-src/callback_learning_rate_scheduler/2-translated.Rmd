Learning rate scheduler.

@description
At the beginning of every epoch, this callback gets the updated learning
rate value from `schedule` function provided at `__init__`, with the current
epoch and current learning rate, and applies the updated learning rate on
the optimizer.

# Examples
```{r}
# This function keeps the initial learning rate for the first ten epochs
# and decreases it exponentially after that.
scheduler <- function(epoch, lr) {
  # browser()
    if (epoch < 10) {
        return(lr)
    } else {
        return(lr * exp(-0.1))
    }
}

model <- keras_model_sequential() %>%
  layer_dense(units = 10)
model %>% compile(optimizer = optimizer_sgd(), loss = 'mse')
model$optimizer$learning_rate |> as.array() |> round(5)
```

```{r}
callback <- callback_learning_rate_scheduler(schedule = scheduler)
history <- model %>% fit(x = array(runif(100), c(5, 20)),
                         y = array(0, c(5, 1)),
                         epochs = 15, callbacks = list(callback), verbose = 0)
model$optimizer$learning_rate |> as.array() |> round(5)
```

@param schedule A function that takes an epoch index (integer, indexed from 0)
    and current learning rate (float) as inputs and returns a new
    learning rate as output (float).
@param verbose Integer. 0: quiet, 1: log update messages.

@export
@family callback
@seealso
+ <https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler>
