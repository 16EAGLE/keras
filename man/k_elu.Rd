% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ops.R
\name{k_elu}
\alias{k_elu}
\title{Exponential Linear Unit activation function.}
\usage{
k_elu(x, alpha = 1)
}
\arguments{
\item{x}{Input tensor.}

\item{alpha}{A scalar, slope of positive section. Defaults to \code{1.0}.}
}
\description{
It is defined as:

\verb{f(x) =  alpha * (exp(x) - 1.) for x < 0}, \verb{f(x) = x for x >= 0}.
}
\section{Returns}{
A tensor with the same shape as \code{x}.
}

\section{Examples}{
\if{html}{\out{<div class="sourceCode python">}}\preformatted{x = np.array([-1., 0., 1.])
x_elu = keras_core.ops.elu(x)
print(x_elu)
# array([-0.63212055, 0., 1.], shape=(3,), dtype=float64)
}\if{html}{\out{</div>}}
}

\seealso{
\itemize{
\item \url{https://www.tensorflow.org/api_docs/python/tf/keras/ops/elu}
}
}
