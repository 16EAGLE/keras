% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layers-activations.R
\name{layer_activation_elu}
\alias{layer_activation_elu}
\title{Applies an Exponential Linear Unit function to an output.}
\usage{
layer_activation_elu(object, alpha = 1, ...)
}
\arguments{
\item{alpha}{float, slope of negative section. Defaults to \code{1.0}.}

\item{...}{Base layer keyword arguments, such as \code{name} and \code{dtype}.}
}
\description{
Formula:

\if{html}{\out{<div class="sourceCode">}}\preformatted{f(x) = alpha * (exp(x) - 1.) for x < 0
f(x) = x for x >= 0
}\if{html}{\out{</div>}}
}
\seealso{
\itemize{
\item \url{https://www.tensorflow.org/api_docs/python/tf/keras/layers/ELU}
}

Other activations layers: 
\code{\link{layer_activation_leaky_relu}()},
\code{\link{layer_activation_parametric_relu}()},
\code{\link{layer_activation_relu}()},
\code{\link{layer_activation_softmax}()},
\code{\link{layer_activation}()}
}
\concept{activations layers}
