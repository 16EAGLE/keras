% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/autogen-layers-normalization.R,
%   R/autogen-layers-preprocessing.R
\name{layer_normalization}
\alias{layer_normalization}
\title{Layer normalization layer (Ba et al., 2016).}
\usage{
layer_normalization(
  object,
  axis = -1L,
  mean = NULL,
  variance = NULL,
  invert = FALSE,
  ...
)

layer_normalization(
  object,
  axis = -1L,
  mean = NULL,
  variance = NULL,
  invert = FALSE,
  ...
)
}
\arguments{
\item{object}{Object to compose the layer with. A tensor, array, or sequential model.}

\item{axis}{Integer, tuple of integers, or None. The axis or axes that should
have a separate mean and variance for each index in the shape.
For example, if shape is \verb{(None, 5)} and \code{axis=1}, the layer will
track 5 separate mean and variance values for the last axis.
If \code{axis} is set to \code{None}, the layer will normalize
all elements in the input by a scalar mean and variance.
When \code{-1}, the last axis of the input is assumed to be a
feature dimension and is normalized per index.
Note that in the specific case of batched scalar inputs where
the only axis is the batch axis, the default will normalize
each index in the batch separately.
In this case, consider passing \code{axis=None}. Defaults to \code{-1}.}

\item{mean}{The mean value(s) to use during normalization. The passed value(s)
will be broadcast to the shape of the kept axes above;
if the value(s) cannot be broadcast, an error will be raised when
this layer's \code{build()} method is called.}

\item{variance}{The variance value(s) to use during normalization. The passed
value(s) will be broadcast to the shape of the kept axes above;
if the value(s) cannot be broadcast, an error will be raised when
this layer's \code{build()} method is called.}

\item{invert}{If \code{True}, this layer will apply the inverse transformation
to its inputs: it would turn a normalized input back into its
original form.}

\item{...}{Passed on to the Python callable}

\item{epsilon}{Small float added to variance to avoid dividing by zero.
Defaults to 1e-3.}

\item{center}{If True, add offset of \code{beta} to normalized tensor. If False,
\code{beta} is ignored. Defaults to \code{True}.}

\item{scale}{If True, multiply by \code{gamma}. If False, \code{gamma} is not used.
When the next layer is linear (also e.g. \code{nn.relu}), this can be
disabled since the scaling will be done by the next layer.
Defaults to \code{True}.}

\item{rms_scaling}{If True, \code{center} and \code{scale} are ignored, and the
inputs are scaled by \code{gamma} and the inverse square root
of the square of all inputs. This is an approximate and faster
approach that avoids ever computing the mean of the input.}

\item{beta_initializer}{Initializer for the beta weight. Defaults to zeros.}

\item{gamma_initializer}{Initializer for the gamma weight. Defaults to ones.}

\item{beta_regularizer}{Optional regularizer for the beta weight.
None by default.}

\item{gamma_regularizer}{Optional regularizer for the gamma weight.
None by default.}

\item{beta_constraint}{Optional constraint for the beta weight.
None by default.}

\item{gamma_constraint}{Optional constraint for the gamma weight.
None by default.}
}
\description{
Normalize the activations of the previous layer for each given example in a
batch independently, rather than across a batch like Batch Normalization.
i.e. applies a transformation that maintains the mean activation within each
example close to 0 and the activation standard deviation close to 1.

If \code{scale} or \code{center} are enabled, the layer will scale the normalized
outputs by broadcasting them with a trainable variable \code{gamma}, and center
the outputs by broadcasting with a trainable variable \code{beta}. \code{gamma} will
default to a ones tensor and \code{beta} will default to a zeros tensor, so that
centering and scaling are no-ops before training has begun.

So, with scaling and centering enabled the normalization equations
are as follows:

Let the intermediate activations for a mini-batch to be the \code{inputs}.

For each sample \code{x_i} in \code{inputs} with \code{k} features, we compute the mean and
variance of the sample:

\if{html}{\out{<div class="sourceCode python">}}\preformatted{mean_i = sum(x_i[j] for j in range(k)) / k
var_i = sum((x_i[j] - mean_i) ** 2 for j in range(k)) / k
}\if{html}{\out{</div>}}

and then compute a normalized \code{x_i_normalized}, including a small factor
\code{epsilon} for numerical stability.

\if{html}{\out{<div class="sourceCode python">}}\preformatted{x_i_normalized = (x_i - mean_i) / sqrt(var_i + epsilon)
}\if{html}{\out{</div>}}

And finally \code{x_i_normalized } is linearly transformed by \code{gamma} and \code{beta},
which are learned parameters:

\if{html}{\out{<div class="sourceCode python">}}\preformatted{output_i = x_i_normalized * gamma + beta
}\if{html}{\out{</div>}}

\code{gamma} and \code{beta} will span the axes of \code{inputs} specified in \code{axis}, and
this part of the inputs' shape must be fully defined.

For example:

\if{html}{\out{<div class="sourceCode python">}}\preformatted{layer = keras.layers.LayerNormalization(axis=[1, 2, 3])
layer.build([5, 20, 30, 40])
print(layer.beta.shape)
# (20, 30, 40)
print(layer.gamma.shape)
# (20, 30, 40)
}\if{html}{\out{</div>}}

Note that other implementations of layer normalization may choose to define
\code{gamma} and \code{beta} over a separate set of axes from the axes being
normalized across. For example, Group Normalization
(\href{https://arxiv.org/abs/1803.08494}{Wu et al. 2018}) with group size of 1
corresponds to a Layer Normalization that normalizes across height, width,
and channel and has \code{gamma} and \code{beta} span only the channel dimension.
So, this Layer Normalization implementation will not match a Group
Normalization layer with group size set to 1.

This layer will shift and scale inputs into a distribution centered around
0 with standard deviation 1. It accomplishes this by precomputing the mean
and variance of the data, and calling \code{(input - mean) / sqrt(var)} at
runtime.

The mean and variance values for the layer must be either supplied on
construction or learned via \code{adapt()}. \code{adapt()} will compute the mean and
variance of the data and store them as the layer's weights. \code{adapt()} should
be called before \code{fit()}, \code{evaluate()}, or \code{predict()}.
}
\section{Reference}{
\itemize{
\item \href{https://arxiv.org/abs/1607.06450}{Lei Ba et al., 2016}.
}
}

\section{Examples}{
Calculate a global mean and variance by analyzing the dataset in \code{adapt()}.

\if{html}{\out{<div class="sourceCode python">}}\preformatted{adapt_data = np.array([1., 2., 3., 4., 5.], dtype='float32')
input_data = np.array([1., 2., 3.], dtype='float32')
layer = keras.layers.Normalization(axis=None)
layer.adapt(adapt_data)
layer(input_data)
# array([-1.4142135, -0.70710677, 0.], dtype=float32)
}\if{html}{\out{</div>}}

Calculate a mean and variance for each index on the last axis.

\if{html}{\out{<div class="sourceCode python">}}\preformatted{adapt_data = np.array([[0., 7., 4.],
                       [2., 9., 6.],
                       [0., 7., 4.],
                       [2., 9., 6.]], dtype='float32')
input_data = np.array([[0., 7., 4.]], dtype='float32')
layer = keras.layers.Normalization(axis=-1)
layer.adapt(adapt_data)
layer(input_data)
# array([-1., -1., -1.], dtype=float32)
}\if{html}{\out{</div>}}

Pass the mean and variance directly.

\if{html}{\out{<div class="sourceCode python">}}\preformatted{input_data = np.array([[1.], [2.], [3.]], dtype='float32')
layer = keras.layers.Normalization(mean=3., variance=2.)
layer(input_data)
# array([[-1.4142135 ],
#        [-0.70710677],
#        [ 0.        ]], dtype=float32)
}\if{html}{\out{</div>}}

Use the layer to de-normalize inputs (after adapting the layer).

\if{html}{\out{<div class="sourceCode python">}}\preformatted{adapt_data = np.array([[0., 7., 4.],
                       [2., 9., 6.],
                       [0., 7., 4.],
                       [2., 9., 6.]], dtype='float32')
input_data = np.array([[1., 2., 3.]], dtype='float32')
layer = keras.layers.Normalization(axis=-1, invert=True)
layer.adapt(adapt_data)
layer(input_data)
# array([2., 10., 8.], dtype=float32)
}\if{html}{\out{</div>}}
}

\seealso{
\itemize{
\item \url{https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization}
}

\itemize{
\item \url{https://www.tensorflow.org/api_docs/python/tf/keras/layers/Normalization}
}

Other normalization layers: 
\code{\link{layer_batch_normalization}()},
\code{\link{layer_group_normalization}()},
\code{\link{layer_spectral_normalization}()},
\code{\link{layer_unit_normalization}()}

Other preprocessing layers: 
\code{\link{layer_category_encoding}()},
\code{\link{layer_center_crop}()},
\code{\link{layer_discretization}()},
\code{\link{layer_hashed_crossing}()},
\code{\link{layer_hashing}()},
\code{\link{layer_integer_lookup}()},
\code{\link{layer_random_brightness}()},
\code{\link{layer_random_contrast}()},
\code{\link{layer_random_crop}()},
\code{\link{layer_random_flip}()},
\code{\link{layer_random_rotation}()},
\code{\link{layer_random_translation}()},
\code{\link{layer_random_zoom}()},
\code{\link{layer_rescaling}()},
\code{\link{layer_resizing}()},
\code{\link{layer_string_lookup}()},
\code{\link{layer_text_vectorization}()}
}
\concept{normalization layers}
\concept{preprocessing layers}
