% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/activations.R
\name{activation_softmax}
\alias{activation_softmax}
\title{Softmax converts a vector of values to a probability distribution.}
\usage{
activation_softmax(x, axis = -1L)
}
\arguments{
\item{x}{Input tensor.}

\item{axis}{Integer, axis along which the softmax is applied.}
}
\description{
The elements of the output vector are in range \verb{[0, 1]} and sum to 1.

Each input vector is handled independently.
The \code{axis} argument sets which axis of the input the function
is applied along.

Softmax is often used as the activation for the last
layer of a classification network because the result could be interpreted as
a probability distribution.

The softmax of each vector x is computed as
\code{exp(x) / sum(exp(x))}.

The input values in are the log-odds of the resulting probability.
}
\seealso{
\itemize{
\item \url{https://keras.io/api/layers/activations#softmax-function}
\item \url{https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax}
}

Other activations: 
\code{\link{activation_elu}()},
\code{\link{activation_exponential}()},
\code{\link{activation_gelu}()},
\code{\link{activation_hard_sigmoid}()},
\code{\link{activation_leaky_relu}()},
\code{\link{activation_linear}()},
\code{\link{activation_log_softmax}()},
\code{\link{activation_mish}()},
\code{\link{activation_relu6}()},
\code{\link{activation_relu}()},
\code{\link{activation_selu}()},
\code{\link{activation_sigmoid}()},
\code{\link{activation_silu}()},
\code{\link{activation_softplus}()},
\code{\link{activation_softsign}()},
\code{\link{activation_tanh}()}
}
\concept{activations}
