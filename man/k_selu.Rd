% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ops.R
\name{k_selu}
\alias{k_selu}
\title{Scaled Exponential Linear Unit (SELU) activation function.}
\usage{
k_selu(x)
}
\arguments{
\item{x}{Input tensor.}
}
\description{
It is defined as:

\verb{f(x) =  scale * alpha * (exp(x) - 1.) for x < 0},
\verb{f(x) = scale * x for x >= 0}.
}
\section{Returns}{
A tensor with the same shape as \code{x}.
}

\section{Examples}{
\if{html}{\out{<div class="sourceCode python">}}\preformatted{x = np.array([-1., 0., 1.])
x_selu = keras_core.ops.selu(x)
print(x_selu)
# array([-1.11133055, 0., 1.05070098], shape=(3,), dtype=float64)
}\if{html}{\out{</div>}}
}

\seealso{
\itemize{
\item \url{https://www.tensorflow.org/api_docs/python/tf/keras/ops/selu}
}
}
