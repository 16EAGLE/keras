% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimizers.R
\name{optimizer_nadam}
\alias{optimizer_nadam}
\title{Nesterov Adam optimizer}
\usage{
optimizer_nadam(lr = 0.002, beta_1 = 0.9, beta_2 = 0.999,
  epsilon = 1e-08, schedule_decay = 0.004, clipnorm = NULL,
  clipvalue = NULL)
}
\arguments{
\item{lr}{Learning rate.}

\item{beta_1}{The exponential decay rate for the 1st moment estimates.}

\item{beta_2}{The exponential decay rate for the 2nd moment estimates.}

\item{epsilon}{Fuzz factor.}

\item{schedule_decay}{Schedule deacy.}

\item{clipnorm}{Gradients will be clipped when their L2 norm exceeds this
value.}

\item{clipvalue}{Gradients will be clipped when their absolute value exceeds
this value.}
}
\description{
Much like Adam is essentially RMSprop with momentum, Nadam is Adam RMSprop 
with Nesterov momentum. See 
\href{http://cs229.stanford.edu/proj2015/054_report.pdf}{Incorporating 
Nesterov Momentum into Adam}.
}
\details{
Default parameters follow those provided in the paper. It is 
  recommended to leave the parameters of this optimizer at their default 
  values.
}
\seealso{
\href{http://www.cs.toronto.edu/~fritz/absps/momentum.pdf}{On the
  importance of initialization and momentum in deep learning}.
}

