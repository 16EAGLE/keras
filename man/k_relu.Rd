% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ops.R
\name{k_relu}
\alias{k_relu}
\title{Rectified linear unit activation function.}
\usage{
k_relu(x)
}
\arguments{
\item{x}{Input tensor.}
}
\description{
It is defined as \code{f(x) = max(0, x)}.
}
\section{Returns}{
A tensor with the same shape as \code{x}.
}

\section{Examples}{
\if{html}{\out{<div class="sourceCode python">}}\preformatted{x1 = keras_core.ops.convert_to_tensor([-1.0, 0.0, 1.0, 0.2])
keras_core.ops.relu(x1)
# array([0.0, 0.0, 1.0, 0.2], dtype=float32)
}\if{html}{\out{</div>}}
}

\seealso{
\itemize{
\item \url{https://www.tensorflow.org/api_docs/python/tf/keras/ops/relu}
}
}
