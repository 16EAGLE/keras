% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/activations.R, R/autogen-activations.R
\name{activation_relu}
\alias{activation_relu}
\title{Applies the rectified linear unit activation function.}
\usage{
activation_relu(x, negative_slope = 0, max_value = NULL, threshold = 0)

activation_relu(x, negative_slope = 0, max_value = NULL, threshold = 0)
}
\arguments{
\item{x}{Input tensor.}

\item{negative_slope}{A \code{float} that controls the slope
for values lower than the threshold.}

\item{max_value}{A \code{float} that sets the saturation threshold (the largest
value the function will return).}

\item{threshold}{A \code{float} giving the threshold value of the activation
function below which values will be damped or set to zero.}
}
\description{
With default values, this returns the standard ReLU activation:
\code{max(x, 0)}, the element-wise maximum of 0 and the input tensor.

Modifying default parameters allows you to use non-zero thresholds,
change the max value of the activation,
and to use a non-zero multiple of the input for values below the threshold.

With default values, this returns the standard ReLU activation:
\code{max(x, 0)}, the element-wise maximum of 0 and the input tensor.

Modifying default parameters allows you to use non-zero thresholds,
change the max value of the activation,
and to use a non-zero multiple of the input for values below the threshold.
}
\section{Examples}{
\if{html}{\out{<div class="sourceCode python">}}\preformatted{x = [-10, -5, 0.0, 5, 10]
keras_core.activations.relu(x)
# [ 0.,  0.,  0.,  5., 10.]
keras_core.activations.relu(x, negative_slope=0.5)
# [-5. , -2.5,  0. ,  5. , 10. ]
keras_core.activations.relu(x, max_value=5.)
# [0., 0., 0., 5., 5.]
keras_core.activations.relu(x, threshold=5.)
# [-0., -0.,  0.,  0., 10.]
}\if{html}{\out{</div>}}

\if{html}{\out{<div class="sourceCode python">}}\preformatted{x = [-10, -5, 0.0, 5, 10]
keras_core.activations.relu(x)
# [ 0.,  0.,  0.,  5., 10.]
keras_core.activations.relu(x, negative_slope=0.5)
# [-5. , -2.5,  0. ,  5. , 10. ]
keras_core.activations.relu(x, max_value=5.)
# [0., 0., 0., 5., 5.]
keras_core.activations.relu(x, threshold=5.)
# [-0., -0.,  0.,  0., 10.]
}\if{html}{\out{</div>}}
}

\section{Returns}{
\if{html}{\out{<div class="sourceCode">}}\preformatted{A tensor with the same shape and dtype as input `x`.
}\if{html}{\out{</div>}}

\if{html}{\out{<div class="sourceCode">}}\preformatted{A tensor with the same shape and dtype as input `x`.
}\if{html}{\out{</div>}}
}

\seealso{
\itemize{
\item \url{https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu}
}

\itemize{
\item \url{https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu}
}

Other activation functions: 
\code{\link{activation_elu}()},
\code{\link{activation_exponential}()},
\code{\link{activation_gelu}()},
\code{\link{activation_hard_sigmoid}()},
\code{\link{activation_leaky_relu}()},
\code{\link{activation_linear}()},
\code{\link{activation_log_softmax}()},
\code{\link{activation_mish}()},
\code{\link{activation_relu_6}()},
\code{\link{activation_selu}()},
\code{\link{activation_sigmoid}()},
\code{\link{activation_silu}()},
\code{\link{activation_softmax}()},
\code{\link{activation_softplus}()},
\code{\link{activation_softsign}()},
\code{\link{activation_tanh}()}

Other activation functions: 
\code{\link{activation_elu}()},
\code{\link{activation_exponential}()},
\code{\link{activation_gelu}()},
\code{\link{activation_hard_sigmoid}()},
\code{\link{activation_leaky_relu}()},
\code{\link{activation_linear}()},
\code{\link{activation_log_softmax}()},
\code{\link{activation_mish}()},
\code{\link{activation_relu_6}()},
\code{\link{activation_selu}()},
\code{\link{activation_sigmoid}()},
\code{\link{activation_silu}()},
\code{\link{activation_softmax}()},
\code{\link{activation_softplus}()},
\code{\link{activation_softsign}()},
\code{\link{activation_tanh}()}
}
\concept{activation functions}
