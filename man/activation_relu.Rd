% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/activations.R
\name{activation_relu}
\alias{activation_relu}
\alias{activation_elu}
\alias{activation_hard_sigmoid}
\alias{activation_linear}
\alias{activation_sigmoid}
\alias{activation_softmax}
\alias{activation_softplus}
\alias{activation_softsign}
\alias{activation_tanh}
\title{Activation functions}
\usage{
activation_relu()

activation_elu()

activation_hard_sigmoid()

activation_linear()

activation_sigmoid()

activation_softmax()

activation_softplus()

activation_softsign()

activation_tanh()
}
\description{
Activations functions can either be used through \code{\link[=layer_activation]{layer_activation()}}, or
through the activation argument supported by all forward layers.
}
