% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/normalization.R
\name{layer_batch_normalization}
\alias{layer_batch_normalization}
\title{Batch normalization layer (Ioffe and Szegedy, 2014).}
\usage{
layer_batch_normalization(x, axis = -1L, momentum = 0.99, epsilon = 0.001,
  center = TRUE, scale = TRUE, beta_initializer = "zeros",
  gamma_initializer = "ones", moving_mean_initializer = "zeros",
  moving_variance_initializer = "ones", beta_regularizer = NULL,
  gamma_regularizer = NULL, beta_constraint = NULL,
  gamma_constraint = NULL, input_shape = NULL)
}
\arguments{
\item{x}{Model or layer}

\item{axis}{Integer, the axis that should be normalized (typically the
features axis). For instance, after a \code{Conv2D} layer with
\code{data_format="channels_first"}, set \code{axis=1} in \code{BatchNormalization}.}

\item{momentum}{Momentum for the moving average.}

\item{epsilon}{Small float added to variance to avoid dividing by zero.}

\item{center}{If TRUE, add offset of \code{beta} to normalized tensor. If FALSE,
\code{beta} is ignored.}

\item{scale}{If TRUE, multiply by \code{gamma}. If FALSE, \code{gamma} is not used.
When the next layer is linear (also e.g. \code{nn.relu}), this can be disabled
since the scaling will be done by the next layer.}

\item{beta_initializer}{Initializer for the beta weight.}

\item{gamma_initializer}{Initializer for the gamma weight.}

\item{moving_mean_initializer}{Initializer for the moving mean.}

\item{moving_variance_initializer}{Initializer for the moving variance.}

\item{beta_regularizer}{Optional regularizer for the beta weight.}

\item{gamma_regularizer}{Optional regularizer for the gamma weight.}

\item{beta_constraint}{Optional constraint for the beta weight.}

\item{gamma_constraint}{Optional constraint for the gamma weight.}

\item{input_shape}{Dimensionality of the input (integer) not including the
samples axis. This argument is required when using this layer as the first
layer in a model.}
}
\description{
Normalize the activations of the previous layer at each batch, i.e. applies a
transformation that maintains the mean activation close to 0 and the
activation standard deviation close to 1.
}
\section{Input shape}{
 Arbitrary. Use the keyword argument \code{input_shape} (list
of integers, does not include the samples axis) when using this layer as
the first layer in a model.
}

\section{Output shape}{
 Same shape as input.
}

\section{References}{

\itemize{
\item \href{https://arxiv.org/abs/1502.03167}{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}
}
}

