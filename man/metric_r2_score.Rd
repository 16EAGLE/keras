% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/autogen-metrics.R
\name{metric_r2_score}
\alias{metric_r2_score}
\title{Computes R2 score.}
\usage{
metric_r2_score(
  ...,
  class_aggregation = "uniform_average",
  num_regressors = 0L,
  name = "r2_score",
  dtype = NULL
)
}
\arguments{
\item{...}{Passed on to the Python callable}

\item{class_aggregation}{Specifies how to aggregate scores corresponding to
different output classes (or target dimensions),
i.e. different dimensions on the last axis of the predictions.
Equivalent to \code{multioutput} argument in Scikit-Learn.
Should be one of
\code{None} (no aggregation), \code{"uniform_average"},
\code{"variance_weighted_average"}.}

\item{num_regressors}{Number of independent regressors used
("Adjusted R2" score). 0 is the standard R2 score.
Defaults to \code{0}.}

\item{name}{Optional. string name of the metric instance.}

\item{dtype}{Optional. data type of the metric result.}
}
\description{
Formula:

\if{html}{\out{<div class="sourceCode python">}}\preformatted{sum_squares_residuals = sum((y_true - y_pred) ** 2)
sum_squares = sum((y_true - mean(y_true)) ** 2)
R2 = 1 - sum_squares_residuals / sum_squares
}\if{html}{\out{</div>}}

This is also called the
\href{https://en.wikipedia.org/wiki/Coefficient_of_determination}{coefficient of determination}.

It indicates how close the fitted regression line
is to ground-truth data.
\itemize{
\item The highest score possible is 1.0. It indicates that the predictors
perfectly accounts for variation in the target.
\item A score of 0.0 indicates that the predictors do not
account for variation in the target.
\item It can also be negative if the model is worse than random.
}

This metric can also compute the "Adjusted R2" score.
}
\section{Examples}{
\if{html}{\out{<div class="sourceCode python">}}\preformatted{y_true = np.array([[1], [4], [3]], dtype=np.float32)
y_pred = np.array([[2], [4], [4]], dtype=np.float32)
metric = keras.metrics.R2Score()
metric.update_state(y_true, y_pred)
result = metric.result()
result
# 0.57142854
}\if{html}{\out{</div>}}
}

\seealso{
\itemize{
\item \url{https://www.tensorflow.org/api_docs/python/tf/keras/metrics/R2Score}
}

Other metric: 
\code{\link{metric_auc}()},
\code{\link{metric_binary_accuracy}()},
\code{\link{metric_binary_crossentropy}()},
\code{\link{metric_binary_iou}()},
\code{\link{metric_categorical_accuracy}()},
\code{\link{metric_categorical_crossentropy}()},
\code{\link{metric_categorical_hinge}()},
\code{\link{metric_cosine_similarity}()},
\code{\link{metric_f1_score}()},
\code{\link{metric_false_negatives}()},
\code{\link{metric_false_positives}()},
\code{\link{metric_fbeta_score}()},
\code{\link{metric_hinge}()},
\code{\link{metric_iou}()},
\code{\link{metric_kl_divergence}()},
\code{\link{metric_log_cosh_error}()},
\code{\link{metric_mean_absolute_error}()},
\code{\link{metric_mean_absolute_percentage_error}()},
\code{\link{metric_mean_iou}()},
\code{\link{metric_mean_metric_wrapper}()},
\code{\link{metric_mean_squared_error}()},
\code{\link{metric_mean_squared_logarithmic_error}()},
\code{\link{metric_mean}()},
\code{\link{metric_one_hot_iou}()},
\code{\link{metric_one_hot_mean_iou}()},
\code{\link{metric_poisson}()},
\code{\link{metric_precision_at_recall}()},
\code{\link{metric_precision}()},
\code{\link{metric_recall_at_precision}()},
\code{\link{metric_recall}()},
\code{\link{metric_root_mean_squared_error}()},
\code{\link{metric_sensitivity_at_specificity}()},
\code{\link{metric_sparse_categorical_accuracy}()},
\code{\link{metric_sparse_categorical_crossentropy}()},
\code{\link{metric_sparse_top_k_categorical_accuracy}()},
\code{\link{metric_specificity_at_sensitivity}()},
\code{\link{metric_squared_hinge}()},
\code{\link{metric_sum}()},
\code{\link{metric_top_k_categorical_accuracy}()},
\code{\link{metric_true_negatives}()},
\code{\link{metric_true_positives}()}
}
\concept{metric}
