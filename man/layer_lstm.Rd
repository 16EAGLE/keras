% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/recurrent.R
\name{layer_lstm}
\alias{layer_lstm}
\title{Long-Short Term Memory unit - Hochreiter 1997.}
\usage{
layer_lstm(x, units, activation = "tanh",
  recurrent_activation = "hard_sigmoid", use_bias = TRUE,
  kernel_initializer = "glorot_uniform",
  recurrent_initializer = "orthogonal", bias_initializer = "zeros",
  unit_forget_bias = TRUE, kernel_regularizer = NULL,
  recurrent_regularizer = NULL, bias_regularizer = NULL,
  activity_regularizer = NULL, kernel_constraint = NULL,
  recurrent_constraint = NULL, bias_constraint = NULL, dropout = 0,
  recurrent_dropout = 0, input_shape = NULL)
}
\arguments{
\item{units}{Positive integer, dimensionality of the output space.}

\item{activation}{Activation function to use. If you don't specify anything,
no activation is applied (ie. "linear" activation: \code{a(x) = x}).}

\item{recurrent_activation}{Activation function to use for the recurrent
step.}

\item{use_bias}{Boolean, whether the layer uses a bias vector.}

\item{kernel_initializer}{Initializer for the \code{kernel} weights matrix, used
for the linear transformation of the inputs..}

\item{recurrent_initializer}{Initializer for the \code{recurrent_kernel} weights
matrix, used for the linear transformation of the recurrent state..}

\item{bias_initializer}{Initializer for the bias vector.}

\item{unit_forget_bias}{Boolean. If TRUE, add 1 to the bias of the forget
gate at initialization. Setting it to true will also force
\code{bias_initializer="zeros"}. This is recommended in \href{http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf}{Jozefowicz etal.}}

\item{kernel_regularizer}{Regularizer function applied to the \code{kernel}
weights matrix.}

\item{recurrent_regularizer}{Regularizer function applied to the
\code{recurrent_kernel} weights matrix.}

\item{bias_regularizer}{Regularizer function applied to the bias vector.}

\item{activity_regularizer}{Regularizer function applied to the output of the
layer (its "activation")..}

\item{kernel_constraint}{Constraint function applied to the \code{kernel} weights
matrix.}

\item{recurrent_constraint}{Constraint function applied to the
\code{recurrent_kernel} weights matrix.}

\item{bias_constraint}{Constraint function applied to the bias vector.}

\item{dropout}{Float between 0 and 1. Fraction of the units to drop for the
linear transformation of the inputs.}

\item{recurrent_dropout}{Float between 0 and 1. Fraction of the units to drop
for the linear transformation of the recurrent state.}
}
\description{
For a step-by-step description of the algorithm, see \href{http://deeplearning.net/tutorial/lstm.html}{thistutorial}.
}
\section{References}{

\itemize{
\item \href{http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf}{Long short-term memory} (original 1997 paper)
\item \href{http://www.cs.toronto.edu/~graves/preprint.pdf}{Supervised sequence labeling with recurrent neural networks}
\item \href{http://arxiv.org/abs/1512.05287}{A Theoretically Grounded Application of Dropout in Recurrent Neural Networks}
}
}

