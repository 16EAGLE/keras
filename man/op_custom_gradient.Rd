% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ops.R
\name{op_custom_gradient}
\alias{op_custom_gradient}
\title{Decorator to define a function with a custom gradient.}
\usage{
op_custom_gradient(f)
}
\arguments{
\item{f}{Function \code{f(...)} that returns a tuple \verb{(y, grad_fn)} where:
\itemize{
\item \code{x} is a sequence of (nested structures of) tensor inputs to the
function.
\item \code{y} is a (nested structure of) tensor outputs of applying
operations in \code{f} to \code{x}.
\item \code{grad_fn} is a function with the signature \code{g(...)} which
returns a list of tensors the same size as (flattened) \code{x}: the
derivatives of tensors in \code{y} with respect to the tensors in
\code{x}. Arguments provided to \code{...} are sequence of tensors the same size as
(flattened) \code{y} holding the initial value gradients for each
tensor in \code{y}.
}}
}
\value{
A function \code{h(x)} which returns the same value as \code{f(x)[0]} and whose
gradient is determined by \code{f(x)[1]}.
}
\description{
This decorator allows fine grained control over the gradients of a sequence
for operations. This may be useful for multiple reasons, including providing
a more efficient or numerically stable gradient for a sequence of
operations.

Note that \code{custom_gradient} only supports TensorFlow and JAX backends.
}
\section{Examples}{
\if{html}{\out{<div class="sourceCode r">}}\preformatted{log1pexp <- op_custom_gradient(\\(x) \{

    e <- op_exp(x)

    grad <- function(upstream) \{
      op_multiply(upstream, 1.0 - 1.0 / op_add(1, e))
    \}

    tuple(op_log(1 + e), grad)
\})
}\if{html}{\out{</div>}}
}

\seealso{
\itemize{
\item \url{https://www.tensorflow.org/api_docs/python/tf/keras/ops/custom_gradient}
}
}
