% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/autogen-layers-activations.R
\name{layer_activation_softmax}
\alias{layer_activation_softmax}
\title{Softmax activation layer.}
\usage{
layer_activation_softmax(object, axis = -1L, ...)
}
\arguments{
\item{axis}{Integer, or list of Integers, axis along which the softmax
normalization is applied.}

\item{...}{Base layer keyword arguments, such as \code{name} and \code{dtype}.}
}
\description{
Formula:

\if{html}{\out{<div class="sourceCode python">}}\preformatted{exp_x = exp(x - max(x))
f(x) = exp_x / sum(exp_x)
}\if{html}{\out{</div>}}
}
\section{Examples}{
\if{html}{\out{<div class="sourceCode python">}}\preformatted{>>>softmax_layer = keras.layers.activations.Softmax()
# >>>input = np.array([1.0, 2.0, 1.0])
# >>>result = softmax_layer(input)
# [0.21194157, 0.5761169, 0.21194157]
}\if{html}{\out{</div>}}
}

\section{Call Arguments}{
inputs: The inputs (logits) to the softmax layer.
mask: A boolean mask of the same shape as \code{inputs}. The mask
specifies 1 to keep and 0 to mask. Defaults to \code{None}.
}

\section{Returns}{
\if{html}{\out{<div class="sourceCode">}}\preformatted{Softmaxed output with the same shape as `inputs`.
}\if{html}{\out{</div>}}
}

\seealso{
\itemize{
\item \url{https://www.tensorflow.org/api_docs/python/tf/keras/layers/Softmax}
}

Other activations layers: 
\code{\link{layer_activation_elu}()},
\code{\link{layer_activation_leaky_relu}()},
\code{\link{layer_activation_parametric_relu}()},
\code{\link{layer_activation_relu}()},
\code{\link{layer_activation}()}
}
\concept{activations layers}
