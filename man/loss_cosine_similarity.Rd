% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/autogen-losses.R
\name{loss_cosine_similarity}
\alias{loss_cosine_similarity}
\title{Computes the cosine similarity between \code{y_true} & \code{y_pred}.}
\usage{
loss_cosine_similarity(
  y_true,
  y_pred,
  axis = -1L,
  ...,
  reduction = "auto",
  name = "cosine_similarity"
)
}
\arguments{
\item{y_true}{Tensor of true targets.}

\item{y_pred}{Tensor of predicted targets.}

\item{axis}{Axis along which to determine similarity. Defaults to \code{-1}.}

\item{...}{Passed on to the Python callable}

\item{reduction}{Type of reduction to apply to the loss. In almost all cases
this should be \code{"sum_over_batch_size"}.
Supported options are \code{"sum"}, \code{"sum_over_batch_size"} or \code{None}.}

\item{name}{Optional name for the loss instance.}
}
\description{
Formula:

\if{html}{\out{<div class="sourceCode python">}}\preformatted{loss = -sum(l2_norm(y_true) * l2_norm(y_pred))
}\if{html}{\out{</div>}}

Note that it is a number between -1 and 1. When it is a negative number
between -1 and 0, 0 indicates orthogonality and values closer to -1
indicate greater similarity. This makes it usable as a loss function in a
setting where you try to maximize the proximity between predictions and
targets. If either \code{y_true} or \code{y_pred} is a zero vector, cosine
similarity will be 0 regardless of the proximity between predictions
and targets.
Note that it is a number between -1 and 1. When it is a negative number
between -1 and 0, 0 indicates orthogonality and values closer to -1
indicate greater similarity. This makes it usable as a loss function in a
setting where you try to maximize the proximity between predictions and
targets. If either \code{y_true} or \code{y_pred} is a zero vector, cosine similarity
will be 0 regardless of the proximity between predictions and targets.

Formula:

\if{html}{\out{<div class="sourceCode python">}}\preformatted{loss = -sum(l2_norm(y_true) * l2_norm(y_pred))
}\if{html}{\out{</div>}}
}
\section{Returns}{
Cosine similarity tensor.
}

\section{Examples}{
\if{html}{\out{<div class="sourceCode python">}}\preformatted{y_true = [[0., 1.], [1., 1.], [1., 1.]]
y_pred = [[1., 0.], [1., 1.], [-1., -1.]]
loss = keras.losses.cosine_similarity(y_true, y_pred, axis=-1)
# [-0., -0.99999994, 0.99999994]
}\if{html}{\out{</div>}}
}

\seealso{
\itemize{
\item \url{https://www.tensorflow.org/api_docs/python/tf/keras/losses/CosineSimilarity}
}

Other loss: 
\code{\link{loss_binary_crossentropy}()},
\code{\link{loss_binary_focal_crossentropy}()},
\code{\link{loss_categorical_crossentropy}()},
\code{\link{loss_categorical_focal_crossentropy}()},
\code{\link{loss_categorical_hinge}()},
\code{\link{loss_hinge}()},
\code{\link{loss_huber}()},
\code{\link{loss_kl_divergence}()},
\code{\link{loss_logcosh}()},
\code{\link{loss_mean_absolute_error}()},
\code{\link{loss_mean_absolute_percentage_error}()},
\code{\link{loss_mean_squared_error}()},
\code{\link{loss_mean_squared_logarithmic_error}()},
\code{\link{loss_poisson}()},
\code{\link{loss_sparse_categorical_crossentropy}()},
\code{\link{loss_squared_hinge}()}
}
\concept{loss}
