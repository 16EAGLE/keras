% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/autogen-activations.R
\name{activation_hard_sigmoid}
\alias{activation_hard_sigmoid}
\title{Hard sigmoid activation function.}
\usage{
activation_hard_sigmoid(x)
}
\arguments{
\item{x}{Input tensor.}
}
\description{
The hard sigmoid activation is defined as:
\itemize{
\item \code{0} if \verb{if x < -2.5}
\item \code{1} if \code{x > 2.5}
\item \code{0.2 * x + 0.5} if \verb{-2.5 <= x <= 2.5}
}

It's a faster, piecewise linear approximation
of the sigmoid activation.
}
\section{Reference}{
\itemize{
\item \href{https://en.wikipedia.org/wiki/Hard_sigmoid}{Wikipedia "Hard sigmoid"}
}
}

\seealso{
\itemize{
\item \url{https://www.tensorflow.org/api_docs/python/tf/keras/activations/hard_sigmoid}
}

Other activation functions: 
\code{\link{activation_elu}()},
\code{\link{activation_exponential}()},
\code{\link{activation_gelu}()},
\code{\link{activation_leaky_relu}()},
\code{\link{activation_linear}()},
\code{\link{activation_log_softmax}()},
\code{\link{activation_mish}()},
\code{\link{activation_relu6}()},
\code{\link{activation_relu}()},
\code{\link{activation_selu}()},
\code{\link{activation_sigmoid}()},
\code{\link{activation_silu}()},
\code{\link{activation_softmax}()},
\code{\link{activation_softplus}()},
\code{\link{activation_softsign}()},
\code{\link{activation_tanh}()}
}
\concept{activation functions}
