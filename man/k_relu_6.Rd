% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ops.R
\name{k_relu_6}
\alias{k_relu_6}
\title{Rectified linear unit activation function with upper bound of 6.}
\usage{
k_relu_6(x)
}
\arguments{
\item{x}{Input tensor.}
}
\description{
It is defined as \code{f(x) = np.clip(x, 0, 6)}.
}
\section{Returns}{
A tensor with the same shape as \code{x}.
}

\section{Examples}{
\if{html}{\out{<div class="sourceCode python">}}\preformatted{x = keras_core.ops.convert_to_tensor([-3.0, -2.0, 0.1, 0.2, 6.0, 8.0])
keras_core.ops.relu6(x)
# array([0.0, 0.0, 0.1, 0.2, 6.0, 6.0], dtype=float32)
}\if{html}{\out{</div>}}
}

\seealso{
\itemize{
\item \url{https://www.tensorflow.org/api_docs/python/tf/keras/ops/relu6}
}
}
