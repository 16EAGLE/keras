% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/autogen-losses.R
\name{loss_categorical_focal_crossentropy}
\alias{loss_categorical_focal_crossentropy}
\title{Computes the alpha balanced focal crossentropy loss.}
\usage{
loss_categorical_focal_crossentropy(
  y_true,
  y_pred,
  alpha = 0.25,
  gamma = 2,
  from_logits = FALSE,
  label_smoothing = 0,
  axis = -1L,
  ...,
  reduction = "sum_over_batch_size",
  name = "categorical_focal_crossentropy"
)
}
\arguments{
\item{y_true}{Tensor of one-hot true targets.}

\item{y_pred}{Tensor of predicted targets.}

\item{alpha}{A weight balancing factor for all classes, default is \code{0.25} as
mentioned in the reference. It can be a list of floats or a scalar.
In the multi-class case, alpha may be set by inverse class
frequency by using \code{compute_class_weight} from \code{sklearn.utils}.}

\item{gamma}{A focusing parameter, default is \code{2.0} as mentioned in the
reference. It helps to gradually reduce the importance given to
simple examples in a smooth manner. When \code{gamma} = 0, there is
no focal effect on the categorical crossentropy.}

\item{from_logits}{Whether \code{output} is expected to be a logits tensor. By
default, we consider that \code{output} encodes a probability
distribution.}

\item{label_smoothing}{Float in \verb{[0, 1].} When > 0, label values are smoothed,
meaning the confidence on label values are relaxed. For example, if
\code{0.1}, use \code{0.1 / num_classes} for non-target labels and
\code{0.9 + 0.1 / num_classes} for target labels.}

\item{axis}{The axis along which to compute crossentropy (the features
axis). Defaults to \code{-1}.}

\item{...}{Passed on to the Python callable}

\item{reduction}{Type of reduction to apply to the loss. In almost all cases
this should be \code{"sum_over_batch_size"}.
Supported options are \code{"sum"}, \code{"sum_over_batch_size"} or \code{None}.}

\item{name}{Optional name for the loss instance.}
}
\description{
Use this crossentropy loss function when there are two or more label
classes and if you want to handle class imbalance without using
\code{class_weights}. We expect labels to be provided in a \code{one_hot}
representation.

According to \href{https://arxiv.org/pdf/1708.02002.pdf}{Lin et al., 2018}, it
helps to apply a focal factor to down-weight easy examples and focus more on
hard examples. The general formula for the focal loss (FL)
is as follows:

\code{FL(p_t) = (1 - p_t) ** gamma * log(p_t)}

where \code{p_t} is defined as follows:
\verb{p_t = output if y_true == 1, else 1 - output}

\code{(1 - p_t) ** gamma} is the \code{modulating_factor}, where \code{gamma} is a focusing
parameter. When \code{gamma} = 0, there is no focal effect on the cross entropy.
\code{gamma} reduces the importance given to simple examples in a smooth manner.

The authors use alpha-balanced variant of focal loss (FL) in the paper:
\code{FL(p_t) = -alpha * (1 - p_t) ** gamma * log(p_t)}

where \code{alpha} is the weight factor for the classes. If \code{alpha} = 1, the
loss won't be able to handle class imbalance properly as all
classes will have the same weight. This can be a constant or a list of
constants. If alpha is a list, it must have the same length as the number
of classes.

The formula above can be generalized to:
\code{FL(p_t) = alpha * (1 - p_t) ** gamma * CrossEntropy(y_true, y_pred)}

where minus comes from \code{CrossEntropy(y_true, y_pred)} (CE).

Extending this to multi-class case is straightforward:
\code{FL(p_t) = alpha * (1 - p_t) ** gamma * CategoricalCE(y_true, y_pred)}

In the snippet below, there is \code{num_classes} floating pointing values per
example. The shape of both \code{y_pred} and \code{y_true} are
\verb{(batch_size, num_classes)}.
}
\section{Examples}{
\if{html}{\out{<div class="sourceCode python">}}\preformatted{y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.9, 0.05], [0.1, 0.85, 0.05]]
loss = keras.losses.categorical_focal_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss
# array([2.63401289e-04, 6.75912094e-01], dtype=float32)
}\if{html}{\out{</div>}}

Standalone usage:

\if{html}{\out{<div class="sourceCode python">}}\preformatted{y_true = [[0., 1., 0.], [0., 0., 1.]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
# Using 'auto'/'sum_over_batch_size' reduction type.
cce = keras.losses.CategoricalFocalCrossentropy()
cce(y_true, y_pred)
# 0.23315276
}\if{html}{\out{</div>}}

\if{html}{\out{<div class="sourceCode python">}}\preformatted{# Calling with 'sample_weight'.
cce(y_true, y_pred, sample_weight=np.array([0.3, 0.7]))
# 0.1632
}\if{html}{\out{</div>}}

\if{html}{\out{<div class="sourceCode python">}}\preformatted{# Using 'sum' reduction type.
cce = keras.losses.CategoricalFocalCrossentropy(
    reduction="sum")
cce(y_true, y_pred)
# 0.46631
}\if{html}{\out{</div>}}

\if{html}{\out{<div class="sourceCode python">}}\preformatted{# Using 'none' reduction type.
cce = keras.losses.CategoricalFocalCrossentropy(
    reduction=None)
cce(y_true, y_pred)
# array([3.2058331e-05, 4.6627346e-01], dtype=float32)
}\if{html}{\out{</div>}}

Usage with the \code{compile()} API:

\if{html}{\out{<div class="sourceCode python">}}\preformatted{model.compile(optimizer='adam',
              loss=keras.losses.CategoricalFocalCrossentropy())
}\if{html}{\out{</div>}}
}

\section{Returns}{
Categorical focal crossentropy loss value.
}

\seealso{
\itemize{
\item \url{https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalFocalCrossentropy}
}

Other loss: 
\code{\link{loss_binary_crossentropy}()},
\code{\link{loss_binary_focal_crossentropy}()},
\code{\link{loss_categorical_crossentropy}()},
\code{\link{loss_categorical_hinge}()},
\code{\link{loss_cosine_similarity}()},
\code{\link{loss_hinge}()},
\code{\link{loss_huber}()},
\code{\link{loss_kl_divergence}()},
\code{\link{loss_log_cosh}()},
\code{\link{loss_mean_absolute_error}()},
\code{\link{loss_mean_absolute_percentage_error}()},
\code{\link{loss_mean_squared_error}()},
\code{\link{loss_mean_squared_logarithmic_error}()},
\code{\link{loss_poisson}()},
\code{\link{loss_sparse_categorical_crossentropy}()},
\code{\link{loss_squared_hinge}()}
}
\concept{loss}
