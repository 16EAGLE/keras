% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ops.R
\name{k_gelu}
\alias{k_gelu}
\title{Gaussian Error Linear Unit (GELU) activation function.}
\usage{
k_gelu(x, approximate = TRUE)
}
\arguments{
\item{x}{Input tensor.}

\item{approximate}{Approximate version of GELU activation. Defaults to \code{True}.}
}
\description{
If \code{approximate} is \code{True}, it is defined as:
\code{f(x) = 0.5 * x * (1 + tanh(sqrt(2 / pi) * (x + 0.044715 * x^3)))}

Or if \code{approximate} is \code{False}, it is defined as:
\code{f(x) = x * P(X <= x) = 0.5 * x * (1 + erf(x / sqrt(2)))},
where \code{P(X) ~ N(0, 1)}.
}
\section{Returns}{
A tensor with the same shape as \code{x}.
}

\section{Examples}{
\if{html}{\out{<div class="sourceCode python">}}\preformatted{x = np.array([-1., 0., 1.])
x_gelu = keras_core.ops.gelu(x)
print(x_gelu)
# array([-0.15865525, 0., 0.84134475], shape=(3,), dtype=float64)
}\if{html}{\out{</div>}}
}

\seealso{
\itemize{
\item \url{https://www.tensorflow.org/api_docs/python/tf/keras/ops/gelu}
}
}
