% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimizers.R
\name{optimizer_adagrad}
\alias{optimizer_adagrad}
\title{Adagrad optimizer.}
\usage{
optimizer_adagrad(lr = 0.01, epsilon = 1e-08, decay = 0,
  clipnorm = NULL, clipvalue = NULL)
}
\arguments{
\item{lr}{Learning rate.}

\item{epsilon}{Fuzz factor.}

\item{decay}{Learning rate decay over each update.}

\item{clipnorm}{Gradients will be clipped when their L2 norm exceeds this
value.}

\item{clipvalue}{Gradients will be clipped when their absolute value exceeds
this value.}
}
\description{
Adagrad optimizer as described in
\href{http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf}{Adaptive 
Subgradient Methods for Online Learning and Stochastic Optimization}.
}
\note{
It is recommended to leave the parameters of this optimizer at their 
  default values.
}

