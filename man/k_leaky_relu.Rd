% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ops.R
\name{k_leaky_relu}
\alias{k_leaky_relu}
\title{Leaky version of a Rectified Linear Unit activation function.}
\usage{
k_leaky_relu(x, negative_slope = 0.2)
}
\arguments{
\item{x}{Input tensor.}

\item{negative_slope}{Slope of the activation function at x < 0.
Defaults to \code{0.2}.}
}
\description{
It allows a small gradient when the unit is not active, it is defined as:

\verb{f(x) = alpha * x for x < 0} or \verb{f(x) = x for x >= 0}.
}
\section{Returns}{
A tensor with the same shape as \code{x}.
}

\section{Examples}{
\if{html}{\out{<div class="sourceCode python">}}\preformatted{x = np.array([-1., 0., 1.])
x_leaky_relu = keras_core.ops.leaky_relu(x)
print(x_leaky_relu)
# array([-0.2,  0. ,  1. ], shape=(3,), dtype=float64)
}\if{html}{\out{</div>}}
}

\seealso{
\itemize{
\item \url{https://www.tensorflow.org/api_docs/python/tf/keras/ops/leaky_relu}
}
}
