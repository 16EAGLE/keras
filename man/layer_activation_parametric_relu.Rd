% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layers-activations.R
\name{layer_activation_parametric_relu}
\alias{layer_activation_parametric_relu}
\title{Parametric Rectified Linear Unit activation layer.}
\usage{
layer_activation_parametric_relu(
  object,
  alpha_initializer = "Zeros",
  alpha_regularizer = NULL,
  alpha_constraint = NULL,
  shared_axes = NULL,
  ...
)
}
\arguments{
\item{alpha_initializer}{Initializer function for the weights.}

\item{alpha_regularizer}{Regularizer for the weights.}

\item{alpha_constraint}{Constraint for the weights.}

\item{shared_axes}{The axes along which to share learnable parameters for the
activation function. For example, if the incoming feature maps are
from a 2D convolution with output shape
\verb{(batch, height, width, channels)}, and you wish to share parameters
across space so that each filter only has one set of parameters,
set \verb{shared_axes=[1, 2]}.}

\item{...}{Base layer keyword arguments, such as \code{name} and \code{dtype}.}
}
\description{
Formula:

\if{html}{\out{<div class="sourceCode python">}}\preformatted{f(x) = alpha * x for x < 0
f(x) = x for x >= 0
}\if{html}{\out{</div>}}

where \code{alpha} is a learned array with the same shape as x.
}
\seealso{
\itemize{
\item \url{https://www.tensorflow.org/api_docs/python/tf/keras/layers/PReLU}
}

Other activations layers: 
\code{\link{layer_activation_elu}()},
\code{\link{layer_activation_leaky_relu}()},
\code{\link{layer_activation_relu}()},
\code{\link{layer_activation_softmax}()},
\code{\link{layer_activation}()}
}
\concept{activations layers}
