% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/activations.R, R/gen-activations.R
\name{activation_silu}
\alias{activation_silu}
\title{Swish (or Silu) activation function.}
\usage{
activation_silu(x)

activation_silu(x)
}
\arguments{
\item{x}{Input tensor.}
}
\description{
It is defined as: \code{swish(x) = x * sigmoid(x)}.

The Swish (or Silu) activation function is a smooth,
non-monotonic function that is unbounded above and
bounded below.

It is defined as: \code{swish(x) = x * sigmoid(x)}.

The Swish (or Silu) activation function is a smooth,
non-monotonic function that is unbounded above and
bounded below.
}
\section{Reference}{
\itemize{
\item \href{https://arxiv.org/abs/1710.05941}{Ramachandran et al., 2017}
}

\itemize{
\item \href{https://arxiv.org/abs/1710.05941}{Ramachandran et al., 2017}
}
}

\seealso{
\itemize{
\item \url{https://www.tensorflow.org/api_docs/python/tf/keras/activations/silu}
}

\itemize{
\item \url{https://www.tensorflow.org/api_docs/python/tf/keras/activations/silu}
}

Other activation functions: 
\code{\link{activation_elu}()},
\code{\link{activation_exponential}()},
\code{\link{activation_gelu}()},
\code{\link{activation_hard_sigmoid}()},
\code{\link{activation_leaky_relu}()},
\code{\link{activation_linear}()},
\code{\link{activation_log_softmax}()},
\code{\link{activation_mish}()},
\code{\link{activation_relu_6}()},
\code{\link{activation_relu}()},
\code{\link{activation_selu}()},
\code{\link{activation_sigmoid}()},
\code{\link{activation_softmax}()},
\code{\link{activation_softplus}()},
\code{\link{activation_softsign}()},
\code{\link{activation_tanh}()}

Other activation functions: 
\code{\link{activation_elu}()},
\code{\link{activation_exponential}()},
\code{\link{activation_gelu}()},
\code{\link{activation_hard_sigmoid}()},
\code{\link{activation_leaky_relu}()},
\code{\link{activation_linear}()},
\code{\link{activation_log_softmax}()},
\code{\link{activation_mish}()},
\code{\link{activation_relu_6}()},
\code{\link{activation_relu}()},
\code{\link{activation_selu}()},
\code{\link{activation_sigmoid}()},
\code{\link{activation_softmax}()},
\code{\link{activation_softplus}()},
\code{\link{activation_softsign}()},
\code{\link{activation_tanh}()}
}
\concept{activation functions}
