% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/activations.R
\name{activation_softsign}
\alias{activation_softsign}
\title{Softsign activation function.}
\usage{
activation_softsign(x)
}
\arguments{
\item{x}{Input tensor.}
}
\description{
Softsign is defined as: \code{softsign(x) = x / (abs(x) + 1)}.
}
\seealso{
\itemize{
\item \url{https://keras.io/api/layers/activations#softsign-function}
}

Other activations: 
\code{\link{activation_elu}()},
\code{\link{activation_exponential}()},
\code{\link{activation_gelu}()},
\code{\link{activation_hard_sigmoid}()},
\code{\link{activation_leaky_relu}()},
\code{\link{activation_linear}()},
\code{\link{activation_log_softmax}()},
\code{\link{activation_mish}()},
\code{\link{activation_relu}()},
\code{\link{activation_relu6}()},
\code{\link{activation_selu}()},
\code{\link{activation_sigmoid}()},
\code{\link{activation_silu}()},
\code{\link{activation_softmax}()},
\code{\link{activation_softplus}()},
\code{\link{activation_tanh}()}
}
\concept{activations}
