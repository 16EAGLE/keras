% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layers-convolutional.R
\name{layer_separable_conv_2d}
\alias{layer_separable_conv_2d}
\title{2D separable convolution layer.}
\usage{
layer_separable_conv_2d(
  object,
  filters,
  kernel_size,
  strides = list(1L, 1L),
  padding = "valid",
  data_format = NULL,
  dilation_rate = list(1L, 1L),
  depth_multiplier = 1L,
  activation = NULL,
  use_bias = TRUE,
  depthwise_initializer = "glorot_uniform",
  pointwise_initializer = "glorot_uniform",
  bias_initializer = "zeros",
  depthwise_regularizer = NULL,
  pointwise_regularizer = NULL,
  bias_regularizer = NULL,
  activity_regularizer = NULL,
  depthwise_constraint = NULL,
  pointwise_constraint = NULL,
  bias_constraint = NULL,
  ...
)
}
\arguments{
\item{object}{Object to compose the layer with. A tensor, array, or sequential model.}

\item{filters}{int, the dimensionality of the output space (i.e. the number
of filters in the pointwise convolution).}

\item{kernel_size}{int or list of 2 integers, specifying the size of the
depthwise convolution window.}

\item{strides}{int or list of 2 integers, specifying the stride length
of the depthwise convolution. If only one int is specified, the same
stride size will be used for all dimensions. \code{strides > 1} is
incompatible with \code{dilation_rate > 1}.}

\item{padding}{string, either \code{"valid"} or \code{"same"} (case-insensitive).
\code{"valid"} means no padding. \code{"same"} results in padding evenly to
the left/right or up/down of the input. When \code{padding="same"} and
\code{strides=1}, the output has the same size as the input.}

\item{data_format}{string, either \code{"channels_last"} or \code{"channels_first"}.
The ordering of the dimensions in the inputs. \code{"channels_last"}
corresponds to inputs with shape \verb{(batch, height, width, channels)}
while \code{"channels_first"} corresponds to inputs with shape
\verb{(batch, channels, height, width)}. It defaults to the
\code{image_data_format} value found in your Keras config file
at \verb{~/.keras/keras.json}.
If you never set it, then it will be \code{"channels_last"}.}

\item{dilation_rate}{int or list of 2 integers, specifying the dilation
rate to use for dilated convolution. If only one int is specified,
the same dilation rate will be used for all dimensions.}

\item{depth_multiplier}{The number of depthwise convolution output channels
for each input channel. The total number of depthwise convolution
output channels will be equal to \code{input_channel * depth_multiplier}.}

\item{activation}{Activation function. If \code{NULL}, no activation is applied.}

\item{use_bias}{bool, if \code{TRUE}, bias will be added to the output.}

\item{depthwise_initializer}{An initializer for the depthwise convolution
kernel. If NULL, then the default initializer (\code{"glorot_uniform"})
will be used.}

\item{pointwise_initializer}{An initializer for the pointwise convolution
kernel. If NULL, then the default initializer (\code{"glorot_uniform"})
will be used.}

\item{bias_initializer}{An initializer for the bias vector. If NULL, the
default initializer ('"zeros"') will be used.}

\item{depthwise_regularizer}{Optional regularizer for the depthwise
convolution kernel.}

\item{pointwise_regularizer}{Optional regularizer for the pointwise
convolution kernel.}

\item{bias_regularizer}{Optional regularizer for the bias vector.}

\item{activity_regularizer}{Optional regularizer function for the output.}

\item{depthwise_constraint}{Optional projection function to be applied to the
depthwise kernel after being updated by an \code{Optimizer} (e.g. used
for norm constraints or value constraints for layer weights). The
function must take as input the unprojected variable and must return
the projected variable (which must have the same shape).}

\item{pointwise_constraint}{Optional projection function to be applied to the
pointwise kernel after being updated by an \code{Optimizer}.}

\item{bias_constraint}{Optional projection function to be applied to the
bias after being updated by an \code{Optimizer}.}

\item{...}{For forward/backward compatability.}
}
\value{
A 4D tensor representing
\code{activation(separable_conv2d(inputs, kernel) + bias)}.
}
\description{
This layer performs a depthwise convolution that acts separately on
channels, followed by a pointwise convolution that mixes channels.
If \code{use_bias} is TRUE and a bias initializer is provided,
it adds a bias vector to the output. It then optionally applies an
activation function to produce the final output.
}
\section{Input Shape}{
\itemize{
\item If \code{data_format="channels_last"}:
A 4D tensor with shape: \verb{(batch_size, height, width, channels)}
\item If \code{data_format="channels_first"}:
A 4D tensor with shape: \verb{(batch_size, channels, height, width)}
}
}

\section{Output Shape}{
\itemize{
\item If \code{data_format="channels_last"}:
A 4D tensor with shape: \verb{(batch_size, new_height, new_width, filters)}
\item If \code{data_format="channels_first"}:
A 4D tensor with shape: \verb{(batch_size, filters, new_height, new_width)}
}
}

\section{Examples}{
\if{html}{\out{<div class="sourceCode r">}}\preformatted{x <- random_uniform(c(4, 10, 10, 12))
y <- layer_separable_conv_2d(x, 3, c(4, 3), 2, activation='relu')
shape(y)
}\if{html}{\out{</div>}}

\if{html}{\out{<div class="sourceCode">}}\preformatted{## shape(4, 4, 4, 3)

}\if{html}{\out{</div>}}
}

\seealso{
\itemize{
\item \url{https://keras.io/api/layers/convolution_layers/separable_convolution2d#separableconv2d-class}
}

Other convolutional layers: 
\code{\link{layer_conv_1d}()},
\code{\link{layer_conv_1d_transpose}()},
\code{\link{layer_conv_2d}()},
\code{\link{layer_conv_2d_transpose}()},
\code{\link{layer_conv_3d}()},
\code{\link{layer_conv_3d_transpose}()},
\code{\link{layer_depthwise_conv_1d}()},
\code{\link{layer_depthwise_conv_2d}()},
\code{\link{layer_separable_conv_1d}()}

Other layers: 
\code{\link{Layer}()},
\code{\link{layer_activation}()},
\code{\link{layer_activation_elu}()},
\code{\link{layer_activation_leaky_relu}()},
\code{\link{layer_activation_parametric_relu}()},
\code{\link{layer_activation_relu}()},
\code{\link{layer_activation_softmax}()},
\code{\link{layer_activity_regularization}()},
\code{\link{layer_add}()},
\code{\link{layer_additive_attention}()},
\code{\link{layer_alpha_dropout}()},
\code{\link{layer_attention}()},
\code{\link{layer_average}()},
\code{\link{layer_average_pooling_1d}()},
\code{\link{layer_average_pooling_2d}()},
\code{\link{layer_average_pooling_3d}()},
\code{\link{layer_batch_normalization}()},
\code{\link{layer_bidirectional}()},
\code{\link{layer_category_encoding}()},
\code{\link{layer_center_crop}()},
\code{\link{layer_concatenate}()},
\code{\link{layer_conv_1d}()},
\code{\link{layer_conv_1d_transpose}()},
\code{\link{layer_conv_2d}()},
\code{\link{layer_conv_2d_transpose}()},
\code{\link{layer_conv_3d}()},
\code{\link{layer_conv_3d_transpose}()},
\code{\link{layer_conv_lstm_1d}()},
\code{\link{layer_conv_lstm_2d}()},
\code{\link{layer_conv_lstm_3d}()},
\code{\link{layer_cropping_1d}()},
\code{\link{layer_cropping_2d}()},
\code{\link{layer_cropping_3d}()},
\code{\link{layer_dense}()},
\code{\link{layer_depthwise_conv_1d}()},
\code{\link{layer_depthwise_conv_2d}()},
\code{\link{layer_discretization}()},
\code{\link{layer_dot}()},
\code{\link{layer_dropout}()},
\code{\link{layer_einsum_dense}()},
\code{\link{layer_embedding}()},
\code{\link{layer_feature_space}()},
\code{\link{layer_flatten}()},
\code{\link{layer_gaussian_dropout}()},
\code{\link{layer_gaussian_noise}()},
\code{\link{layer_global_average_pooling_1d}()},
\code{\link{layer_global_average_pooling_2d}()},
\code{\link{layer_global_average_pooling_3d}()},
\code{\link{layer_global_max_pooling_1d}()},
\code{\link{layer_global_max_pooling_2d}()},
\code{\link{layer_global_max_pooling_3d}()},
\code{\link{layer_group_normalization}()},
\code{\link{layer_group_query_attention}()},
\code{\link{layer_gru}()},
\code{\link{layer_hashed_crossing}()},
\code{\link{layer_hashing}()},
\code{\link{layer_identity}()},
\code{\link{layer_integer_lookup}()},
\code{\link{layer_lambda}()},
\code{\link{layer_layer_normalization}()},
\code{\link{layer_lstm}()},
\code{\link{layer_masking}()},
\code{\link{layer_max_pooling_1d}()},
\code{\link{layer_max_pooling_2d}()},
\code{\link{layer_max_pooling_3d}()},
\code{\link{layer_maximum}()},
\code{\link{layer_minimum}()},
\code{\link{layer_multi_head_attention}()},
\code{\link{layer_multiply}()},
\code{\link{layer_normalization}()},
\code{\link{layer_permute}()},
\code{\link{layer_random_brightness}()},
\code{\link{layer_random_contrast}()},
\code{\link{layer_random_crop}()},
\code{\link{layer_random_flip}()},
\code{\link{layer_random_rotation}()},
\code{\link{layer_random_translation}()},
\code{\link{layer_random_zoom}()},
\code{\link{layer_repeat_vector}()},
\code{\link{layer_rescaling}()},
\code{\link{layer_reshape}()},
\code{\link{layer_resizing}()},
\code{\link{layer_rnn}()},
\code{\link{layer_separable_conv_1d}()},
\code{\link{layer_simple_rnn}()},
\code{\link{layer_spatial_dropout_1d}()},
\code{\link{layer_spatial_dropout_2d}()},
\code{\link{layer_spatial_dropout_3d}()},
\code{\link{layer_spectral_normalization}()},
\code{\link{layer_string_lookup}()},
\code{\link{layer_subtract}()},
\code{\link{layer_text_vectorization}()},
\code{\link{layer_tfsm}()},
\code{\link{layer_time_distributed}()},
\code{\link{layer_torch_module_wrapper}()},
\code{\link{layer_unit_normalization}()},
\code{\link{layer_upsampling_1d}()},
\code{\link{layer_upsampling_2d}()},
\code{\link{layer_upsampling_3d}()},
\code{\link{layer_zero_padding_1d}()},
\code{\link{layer_zero_padding_2d}()},
\code{\link{layer_zero_padding_3d}()},
\code{\link{rnn_cell_gru}()},
\code{\link{rnn_cell_lstm}()},
\code{\link{rnn_cell_simple}()},
\code{\link{rnn_cells_stack}()}
}
\concept{convolutional layers}
\concept{layers}
