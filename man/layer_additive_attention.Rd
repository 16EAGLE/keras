% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/autogen-layers-attention.R
\name{layer_additive_attention}
\alias{layer_additive_attention}
\title{Additive attention layer, a.k.a. Bahdanau-style attention.}
\usage{
layer_additive_attention(object, use_scale = TRUE, dropout = 0, ...)
}
\arguments{
\item{object}{Object to compose the layer with. A tensor, array, or sequential model.}

\item{use_scale}{If \code{True}, will create a scalar variable to scale the
attention scores.}

\item{dropout}{Float between 0 and 1. Fraction of the units to drop for the
attention scores. Defaults to \code{0.0}.}

\item{...}{Passed on to the Python callable}
}
\description{
Inputs are a list with 2 or 3 elements:
\enumerate{
\item A \code{query} tensor of shape \verb{(batch_size, Tq, dim)}.
\item A \code{value} tensor of shape \verb{(batch_size, Tv, dim)}.
\item A optional \code{key} tensor of shape \verb{(batch_size, Tv, dim)}. If none
supplied, \code{value} will be used as \code{key}.
}

The calculation follows the steps:
\enumerate{
\item Calculate attention scores using \code{query} and \code{key} with shape
\verb{(batch_size, Tq, Tv)} as a non-linear sum
\code{scores = reduce_sum(tanh(query + key), axis=-1)}.
\item Use scores to calculate a softmax distribution with shape
\verb{(batch_size, Tq, Tv)}.
\item Use the softmax distribution to create a linear combination of \code{value}
with shape \verb{(batch_size, Tq, dim)}.
}
}
\section{Call Arguments}{
\itemize{
\item \code{inputs}: List of the following tensors:
\itemize{
\item \code{query}: Query tensor of shape \verb{(batch_size, Tq, dim)}.
\item \code{value}: Value tensor of shape \verb{(batch_size, Tv, dim)}.
\item \code{key}: Optional key tensor of shape \verb{(batch_size, Tv, dim)}. If
not given, will use \code{value} for both \code{key} and \code{value}, which is
the most common case.
}
\item \code{mask}: List of the following tensors:
\itemize{
\item \code{query_mask}: A boolean mask tensor of shape \verb{(batch_size, Tq)}.
If given, the output will be zero at the positions where
\code{mask==False}.
\item \code{value_mask}: A boolean mask tensor of shape \verb{(batch_size, Tv)}.
If given, will apply the mask such that values at positions
where \code{mask==False} do not contribute to the result.
}
\item \code{return_attention_scores}: bool, it \code{True}, returns the attention scores
(after masking and softmax) as an additional output argument.
\item \code{training}: Python boolean indicating whether the layer should behave in
training mode (adding dropout) or in inference mode (no dropout).
\item \code{use_causal_mask}: Boolean. Set to \code{True} for decoder self-attention. Adds
a mask such that position \code{i} cannot attend to positions \code{j > i}.
This prevents the flow of information from the future towards the
past. Defaults to \code{False}.
}
}

\section{Output}{
Attention outputs of shape \verb{(batch_size, Tq, dim)}.
(Optional) Attention scores after masking and softmax with shape
\verb{(batch_size, Tq, Tv)}.
}

\seealso{
\itemize{
\item \url{https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention}
}

Other attention layers: 
\code{\link{layer_attention}()},
\code{\link{layer_multi_head_attention}()}
}
\concept{attention layers}
