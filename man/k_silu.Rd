% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ops.R
\name{k_silu}
\alias{k_silu}
\title{Sigmoid Linear Unit (SiLU) activation function, also known as Swish.}
\usage{
k_silu(x)
}
\arguments{
\item{x}{Input tensor.}
}
\description{
The SiLU activation function is computed by the sigmoid function multiplied
by its input. It is defined as \code{f(x) = x * sigmoid(x)}.
}
\section{Returns}{
A tensor with the same shape as \code{x}.
}

\section{Examples}{
\if{html}{\out{<div class="sourceCode python">}}\preformatted{x = keras_core.ops.convert_to_tensor([-6.0, 1.0, 0.0, 1.0, 6.0])
keras_core.ops.sigmoid(x)
# array([0.00247262, 0.7310586, 0.5, 0.7310586, 0.9975274], dtype=float32)
keras_core.ops.silu(x)
# array([-0.0148357, 0.7310586, 0.0, 0.7310586, 5.9851646], dtype=float32)
}\if{html}{\out{</div>}}
}

\seealso{
\itemize{
\item \url{https://www.tensorflow.org/api_docs/python/tf/keras/ops/silu}
}
}
