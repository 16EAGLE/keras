<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Multi-GPU and distributed training • keras</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet">
<script src="../extra.js"></script><meta property="og:title" content="Multi-GPU and distributed training">
<meta property="og:description" content="Guide to multi-GPU &amp; distributed training for Keras models.">
<meta property="og:image" content="https://keras.posit.co/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">keras</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">2.13.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Tutorials
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/getting_started.html">Getting Started</a>
    </li>
    <li>
      <a href="../articles/tutorial_basic_classification.html">Basic Classification</a>
    </li>
    <li>
      <a href="../articles/tutorial_basic_text_classification.html">Text Classification</a>
    </li>
    <li>
      <a href="../articles/tutorial_basic_regression.html">Basic Regression</a>
    </li>
    <li>
      <a href="../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    </li>
    <li>
      <a href="../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Guides
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li class="dropdown-header">Guides (New for TF 2.6)</li>
    <li>
      <a href="../articles/new-guides/python_subclasses.html">Python Subclasses</a>
    </li>
    <li>
      <a href="../articles/new-guides/making_new_layers_and_models_via_subclassing.html">Making New Layers and Models via Subclassing</a>
    </li>
    <li>
      <a href="../articles/new-guides/customizing_what_happens_in_fit.html">Customizing What Happens in Fit</a>
    </li>
    <li>
      <a href="../articles/new-guides/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    </li>
    <li>
      <a href="../articles/new-guides/preprocessing_layers.html">Working with Preprocessing Layers</a>
    </li>
    <li>
      <a href="../argicles/new-guides/working_with_rnns.html">Working with RNNs</a>
    </li>
    <li class="divider">
    </li>
<li class="dropdown-header">Using Keras</li>
    <li>
      <a href="../articles/guide_keras.html">Guide to Keras Basics</a>
    </li>
    <li>
      <a href="../articles/sequential_model.html">Sequential Model in Depth</a>
    </li>
    <li>
      <a href="../articles/functional_api.html">Functional API in Depth</a>
    </li>
    <li>
      <a href="../articles/about_keras_models.html">About Keras Models</a>
    </li>
    <li>
      <a href="../articles/about_keras_layers.html">About Keras Layers</a>
    </li>
    <li>
      <a href="../articles/training_visualization.html">Training Visualization</a>
    </li>
    <li>
      <a href="../articles/applications.html">Pre-Trained Models</a>
    </li>
    <li>
      <a href="../articles/faq.html">Frequently Asked Questions</a>
    </li>
    <li>
      <a href="../articles/why_use_keras.html">Why Use Keras?</a>
    </li>
    <li class="divider">
    </li>
<li class="dropdown-header">Advanced</li>
    <li>
      <a href="../articles/eager_guide.html">Eager Execution</a>
    </li>
    <li>
      <a href="../articles/training_callbacks.html">Training Callbacks</a>
    </li>
    <li>
      <a href="../articles/backend.html">Keras Backend</a>
    </li>
    <li>
      <a href="../articles/custom_layers.html">Custom Layers</a>
    </li>
    <li>
      <a href="../articles/custom_models.html">Custom Models</a>
    </li>
    <li>
      <a href="../articles/saving_serializing.html">Saving and serializing</a>
    </li>
  </ul>
</li>
<li>
  <a href="../articles/learn.html">Learn</a>
</li>
<li>
  <a href="../articles/tools.html">Tools</a>
</li>
<li>
  <a href="../articles/examples/index.html">Examples</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/rstudio/keras/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Multi-GPU and distributed training</h1>
                        <h4 data-toc-skip class="author"><a href="https://twitter.com/fchollet" class="external-link">fchollet</a></h4>
            
            <h4 data-toc-skip class="date">Last Modified: 2023-11-10;
Last Rendered: 2023-11-10</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/distributed_training.Rmd" class="external-link"><code>vignettes/distributed_training.Rmd</code></a></small>
      <div class="hidden name"><code>distributed_training.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>There are generally two ways to distribute computation across
multiple devices:</p>
<p><strong>Data parallelism</strong>, where a single model gets
replicated on multiple devices or multiple machines. Each of them
processes different batches of data, then they merge their results.
There exist many variants of this setup, that differ in how the
different model replicas merge results, in whether they stay in sync at
every batch or whether they are more loosely coupled, etc.</p>
<p><strong>Model parallelism</strong>, where different parts of a single
model run on different devices, processing a single batch of data
together. This works best with models that have a naturally-parallel
architecture, such as models that feature multiple branches.</p>
<p>This guide focuses on data parallelism, in particular
<strong>synchronous data parallelism</strong>, where the different
replicas of the model stay in sync after each batch they process.
Synchronicity keeps the model convergence behavior identical to what you
would see for single-device training.</p>
<p>Specifically, this guide teaches you how to use the
<code>tf.distribute</code> API to train Keras models on multiple GPUs,
with minimal changes to your code, in the following two setups:</p>
<ul>
<li>On multiple GPUs (typically 2 to 8) installed on a single machine
(single host, multi-device training). This is the most common setup for
researchers and small-scale industry workflows.</li>
<li>On a cluster of many machines, each hosting one or multiple GPUs
(multi-worker distributed training). This is a good setup for
large-scale industry workflows, e.g. training high-resolution image
classification models on tens of millions of images using 20-100
GPUs.</li>
</ul>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> keras</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="single-host-multi-device-synchronous-training">Single-host, multi-device synchronous training<a class="anchor" aria-label="anchor" href="#single-host-multi-device-synchronous-training"></a>
</h2>
<p>In this setup, you have one machine with several GPUs on it
(typically 2 to 8). Each device will run a copy of your model (called a
<strong>replica</strong>). For simplicity, in what follows, we’ll assume
we’re dealing with 8 GPUs, at no loss of generality.</p>
<p><strong>How it works</strong></p>
<p>At each step of training:</p>
<ul>
<li>The current batch of data (called <strong>global batch</strong>) is
split into 8 different sub-batches (called <strong>local
batches</strong>). For instance, if the global batch has 512 samples,
each of the 8 local batches will have 64 samples.</li>
<li>Each of the 8 replicas independently processes a local batch: they
run a forward pass, then a backward pass, outputting the gradient of the
weights with respect to the loss of the model on the local batch.</li>
<li>The weight updates originating from local gradients are efficiently
merged across the 8 replicas. Because this is done at the end of every
step, the replicas always stay in sync.</li>
</ul>
<p>In practice, the process of synchronously updating the weights of the
model replicas is handled at the level of each individual weight
variable. This is done through a <strong>mirrored variable</strong>
object.</p>
<p><strong>How to use it</strong></p>
<p>To do single-host, multi-device synchronous training with a Keras
model, you would use the <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy" class="external-link"><code>tf.distribute.MirroredStrategy</code>
API</a>. Here’s how it works:</p>
<ul>
<li>Instantiate a <code>MirroredStrategy</code>, optionally configuring
which specific devices you want to use (by default the strategy will use
all GPUs available).</li>
<li>Use the strategy object to open a scope, and within this scope,
create all the Keras objects you need that contain variables. Typically,
that means <strong>creating &amp; compiling the model</strong> inside
the distribution scope.</li>
<li>Train the model via <code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code> as usual.</li>
</ul>
<p>Importantly, we recommend that you use <code>tf.data.Dataset</code>
objects to load data in a multi-device or distributed workflow.</p>
<p>Schematically, it looks like this:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># Create a MirroredStrategy.</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>strategy <span class="op">=</span> tf.distribute.MirroredStrategy()</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Number of devices: </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(strategy.num_replicas_in_sync))</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a><span class="co"># Open a strategy scope.</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="cf">with</span> strategy.scope():</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>  <span class="co"># Everything that creates variables should be under the strategy scope.</span></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>  <span class="co"># In general this is only model construction &amp; `compile()`.</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>  model <span class="op">=</span> Model(...)</span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>  model.<span class="bu">compile</span>(...)</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a><span class="co"># Train the model on all available devices.</span></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>model.fit(train_dataset, validation_data<span class="op">=</span>val_dataset, ...)</span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a><span class="co"># Test the model on all available devices.</span></span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a>model.evaluate(test_dataset)</span></code></pre></div>
<p>Here’s a simple end-to-end runnable example:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="kw">def</span> get_compiled_model():</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>    <span class="co"># Make a simple 2-layer densely-connected neural network.</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>    inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">784</span>,))</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Dense(<span class="dv">256</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(inputs)</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Dense(<span class="dv">256</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>    outputs <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>)(x)</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>    model <span class="op">=</span> keras.Model(inputs, outputs)</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>    model.<span class="bu">compile</span>(</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>        optimizer<span class="op">=</span>keras.optimizers.Adam(),</span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>        loss<span class="op">=</span>keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>        metrics<span class="op">=</span>[keras.metrics.SparseCategoricalAccuracy()],</span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>    )</span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a><span class="kw">def</span> get_dataset():</span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a>    batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a>    num_val_samples <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" tabindex="-1"></a>    <span class="co"># Return the MNIST dataset in the form of a `tf.data.Dataset`.</span></span>
<span id="cb3-21"><a href="#cb3-21" tabindex="-1"></a>    (x_train, y_train), (x_test, y_test) <span class="op">=</span> keras.datasets.mnist.load_data()</span>
<span id="cb3-22"><a href="#cb3-22" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" tabindex="-1"></a>    <span class="co"># Preprocess the data (these are Numpy arrays)</span></span>
<span id="cb3-24"><a href="#cb3-24" tabindex="-1"></a>    x_train <span class="op">=</span> x_train.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">784</span>).astype(<span class="st">"float32"</span>) <span class="op">/</span> <span class="dv">255</span></span>
<span id="cb3-25"><a href="#cb3-25" tabindex="-1"></a>    x_test <span class="op">=</span> x_test.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">784</span>).astype(<span class="st">"float32"</span>) <span class="op">/</span> <span class="dv">255</span></span>
<span id="cb3-26"><a href="#cb3-26" tabindex="-1"></a>    y_train <span class="op">=</span> y_train.astype(<span class="st">"float32"</span>)</span>
<span id="cb3-27"><a href="#cb3-27" tabindex="-1"></a>    y_test <span class="op">=</span> y_test.astype(<span class="st">"float32"</span>)</span>
<span id="cb3-28"><a href="#cb3-28" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" tabindex="-1"></a>    <span class="co"># Reserve num_val_samples samples for validation</span></span>
<span id="cb3-30"><a href="#cb3-30" tabindex="-1"></a>    x_val <span class="op">=</span> x_train[<span class="op">-</span>num_val_samples:]</span>
<span id="cb3-31"><a href="#cb3-31" tabindex="-1"></a>    y_val <span class="op">=</span> y_train[<span class="op">-</span>num_val_samples:]</span>
<span id="cb3-32"><a href="#cb3-32" tabindex="-1"></a>    x_train <span class="op">=</span> x_train[:<span class="op">-</span>num_val_samples]</span>
<span id="cb3-33"><a href="#cb3-33" tabindex="-1"></a>    y_train <span class="op">=</span> y_train[:<span class="op">-</span>num_val_samples]</span>
<span id="cb3-34"><a href="#cb3-34" tabindex="-1"></a>    <span class="cf">return</span> (</span>
<span id="cb3-35"><a href="#cb3-35" tabindex="-1"></a>        tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size),</span>
<span id="cb3-36"><a href="#cb3-36" tabindex="-1"></a>        tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size),</span>
<span id="cb3-37"><a href="#cb3-37" tabindex="-1"></a>        tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size),</span>
<span id="cb3-38"><a href="#cb3-38" tabindex="-1"></a>    )</span>
<span id="cb3-39"><a href="#cb3-39" tabindex="-1"></a></span>
<span id="cb3-40"><a href="#cb3-40" tabindex="-1"></a></span>
<span id="cb3-41"><a href="#cb3-41" tabindex="-1"></a><span class="co"># Create a MirroredStrategy.</span></span>
<span id="cb3-42"><a href="#cb3-42" tabindex="-1"></a>strategy <span class="op">=</span> tf.distribute.MirroredStrategy()</span>
<span id="cb3-43"><a href="#cb3-43" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of devices: </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(strategy.num_replicas_in_sync))</span>
<span id="cb3-44"><a href="#cb3-44" tabindex="-1"></a></span>
<span id="cb3-45"><a href="#cb3-45" tabindex="-1"></a><span class="co"># Open a strategy scope.</span></span>
<span id="cb3-46"><a href="#cb3-46" tabindex="-1"></a><span class="cf">with</span> strategy.scope():</span>
<span id="cb3-47"><a href="#cb3-47" tabindex="-1"></a>    <span class="co"># Everything that creates variables should be under the strategy scope.</span></span>
<span id="cb3-48"><a href="#cb3-48" tabindex="-1"></a>    <span class="co"># In general this is only model construction &amp; `compile()`.</span></span>
<span id="cb3-49"><a href="#cb3-49" tabindex="-1"></a>    model <span class="op">=</span> get_compiled_model()</span>
<span id="cb3-50"><a href="#cb3-50" tabindex="-1"></a></span>
<span id="cb3-51"><a href="#cb3-51" tabindex="-1"></a><span class="co"># Train the model on all available devices.</span></span>
<span id="cb3-52"><a href="#cb3-52" tabindex="-1"></a>train_dataset, val_dataset, test_dataset <span class="op">=</span> get_dataset()</span>
<span id="cb3-53"><a href="#cb3-53" tabindex="-1"></a>model.fit(train_dataset, epochs<span class="op">=</span><span class="dv">2</span>, validation_data<span class="op">=</span>val_dataset)</span>
<span id="cb3-54"><a href="#cb3-54" tabindex="-1"></a></span>
<span id="cb3-55"><a href="#cb3-55" tabindex="-1"></a><span class="co"># Test the model on all available devices.</span></span>
<span id="cb3-56"><a href="#cb3-56" tabindex="-1"></a>model.evaluate(test_dataset)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="using-callbacks-to-ensure-fault-tolerance">Using callbacks to ensure fault tolerance<a class="anchor" aria-label="anchor" href="#using-callbacks-to-ensure-fault-tolerance"></a>
</h2>
<p>When using distributed training, you should always make sure you have
a strategy to recover from failure (fault tolerance). The simplest way
to handle this is to pass <code>ModelCheckpoint</code> callback to
<code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code>, to save your model at regular intervals (e.g. every
100 batches or every epoch). You can then restart training from your
saved model.</p>
<p>Here’s a simple example:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a><span class="co"># Prepare a directory to store all the checkpoints.</span></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>checkpoint_dir <span class="op">=</span> <span class="st">"./ckpt"</span></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> os.path.exists(checkpoint_dir):</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>    os.makedirs(checkpoint_dir)</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a><span class="kw">def</span> make_or_restore_model():</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>    <span class="co"># Either restore the latest model, or create a fresh one</span></span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>    <span class="co"># if there is no checkpoint available.</span></span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a>    checkpoints <span class="op">=</span> [checkpoint_dir <span class="op">+</span> <span class="st">"/"</span> <span class="op">+</span> name <span class="cf">for</span> name <span class="kw">in</span> os.listdir(checkpoint_dir)]</span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>    <span class="cf">if</span> checkpoints:</span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a>        latest_checkpoint <span class="op">=</span> <span class="bu">max</span>(checkpoints, key<span class="op">=</span>os.path.getctime)</span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Restoring from"</span>, latest_checkpoint)</span>
<span id="cb4-17"><a href="#cb4-17" tabindex="-1"></a>        <span class="cf">return</span> keras.models.load_model(latest_checkpoint)</span>
<span id="cb4-18"><a href="#cb4-18" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Creating a new model"</span>)</span>
<span id="cb4-19"><a href="#cb4-19" tabindex="-1"></a>    <span class="cf">return</span> get_compiled_model()</span>
<span id="cb4-20"><a href="#cb4-20" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" tabindex="-1"></a><span class="kw">def</span> run_training(epochs<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb4-23"><a href="#cb4-23" tabindex="-1"></a>    <span class="co"># Create a MirroredStrategy.</span></span>
<span id="cb4-24"><a href="#cb4-24" tabindex="-1"></a>    strategy <span class="op">=</span> tf.distribute.MirroredStrategy()</span>
<span id="cb4-25"><a href="#cb4-25" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" tabindex="-1"></a>    <span class="co"># Open a strategy scope and create/restore the model</span></span>
<span id="cb4-27"><a href="#cb4-27" tabindex="-1"></a>    <span class="cf">with</span> strategy.scope():</span>
<span id="cb4-28"><a href="#cb4-28" tabindex="-1"></a>        model <span class="op">=</span> make_or_restore_model()</span>
<span id="cb4-29"><a href="#cb4-29" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" tabindex="-1"></a>    callbacks <span class="op">=</span> [</span>
<span id="cb4-31"><a href="#cb4-31" tabindex="-1"></a>        <span class="co"># This callback saves a SavedModel every epoch</span></span>
<span id="cb4-32"><a href="#cb4-32" tabindex="-1"></a>        <span class="co"># We include the current epoch in the folder name.</span></span>
<span id="cb4-33"><a href="#cb4-33" tabindex="-1"></a>        keras.callbacks.ModelCheckpoint(</span>
<span id="cb4-34"><a href="#cb4-34" tabindex="-1"></a>            filepath<span class="op">=</span>checkpoint_dir <span class="op">+</span> <span class="st">"/ckpt-</span><span class="sc">{epoch}</span><span class="st">"</span>, save_freq<span class="op">=</span><span class="st">"epoch"</span></span>
<span id="cb4-35"><a href="#cb4-35" tabindex="-1"></a>        )</span>
<span id="cb4-36"><a href="#cb4-36" tabindex="-1"></a>    ]</span>
<span id="cb4-37"><a href="#cb4-37" tabindex="-1"></a>    model.fit(</span>
<span id="cb4-38"><a href="#cb4-38" tabindex="-1"></a>        train_dataset,</span>
<span id="cb4-39"><a href="#cb4-39" tabindex="-1"></a>        epochs<span class="op">=</span>epochs,</span>
<span id="cb4-40"><a href="#cb4-40" tabindex="-1"></a>        callbacks<span class="op">=</span>callbacks,</span>
<span id="cb4-41"><a href="#cb4-41" tabindex="-1"></a>        validation_data<span class="op">=</span>val_dataset,</span>
<span id="cb4-42"><a href="#cb4-42" tabindex="-1"></a>        verbose<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb4-43"><a href="#cb4-43" tabindex="-1"></a>    )</span>
<span id="cb4-44"><a href="#cb4-44" tabindex="-1"></a></span>
<span id="cb4-45"><a href="#cb4-45" tabindex="-1"></a></span>
<span id="cb4-46"><a href="#cb4-46" tabindex="-1"></a><span class="co"># Running the first time creates the model</span></span>
<span id="cb4-47"><a href="#cb4-47" tabindex="-1"></a>run_training(epochs<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-48"><a href="#cb4-48" tabindex="-1"></a></span>
<span id="cb4-49"><a href="#cb4-49" tabindex="-1"></a><span class="co"># Calling the same function again will resume from where we left off</span></span>
<span id="cb4-50"><a href="#cb4-50" tabindex="-1"></a>run_training(epochs<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="tf-data-performance-tips">
<code>tf.data</code> performance tips<a class="anchor" aria-label="anchor" href="#tf-data-performance-tips"></a>
</h2>
<p>When doing distributed training, the efficiency with which you load
data can often become critical. Here are a few tips to make sure your
<code>tf.data</code> pipelines run as fast as possible.</p>
<p><strong>Note about dataset batching</strong></p>
<p>When creating your dataset, make sure it is batched with the global
batch size. For instance, if each of your 8 GPUs is capable of running a
batch of 64 samples, you call use a global batch size of 512.</p>
<p><strong>Calling <code>dataset.cache()</code></strong></p>
<p>If you call <code>.cache()</code> on a dataset, its data will be
cached after running through the first iteration over the data. Every
subsequent iteration will use the cached data. The cache can be in
memory (default) or to a local file you specify.</p>
<p>This can improve performance when:</p>
<ul>
<li>Your data is not expected to change from iteration to iteration</li>
<li>You are reading data from a remote distributed filesystem</li>
<li>You are reading data from local disk, but your data would fit in
memory and your workflow is significantly IO-bound (e.g. reading &amp;
decoding image files).</li>
</ul>
<p><strong>Calling
<code>dataset.prefetch(buffer_size)</code></strong></p>
<p>You should almost always call <code>.prefetch(buffer_size)</code>
after creating a dataset. It means your data pipeline will run
asynchronously from your model, with new samples being preprocessed and
stored in a buffer while the current batch samples are used to train the
model. The next batch will be prefetched in GPU memory by the time the
current batch is over.</p>
</div>
<div class="section level2">
<h2 id="multi-worker-distributed-synchronous-training">Multi-worker distributed synchronous training<a class="anchor" aria-label="anchor" href="#multi-worker-distributed-synchronous-training"></a>
</h2>
<p><strong>How it works</strong></p>
<p>In this setup, you have multiple machines (called
<strong>workers</strong>), each with one or several GPUs on them. Much
like what happens for single-host training, each available GPU will run
one model replica, and the value of the variables of each replica is
kept in sync after each batch.</p>
<p>Importantly, the current implementation assumes that all workers have
the same number of GPUs (homogeneous cluster).</p>
<p><strong>How to use it</strong></p>
<ol style="list-style-type: decimal">
<li>Set up a cluster (we provide pointers below).</li>
<li>Set up an appropriate <code>TF_CONFIG</code> environment variable on
each worker. This tells the worker what its role is and how to
communicate with its peers.</li>
<li>On each worker, run your model construction &amp; compilation code
within the scope of a <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy" class="external-link"><code>MultiWorkerMirroredStrategy</code>
object</a>, similarly to we did for single-host training.</li>
<li>Run evaluation code on a designated evaluator machine.</li>
</ol>
<p><strong>Setting up a cluster</strong></p>
<p>First, set up a cluster (collective of machines). Each machine
individually should be setup so as to be able to run your model
(typically, each machine will run the same Docker image) and to able to
access your data source (e.g. GCS).</p>
<p>Cluster management is beyond the scope of this guide. <a href="https://cloud.google.com/ai-platform/training/docs/distributed-training-containers" class="external-link">Here
is a document</a> to help you get started. You can also take a look at
<a href="https://www.kubeflow.org/" class="external-link">Kubeflow</a>.</p>
<p><strong>Setting up the <code>TF_CONFIG</code> environment
variable</strong></p>
<p>While the code running on each worker is almost the same as the code
used in the single-host workflow (except with a different
<code>tf.distribute</code> strategy object), one significant difference
between the single-host workflow and the multi-worker workflow is that
you need to set a <code>TF_CONFIG</code> environment variable on each
machine running in your cluster.</p>
<p>The <code>TF_CONFIG</code> environment variable is a JSON string that
specifies:</p>
<ul>
<li>The cluster configuration, while the list of addresses &amp; ports
of the machines that make up the cluster</li>
<li>The worker’s “task”, which is the role that this specific machine
has to play within the cluster.</li>
</ul>
<p>One example of TF_CONFIG is:</p>
<pre><code>os.environ['TF_CONFIG'] = json.dumps({
    'cluster': {
        'worker': ["localhost:12345", "localhost:23456"]
    },
    'task': {'type': 'worker', 'index': 0}
})</code></pre>
<p>In the multi-worker synchronous training setup, valid roles (task
types) for the machines are “worker” and “evaluator”.</p>
<p>For example, if you have 8 machines with 4 GPUs each, you could have
7 workers and one evaluator.</p>
<ul>
<li>The workers train the model, each one processing sub-batches of a
global batch.</li>
<li>One of the workers (worker 0) will serve as “chief”, a particular
kind of worker that is responsible for saving logs and checkpoints for
later reuse (typically to a Cloud storage location).</li>
<li>The evaluator runs a continuous loop that loads the latest
checkpoint saved by the chief worker, runs evaluation on it
(asynchronously from the other workers) and writes evaluation logs
(e.g. TensorBoard logs).</li>
</ul>
<p><strong>Running code on each worker</strong></p>
<p>You would run training code on each worker (including the chief) and
evaluation code on the evaluator.</p>
<p>The training code is basically the same as what you would use in the
single-host setup, except using <code>MultiWorkerMirroredStrategy</code>
instead of <code>MirroredStrategy</code>.</p>
<p>Each worker would run the same code (minus the difference explained
in the note below), including the same callbacks.</p>
<p><strong>Note:</strong> Callbacks that save model checkpoints or logs
should save to a different directory for each worker. It is standard
practice that all workers should save to local disk (which is typically
temporary), <strong>except worker 0</strong>, which would save
TensorBoard logs checkpoints to a Cloud storage location for later
access &amp; reuse.</p>
<p>The evaluator would simply use <code>MirroredStrategy</code> (since
it runs on a single machine and does not need to communicate with other
machines) and call <code>model.evaluate()</code>. It would be loading
the latest checkpoint saved by the chief worker to a Cloud storage
location, and would save evaluation logs to the same location as the
chief logs.</p>
<div class="section level3">
<h3 id="example-code-running-in-a-multi-worker-setup">Example: code running in a multi-worker setup<a class="anchor" aria-label="anchor" href="#example-code-running-in-a-multi-worker-setup"></a>
</h3>
<p>On the chief (worker 0):</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="co"># Set TF_CONFIG</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>os.environ[<span class="st">'TF_CONFIG'</span>] <span class="op">=</span> json.dumps({</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>    <span class="st">'cluster'</span>: {</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>        <span class="st">'worker'</span>: [<span class="st">"localhost:12345"</span>, <span class="st">"localhost:23456"</span>]</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>    },</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>    <span class="st">'task'</span>: {<span class="st">'type'</span>: <span class="st">'worker'</span>, <span class="st">'index'</span>: <span class="dv">0</span>}</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>})</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a><span class="co"># Open a strategy scope and create/restore the model.</span></span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>strategy <span class="op">=</span> tf.distribute.experimental.MultiWorkerMirroredStrategy()</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a><span class="cf">with</span> strategy.scope():</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>  model <span class="op">=</span> make_or_restore_model()</span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a>callbacks <span class="op">=</span> [</span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a>    <span class="co"># This callback saves a SavedModel every 100 batches</span></span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a>    keras.callbacks.ModelCheckpoint(filepath<span class="op">=</span><span class="st">'path/to/cloud/location/ckpt'</span>,</span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a>                                    save_freq<span class="op">=</span><span class="dv">100</span>),</span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a>    keras.callbacks.TensorBoard(<span class="st">'path/to/cloud/location/tb/'</span>)</span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a>]</span>
<span id="cb6-20"><a href="#cb6-20" tabindex="-1"></a>model.fit(train_dataset,</span>
<span id="cb6-21"><a href="#cb6-21" tabindex="-1"></a>          callbacks<span class="op">=</span>callbacks,</span>
<span id="cb6-22"><a href="#cb6-22" tabindex="-1"></a>          ...)</span></code></pre></div>
<p>On other workers:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="co"># Set TF_CONFIG</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>worker_index <span class="op">=</span> <span class="dv">1</span>  <span class="co"># For instance</span></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>os.environ[<span class="st">'TF_CONFIG'</span>] <span class="op">=</span> json.dumps({</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>    <span class="st">'cluster'</span>: {</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>        <span class="st">'worker'</span>: [<span class="st">"localhost:12345"</span>, <span class="st">"localhost:23456"</span>]</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>    },</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>    <span class="st">'task'</span>: {<span class="st">'type'</span>: <span class="st">'worker'</span>, <span class="st">'index'</span>: worker_index}</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>})</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a><span class="co"># Open a strategy scope and create/restore the model.</span></span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a><span class="co"># You can restore from the checkpoint saved by the chief.</span></span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>strategy <span class="op">=</span> tf.distribute.experimental.MultiWorkerMirroredStrategy()</span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a><span class="cf">with</span> strategy.scope():</span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a>  model <span class="op">=</span> make_or_restore_model()</span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a>callbacks <span class="op">=</span> [</span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a>    keras.callbacks.ModelCheckpoint(filepath<span class="op">=</span><span class="st">'local/path/ckpt'</span>, save_freq<span class="op">=</span><span class="dv">100</span>),</span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a>    keras.callbacks.TensorBoard(<span class="st">'local/path/tb/'</span>)</span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a>]</span>
<span id="cb7-20"><a href="#cb7-20" tabindex="-1"></a>model.fit(train_dataset,</span>
<span id="cb7-21"><a href="#cb7-21" tabindex="-1"></a>          callbacks<span class="op">=</span>callbacks,</span>
<span id="cb7-22"><a href="#cb7-22" tabindex="-1"></a>          ...)</span></code></pre></div>
<p>On the evaluator:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>strategy <span class="op">=</span> tf.distribute.MirroredStrategy()</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a><span class="cf">with</span> strategy.scope():</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>  model <span class="op">=</span> make_or_restore_model()  <span class="co"># Restore from the checkpoint saved by the chief.</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>results <span class="op">=</span> model.evaluate(val_dataset)</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a><span class="co"># Then, log the results on a shared location, write TensorBoard logs, etc</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="further-reading">Further reading<a class="anchor" aria-label="anchor" href="#further-reading"></a>
</h3>
<ol style="list-style-type: decimal">
<li><a href="https://www.tensorflow.org/guide/distributed_training" class="external-link">TensorFlow
distributed training guide</a></li>
<li><a href="https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras" class="external-link">Tutorial
on multi-worker training with Keras</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy" class="external-link">MirroredStrategy
docs</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy" class="external-link">MultiWorkerMirroredStrategy
docs</a></li>
<li><a href="https://towardsdatascience.com/distributed-training-in-tf-keras-with-w-b-ccf021f9322e" class="external-link">Distributed
training in tf.keras with Weights &amp; Biases</a></li>
</ol>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, RStudio, Google.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
