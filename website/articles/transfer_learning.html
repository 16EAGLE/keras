<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Transfer learning &amp; fine-tuning • keras</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet">
<script src="../extra.js"></script><meta property="og:title" content="Transfer learning &amp; fine-tuning">
<meta property="og:description" content="Complete guide to transfer learning &amp; fine-tuning in Keras.">
<meta property="og:image" content="https://keras.posit.co/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">keras</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">2.13.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Tutorials
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/getting_started.html">Getting Started</a>
    </li>
    <li>
      <a href="../articles/tutorial_basic_classification.html">Basic Classification</a>
    </li>
    <li>
      <a href="../articles/tutorial_basic_text_classification.html">Text Classification</a>
    </li>
    <li>
      <a href="../articles/tutorial_basic_regression.html">Basic Regression</a>
    </li>
    <li>
      <a href="../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    </li>
    <li>
      <a href="../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Guides
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li class="dropdown-header">Guides (New for TF 2.6)</li>
    <li>
      <a href="../articles/new-guides/python_subclasses.html">Python Subclasses</a>
    </li>
    <li>
      <a href="../articles/new-guides/making_new_layers_and_models_via_subclassing.html">Making New Layers and Models via Subclassing</a>
    </li>
    <li>
      <a href="../articles/new-guides/customizing_what_happens_in_fit.html">Customizing What Happens in Fit</a>
    </li>
    <li>
      <a href="../articles/new-guides/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    </li>
    <li>
      <a href="../articles/new-guides/preprocessing_layers.html">Working with Preprocessing Layers</a>
    </li>
    <li>
      <a href="../argicles/new-guides/working_with_rnns.html">Working with RNNs</a>
    </li>
    <li class="divider">
    </li>
<li class="dropdown-header">Using Keras</li>
    <li>
      <a href="../articles/guide_keras.html">Guide to Keras Basics</a>
    </li>
    <li>
      <a href="../articles/sequential_model.html">Sequential Model in Depth</a>
    </li>
    <li>
      <a href="../articles/functional_api.html">Functional API in Depth</a>
    </li>
    <li>
      <a href="../articles/about_keras_models.html">About Keras Models</a>
    </li>
    <li>
      <a href="../articles/about_keras_layers.html">About Keras Layers</a>
    </li>
    <li>
      <a href="../articles/training_visualization.html">Training Visualization</a>
    </li>
    <li>
      <a href="../articles/applications.html">Pre-Trained Models</a>
    </li>
    <li>
      <a href="../articles/faq.html">Frequently Asked Questions</a>
    </li>
    <li>
      <a href="../articles/why_use_keras.html">Why Use Keras?</a>
    </li>
    <li class="divider">
    </li>
<li class="dropdown-header">Advanced</li>
    <li>
      <a href="../articles/eager_guide.html">Eager Execution</a>
    </li>
    <li>
      <a href="../articles/training_callbacks.html">Training Callbacks</a>
    </li>
    <li>
      <a href="../articles/backend.html">Keras Backend</a>
    </li>
    <li>
      <a href="../articles/custom_layers.html">Custom Layers</a>
    </li>
    <li>
      <a href="../articles/custom_models.html">Custom Models</a>
    </li>
    <li>
      <a href="../articles/saving_serializing.html">Saving and serializing</a>
    </li>
  </ul>
</li>
<li>
  <a href="../articles/learn.html">Learn</a>
</li>
<li>
  <a href="../articles/tools.html">Tools</a>
</li>
<li>
  <a href="../articles/examples/index.html">Examples</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/rstudio/keras/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Transfer learning &amp; fine-tuning</h1>
                        <h4 data-toc-skip class="author"><a href="https://twitter.com/fchollet" class="external-link">fchollet</a></h4>
            
            <h4 data-toc-skip class="date">Last Modified: 2023-11-10;
Last Rendered: 2023-11-10</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/transfer_learning.Rmd" class="external-link"><code>vignettes/transfer_learning.Rmd</code></a></small>
      <div class="hidden name"><code>transfer_learning.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> keras</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">import</span> tensorflow_datasets <span class="im">as</span> tfds</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p><strong>Transfer learning</strong> consists of taking features
learned on one problem, and leveraging them on a new, similar problem.
For instance, features from a model that has learned to identify racoons
may be useful to kick-start a model meant to identify tanukis.</p>
<p>Transfer learning is usually done for tasks where your dataset has
too little data to train a full-scale model from scratch.</p>
<p>The most common incarnation of transfer learning in the context of
deep learning is the following workflow:</p>
<ol style="list-style-type: decimal">
<li>Take layers from a previously trained model.</li>
<li>Freeze them, so as to avoid destroying any of the information they
contain during future training rounds.</li>
<li>Add some new, trainable layers on top of the frozen layers. They
will learn to turn the old features into predictions on a new
dataset.</li>
<li>Train the new layers on your dataset.</li>
</ol>
<p>A last, optional step, is <strong>fine-tuning</strong>, which
consists of unfreezing the entire model you obtained above (or part of
it), and re-training it on the new data with a very low learning rate.
This can potentially achieve meaningful improvements, by incrementally
adapting the pretrained features to the new data.</p>
<p>First, we will go over the Keras <code>trainable</code> API in
detail, which underlies most transfer learning &amp; fine-tuning
workflows.</p>
<p>Then, we’ll demonstrate the typical workflow by taking a model
pretrained on the ImageNet dataset, and retraining it on the Kaggle
“cats vs dogs” classification dataset.</p>
<p>This is adapted from <a href="https://www.manning.com/books/deep-learning-with-python" class="external-link">Deep
Learning with Python</a> and the 2016 blog post <a href="https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html" class="external-link">“building
powerful image classification models using very little data”</a>.</p>
</div>
<div class="section level2">
<h2 id="freezing-layers-understanding-the-trainable-attribute">Freezing layers: understanding the <code>trainable</code>
attribute<a class="anchor" aria-label="anchor" href="#freezing-layers-understanding-the-trainable-attribute"></a>
</h2>
<p>Layers &amp; models have three weight attributes:</p>
<ul>
<li>
<code>weights</code> is the list of all weights variables of the
layer.</li>
<li>
<code>trainable_weights</code> is the list of those that are meant
to be updated (via gradient descent) to minimize the loss during
training.</li>
<li>
<code>non_trainable_weights</code> is the list of those that aren’t
meant to be trained. Typically they are updated by the model during the
forward pass.</li>
</ul>
<p><strong>Example: the <code>Dense</code> layer has 2 trainable weights
(kernel &amp; bias)</strong></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>layer <span class="op">=</span> keras.layers.Dense(<span class="dv">3</span>)</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>layer.build((<span class="va">None</span>, <span class="dv">4</span>))  <span class="co"># Create the weights</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"weights:"</span>, <span class="bu">len</span>(layer.weights))</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"trainable_weights:"</span>, <span class="bu">len</span>(layer.trainable_weights))</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"non_trainable_weights:"</span>, <span class="bu">len</span>(layer.non_trainable_weights))</span></code></pre></div>
<p>In general, all weights are trainable weights. The only built-in
layer that has non-trainable weights is the
<code>BatchNormalization</code> layer. It uses non-trainable weights to
keep track of the mean and variance of its inputs during training. To
learn how to use non-trainable weights in your own custom layers, see
the <a href="https://keras.io/guides/making_new_layers_and_models_via_subclassing/" class="external-link">guide
to writing new layers from scratch</a>.</p>
<p><strong>Example: the <code>BatchNormalization</code> layer has 2
trainable weights and 2 non-trainable weights</strong></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>layer <span class="op">=</span> keras.layers.BatchNormalization()</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>layer.build((<span class="va">None</span>, <span class="dv">4</span>))  <span class="co"># Create the weights</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"weights:"</span>, <span class="bu">len</span>(layer.weights))</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"trainable_weights:"</span>, <span class="bu">len</span>(layer.trainable_weights))</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"non_trainable_weights:"</span>, <span class="bu">len</span>(layer.non_trainable_weights))</span></code></pre></div>
<p>Layers &amp; models also feature a boolean attribute
<code>trainable</code>. Its value can be changed. Setting
<code>layer.trainable</code> to <code>False</code> moves all the layer’s
weights from trainable to non-trainable. This is called “freezing” the
layer: the state of a frozen layer won’t be updated during training
(either when training with <code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code> or when training with any
custom loop that relies on <code>trainable_weights</code> to apply
gradient updates).</p>
<p><strong>Example: setting <code>trainable</code> to
<code>False</code></strong></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>layer <span class="op">=</span> keras.layers.Dense(<span class="dv">3</span>)</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>layer.build((<span class="va">None</span>, <span class="dv">4</span>))  <span class="co"># Create the weights</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>layer.trainable <span class="op">=</span> <span class="va">False</span>  <span class="co"># Freeze the layer</span></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"weights:"</span>, <span class="bu">len</span>(layer.weights))</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"trainable_weights:"</span>, <span class="bu">len</span>(layer.trainable_weights))</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"non_trainable_weights:"</span>, <span class="bu">len</span>(layer.non_trainable_weights))</span></code></pre></div>
<p>When a trainable weight becomes non-trainable, its value is no longer
updated during training.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># Make a model with 2 layers</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>layer1 <span class="op">=</span> keras.layers.Dense(<span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>layer2 <span class="op">=</span> keras.layers.Dense(<span class="dv">3</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span>)</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>model <span class="op">=</span> keras.Sequential([keras.Input(shape<span class="op">=</span>(<span class="dv">3</span>,)), layer1, layer2])</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a><span class="co"># Freeze the first layer</span></span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>layer1.trainable <span class="op">=</span> <span class="va">False</span></span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a><span class="co"># Keep a copy of the weights of layer1 for later reference</span></span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>initial_layer1_weights_values <span class="op">=</span> layer1.get_weights()</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"adam"</span>, loss<span class="op">=</span><span class="st">"mse"</span>)</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>model.fit(np.random.random((<span class="dv">2</span>, <span class="dv">3</span>)), np.random.random((<span class="dv">2</span>, <span class="dv">3</span>)))</span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a><span class="co"># Check that the weights of layer1 have not changed during training</span></span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>final_layer1_weights_values <span class="op">=</span> layer1.get_weights()</span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a>np.testing.assert_allclose(</span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a>    initial_layer1_weights_values[<span class="dv">0</span>], final_layer1_weights_values[<span class="dv">0</span>]</span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a>)</span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a>np.testing.assert_allclose(</span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a>    initial_layer1_weights_values[<span class="dv">1</span>], final_layer1_weights_values[<span class="dv">1</span>]</span>
<span id="cb5-23"><a href="#cb5-23" tabindex="-1"></a>)</span></code></pre></div>
<p>Do not confuse the <code>layer.trainable</code> attribute with the
argument <code>training</code> in <code>layer.__call__()</code> (which
controls whether the layer should run its forward pass in inference mode
or training mode). For more information, see the <a href="https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute" class="external-link">Keras
FAQ</a>.</p>
</div>
<div class="section level2">
<h2 id="recursive-setting-of-the-trainable-attribute">Recursive setting of the <code>trainable</code> attribute<a class="anchor" aria-label="anchor" href="#recursive-setting-of-the-trainable-attribute"></a>
</h2>
<p>If you set <code>trainable = False</code> on a model or on any layer
that has sublayers, all children layers become non-trainable as
well.</p>
<p><strong>Example:</strong></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>inner_model <span class="op">=</span> keras.Sequential(</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>    [</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>        keras.Input(shape<span class="op">=</span>(<span class="dv">3</span>,)),</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>        keras.layers.Dense(<span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>        keras.layers.Dense(<span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>    ]</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>)</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>model <span class="op">=</span> keras.Sequential(</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>    [</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>        keras.Input(shape<span class="op">=</span>(<span class="dv">3</span>,)),</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>        inner_model,</span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a>        keras.layers.Dense(<span class="dv">3</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span>),</span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a>    ]</span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a>)</span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a>model.trainable <span class="op">=</span> <span class="va">False</span>  <span class="co"># Freeze the outer model</span></span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a><span class="cf">assert</span> inner_model.trainable <span class="op">==</span> <span class="va">False</span>  <span class="co"># All layers in `model` are now frozen</span></span>
<span id="cb6-20"><a href="#cb6-20" tabindex="-1"></a><span class="cf">assert</span> (</span>
<span id="cb6-21"><a href="#cb6-21" tabindex="-1"></a>    inner_model.layers[<span class="dv">0</span>].trainable <span class="op">==</span> <span class="va">False</span></span>
<span id="cb6-22"><a href="#cb6-22" tabindex="-1"></a>)  <span class="co"># `trainable` is propagated recursively</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="the-typical-transfer-learning-workflow">The typical transfer-learning workflow<a class="anchor" aria-label="anchor" href="#the-typical-transfer-learning-workflow"></a>
</h2>
<p>This leads us to how a typical transfer learning workflow can be
implemented in Keras:</p>
<ol style="list-style-type: decimal">
<li>Instantiate a base model and load pre-trained weights into it.</li>
<li>Freeze all layers in the base model by setting
<code>trainable = False</code>.</li>
<li>Create a new model on top of the output of one (or several) layers
from the base model.</li>
<li>Train your new model on your new dataset.</li>
</ol>
<p>Note that an alternative, more lightweight workflow could also
be:</p>
<ol style="list-style-type: decimal">
<li>Instantiate a base model and load pre-trained weights into it.</li>
<li>Run your new dataset through it and record the output of one (or
several) layers from the base model. This is called <strong>feature
extraction</strong>.</li>
<li>Use that output as input data for a new, smaller model.</li>
</ol>
<p>A key advantage of that second workflow is that you only run the base
model once on your data, rather than once per epoch of training. So it’s
a lot faster &amp; cheaper.</p>
<p>An issue with that second workflow, though, is that it doesn’t allow
you to dynamically modify the input data of your new model during
training, which is required when doing data augmentation, for instance.
Transfer learning is typically used for tasks when your new dataset has
too little data to train a full-scale model from scratch, and in such
scenarios data augmentation is very important. So in what follows, we
will focus on the first workflow.</p>
<p>Here’s what the first workflow looks like in Keras:</p>
<p>First, instantiate a base model with pre-trained weights.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>base_model <span class="op">=</span> keras.applications.Xception(</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>    weights<span class="op">=</span><span class="st">'imagenet'</span>,  <span class="co"># Load weights pre-trained on ImageNet.</span></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>    input_shape<span class="op">=</span>(<span class="dv">150</span>, <span class="dv">150</span>, <span class="dv">3</span>),</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>    include_top<span class="op">=</span><span class="va">False</span>)  <span class="co"># Do not include the ImageNet classifier at the top.</span></span></code></pre></div>
<p>Then, freeze the base model.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>base_model.trainable <span class="op">=</span> <span class="va">False</span></span></code></pre></div>
<p>Create a new model on top.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">150</span>, <span class="dv">150</span>, <span class="dv">3</span>))</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="co"># We make sure that the base_model is running in inference mode here,</span></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a><span class="co"># by passing `training=False`. This is important for fine-tuning, as you will</span></span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a><span class="co"># learn in a few paragraphs.</span></span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>x <span class="op">=</span> base_model(inputs, training<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a><span class="co"># Convert features of shape `base_model.output_shape[1:]` to vectors</span></span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>x <span class="op">=</span> keras.layers.GlobalAveragePooling2D()(x)</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a><span class="co"># A Dense classifier with a single unit (binary classification)</span></span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a>outputs <span class="op">=</span> keras.layers.Dense(<span class="dv">1</span>)(x)</span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a>model <span class="op">=</span> keras.Model(inputs, outputs)</span></code></pre></div>
<p>Train the model on new data.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span>keras.optimizers.Adam(),</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>              loss<span class="op">=</span>keras.losses.BinaryCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>              metrics<span class="op">=</span>[keras.metrics.BinaryAccuracy()])</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>model.fit(new_dataset, epochs<span class="op">=</span><span class="dv">20</span>, callbacks<span class="op">=</span>..., validation_data<span class="op">=</span>...)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="fine-tuning">Fine-tuning<a class="anchor" aria-label="anchor" href="#fine-tuning"></a>
</h2>
<p>Once your model has converged on the new data, you can try to
unfreeze all or part of the base model and retrain the whole model
end-to-end with a very low learning rate.</p>
<p>This is an optional last step that can potentially give you
incremental improvements. It could also potentially lead to quick
overfitting – keep that in mind.</p>
<p>It is critical to only do this step <em>after</em> the model with
frozen layers has been trained to convergence. If you mix
randomly-initialized trainable layers with trainable layers that hold
pre-trained features, the randomly-initialized layers will cause very
large gradient updates during training, which will destroy your
pre-trained features.</p>
<p>It’s also critical to use a very low learning rate at this stage,
because you are training a much larger model than in the first round of
training, on a dataset that is typically very small. As a result, you
are at risk of overfitting very quickly if you apply large weight
updates. Here, you only want to readapt the pretrained weights in an
incremental way.</p>
<p>This is how to implement fine-tuning of the whole base model:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="co"># Unfreeze the base model</span></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>base_model.trainable <span class="op">=</span> <span class="va">True</span></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a><span class="co"># It's important to recompile your model after you make any changes</span></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a><span class="co"># to the `trainable` attribute of any inner layer, so that your changes</span></span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a><span class="co"># are take into account</span></span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span>keras.optimizers.Adam(<span class="fl">1e-5</span>),  <span class="co"># Very low learning rate</span></span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a>              loss<span class="op">=</span>keras.losses.BinaryCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a>              metrics<span class="op">=</span>[keras.metrics.BinaryAccuracy()])</span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a><span class="co"># Train end-to-end. Be careful to stop before you overfit!</span></span>
<span id="cb11-12"><a href="#cb11-12" tabindex="-1"></a>model.fit(new_dataset, epochs<span class="op">=</span><span class="dv">10</span>, callbacks<span class="op">=</span>..., validation_data<span class="op">=</span>...)</span></code></pre></div>
<p><strong>Important note about <code><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile()</a></code> and
<code>trainable</code></strong></p>
<p>Calling <code><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile()</a></code> on a model is meant to “freeze” the
behavior of that model. This implies that the <code>trainable</code>
attribute values at the time the model is compiled should be preserved
throughout the lifetime of that model, until <code>compile</code> is
called again. Hence, if you change any <code>trainable</code> value,
make sure to call <code><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile()</a></code> again on your model for your
changes to be taken into account.</p>
<p><strong>Important notes about <code>BatchNormalization</code>
layer</strong></p>
<p>Many image models contain <code>BatchNormalization</code> layers.
That layer is a special case on every imaginable count. Here are a few
things to keep in mind.</p>
<ul>
<li>
<code>BatchNormalization</code> contains 2 non-trainable weights
that get updated during training. These are the variables tracking the
mean and variance of the inputs.</li>
<li>When you set <code>bn_layer.trainable = False</code>, the
<code>BatchNormalization</code> layer will run in inference mode, and
will not update its mean &amp; variance statistics. This is not the case
for other layers in general, as <a href="https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute" class="external-link">weight
trainability &amp; inference/training modes are two orthogonal
concepts</a>. But the two are tied in the case of the
<code>BatchNormalization</code> layer.</li>
<li>When you unfreeze a model that contains
<code>BatchNormalization</code> layers in order to do fine-tuning, you
should keep the <code>BatchNormalization</code> layers in inference mode
by passing <code>training=False</code> when calling the base model.
Otherwise the updates applied to the non-trainable weights will suddenly
destroy what the model has learned.</li>
</ul>
<p>You’ll see this pattern in action in the end-to-end example at the
end of this guide.</p>
</div>
<div class="section level2">
<h2 id="an-end-to-end-example-fine-tuning-an-image-classification-model-on-a-cats-vs--dogs-dataset">An end-to-end example: fine-tuning an image classification model on
a cats vs. dogs dataset<a class="anchor" aria-label="anchor" href="#an-end-to-end-example-fine-tuning-an-image-classification-model-on-a-cats-vs--dogs-dataset"></a>
</h2>
<p>To solidify these concepts, let’s walk you through a concrete
end-to-end transfer learning &amp; fine-tuning example. We will load the
Xception model, pre-trained on ImageNet, and use it on the Kaggle “cats
vs. dogs” classification dataset.</p>
<div class="section level3">
<h3 id="getting-the-data">Getting the data<a class="anchor" aria-label="anchor" href="#getting-the-data"></a>
</h3>
<p>First, let’s fetch the cats vs. dogs dataset using TFDS. If you have
your own dataset, you’ll probably want to use the utility
<code>keras.utils.image_dataset_from_directory</code> to generate
similar labeled dataset objects from a set of images on disk filed into
class-specific folders.</p>
<p>Transfer learning is most useful when working with very small
datasets. To keep our dataset small, we will use 40% of the original
training data (25,000 images) for training, 10% for validation, and 10%
for testing.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>tfds.disable_progress_bar()</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>train_ds, validation_ds, test_ds <span class="op">=</span> tfds.load(</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>    <span class="st">"cats_vs_dogs"</span>,</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>    <span class="co"># Reserve 10% for validation and 10% for test</span></span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>    split<span class="op">=</span>[<span class="st">"train[:40%]"</span>, <span class="st">"train[40%:50%]"</span>, <span class="st">"train[50%:60%]"</span>],</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a>    as_supervised<span class="op">=</span><span class="va">True</span>,  <span class="co"># Include labels</span></span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a>)</span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of training samples: </span><span class="sc">{</span>train_ds<span class="sc">.</span>cardinality()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-11"><a href="#cb12-11" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of validation samples: </span><span class="sc">{</span>validation_ds<span class="sc">.</span>cardinality()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-12"><a href="#cb12-12" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of test samples: </span><span class="sc">{</span>test_ds<span class="sc">.</span>cardinality()<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
<p>These are the first 9 images in the training dataset – as you can
see, they’re all different sizes.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a><span class="cf">for</span> i, (image, label) <span class="kw">in</span> <span class="bu">enumerate</span>(train_ds.take(<span class="dv">9</span>)):</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>    ax <span class="op">=</span> plt.subplot(<span class="dv">3</span>, <span class="dv">3</span>, i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>    plt.imshow(image)</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>    plt.title(<span class="bu">int</span>(label))</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>    plt.axis(<span class="st">"off"</span>)</span></code></pre></div>
<p>We can also see that label 1 is “dog” and label 0 is “cat”.</p>
</div>
<div class="section level3">
<h3 id="standardizing-the-data">Standardizing the data<a class="anchor" aria-label="anchor" href="#standardizing-the-data"></a>
</h3>
<p>Our raw images have a variety of sizes. In addition, each pixel
consists of 3 integer values between 0 and 255 (RGB level values). This
isn’t a great fit for feeding a neural network. We need to do 2
things:</p>
<ul>
<li>Standardize to a fixed image size. We pick 150x150.</li>
<li>Normalize pixel values between -1 and 1. We’ll do this using a
<code>Normalization</code> layer as part of the model itself.</li>
</ul>
<p>In general, it’s a good practice to develop models that take raw data
as input, as opposed to models that take already-preprocessed data. The
reason being that, if your model expects preprocessed data, any time you
export your model to use it elsewhere (in a web browser, in a mobile
app), you’ll need to reimplement the exact same preprocessing pipeline.
This gets very tricky very quickly. So we should do the least possible
amount of preprocessing before hitting the model.</p>
<p>Here, we’ll do image resizing in the data pipeline (because a deep
neural network can only process contiguous batches of data), and we’ll
do the input value scaling as part of the model, when we create it.</p>
<p>Let’s resize images to 150x150:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>resize_fn <span class="op">=</span> keras.layers.Resizing(<span class="dv">150</span>, <span class="dv">150</span>)</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>train_ds <span class="op">=</span> train_ds.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: (resize_fn(x), y))</span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>validation_ds <span class="op">=</span> validation_ds.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: (resize_fn(x), y))</span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>test_ds <span class="op">=</span> test_ds.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: (resize_fn(x), y))</span></code></pre></div>
</div>
<div class="section level3">
<h3 id="using-random-data-augmentation">Using random data augmentation<a class="anchor" aria-label="anchor" href="#using-random-data-augmentation"></a>
</h3>
<p>When you don’t have a large image dataset, it’s a good practice to
artificially introduce sample diversity by applying random yet realistic
transformations to the training images, such as random horizontal
flipping or small random rotations. This helps expose the model to
different aspects of the training data while slowing down
overfitting.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>augmentation_layers <span class="op">=</span> [</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>    layers.RandomFlip(<span class="st">"horizontal"</span>),</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>    layers.RandomRotation(<span class="fl">0.1</span>),</span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>]</span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a><span class="kw">def</span> data_augmentation(x):</span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a>    <span class="cf">for</span> layer <span class="kw">in</span> augmentation_layers:</span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a>        x <span class="op">=</span> layer(x)</span>
<span id="cb15-10"><a href="#cb15-10" tabindex="-1"></a>    <span class="cf">return</span> x</span>
<span id="cb15-11"><a href="#cb15-11" tabindex="-1"></a></span>
<span id="cb15-12"><a href="#cb15-12" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" tabindex="-1"></a>train_ds <span class="op">=</span> train_ds.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: (data_augmentation(x), y))</span></code></pre></div>
<p>Let’s batch the data and use prefetching to optimize loading
speed.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> data <span class="im">as</span> tf_data</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>train_ds <span class="op">=</span> train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()</span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a>validation_ds <span class="op">=</span> (</span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a>    validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()</span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a>)</span>
<span id="cb16-9"><a href="#cb16-9" tabindex="-1"></a>test_ds <span class="op">=</span> test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()</span></code></pre></div>
<p>Let’s visualize what the first image of the first batch looks like
after various random transformations:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="cf">for</span> images, labels <span class="kw">in</span> train_ds.take(<span class="dv">1</span>):</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a>    first_image <span class="op">=</span> images[<span class="dv">0</span>]</span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">9</span>):</span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a>        ax <span class="op">=</span> plt.subplot(<span class="dv">3</span>, <span class="dv">3</span>, i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a>        augmented_image <span class="op">=</span> data_augmentation(np.expand_dims(first_image, <span class="dv">0</span>))</span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a>        plt.imshow(np.array(augmented_image[<span class="dv">0</span>]).astype(<span class="st">"int32"</span>))</span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a>        plt.title(<span class="bu">int</span>(labels[<span class="dv">0</span>]))</span>
<span id="cb17-9"><a href="#cb17-9" tabindex="-1"></a>        plt.axis(<span class="st">"off"</span>)</span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="build-a-model">Build a model<a class="anchor" aria-label="anchor" href="#build-a-model"></a>
</h2>
<p>Now let’s built a model that follows the blueprint we’ve explained
earlier.</p>
<p>Note that:</p>
<ul>
<li>We add a <code>Rescaling</code> layer to scale input values
(initially in the <code>[0, 255]</code> range) to the
<code>[-1, 1]</code> range.</li>
<li>We add a <code>Dropout</code> layer before the classification layer,
for regularization.</li>
<li>We make sure to pass <code>training=False</code> when calling the
base model, so that it runs in inference mode, so that batchnorm
statistics don’t get updated even after we unfreeze the base model for
fine-tuning.</li>
</ul>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a>base_model <span class="op">=</span> keras.applications.Xception(</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a>    weights<span class="op">=</span><span class="st">"imagenet"</span>,  <span class="co"># Load weights pre-trained on ImageNet.</span></span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>    input_shape<span class="op">=</span>(<span class="dv">150</span>, <span class="dv">150</span>, <span class="dv">3</span>),</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>    include_top<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a>)  <span class="co"># Do not include the ImageNet classifier at the top.</span></span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a><span class="co"># Freeze the base_model</span></span>
<span id="cb18-8"><a href="#cb18-8" tabindex="-1"></a>base_model.trainable <span class="op">=</span> <span class="va">False</span></span>
<span id="cb18-9"><a href="#cb18-9" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" tabindex="-1"></a><span class="co"># Create new model on top</span></span>
<span id="cb18-11"><a href="#cb18-11" tabindex="-1"></a>inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">150</span>, <span class="dv">150</span>, <span class="dv">3</span>))</span>
<span id="cb18-12"><a href="#cb18-12" tabindex="-1"></a></span>
<span id="cb18-13"><a href="#cb18-13" tabindex="-1"></a><span class="co"># Pre-trained Xception weights requires that input be scaled</span></span>
<span id="cb18-14"><a href="#cb18-14" tabindex="-1"></a><span class="co"># from (0, 255) to a range of (-1., +1.), the rescaling layer</span></span>
<span id="cb18-15"><a href="#cb18-15" tabindex="-1"></a><span class="co"># outputs: `(inputs * scale) + offset`</span></span>
<span id="cb18-16"><a href="#cb18-16" tabindex="-1"></a>scale_layer <span class="op">=</span> keras.layers.Rescaling(scale<span class="op">=</span><span class="dv">1</span> <span class="op">/</span> <span class="fl">127.5</span>, offset<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb18-17"><a href="#cb18-17" tabindex="-1"></a>x <span class="op">=</span> scale_layer(inputs)</span>
<span id="cb18-18"><a href="#cb18-18" tabindex="-1"></a></span>
<span id="cb18-19"><a href="#cb18-19" tabindex="-1"></a><span class="co"># The base model contains batchnorm layers. We want to keep them in inference mode</span></span>
<span id="cb18-20"><a href="#cb18-20" tabindex="-1"></a><span class="co"># when we unfreeze the base model for fine-tuning, so we make sure that the</span></span>
<span id="cb18-21"><a href="#cb18-21" tabindex="-1"></a><span class="co"># base_model is running in inference mode here.</span></span>
<span id="cb18-22"><a href="#cb18-22" tabindex="-1"></a>x <span class="op">=</span> base_model(x, training<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb18-23"><a href="#cb18-23" tabindex="-1"></a>x <span class="op">=</span> keras.layers.GlobalAveragePooling2D()(x)</span>
<span id="cb18-24"><a href="#cb18-24" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dropout(<span class="fl">0.2</span>)(x)  <span class="co"># Regularize with dropout</span></span>
<span id="cb18-25"><a href="#cb18-25" tabindex="-1"></a>outputs <span class="op">=</span> keras.layers.Dense(<span class="dv">1</span>)(x)</span>
<span id="cb18-26"><a href="#cb18-26" tabindex="-1"></a>model <span class="op">=</span> keras.Model(inputs, outputs)</span>
<span id="cb18-27"><a href="#cb18-27" tabindex="-1"></a></span>
<span id="cb18-28"><a href="#cb18-28" tabindex="-1"></a>model.summary(show_trainable<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="train-the-top-layer">Train the top layer<a class="anchor" aria-label="anchor" href="#train-the-top-layer"></a>
</h2>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>    optimizer<span class="op">=</span>keras.optimizers.Adam(),</span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a>    loss<span class="op">=</span>keras.losses.BinaryCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a>    metrics<span class="op">=</span>[keras.metrics.BinaryAccuracy()],</span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a>)</span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb19-8"><a href="#cb19-8" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Fitting the top layer of the model"</span>)</span>
<span id="cb19-9"><a href="#cb19-9" tabindex="-1"></a>model.fit(train_ds, epochs<span class="op">=</span>epochs, validation_data<span class="op">=</span>validation_ds)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="do-a-round-of-fine-tuning-of-the-entire-model">Do a round of fine-tuning of the entire model<a class="anchor" aria-label="anchor" href="#do-a-round-of-fine-tuning-of-the-entire-model"></a>
</h2>
<p>Finally, let’s unfreeze the base model and train the entire model
end-to-end with a low learning rate.</p>
<p>Importantly, although the base model becomes trainable, it is still
running in inference mode since we passed <code>training=False</code>
when calling it when we built the model. This means that the batch
normalization layers inside won’t update their batch statistics. If they
did, they would wreck havoc on the representations learned by the model
so far.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="co"># Unfreeze the base_model. Note that it keeps running in inference mode</span></span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a><span class="co"># since we passed `training=False` when calling it. This means that</span></span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a><span class="co"># the batchnorm layers will not update their batch statistics.</span></span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a><span class="co"># This prevents the batchnorm layers from undoing all the training</span></span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a><span class="co"># we've done so far.</span></span>
<span id="cb20-6"><a href="#cb20-6" tabindex="-1"></a>base_model.trainable <span class="op">=</span> <span class="va">True</span></span>
<span id="cb20-7"><a href="#cb20-7" tabindex="-1"></a>model.summary(show_trainable<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-8"><a href="#cb20-8" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb20-10"><a href="#cb20-10" tabindex="-1"></a>    optimizer<span class="op">=</span>keras.optimizers.Adam(<span class="fl">1e-5</span>),  <span class="co"># Low learning rate</span></span>
<span id="cb20-11"><a href="#cb20-11" tabindex="-1"></a>    loss<span class="op">=</span>keras.losses.BinaryCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb20-12"><a href="#cb20-12" tabindex="-1"></a>    metrics<span class="op">=</span>[keras.metrics.BinaryAccuracy()],</span>
<span id="cb20-13"><a href="#cb20-13" tabindex="-1"></a>)</span>
<span id="cb20-14"><a href="#cb20-14" tabindex="-1"></a></span>
<span id="cb20-15"><a href="#cb20-15" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb20-16"><a href="#cb20-16" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Fitting the end-to-end model"</span>)</span>
<span id="cb20-17"><a href="#cb20-17" tabindex="-1"></a>model.fit(train_ds, epochs<span class="op">=</span>epochs, validation_data<span class="op">=</span>validation_ds)</span></code></pre></div>
<p>After 10 epochs, fine-tuning gains us a nice improvement here. Let’s
evaluate the model on the test dataset:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test dataset evaluation"</span>)</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a>model.evaluate(test_ds)</span></code></pre></div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, RStudio, Google.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
