<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>The Functional API • keras</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../../bootstrap-toc.css">
<script src="../../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../../pkgdown.css" rel="stylesheet">
<script src="../../pkgdown.js"></script><link href="../../extra.css" rel="stylesheet">
<script src="../../extra.js"></script><meta property="og:title" content="The Functional API">
<meta property="og:description" content="Complete guide to the functional API.">
<meta property="og:image" content="/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../../index.html">keras</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">2.13.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../../index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Tutorials
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/getting_started.html">Getting Started</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    </li>
    <li>
      <a href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    </li>
    <li>
      <a href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li class="dropdown-header">Guides (New for TF 2.6)</li>
    <li>
      <a href="../../articles/new-guides/python_subclasses.html">Python Subclasses</a>
    </li>
    <li>
      <a href="../../articles/new-guides/making_new_layers_and_models_via_subclassing.html">Making New Layers and Models via Subclassing</a>
    </li>
    <li>
      <a href="../../articles/new-guides/customizing_what_happens_in_fit.html">Customizing What Happens in Fit</a>
    </li>
    <li>
      <a href="../../articles/new-guides/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    </li>
    <li>
      <a href="../../articles/new-guides/preprocessing_layers.html">Working with Preprocessing Layers</a>
    </li>
    <li>
      <a href="../../argicles/new-guides/working_with_rnns.html">Working with RNNs</a>
    </li>
    <li class="divider">
    </li>
<li class="dropdown-header">Using Keras</li>
    <li>
      <a href="../../articles/guide_keras.html">Guide to Keras Basics</a>
    </li>
    <li>
      <a href="../../articles/sequential_model.html">Sequential Model in Depth</a>
    </li>
    <li>
      <a href="../../articles/functional_api.html">Functional API in Depth</a>
    </li>
    <li>
      <a href="../../articles/about_keras_models.html">About Keras Models</a>
    </li>
    <li>
      <a href="../../articles/about_keras_layers.html">About Keras Layers</a>
    </li>
    <li>
      <a href="../../articles/training_visualization.html">Training Visualization</a>
    </li>
    <li>
      <a href="../../articles/applications.html">Pre-Trained Models</a>
    </li>
    <li>
      <a href="../../articles/faq.html">Frequently Asked Questions</a>
    </li>
    <li>
      <a href="../../articles/why_use_keras.html">Why Use Keras?</a>
    </li>
    <li class="divider">
    </li>
<li class="dropdown-header">Advanced</li>
    <li>
      <a href="../../articles/eager_guide.html">Eager Execution</a>
    </li>
    <li>
      <a href="../../articles/training_callbacks.html">Training Callbacks</a>
    </li>
    <li>
      <a href="../../articles/backend.html">Keras Backend</a>
    </li>
    <li>
      <a href="../../articles/custom_layers.html">Custom Layers</a>
    </li>
    <li>
      <a href="../../articles/custom_models.html">Custom Models</a>
    </li>
    <li>
      <a href="../../articles/saving_serializing.html">Saving and serializing</a>
    </li>
  </ul>
</li>
<li>
  <a href="../../articles/learn.html">Learn</a>
</li>
<li>
  <a href="../../articles/tools.html">Tools</a>
</li>
<li>
  <a href="../../articles/examples/index.html">Examples</a>
</li>
<li>
  <a href="../../reference/index.html">Reference</a>
</li>
<li>
  <a href="../../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/rstudio/keras/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>The Functional API</h1>
                        <h4 data-toc-skip class="author"><a href="https://twitter.com/fchollet" class="external-link">fchollet</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/guides/functional_api.Rmd" class="external-link"><code>vignettes/guides/functional_api.Rmd</code></a></small>
      <div class="hidden name"><code>functional_api.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> ops</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>The Keras <em>functional API</em> is a way to create models that are
more flexible than the <code>keras.Sequential</code> API. The functional
API can handle models with non-linear topology, shared layers, and even
multiple inputs or outputs.</p>
<p>The main idea is that a deep learning model is usually a directed
acyclic graph (DAG) of layers. So the functional API is a way to build
<em>graphs of layers</em>.</p>
<p>Consider the following model:</p>
<div class="k-default-codeblock">
<pre><code>(input: 784-dimensional vectors)
       ↧
[Dense (64 units, relu activation)]
       ↧
[Dense (64 units, relu activation)]
       ↧
[Dense (10 units, softmax activation)]
       ↧
(output: logits of a probability distribution over 10 classes)</code></pre>
</div>
<p>This is a basic graph with three layers. To build this model using
the functional API, start by creating an input node:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">784</span>,))</span></code></pre></div>
<p>The shape of the data is set as a 784-dimensional vector. The batch
size is always omitted since only the shape of each sample is
specified.</p>
<p>If, for example, you have an image input with a shape of
<code>(32, 32, 3)</code>, you would use:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="co"># Just for demonstration purposes.</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>img_inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">32</span>, <span class="dv">32</span>, <span class="dv">3</span>))</span></code></pre></div>
<p>The <code>inputs</code> that is returned contains information about
the shape and <code>dtype</code> of the input data that you feed to your
model. Here’s the shape:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>inputs.shape</span></code></pre></div>
<p>Here’s the dtype:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>inputs.dtype</span></code></pre></div>
<p>You create a new node in the graph of layers by calling a layer on
this <code>inputs</code> object:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>dense <span class="op">=</span> layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">"relu"</span>)</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>x <span class="op">=</span> dense(inputs)</span></code></pre></div>
<p>The “layer call” action is like drawing an arrow from “inputs” to
this layer you created. You’re “passing” the inputs to the
<code>dense</code> layer, and you get <code>x</code> as the output.</p>
<p>Let’s add a few more layers to the graph of layers:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>x <span class="op">=</span> layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>outputs <span class="op">=</span> layers.Dense(<span class="dv">10</span>)(x)</span></code></pre></div>
<p>At this point, you can create a <code>Model</code> by specifying its
inputs and outputs in the graph of layers:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs, name<span class="op">=</span><span class="st">"mnist_model"</span>)</span></code></pre></div>
<p>Let’s check out what the model summary looks like:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>model.summary()</span></code></pre></div>
<p>You can also plot the model as a graph:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>keras.utils.plot_model(model, <span class="st">"my_first_model.png"</span>)</span></code></pre></div>
<p>And, optionally, display the input and output shapes of each layer in
the plotted graph:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>keras.utils.plot_model(</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>    model, <span class="st">"my_first_model_with_shape_info.png"</span>, show_shapes<span class="op">=</span><span class="va">True</span></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>)</span></code></pre></div>
<p>This figure and the code are almost identical. In the code version,
the connection arrows are replaced by the call operation.</p>
<p>A “graph of layers” is an intuitive mental image for a deep learning
model, and the functional API is a way to create models that closely
mirrors this.</p>
</div>
<div class="section level2">
<h2 id="training-evaluation-and-inference">Training, evaluation, and inference<a class="anchor" aria-label="anchor" href="#training-evaluation-and-inference"></a>
</h2>
<p>Training, evaluation, and inference work exactly in the same way for
models built using the functional API as for <code>Sequential</code>
models.</p>
<p>The <code>Model</code> class offers a built-in training loop (the
<code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code> method) and a built-in evaluation loop (the
<code><a href="https://rdrr.io/pkg/tensorflow/man/evaluate.html" class="external-link">evaluate()</a></code> method). Note that you can easily <a href="/guides/customizing_what_happens_in_fit/">customize these
loops</a> to implement training routines beyond supervised learning
(e.g. <a href="https://keras.io/examples/generative/dcgan_overriding_train_step/" class="external-link">GANs</a>).</p>
<p>Here, load the MNIST image data, reshape it into vectors, fit the
model on the data (while monitoring performance on a validation split),
then evaluate the model on the test data:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>(x_train, y_train), (x_test, y_test) <span class="op">=</span> keras.datasets.mnist.load_data()</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>x_train <span class="op">=</span> x_train.reshape(<span class="dv">60000</span>, <span class="dv">784</span>).astype(<span class="st">"float32"</span>) <span class="op">/</span> <span class="dv">255</span></span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>x_test <span class="op">=</span> x_test.reshape(<span class="dv">10000</span>, <span class="dv">784</span>).astype(<span class="st">"float32"</span>) <span class="op">/</span> <span class="dv">255</span></span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a>    loss<span class="op">=</span>keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a>    optimizer<span class="op">=</span>keras.optimizers.RMSprop(),</span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">"accuracy"</span>],</span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a>)</span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" tabindex="-1"></a>history <span class="op">=</span> model.fit(</span>
<span id="cb13-13"><a href="#cb13-13" tabindex="-1"></a>    x_train, y_train, batch_size<span class="op">=</span><span class="dv">64</span>, epochs<span class="op">=</span><span class="dv">2</span>, validation_split<span class="op">=</span><span class="fl">0.2</span></span>
<span id="cb13-14"><a href="#cb13-14" tabindex="-1"></a>)</span>
<span id="cb13-15"><a href="#cb13-15" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" tabindex="-1"></a>test_scores <span class="op">=</span> model.evaluate(x_test, y_test, verbose<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb13-17"><a href="#cb13-17" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test loss:"</span>, test_scores[<span class="dv">0</span>])</span>
<span id="cb13-18"><a href="#cb13-18" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test accuracy:"</span>, test_scores[<span class="dv">1</span>])</span></code></pre></div>
<p>For further reading, see the <a href="/guides/training_with_built_in_methods/">training and
evaluation</a> guide.</p>
</div>
<div class="section level2">
<h2 id="save-and-serialize">Save and serialize<a class="anchor" aria-label="anchor" href="#save-and-serialize"></a>
</h2>
<p>Saving the model and serialization work the same way for models built
using the functional API as they do for <code>Sequential</code> models.
The standard way to save a functional model is to call
<code>model.save()</code> to save the entire model as a single file. You
can later recreate the same model from this file, even if the code that
built the model is no longer available.</p>
<p>This saved file includes the: - model architecture - model weight
values (that were learned during training) - model training config, if
any (as passed to <code><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile()</a></code>) - optimizer and its state, if
any (to restart training where you left off)</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>model.save(<span class="st">"my_model.keras"</span>)</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="kw">del</span> model</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a><span class="co"># Recreate the exact same model purely from the file:</span></span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>model <span class="op">=</span> keras.models.load_model(<span class="st">"my_model.keras"</span>)</span></code></pre></div>
<p>For details, read the model <a href="/guides/serialization_and_saving/">serialization &amp; saving</a>
guide.</p>
</div>
<div class="section level2">
<h2 id="use-the-same-graph-of-layers-to-define-multiple-models">Use the same graph of layers to define multiple models<a class="anchor" aria-label="anchor" href="#use-the-same-graph-of-layers-to-define-multiple-models"></a>
</h2>
<p>In the functional API, models are created by specifying their inputs
and outputs in a graph of layers. That means that a single graph of
layers can be used to generate multiple models.</p>
<p>In the example below, you use the same stack of layers to instantiate
two models: an <code>encoder</code> model that turns image inputs into
16-dimensional vectors, and an end-to-end <code>autoencoder</code> model
for training.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>encoder_input <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>), name<span class="op">=</span><span class="st">"img"</span>)</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">16</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(encoder_input)</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">32</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>x <span class="op">=</span> layers.MaxPooling2D(<span class="dv">3</span>)(x)</span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">32</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">16</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a>encoder_output <span class="op">=</span> layers.GlobalMaxPooling2D()(x)</span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a>encoder <span class="op">=</span> keras.Model(encoder_input, encoder_output, name<span class="op">=</span><span class="st">"encoder"</span>)</span>
<span id="cb15-10"><a href="#cb15-10" tabindex="-1"></a>encoder.summary()</span>
<span id="cb15-11"><a href="#cb15-11" tabindex="-1"></a></span>
<span id="cb15-12"><a href="#cb15-12" tabindex="-1"></a>x <span class="op">=</span> layers.Reshape((<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">1</span>))(encoder_output)</span>
<span id="cb15-13"><a href="#cb15-13" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2DTranspose(<span class="dv">16</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb15-14"><a href="#cb15-14" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2DTranspose(<span class="dv">32</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb15-15"><a href="#cb15-15" tabindex="-1"></a>x <span class="op">=</span> layers.UpSampling2D(<span class="dv">3</span>)(x)</span>
<span id="cb15-16"><a href="#cb15-16" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2DTranspose(<span class="dv">16</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb15-17"><a href="#cb15-17" tabindex="-1"></a>decoder_output <span class="op">=</span> layers.Conv2DTranspose(<span class="dv">1</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb15-18"><a href="#cb15-18" tabindex="-1"></a></span>
<span id="cb15-19"><a href="#cb15-19" tabindex="-1"></a>autoencoder <span class="op">=</span> keras.Model(encoder_input, decoder_output, name<span class="op">=</span><span class="st">"autoencoder"</span>)</span>
<span id="cb15-20"><a href="#cb15-20" tabindex="-1"></a>autoencoder.summary()</span></code></pre></div>
<p>Here, the decoding architecture is strictly symmetrical to the
encoding architecture, so the output shape is the same as the input
shape <code>(28, 28, 1)</code>.</p>
<p>The reverse of a <code>Conv2D</code> layer is a
<code>Conv2DTranspose</code> layer, and the reverse of a
<code>MaxPooling2D</code> layer is an <code>UpSampling2D</code>
layer.</p>
</div>
<div class="section level2">
<h2 id="all-models-are-callable-just-like-layers">All models are callable, just like layers<a class="anchor" aria-label="anchor" href="#all-models-are-callable-just-like-layers"></a>
</h2>
<p>You can treat any model as if it were a layer by invoking it on an
<code>Input</code> or on the output of another layer. By calling a model
you aren’t just reusing the architecture of the model, you’re also
reusing its weights.</p>
<p>To see this in action, here’s a different take on the autoencoder
example that creates an encoder model, a decoder model, and chains them
in two calls to obtain the autoencoder model:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>encoder_input <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>), name<span class="op">=</span><span class="st">"original_img"</span>)</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">16</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(encoder_input)</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">32</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>x <span class="op">=</span> layers.MaxPooling2D(<span class="dv">3</span>)(x)</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">32</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">16</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a>encoder_output <span class="op">=</span> layers.GlobalMaxPooling2D()(x)</span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" tabindex="-1"></a>encoder <span class="op">=</span> keras.Model(encoder_input, encoder_output, name<span class="op">=</span><span class="st">"encoder"</span>)</span>
<span id="cb16-10"><a href="#cb16-10" tabindex="-1"></a>encoder.summary()</span>
<span id="cb16-11"><a href="#cb16-11" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" tabindex="-1"></a>decoder_input <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">16</span>,), name<span class="op">=</span><span class="st">"encoded_img"</span>)</span>
<span id="cb16-13"><a href="#cb16-13" tabindex="-1"></a>x <span class="op">=</span> layers.Reshape((<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">1</span>))(decoder_input)</span>
<span id="cb16-14"><a href="#cb16-14" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2DTranspose(<span class="dv">16</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb16-15"><a href="#cb16-15" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2DTranspose(<span class="dv">32</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb16-16"><a href="#cb16-16" tabindex="-1"></a>x <span class="op">=</span> layers.UpSampling2D(<span class="dv">3</span>)(x)</span>
<span id="cb16-17"><a href="#cb16-17" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2DTranspose(<span class="dv">16</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb16-18"><a href="#cb16-18" tabindex="-1"></a>decoder_output <span class="op">=</span> layers.Conv2DTranspose(<span class="dv">1</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb16-19"><a href="#cb16-19" tabindex="-1"></a></span>
<span id="cb16-20"><a href="#cb16-20" tabindex="-1"></a>decoder <span class="op">=</span> keras.Model(decoder_input, decoder_output, name<span class="op">=</span><span class="st">"decoder"</span>)</span>
<span id="cb16-21"><a href="#cb16-21" tabindex="-1"></a>decoder.summary()</span>
<span id="cb16-22"><a href="#cb16-22" tabindex="-1"></a></span>
<span id="cb16-23"><a href="#cb16-23" tabindex="-1"></a>autoencoder_input <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>), name<span class="op">=</span><span class="st">"img"</span>)</span>
<span id="cb16-24"><a href="#cb16-24" tabindex="-1"></a>encoded_img <span class="op">=</span> encoder(autoencoder_input)</span>
<span id="cb16-25"><a href="#cb16-25" tabindex="-1"></a>decoded_img <span class="op">=</span> decoder(encoded_img)</span>
<span id="cb16-26"><a href="#cb16-26" tabindex="-1"></a>autoencoder <span class="op">=</span> keras.Model(autoencoder_input, decoded_img, name<span class="op">=</span><span class="st">"autoencoder"</span>)</span>
<span id="cb16-27"><a href="#cb16-27" tabindex="-1"></a>autoencoder.summary()</span></code></pre></div>
<p>As you can see, the model can be nested: a model can contain
sub-models (since a model is just like a layer). A common use case for
model nesting is <em>ensembling</em>. For example, here’s how to
ensemble a set of models into a single model that averages their
predictions:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="kw">def</span> get_model():</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>    inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">128</span>,))</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a>    outputs <span class="op">=</span> layers.Dense(<span class="dv">1</span>)(inputs)</span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>    <span class="cf">return</span> keras.Model(inputs, outputs)</span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a>model1 <span class="op">=</span> get_model()</span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a>model2 <span class="op">=</span> get_model()</span>
<span id="cb17-9"><a href="#cb17-9" tabindex="-1"></a>model3 <span class="op">=</span> get_model()</span>
<span id="cb17-10"><a href="#cb17-10" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" tabindex="-1"></a>inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">128</span>,))</span>
<span id="cb17-12"><a href="#cb17-12" tabindex="-1"></a>y1 <span class="op">=</span> model1(inputs)</span>
<span id="cb17-13"><a href="#cb17-13" tabindex="-1"></a>y2 <span class="op">=</span> model2(inputs)</span>
<span id="cb17-14"><a href="#cb17-14" tabindex="-1"></a>y3 <span class="op">=</span> model3(inputs)</span>
<span id="cb17-15"><a href="#cb17-15" tabindex="-1"></a>outputs <span class="op">=</span> layers.average([y1, y2, y3])</span>
<span id="cb17-16"><a href="#cb17-16" tabindex="-1"></a>ensemble_model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="manipulate-complex-graph-topologies">Manipulate complex graph topologies<a class="anchor" aria-label="anchor" href="#manipulate-complex-graph-topologies"></a>
</h2>
<div class="section level3">
<h3 id="models-with-multiple-inputs-and-outputs">Models with multiple inputs and outputs<a class="anchor" aria-label="anchor" href="#models-with-multiple-inputs-and-outputs"></a>
</h3>
<p>The functional API makes it easy to manipulate multiple inputs and
outputs. This cannot be handled with the <code>Sequential</code>
API.</p>
<p>For example, if you’re building a system for ranking customer issue
tickets by priority and routing them to the correct department, then the
model will have three inputs:</p>
<ul>
<li>the title of the ticket (text input),</li>
<li>the text body of the ticket (text input), and</li>
<li>any tags added by the user (categorical input)</li>
</ul>
<p>This model will have two outputs:</p>
<ul>
<li>the priority score between 0 and 1 (scalar sigmoid output), and</li>
<li>the department that should handle the ticket (softmax output over
the set of departments).</li>
</ul>
<p>You can build this model in a few lines with the functional API:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a>num_tags <span class="op">=</span> <span class="dv">12</span>  <span class="co"># Number of unique issue tags</span></span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a>num_words <span class="op">=</span> <span class="dv">10000</span>  <span class="co"># Size of vocabulary obtained when preprocessing text data</span></span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>num_departments <span class="op">=</span> <span class="dv">4</span>  <span class="co"># Number of departments for predictions</span></span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a>title_input <span class="op">=</span> keras.Input(</span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a>    shape<span class="op">=</span>(<span class="va">None</span>,), name<span class="op">=</span><span class="st">"title"</span></span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a>)  <span class="co"># Variable-length sequence of ints</span></span>
<span id="cb18-8"><a href="#cb18-8" tabindex="-1"></a>body_input <span class="op">=</span> keras.Input(</span>
<span id="cb18-9"><a href="#cb18-9" tabindex="-1"></a>    shape<span class="op">=</span>(<span class="va">None</span>,), name<span class="op">=</span><span class="st">"body"</span></span>
<span id="cb18-10"><a href="#cb18-10" tabindex="-1"></a>)  <span class="co"># Variable-length sequence of ints</span></span>
<span id="cb18-11"><a href="#cb18-11" tabindex="-1"></a>tags_input <span class="op">=</span> keras.Input(</span>
<span id="cb18-12"><a href="#cb18-12" tabindex="-1"></a>    shape<span class="op">=</span>(num_tags,), name<span class="op">=</span><span class="st">"tags"</span></span>
<span id="cb18-13"><a href="#cb18-13" tabindex="-1"></a>)  <span class="co"># Binary vectors of size `num_tags`</span></span>
<span id="cb18-14"><a href="#cb18-14" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" tabindex="-1"></a><span class="co"># Embed each word in the title into a 64-dimensional vector</span></span>
<span id="cb18-16"><a href="#cb18-16" tabindex="-1"></a>title_features <span class="op">=</span> layers.Embedding(num_words, <span class="dv">64</span>)(title_input)</span>
<span id="cb18-17"><a href="#cb18-17" tabindex="-1"></a><span class="co"># Embed each word in the text into a 64-dimensional vector</span></span>
<span id="cb18-18"><a href="#cb18-18" tabindex="-1"></a>body_features <span class="op">=</span> layers.Embedding(num_words, <span class="dv">64</span>)(body_input)</span>
<span id="cb18-19"><a href="#cb18-19" tabindex="-1"></a></span>
<span id="cb18-20"><a href="#cb18-20" tabindex="-1"></a><span class="co"># Reduce sequence of embedded words in the title into a single 128-dimensional vector</span></span>
<span id="cb18-21"><a href="#cb18-21" tabindex="-1"></a>title_features <span class="op">=</span> layers.LSTM(<span class="dv">128</span>)(title_features)</span>
<span id="cb18-22"><a href="#cb18-22" tabindex="-1"></a><span class="co"># Reduce sequence of embedded words in the body into a single 32-dimensional vector</span></span>
<span id="cb18-23"><a href="#cb18-23" tabindex="-1"></a>body_features <span class="op">=</span> layers.LSTM(<span class="dv">32</span>)(body_features)</span>
<span id="cb18-24"><a href="#cb18-24" tabindex="-1"></a></span>
<span id="cb18-25"><a href="#cb18-25" tabindex="-1"></a><span class="co"># Merge all available features into a single large vector via concatenation</span></span>
<span id="cb18-26"><a href="#cb18-26" tabindex="-1"></a>x <span class="op">=</span> layers.concatenate([title_features, body_features, tags_input])</span>
<span id="cb18-27"><a href="#cb18-27" tabindex="-1"></a></span>
<span id="cb18-28"><a href="#cb18-28" tabindex="-1"></a><span class="co"># Stick a logistic regression for priority prediction on top of the features</span></span>
<span id="cb18-29"><a href="#cb18-29" tabindex="-1"></a>priority_pred <span class="op">=</span> layers.Dense(<span class="dv">1</span>, name<span class="op">=</span><span class="st">"priority"</span>)(x)</span>
<span id="cb18-30"><a href="#cb18-30" tabindex="-1"></a><span class="co"># Stick a department classifier on top of the features</span></span>
<span id="cb18-31"><a href="#cb18-31" tabindex="-1"></a>department_pred <span class="op">=</span> layers.Dense(num_departments, name<span class="op">=</span><span class="st">"department"</span>)(x)</span>
<span id="cb18-32"><a href="#cb18-32" tabindex="-1"></a></span>
<span id="cb18-33"><a href="#cb18-33" tabindex="-1"></a><span class="co"># Instantiate an end-to-end model predicting both priority and department</span></span>
<span id="cb18-34"><a href="#cb18-34" tabindex="-1"></a>model <span class="op">=</span> keras.Model(</span>
<span id="cb18-35"><a href="#cb18-35" tabindex="-1"></a>    inputs<span class="op">=</span>[title_input, body_input, tags_input],</span>
<span id="cb18-36"><a href="#cb18-36" tabindex="-1"></a>    outputs<span class="op">=</span>{<span class="st">"priority"</span>: priority_pred, <span class="st">"department"</span>: department_pred},</span>
<span id="cb18-37"><a href="#cb18-37" tabindex="-1"></a>)</span></code></pre></div>
<p>Now plot the model:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>keras.utils.plot_model(</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>    model, <span class="st">"multi_input_and_output_model.png"</span>, show_shapes<span class="op">=</span><span class="va">True</span></span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a>)</span></code></pre></div>
<p>When compiling this model, you can assign different losses to each
output. You can even assign different weights to each loss – to modulate
their contribution to the total training loss.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a>    optimizer<span class="op">=</span>keras.optimizers.RMSprop(<span class="fl">1e-3</span>),</span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a>    loss<span class="op">=</span>[</span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a>        keras.losses.BinaryCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a>        keras.losses.CategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb20-6"><a href="#cb20-6" tabindex="-1"></a>    ],</span>
<span id="cb20-7"><a href="#cb20-7" tabindex="-1"></a>    loss_weights<span class="op">=</span>[<span class="fl">1.0</span>, <span class="fl">0.2</span>],</span>
<span id="cb20-8"><a href="#cb20-8" tabindex="-1"></a>)</span></code></pre></div>
<p>Since the output layers have different names, you could also specify
the losses and loss weights with the corresponding layer names:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a>    optimizer<span class="op">=</span>keras.optimizers.RMSprop(<span class="fl">1e-3</span>),</span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a>    loss<span class="op">=</span>{</span>
<span id="cb21-4"><a href="#cb21-4" tabindex="-1"></a>        <span class="st">"priority"</span>: keras.losses.BinaryCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb21-5"><a href="#cb21-5" tabindex="-1"></a>        <span class="st">"department"</span>: keras.losses.CategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb21-6"><a href="#cb21-6" tabindex="-1"></a>    },</span>
<span id="cb21-7"><a href="#cb21-7" tabindex="-1"></a>    loss_weights<span class="op">=</span>{<span class="st">"priority"</span>: <span class="fl">1.0</span>, <span class="st">"department"</span>: <span class="fl">0.2</span>},</span>
<span id="cb21-8"><a href="#cb21-8" tabindex="-1"></a>)</span></code></pre></div>
<p>Train the model by passing lists of NumPy arrays of inputs and
targets:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a><span class="co"># Dummy input data</span></span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a>title_data <span class="op">=</span> np.random.randint(num_words, size<span class="op">=</span>(<span class="dv">1280</span>, <span class="dv">10</span>))</span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a>body_data <span class="op">=</span> np.random.randint(num_words, size<span class="op">=</span>(<span class="dv">1280</span>, <span class="dv">100</span>))</span>
<span id="cb22-4"><a href="#cb22-4" tabindex="-1"></a>tags_data <span class="op">=</span> np.random.randint(<span class="dv">2</span>, size<span class="op">=</span>(<span class="dv">1280</span>, num_tags)).astype(<span class="st">"float32"</span>)</span>
<span id="cb22-5"><a href="#cb22-5" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" tabindex="-1"></a><span class="co"># Dummy target data</span></span>
<span id="cb22-7"><a href="#cb22-7" tabindex="-1"></a>priority_targets <span class="op">=</span> np.random.random(size<span class="op">=</span>(<span class="dv">1280</span>, <span class="dv">1</span>))</span>
<span id="cb22-8"><a href="#cb22-8" tabindex="-1"></a>dept_targets <span class="op">=</span> np.random.randint(<span class="dv">2</span>, size<span class="op">=</span>(<span class="dv">1280</span>, num_departments))</span>
<span id="cb22-9"><a href="#cb22-9" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" tabindex="-1"></a>model.fit(</span>
<span id="cb22-11"><a href="#cb22-11" tabindex="-1"></a>    {<span class="st">"title"</span>: title_data, <span class="st">"body"</span>: body_data, <span class="st">"tags"</span>: tags_data},</span>
<span id="cb22-12"><a href="#cb22-12" tabindex="-1"></a>    {<span class="st">"priority"</span>: priority_targets, <span class="st">"department"</span>: dept_targets},</span>
<span id="cb22-13"><a href="#cb22-13" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb22-14"><a href="#cb22-14" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb22-15"><a href="#cb22-15" tabindex="-1"></a>)</span></code></pre></div>
<p>When calling fit with a <code>Dataset</code> object, it should yield
either a tuple of lists like
<code>([title_data, body_data, tags_data], [priority_targets, dept_targets])</code>
or a tuple of dictionaries like
<code>({'title': title_data, 'body': body_data, 'tags': tags_data}, {'priority': priority_targets, 'department': dept_targets})</code>.</p>
<p>For more detailed explanation, refer to the <a href="/guides/training_with_built_in_methods/">training and
evaluation</a> guide.</p>
</div>
<div class="section level3">
<h3 id="a-toy-resnet-model">A toy ResNet model<a class="anchor" aria-label="anchor" href="#a-toy-resnet-model"></a>
</h3>
<p>In addition to models with multiple inputs and outputs, the
functional API makes it easy to manipulate non-linear connectivity
topologies – these are models with layers that are not connected
sequentially, which the <code>Sequential</code> API cannot handle.</p>
<p>A common use case for this is residual connections. Let’s build a toy
ResNet model for CIFAR10 to demonstrate this:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a>inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">32</span>, <span class="dv">32</span>, <span class="dv">3</span>), name<span class="op">=</span><span class="st">"img"</span>)</span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">32</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(inputs)</span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">64</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb23-4"><a href="#cb23-4" tabindex="-1"></a>block_1_output <span class="op">=</span> layers.MaxPooling2D(<span class="dv">3</span>)(x)</span>
<span id="cb23-5"><a href="#cb23-5" tabindex="-1"></a></span>
<span id="cb23-6"><a href="#cb23-6" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">64</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>, padding<span class="op">=</span><span class="st">"same"</span>)(block_1_output)</span>
<span id="cb23-7"><a href="#cb23-7" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">64</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>, padding<span class="op">=</span><span class="st">"same"</span>)(x)</span>
<span id="cb23-8"><a href="#cb23-8" tabindex="-1"></a>block_2_output <span class="op">=</span> layers.add([x, block_1_output])</span>
<span id="cb23-9"><a href="#cb23-9" tabindex="-1"></a></span>
<span id="cb23-10"><a href="#cb23-10" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">64</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>, padding<span class="op">=</span><span class="st">"same"</span>)(block_2_output)</span>
<span id="cb23-11"><a href="#cb23-11" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">64</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>, padding<span class="op">=</span><span class="st">"same"</span>)(x)</span>
<span id="cb23-12"><a href="#cb23-12" tabindex="-1"></a>block_3_output <span class="op">=</span> layers.add([x, block_2_output])</span>
<span id="cb23-13"><a href="#cb23-13" tabindex="-1"></a></span>
<span id="cb23-14"><a href="#cb23-14" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(<span class="dv">64</span>, <span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(block_3_output)</span>
<span id="cb23-15"><a href="#cb23-15" tabindex="-1"></a>x <span class="op">=</span> layers.GlobalAveragePooling2D()(x)</span>
<span id="cb23-16"><a href="#cb23-16" tabindex="-1"></a>x <span class="op">=</span> layers.Dense(<span class="dv">256</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb23-17"><a href="#cb23-17" tabindex="-1"></a>x <span class="op">=</span> layers.Dropout(<span class="fl">0.5</span>)(x)</span>
<span id="cb23-18"><a href="#cb23-18" tabindex="-1"></a>outputs <span class="op">=</span> layers.Dense(<span class="dv">10</span>)(x)</span>
<span id="cb23-19"><a href="#cb23-19" tabindex="-1"></a></span>
<span id="cb23-20"><a href="#cb23-20" tabindex="-1"></a>model <span class="op">=</span> keras.Model(inputs, outputs, name<span class="op">=</span><span class="st">"toy_resnet"</span>)</span>
<span id="cb23-21"><a href="#cb23-21" tabindex="-1"></a>model.summary()</span></code></pre></div>
<p>Plot the model:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a>keras.utils.plot_model(model, <span class="st">"mini_resnet.png"</span>, show_shapes<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<p>Now train the model:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a>(x_train, y_train), (x_test, y_test) <span class="op">=</span> keras.datasets.cifar10.load_data()</span>
<span id="cb25-2"><a href="#cb25-2" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" tabindex="-1"></a>x_train <span class="op">=</span> x_train.astype(<span class="st">"float32"</span>) <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb25-4"><a href="#cb25-4" tabindex="-1"></a>x_test <span class="op">=</span> x_test.astype(<span class="st">"float32"</span>) <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb25-5"><a href="#cb25-5" tabindex="-1"></a>y_train <span class="op">=</span> keras.utils.to_categorical(y_train, <span class="dv">10</span>)</span>
<span id="cb25-6"><a href="#cb25-6" tabindex="-1"></a>y_test <span class="op">=</span> keras.utils.to_categorical(y_test, <span class="dv">10</span>)</span>
<span id="cb25-7"><a href="#cb25-7" tabindex="-1"></a></span>
<span id="cb25-8"><a href="#cb25-8" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb25-9"><a href="#cb25-9" tabindex="-1"></a>    optimizer<span class="op">=</span>keras.optimizers.RMSprop(<span class="fl">1e-3</span>),</span>
<span id="cb25-10"><a href="#cb25-10" tabindex="-1"></a>    loss<span class="op">=</span>keras.losses.CategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb25-11"><a href="#cb25-11" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">"acc"</span>],</span>
<span id="cb25-12"><a href="#cb25-12" tabindex="-1"></a>)</span>
<span id="cb25-13"><a href="#cb25-13" tabindex="-1"></a><span class="co"># We restrict the data to the first 1000 samples so as to limit execution time</span></span>
<span id="cb25-14"><a href="#cb25-14" tabindex="-1"></a><span class="co"># on Colab. Try to train on the entire dataset until convergence!</span></span>
<span id="cb25-15"><a href="#cb25-15" tabindex="-1"></a>model.fit(</span>
<span id="cb25-16"><a href="#cb25-16" tabindex="-1"></a>    x_train[:<span class="dv">1000</span>],</span>
<span id="cb25-17"><a href="#cb25-17" tabindex="-1"></a>    y_train[:<span class="dv">1000</span>],</span>
<span id="cb25-18"><a href="#cb25-18" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb25-19"><a href="#cb25-19" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb25-20"><a href="#cb25-20" tabindex="-1"></a>    validation_split<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb25-21"><a href="#cb25-21" tabindex="-1"></a>)</span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="shared-layers">Shared layers<a class="anchor" aria-label="anchor" href="#shared-layers"></a>
</h2>
<p>Another good use for the functional API are models that use
<em>shared layers</em>. Shared layers are layer instances that are
reused multiple times in the same model – they learn features that
correspond to multiple paths in the graph-of-layers.</p>
<p>Shared layers are often used to encode inputs from similar spaces
(say, two different pieces of text that feature similar vocabulary).
They enable sharing of information across these different inputs, and
they make it possible to train such a model on less data. If a given
word is seen in one of the inputs, that will benefit the processing of
all inputs that pass through the shared layer.</p>
<p>To share a layer in the functional API, call the same layer instance
multiple times. For instance, here’s an <code>Embedding</code> layer
shared across two different text inputs:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a><span class="co"># Embedding for 1000 unique words mapped to 128-dimensional vectors</span></span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a>shared_embedding <span class="op">=</span> layers.Embedding(<span class="dv">1000</span>, <span class="dv">128</span>)</span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" tabindex="-1"></a><span class="co"># Variable-length sequence of integers</span></span>
<span id="cb26-5"><a href="#cb26-5" tabindex="-1"></a>text_input_a <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="va">None</span>,), dtype<span class="op">=</span><span class="st">"int32"</span>)</span>
<span id="cb26-6"><a href="#cb26-6" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" tabindex="-1"></a><span class="co"># Variable-length sequence of integers</span></span>
<span id="cb26-8"><a href="#cb26-8" tabindex="-1"></a>text_input_b <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="va">None</span>,), dtype<span class="op">=</span><span class="st">"int32"</span>)</span>
<span id="cb26-9"><a href="#cb26-9" tabindex="-1"></a></span>
<span id="cb26-10"><a href="#cb26-10" tabindex="-1"></a><span class="co"># Reuse the same layer to encode both inputs</span></span>
<span id="cb26-11"><a href="#cb26-11" tabindex="-1"></a>encoded_input_a <span class="op">=</span> shared_embedding(text_input_a)</span>
<span id="cb26-12"><a href="#cb26-12" tabindex="-1"></a>encoded_input_b <span class="op">=</span> shared_embedding(text_input_b)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="extract-and-reuse-nodes-in-the-graph-of-layers">Extract and reuse nodes in the graph of layers<a class="anchor" aria-label="anchor" href="#extract-and-reuse-nodes-in-the-graph-of-layers"></a>
</h2>
<p>Because the graph of layers you are manipulating is a static data
structure, it can be accessed and inspected. And this is how you are
able to plot functional models as images.</p>
<p>This also means that you can access the activations of intermediate
layers (“nodes” in the graph) and reuse them elsewhere – which is very
useful for something like feature extraction.</p>
<p>Let’s look at an example. This is a VGG19 model with weights
pretrained on ImageNet:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a>vgg19 <span class="op">=</span> keras.applications.VGG19()</span></code></pre></div>
<p>And these are the intermediate activations of the model, obtained by
querying the graph data structure:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a>features_list <span class="op">=</span> [layer.output <span class="cf">for</span> layer <span class="kw">in</span> vgg19.layers]</span></code></pre></div>
<p>Use these features to create a new feature-extraction model that
returns the values of the intermediate layer activations:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a>feat_extraction_model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>vgg19.<span class="bu">input</span>, outputs<span class="op">=</span>features_list)</span>
<span id="cb29-2"><a href="#cb29-2" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" tabindex="-1"></a>img <span class="op">=</span> np.random.random((<span class="dv">1</span>, <span class="dv">224</span>, <span class="dv">224</span>, <span class="dv">3</span>)).astype(<span class="st">"float32"</span>)</span>
<span id="cb29-4"><a href="#cb29-4" tabindex="-1"></a>extracted_features <span class="op">=</span> feat_extraction_model(img)</span></code></pre></div>
<p>This comes in handy for tasks like <a href="https://keras.io/examples/generative/neural_style_transfer/" class="external-link">neural
style transfer</a>, among other things.</p>
</div>
<div class="section level2">
<h2 id="extend-the-api-using-custom-layers">Extend the API using custom layers<a class="anchor" aria-label="anchor" href="#extend-the-api-using-custom-layers"></a>
</h2>
<p><code>keras</code> includes a wide range of built-in layers, for
example:</p>
<ul>
<li>Convolutional layers: <code>Conv1D</code>, <code>Conv2D</code>,
<code>Conv3D</code>, <code>Conv2DTranspose</code>
</li>
<li>Pooling layers: <code>MaxPooling1D</code>,
<code>MaxPooling2D</code>, <code>MaxPooling3D</code>,
<code>AveragePooling1D</code>
</li>
<li>RNN layers: <code>GRU</code>, <code>LSTM</code>,
<code>ConvLSTM2D</code>
</li>
<li>
<code>BatchNormalization</code>, <code>Dropout</code>,
<code>Embedding</code>, etc.</li>
</ul>
<p>But if you don’t find what you need, it’s easy to extend the API by
creating your own layers. All layers subclass the <code>Layer</code>
class and implement:</p>
<ul>
<li>
<code>call</code> method, that specifies the computation done by the
layer.</li>
<li>
<code>build</code> method, that creates the weights of the layer
(this is just a style convention since you can create weights in
<code>__init__</code>, as well).</li>
</ul>
<p>To learn more about creating layers from scratch, read <a href="/guides/making_new_layers_and_models_via_subclassing">custom
layers and models</a> guide.</p>
<p>The following is a basic implementation of
<code>keras.layers.Dense</code>:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a><span class="kw">class</span> CustomDense(layers.Layer):</span>
<span id="cb30-2"><a href="#cb30-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, units<span class="op">=</span><span class="dv">32</span>):</span>
<span id="cb30-3"><a href="#cb30-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb30-4"><a href="#cb30-4" tabindex="-1"></a>        <span class="va">self</span>.units <span class="op">=</span> units</span>
<span id="cb30-5"><a href="#cb30-5" tabindex="-1"></a></span>
<span id="cb30-6"><a href="#cb30-6" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb30-7"><a href="#cb30-7" tabindex="-1"></a>        <span class="va">self</span>.w <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb30-8"><a href="#cb30-8" tabindex="-1"></a>            shape<span class="op">=</span>(input_shape[<span class="op">-</span><span class="dv">1</span>], <span class="va">self</span>.units),</span>
<span id="cb30-9"><a href="#cb30-9" tabindex="-1"></a>            initializer<span class="op">=</span><span class="st">"random_normal"</span>,</span>
<span id="cb30-10"><a href="#cb30-10" tabindex="-1"></a>            trainable<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb30-11"><a href="#cb30-11" tabindex="-1"></a>        )</span>
<span id="cb30-12"><a href="#cb30-12" tabindex="-1"></a>        <span class="va">self</span>.b <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb30-13"><a href="#cb30-13" tabindex="-1"></a>            shape<span class="op">=</span>(<span class="va">self</span>.units,), initializer<span class="op">=</span><span class="st">"random_normal"</span>, trainable<span class="op">=</span><span class="va">True</span></span>
<span id="cb30-14"><a href="#cb30-14" tabindex="-1"></a>        )</span>
<span id="cb30-15"><a href="#cb30-15" tabindex="-1"></a></span>
<span id="cb30-16"><a href="#cb30-16" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb30-17"><a href="#cb30-17" tabindex="-1"></a>        <span class="cf">return</span> ops.matmul(inputs, <span class="va">self</span>.w) <span class="op">+</span> <span class="va">self</span>.b</span>
<span id="cb30-18"><a href="#cb30-18" tabindex="-1"></a></span>
<span id="cb30-19"><a href="#cb30-19" tabindex="-1"></a></span>
<span id="cb30-20"><a href="#cb30-20" tabindex="-1"></a>inputs <span class="op">=</span> keras.Input((<span class="dv">4</span>,))</span>
<span id="cb30-21"><a href="#cb30-21" tabindex="-1"></a>outputs <span class="op">=</span> CustomDense(<span class="dv">10</span>)(inputs)</span>
<span id="cb30-22"><a href="#cb30-22" tabindex="-1"></a></span>
<span id="cb30-23"><a href="#cb30-23" tabindex="-1"></a>model <span class="op">=</span> keras.Model(inputs, outputs)</span></code></pre></div>
<p>For serialization support in your custom layer, define a
<code><a href="../../reference/get_config.html">get_config()</a></code> method that returns the constructor arguments
of the layer instance:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a><span class="kw">class</span> CustomDense(layers.Layer):</span>
<span id="cb31-2"><a href="#cb31-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, units<span class="op">=</span><span class="dv">32</span>):</span>
<span id="cb31-3"><a href="#cb31-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb31-4"><a href="#cb31-4" tabindex="-1"></a>        <span class="va">self</span>.units <span class="op">=</span> units</span>
<span id="cb31-5"><a href="#cb31-5" tabindex="-1"></a></span>
<span id="cb31-6"><a href="#cb31-6" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb31-7"><a href="#cb31-7" tabindex="-1"></a>        <span class="va">self</span>.w <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb31-8"><a href="#cb31-8" tabindex="-1"></a>            shape<span class="op">=</span>(input_shape[<span class="op">-</span><span class="dv">1</span>], <span class="va">self</span>.units),</span>
<span id="cb31-9"><a href="#cb31-9" tabindex="-1"></a>            initializer<span class="op">=</span><span class="st">"random_normal"</span>,</span>
<span id="cb31-10"><a href="#cb31-10" tabindex="-1"></a>            trainable<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb31-11"><a href="#cb31-11" tabindex="-1"></a>        )</span>
<span id="cb31-12"><a href="#cb31-12" tabindex="-1"></a>        <span class="va">self</span>.b <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb31-13"><a href="#cb31-13" tabindex="-1"></a>            shape<span class="op">=</span>(<span class="va">self</span>.units,), initializer<span class="op">=</span><span class="st">"random_normal"</span>, trainable<span class="op">=</span><span class="va">True</span></span>
<span id="cb31-14"><a href="#cb31-14" tabindex="-1"></a>        )</span>
<span id="cb31-15"><a href="#cb31-15" tabindex="-1"></a></span>
<span id="cb31-16"><a href="#cb31-16" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb31-17"><a href="#cb31-17" tabindex="-1"></a>        <span class="cf">return</span> ops.matmul(inputs, <span class="va">self</span>.w) <span class="op">+</span> <span class="va">self</span>.b</span>
<span id="cb31-18"><a href="#cb31-18" tabindex="-1"></a></span>
<span id="cb31-19"><a href="#cb31-19" tabindex="-1"></a>    <span class="kw">def</span> get_config(<span class="va">self</span>):</span>
<span id="cb31-20"><a href="#cb31-20" tabindex="-1"></a>        <span class="cf">return</span> {<span class="st">"units"</span>: <span class="va">self</span>.units}</span>
<span id="cb31-21"><a href="#cb31-21" tabindex="-1"></a></span>
<span id="cb31-22"><a href="#cb31-22" tabindex="-1"></a></span>
<span id="cb31-23"><a href="#cb31-23" tabindex="-1"></a>inputs <span class="op">=</span> keras.Input((<span class="dv">4</span>,))</span>
<span id="cb31-24"><a href="#cb31-24" tabindex="-1"></a>outputs <span class="op">=</span> CustomDense(<span class="dv">10</span>)(inputs)</span>
<span id="cb31-25"><a href="#cb31-25" tabindex="-1"></a></span>
<span id="cb31-26"><a href="#cb31-26" tabindex="-1"></a>model <span class="op">=</span> keras.Model(inputs, outputs)</span>
<span id="cb31-27"><a href="#cb31-27" tabindex="-1"></a>config <span class="op">=</span> model.get_config()</span>
<span id="cb31-28"><a href="#cb31-28" tabindex="-1"></a></span>
<span id="cb31-29"><a href="#cb31-29" tabindex="-1"></a>new_model <span class="op">=</span> keras.Model.from_config(</span>
<span id="cb31-30"><a href="#cb31-30" tabindex="-1"></a>    config, custom_objects<span class="op">=</span>{<span class="st">"CustomDense"</span>: CustomDense}</span>
<span id="cb31-31"><a href="#cb31-31" tabindex="-1"></a>)</span></code></pre></div>
<p>Optionally, implement the class method
<code>from_config(cls, config)</code> which is used when recreating a
layer instance given its config dictionary. The default implementation
of <code>from_config</code> is:</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" tabindex="-1"></a><span class="kw">def</span> from_config(cls, config):</span>
<span id="cb32-2"><a href="#cb32-2" tabindex="-1"></a>  <span class="cf">return</span> cls(<span class="op">**</span>config)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="when-to-use-the-functional-api">When to use the functional API<a class="anchor" aria-label="anchor" href="#when-to-use-the-functional-api"></a>
</h2>
<p>Should you use the Keras functional API to create a new model, or
just subclass the <code>Model</code> class directly? In general, the
functional API is higher-level, easier and safer, and has a number of
features that subclassed models do not support.</p>
<p>However, model subclassing provides greater flexibility when building
models that are not easily expressible as directed acyclic graphs of
layers. For example, you could not implement a Tree-RNN with the
functional API and would have to subclass <code>Model</code>
directly.</p>
<p>For an in-depth look at the differences between the functional API
and model subclassing, read <a href="https://blog.tensorflow.org/2019/01/what-are-symbolic-and-imperative-apis.html" class="external-link">What
are Symbolic and Imperative APIs in TensorFlow 2.0?</a>.</p>
<div class="section level3">
<h3 id="functional-api-strengths">Functional API strengths:<a class="anchor" aria-label="anchor" href="#functional-api-strengths"></a>
</h3>
<p>The following properties are also true for Sequential models (which
are also data structures), but are not true for subclassed models (which
are Python bytecode, not data structures).</p>
<div class="section level4">
<h4 id="less-verbose">Less verbose<a class="anchor" aria-label="anchor" href="#less-verbose"></a>
</h4>
<p>There is no <code>super().__init__(...)</code>, no
<code>def call(self, ...):</code>, etc.</p>
<p>Compare:</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" tabindex="-1"></a>inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">32</span>,))</span>
<span id="cb33-2"><a href="#cb33-2" tabindex="-1"></a>x <span class="op">=</span> layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(inputs)</span>
<span id="cb33-3"><a href="#cb33-3" tabindex="-1"></a>outputs <span class="op">=</span> layers.Dense(<span class="dv">10</span>)(x)</span>
<span id="cb33-4"><a href="#cb33-4" tabindex="-1"></a>mlp <span class="op">=</span> keras.Model(inputs, outputs)</span></code></pre></div>
<p>With the subclassed version:</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" tabindex="-1"></a><span class="kw">class</span> MLP(keras.Model):</span>
<span id="cb34-2"><a href="#cb34-2" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">**</span>kwargs):</span>
<span id="cb34-4"><a href="#cb34-4" tabindex="-1"></a>    <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb34-5"><a href="#cb34-5" tabindex="-1"></a>    <span class="va">self</span>.dense_1 <span class="op">=</span> layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>)</span>
<span id="cb34-6"><a href="#cb34-6" tabindex="-1"></a>    <span class="va">self</span>.dense_2 <span class="op">=</span> layers.Dense(<span class="dv">10</span>)</span>
<span id="cb34-7"><a href="#cb34-7" tabindex="-1"></a></span>
<span id="cb34-8"><a href="#cb34-8" tabindex="-1"></a>  <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb34-9"><a href="#cb34-9" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.dense_1(inputs)</span>
<span id="cb34-10"><a href="#cb34-10" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span>.dense_2(x)</span>
<span id="cb34-11"><a href="#cb34-11" tabindex="-1"></a></span>
<span id="cb34-12"><a href="#cb34-12" tabindex="-1"></a><span class="co"># Instantiate the model.</span></span>
<span id="cb34-13"><a href="#cb34-13" tabindex="-1"></a>mlp <span class="op">=</span> MLP()</span>
<span id="cb34-14"><a href="#cb34-14" tabindex="-1"></a><span class="co"># Necessary to create the model's state.</span></span>
<span id="cb34-15"><a href="#cb34-15" tabindex="-1"></a><span class="co"># The model doesn't have a state until it's called at least once.</span></span>
<span id="cb34-16"><a href="#cb34-16" tabindex="-1"></a>_ <span class="op">=</span> mlp(ops.zeros((<span class="dv">1</span>, <span class="dv">32</span>)))</span></code></pre></div>
</div>
<div class="section level4">
<h4 id="model-validation-while-defining-its-connectivity-graph">Model validation while defining its connectivity graph<a class="anchor" aria-label="anchor" href="#model-validation-while-defining-its-connectivity-graph"></a>
</h4>
<p>In the functional API, the input specification (shape and dtype) is
created in advance (using <code>Input</code>). Every time you call a
layer, the layer checks that the specification passed to it matches its
assumptions, and it will raise a helpful error message if not.</p>
<p>This guarantees that any model you can build with the functional API
will run. All debugging – other than convergence-related debugging –
happens statically during the model construction and not at execution
time. This is similar to type checking in a compiler.</p>
</div>
<div class="section level4">
<h4 id="a-functional-model-is-plottable-and-inspectable">A functional model is plottable and inspectable<a class="anchor" aria-label="anchor" href="#a-functional-model-is-plottable-and-inspectable"></a>
</h4>
<p>You can plot the model as a graph, and you can easily access
intermediate nodes in this graph. For example, to extract and reuse the
activations of intermediate layers (as seen in a previous example):</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" tabindex="-1"></a>features_list <span class="op">=</span> [layer.output <span class="cf">for</span> layer <span class="kw">in</span> vgg19.layers]</span>
<span id="cb35-2"><a href="#cb35-2" tabindex="-1"></a>feat_extraction_model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>vgg19.<span class="bu">input</span>, outputs<span class="op">=</span>features_list)</span></code></pre></div>
</div>
<div class="section level4">
<h4 id="a-functional-model-can-be-serialized-or-cloned">A functional model can be serialized or cloned<a class="anchor" aria-label="anchor" href="#a-functional-model-can-be-serialized-or-cloned"></a>
</h4>
<p>Because a functional model is a data structure rather than a piece of
code, it is safely serializable and can be saved as a single file that
allows you to recreate the exact same model without having access to any
of the original code. See the <a href="/guides/serialization_and_saving/">serialization &amp; saving
guide</a>.</p>
<p>To serialize a subclassed model, it is necessary for the implementer
to specify a <code><a href="../../reference/get_config.html">get_config()</a></code> and <code><a href="../../reference/get_config.html">from_config()</a></code>
method at the model level.</p>
</div>
</div>
<div class="section level3">
<h3 id="functional-api-weakness">Functional API weakness:<a class="anchor" aria-label="anchor" href="#functional-api-weakness"></a>
</h3>
<div class="section level4">
<h4 id="it-does-not-support-dynamic-architectures">It does not support dynamic architectures<a class="anchor" aria-label="anchor" href="#it-does-not-support-dynamic-architectures"></a>
</h4>
<p>The functional API treats models as DAGs of layers. This is true for
most deep learning architectures, but not all – for example, recursive
networks or Tree RNNs do not follow this assumption and cannot be
implemented in the functional API.</p>
</div>
</div>
</div>
<div class="section level2">
<h2 id="mix-and-match-api-styles">Mix-and-match API styles<a class="anchor" aria-label="anchor" href="#mix-and-match-api-styles"></a>
</h2>
<p>Choosing between the functional API or Model subclassing isn’t a
binary decision that restricts you into one category of models. All
models in the <code>keras</code> API can interact with each other,
whether they’re <code>Sequential</code> models, functional models, or
subclassed models that are written from scratch.</p>
<p>You can always use a functional model or <code>Sequential</code>
model as part of a subclassed model or layer:</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" tabindex="-1"></a>units <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb36-2"><a href="#cb36-2" tabindex="-1"></a>timesteps <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb36-3"><a href="#cb36-3" tabindex="-1"></a>input_dim <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb36-4"><a href="#cb36-4" tabindex="-1"></a></span>
<span id="cb36-5"><a href="#cb36-5" tabindex="-1"></a><span class="co"># Define a Functional model</span></span>
<span id="cb36-6"><a href="#cb36-6" tabindex="-1"></a>inputs <span class="op">=</span> keras.Input((<span class="va">None</span>, units))</span>
<span id="cb36-7"><a href="#cb36-7" tabindex="-1"></a>x <span class="op">=</span> layers.GlobalAveragePooling1D()(inputs)</span>
<span id="cb36-8"><a href="#cb36-8" tabindex="-1"></a>outputs <span class="op">=</span> layers.Dense(<span class="dv">1</span>)(x)</span>
<span id="cb36-9"><a href="#cb36-9" tabindex="-1"></a>model <span class="op">=</span> keras.Model(inputs, outputs)</span>
<span id="cb36-10"><a href="#cb36-10" tabindex="-1"></a></span>
<span id="cb36-11"><a href="#cb36-11" tabindex="-1"></a></span>
<span id="cb36-12"><a href="#cb36-12" tabindex="-1"></a><span class="kw">class</span> CustomRNN(layers.Layer):</span>
<span id="cb36-13"><a href="#cb36-13" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb36-14"><a href="#cb36-14" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb36-15"><a href="#cb36-15" tabindex="-1"></a>        <span class="va">self</span>.units <span class="op">=</span> units</span>
<span id="cb36-16"><a href="#cb36-16" tabindex="-1"></a>        <span class="va">self</span>.projection_1 <span class="op">=</span> layers.Dense(units<span class="op">=</span>units, activation<span class="op">=</span><span class="st">"tanh"</span>)</span>
<span id="cb36-17"><a href="#cb36-17" tabindex="-1"></a>        <span class="va">self</span>.projection_2 <span class="op">=</span> layers.Dense(units<span class="op">=</span>units, activation<span class="op">=</span><span class="st">"tanh"</span>)</span>
<span id="cb36-18"><a href="#cb36-18" tabindex="-1"></a>        <span class="co"># Our previously-defined Functional model</span></span>
<span id="cb36-19"><a href="#cb36-19" tabindex="-1"></a>        <span class="va">self</span>.classifier <span class="op">=</span> model</span>
<span id="cb36-20"><a href="#cb36-20" tabindex="-1"></a></span>
<span id="cb36-21"><a href="#cb36-21" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb36-22"><a href="#cb36-22" tabindex="-1"></a>        outputs <span class="op">=</span> []</span>
<span id="cb36-23"><a href="#cb36-23" tabindex="-1"></a>        state <span class="op">=</span> ops.zeros(shape<span class="op">=</span>(inputs.shape[<span class="dv">0</span>], <span class="va">self</span>.units))</span>
<span id="cb36-24"><a href="#cb36-24" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(inputs.shape[<span class="dv">1</span>]):</span>
<span id="cb36-25"><a href="#cb36-25" tabindex="-1"></a>            x <span class="op">=</span> inputs[:, t, :]</span>
<span id="cb36-26"><a href="#cb36-26" tabindex="-1"></a>            h <span class="op">=</span> <span class="va">self</span>.projection_1(x)</span>
<span id="cb36-27"><a href="#cb36-27" tabindex="-1"></a>            y <span class="op">=</span> h <span class="op">+</span> <span class="va">self</span>.projection_2(state)</span>
<span id="cb36-28"><a href="#cb36-28" tabindex="-1"></a>            state <span class="op">=</span> y</span>
<span id="cb36-29"><a href="#cb36-29" tabindex="-1"></a>            outputs.append(y)</span>
<span id="cb36-30"><a href="#cb36-30" tabindex="-1"></a>        features <span class="op">=</span> ops.stack(outputs, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb36-31"><a href="#cb36-31" tabindex="-1"></a>        <span class="bu">print</span>(features.shape)</span>
<span id="cb36-32"><a href="#cb36-32" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.classifier(features)</span>
<span id="cb36-33"><a href="#cb36-33" tabindex="-1"></a></span>
<span id="cb36-34"><a href="#cb36-34" tabindex="-1"></a></span>
<span id="cb36-35"><a href="#cb36-35" tabindex="-1"></a>rnn_model <span class="op">=</span> CustomRNN()</span>
<span id="cb36-36"><a href="#cb36-36" tabindex="-1"></a>_ <span class="op">=</span> rnn_model(ops.zeros((<span class="dv">1</span>, timesteps, input_dim)))</span></code></pre></div>
<p>You can use any subclassed layer or model in the functional API as
long as it implements a <code>call</code> method that follows one of the
following patterns:</p>
<ul>
<li>
<code>call(self, inputs, **kwargs)</code> – Where
<code>inputs</code> is a tensor or a nested structure of tensors (e.g. a
list of tensors), and where <code>**kwargs</code> are non-tensor
arguments (non-inputs).</li>
<li>
<code>call(self, inputs, training=None, **kwargs)</code> – Where
<code>training</code> is a boolean indicating whether the layer should
behave in training mode and inference mode.</li>
<li>
<code>call(self, inputs, mask=None, **kwargs)</code> – Where
<code>mask</code> is a boolean mask tensor (useful for RNNs, for
instance).</li>
<li>
<code>call(self, inputs, training=None, mask=None, **kwargs)</code>
– Of course, you can have both masking and training-specific behavior at
the same time.</li>
</ul>
<p>Additionally, if you implement the <code>get_config</code> method on
your custom Layer or model, the functional models you create will still
be serializable and cloneable.</p>
<p>Here’s a quick example of a custom RNN, written from scratch, being
used in a functional model:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" tabindex="-1"></a>units <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb37-2"><a href="#cb37-2" tabindex="-1"></a>timesteps <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb37-3"><a href="#cb37-3" tabindex="-1"></a>input_dim <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb37-4"><a href="#cb37-4" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb37-5"><a href="#cb37-5" tabindex="-1"></a></span>
<span id="cb37-6"><a href="#cb37-6" tabindex="-1"></a></span>
<span id="cb37-7"><a href="#cb37-7" tabindex="-1"></a><span class="kw">class</span> CustomRNN(layers.Layer):</span>
<span id="cb37-8"><a href="#cb37-8" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb37-9"><a href="#cb37-9" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb37-10"><a href="#cb37-10" tabindex="-1"></a>        <span class="va">self</span>.units <span class="op">=</span> units</span>
<span id="cb37-11"><a href="#cb37-11" tabindex="-1"></a>        <span class="va">self</span>.projection_1 <span class="op">=</span> layers.Dense(units<span class="op">=</span>units, activation<span class="op">=</span><span class="st">"tanh"</span>)</span>
<span id="cb37-12"><a href="#cb37-12" tabindex="-1"></a>        <span class="va">self</span>.projection_2 <span class="op">=</span> layers.Dense(units<span class="op">=</span>units, activation<span class="op">=</span><span class="st">"tanh"</span>)</span>
<span id="cb37-13"><a href="#cb37-13" tabindex="-1"></a>        <span class="va">self</span>.classifier <span class="op">=</span> layers.Dense(<span class="dv">1</span>)</span>
<span id="cb37-14"><a href="#cb37-14" tabindex="-1"></a></span>
<span id="cb37-15"><a href="#cb37-15" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb37-16"><a href="#cb37-16" tabindex="-1"></a>        outputs <span class="op">=</span> []</span>
<span id="cb37-17"><a href="#cb37-17" tabindex="-1"></a>        state <span class="op">=</span> ops.zeros(shape<span class="op">=</span>(inputs.shape[<span class="dv">0</span>], <span class="va">self</span>.units))</span>
<span id="cb37-18"><a href="#cb37-18" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(inputs.shape[<span class="dv">1</span>]):</span>
<span id="cb37-19"><a href="#cb37-19" tabindex="-1"></a>            x <span class="op">=</span> inputs[:, t, :]</span>
<span id="cb37-20"><a href="#cb37-20" tabindex="-1"></a>            h <span class="op">=</span> <span class="va">self</span>.projection_1(x)</span>
<span id="cb37-21"><a href="#cb37-21" tabindex="-1"></a>            y <span class="op">=</span> h <span class="op">+</span> <span class="va">self</span>.projection_2(state)</span>
<span id="cb37-22"><a href="#cb37-22" tabindex="-1"></a>            state <span class="op">=</span> y</span>
<span id="cb37-23"><a href="#cb37-23" tabindex="-1"></a>            outputs.append(y)</span>
<span id="cb37-24"><a href="#cb37-24" tabindex="-1"></a>        features <span class="op">=</span> ops.stack(outputs, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb37-25"><a href="#cb37-25" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.classifier(features)</span>
<span id="cb37-26"><a href="#cb37-26" tabindex="-1"></a></span>
<span id="cb37-27"><a href="#cb37-27" tabindex="-1"></a></span>
<span id="cb37-28"><a href="#cb37-28" tabindex="-1"></a><span class="co"># Note that you specify a static batch size for the inputs with the `batch_shape`</span></span>
<span id="cb37-29"><a href="#cb37-29" tabindex="-1"></a><span class="co"># arg, because the inner computation of `CustomRNN` requires a static batch size</span></span>
<span id="cb37-30"><a href="#cb37-30" tabindex="-1"></a><span class="co"># (when you create the `state` zeros tensor).</span></span>
<span id="cb37-31"><a href="#cb37-31" tabindex="-1"></a>inputs <span class="op">=</span> keras.Input(batch_shape<span class="op">=</span>(batch_size, timesteps, input_dim))</span>
<span id="cb37-32"><a href="#cb37-32" tabindex="-1"></a>x <span class="op">=</span> layers.Conv1D(<span class="dv">32</span>, <span class="dv">3</span>)(inputs)</span>
<span id="cb37-33"><a href="#cb37-33" tabindex="-1"></a>outputs <span class="op">=</span> CustomRNN()(x)</span>
<span id="cb37-34"><a href="#cb37-34" tabindex="-1"></a></span>
<span id="cb37-35"><a href="#cb37-35" tabindex="-1"></a>model <span class="op">=</span> keras.Model(inputs, outputs)</span>
<span id="cb37-36"><a href="#cb37-36" tabindex="-1"></a></span>
<span id="cb37-37"><a href="#cb37-37" tabindex="-1"></a>rnn_model <span class="op">=</span> CustomRNN()</span>
<span id="cb37-38"><a href="#cb37-38" tabindex="-1"></a>_ <span class="op">=</span> rnn_model(ops.zeros((<span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">5</span>)))</span></code></pre></div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, RStudio, Google.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
