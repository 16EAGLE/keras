<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Making new layers and models via subclassing • keras</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../../bootstrap-toc.css">
<script src="../../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../../pkgdown.css" rel="stylesheet">
<script src="../../pkgdown.js"></script><link href="../../extra.css" rel="stylesheet">
<script src="../../extra.js"></script><meta property="og:title" content="Making new layers and models via subclassing">
<meta property="og:description" content="Complete guide to writing `Layer` and `Model` objects from scratch.">
<meta property="og:image" content="/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../../index.html">keras</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">2.13.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../../index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Tutorials
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/getting_started.html">Getting Started</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    </li>
    <li>
      <a href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    </li>
    <li>
      <a href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li class="dropdown-header">Guides (New for TF 2.6)</li>
    <li>
      <a href="../../articles/new-guides/python_subclasses.html">Python Subclasses</a>
    </li>
    <li>
      <a href="../../articles/new-guides/making_new_layers_and_models_via_subclassing.html">Making New Layers and Models via Subclassing</a>
    </li>
    <li>
      <a href="../../articles/new-guides/customizing_what_happens_in_fit.html">Customizing What Happens in Fit</a>
    </li>
    <li>
      <a href="../../articles/new-guides/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    </li>
    <li>
      <a href="../../articles/new-guides/preprocessing_layers.html">Working with Preprocessing Layers</a>
    </li>
    <li>
      <a href="../../argicles/new-guides/working_with_rnns.html">Working with RNNs</a>
    </li>
    <li class="divider">
    </li>
<li class="dropdown-header">Using Keras</li>
    <li>
      <a href="../../articles/guide_keras.html">Guide to Keras Basics</a>
    </li>
    <li>
      <a href="../../articles/sequential_model.html">Sequential Model in Depth</a>
    </li>
    <li>
      <a href="../../articles/functional_api.html">Functional API in Depth</a>
    </li>
    <li>
      <a href="../../articles/about_keras_models.html">About Keras Models</a>
    </li>
    <li>
      <a href="../../articles/about_keras_layers.html">About Keras Layers</a>
    </li>
    <li>
      <a href="../../articles/training_visualization.html">Training Visualization</a>
    </li>
    <li>
      <a href="../../articles/applications.html">Pre-Trained Models</a>
    </li>
    <li>
      <a href="../../articles/faq.html">Frequently Asked Questions</a>
    </li>
    <li>
      <a href="../../articles/why_use_keras.html">Why Use Keras?</a>
    </li>
    <li class="divider">
    </li>
<li class="dropdown-header">Advanced</li>
    <li>
      <a href="../../articles/eager_guide.html">Eager Execution</a>
    </li>
    <li>
      <a href="../../articles/training_callbacks.html">Training Callbacks</a>
    </li>
    <li>
      <a href="../../articles/backend.html">Keras Backend</a>
    </li>
    <li>
      <a href="../../articles/custom_layers.html">Custom Layers</a>
    </li>
    <li>
      <a href="../../articles/custom_models.html">Custom Models</a>
    </li>
    <li>
      <a href="../../articles/saving_serializing.html">Saving and serializing</a>
    </li>
  </ul>
</li>
<li>
  <a href="../../articles/learn.html">Learn</a>
</li>
<li>
  <a href="../../articles/tools.html">Tools</a>
</li>
<li>
  <a href="../../articles/examples/index.html">Examples</a>
</li>
<li>
  <a href="../../reference/index.html">Reference</a>
</li>
<li>
  <a href="../../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/rstudio/keras/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Making new layers and models via
subclassing</h1>
                        <h4 data-toc-skip class="author"><a href="https://twitter.com/fchollet" class="external-link">fchollet</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/guides/making_new_layers_and_models_via_subclassing.Rmd" class="external-link"><code>vignettes/guides/making_new_layers_and_models_via_subclassing.Rmd</code></a></small>
      <div class="hidden name"><code>making_new_layers_and_models_via_subclassing.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>This guide will cover everything you need to know to build your own
subclassed layers and models. In particular, you’ll learn about the
following features:</p>
<ul>
<li>The <code>Layer</code> class</li>
<li>The <code>add_weight()</code> method</li>
<li>Trainable and non-trainable weights</li>
<li>The <code>build()</code> method</li>
<li>Making sure your layers can be used with any backend</li>
<li>The <code>add_loss()</code> method</li>
<li>The <code>training</code> argument in <code><a href="https://rdrr.io/r/base/call.html" class="external-link">call()</a></code>
</li>
<li>The <code>mask</code> argument in <code><a href="https://rdrr.io/r/base/call.html" class="external-link">call()</a></code>
</li>
<li>Making sure your layers can be serialized</li>
</ul>
<p>Let’s dive in.</p>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> ops</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="the-layer-class-the-combination-of-state-weights-and-some-computation">The <code>Layer</code> class: the combination of state (weights) and
some computation<a class="anchor" aria-label="anchor" href="#the-layer-class-the-combination-of-state-weights-and-some-computation"></a>
</h2>
<p>One of the central abstractions in Keras is the <code>Layer</code>
class. A layer encapsulates both a state (the layer’s “weights”) and a
transformation from inputs to outputs (a “call”, the layer’s forward
pass).</p>
<p>Here’s a densely-connected layer. It has two state variables: the
variables <code>w</code> and <code>b</code>.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="kw">class</span> Linear(keras.layers.Layer):</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, units<span class="op">=</span><span class="dv">32</span>, input_dim<span class="op">=</span><span class="dv">32</span>):</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>        <span class="va">self</span>.w <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>            shape<span class="op">=</span>(input_dim, units),</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>            initializer<span class="op">=</span><span class="st">"random_normal"</span>,</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>            trainable<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>        )</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>        <span class="va">self</span>.b <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>            shape<span class="op">=</span>(units,), initializer<span class="op">=</span><span class="st">"zeros"</span>, trainable<span class="op">=</span><span class="va">True</span></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>        )</span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a>        <span class="cf">return</span> ops.matmul(inputs, <span class="va">self</span>.w) <span class="op">+</span> <span class="va">self</span>.b</span></code></pre></div>
<p>You would use a layer by calling it on some tensor input(s), much
like a Python function.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>x <span class="op">=</span> ops.ones((<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>linear_layer <span class="op">=</span> Linear(<span class="dv">4</span>, <span class="dv">2</span>)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>y <span class="op">=</span> linear_layer(x)</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="bu">print</span>(y)</span></code></pre></div>
<p>Note that the weights <code>w</code> and <code>b</code> are
automatically tracked by the layer upon being set as layer
attributes:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="cf">assert</span> linear_layer.weights <span class="op">==</span> [linear_layer.w, linear_layer.b]</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="layers-can-have-non-trainable-weights">Layers can have non-trainable weights<a class="anchor" aria-label="anchor" href="#layers-can-have-non-trainable-weights"></a>
</h2>
<p>Besides trainable weights, you can add non-trainable weights to a
layer as well. Such weights are meant not to be taken into account
during backpropagation, when you are training the layer.</p>
<p>Here’s how to add and use a non-trainable weight:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="kw">class</span> ComputeSum(keras.layers.Layer):</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim):</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>        <span class="va">self</span>.total <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>            initializer<span class="op">=</span><span class="st">"zeros"</span>, shape<span class="op">=</span>(input_dim,), trainable<span class="op">=</span><span class="va">False</span></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>        )</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>        <span class="va">self</span>.total.assign_add(ops.<span class="bu">sum</span>(inputs, axis<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.total</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>x <span class="op">=</span> ops.ones((<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>my_sum <span class="op">=</span> ComputeSum(<span class="dv">2</span>)</span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a>y <span class="op">=</span> my_sum(x)</span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a><span class="bu">print</span>(y.numpy())</span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>y <span class="op">=</span> my_sum(x)</span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a><span class="bu">print</span>(y.numpy())</span></code></pre></div>
<p>It’s part of <code>layer.weights</code>, but it gets categorized as a
non-trainable weight:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"weights:"</span>, <span class="bu">len</span>(my_sum.weights))</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"non-trainable weights:"</span>, <span class="bu">len</span>(my_sum.non_trainable_weights))</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a><span class="co"># It's not included in the trainable weights:</span></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"trainable_weights:"</span>, my_sum.trainable_weights)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="best-practice-deferring-weight-creation-until-the-shape-of-the-inputs-is-known">Best practice: deferring weight creation until the shape of the
inputs is known<a class="anchor" aria-label="anchor" href="#best-practice-deferring-weight-creation-until-the-shape-of-the-inputs-is-known"></a>
</h2>
<p>Our <code>Linear</code> layer above took an <code>input_dim</code>
argument that was used to compute the shape of the weights
<code>w</code> and <code>b</code> in <code>__init__()</code>:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="kw">class</span> Linear(keras.layers.Layer):</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, units<span class="op">=</span><span class="dv">32</span>, input_dim<span class="op">=</span><span class="dv">32</span>):</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>        <span class="va">self</span>.w <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>            shape<span class="op">=</span>(input_dim, units),</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>            initializer<span class="op">=</span><span class="st">"random_normal"</span>,</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>            trainable<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>        )</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>        <span class="va">self</span>.b <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>            shape<span class="op">=</span>(units,), initializer<span class="op">=</span><span class="st">"zeros"</span>, trainable<span class="op">=</span><span class="va">True</span></span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>        )</span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a>        <span class="cf">return</span> ops.matmul(inputs, <span class="va">self</span>.w) <span class="op">+</span> <span class="va">self</span>.b</span></code></pre></div>
<p>In many cases, you may not know in advance the size of your inputs,
and you would like to lazily create weights when that value becomes
known, some time after instantiating the layer.</p>
<p>In the Keras API, we recommend creating layer weights in the
<code>build(self, inputs_shape)</code> method of your layer. Like
this:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="kw">class</span> Linear(keras.layers.Layer):</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, units<span class="op">=</span><span class="dv">32</span>):</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>        <span class="va">self</span>.units <span class="op">=</span> units</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>        <span class="va">self</span>.w <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>            shape<span class="op">=</span>(input_shape[<span class="op">-</span><span class="dv">1</span>], <span class="va">self</span>.units),</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>            initializer<span class="op">=</span><span class="st">"random_normal"</span>,</span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>            trainable<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>        )</span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>        <span class="va">self</span>.b <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>            shape<span class="op">=</span>(<span class="va">self</span>.units,), initializer<span class="op">=</span><span class="st">"random_normal"</span>, trainable<span class="op">=</span><span class="va">True</span></span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>        )</span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a>        <span class="cf">return</span> ops.matmul(inputs, <span class="va">self</span>.w) <span class="op">+</span> <span class="va">self</span>.b</span></code></pre></div>
<p>The <code>__call__()</code> method of your layer will automatically
run build the first time it is called. You now have a layer that’s lazy
and thus easier to use:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="co"># At instantiation, we don't know on what inputs this is going to get called</span></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>linear_layer <span class="op">=</span> Linear(<span class="dv">32</span>)</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a><span class="co"># The layer's weights are created dynamically the first time the layer is called</span></span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>y <span class="op">=</span> linear_layer(x)</span></code></pre></div>
<p>Implementing <code>build()</code> separately as shown above nicely
separates creating weights only once from using weights in every
call.</p>
</div>
<div class="section level2">
<h2 id="layers-are-recursively-composable">Layers are recursively composable<a class="anchor" aria-label="anchor" href="#layers-are-recursively-composable"></a>
</h2>
<p>If you assign a Layer instance as an attribute of another Layer, the
outer layer will start tracking the weights created by the inner
layer.</p>
<p>We recommend creating such sublayers in the <code>__init__()</code>
method and leave it to the first <code>__call__()</code> to trigger
building their weights.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="kw">class</span> MLPBlock(keras.layers.Layer):</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>        <span class="va">self</span>.linear_1 <span class="op">=</span> Linear(<span class="dv">32</span>)</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>        <span class="va">self</span>.linear_2 <span class="op">=</span> Linear(<span class="dv">32</span>)</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>        <span class="va">self</span>.linear_3 <span class="op">=</span> Linear(<span class="dv">1</span>)</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_1(inputs)</span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a>        x <span class="op">=</span> keras.activations.relu(x)</span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_2(x)</span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a>        x <span class="op">=</span> keras.activations.relu(x)</span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.linear_3(x)</span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" tabindex="-1"></a>mlp <span class="op">=</span> MLPBlock()</span>
<span id="cb10-17"><a href="#cb10-17" tabindex="-1"></a>y <span class="op">=</span> mlp(</span>
<span id="cb10-18"><a href="#cb10-18" tabindex="-1"></a>    ops.ones(shape<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">64</span>))</span>
<span id="cb10-19"><a href="#cb10-19" tabindex="-1"></a>)  <span class="co"># The first call to the `mlp` will create the weights</span></span>
<span id="cb10-20"><a href="#cb10-20" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"weights:"</span>, <span class="bu">len</span>(mlp.weights))</span>
<span id="cb10-21"><a href="#cb10-21" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"trainable weights:"</span>, <span class="bu">len</span>(mlp.trainable_weights))</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="backend-agnostic-layers-and-backend-specific-layers">Backend-agnostic layers and backend-specific layers<a class="anchor" aria-label="anchor" href="#backend-agnostic-layers-and-backend-specific-layers"></a>
</h2>
<p>As long as a layer only uses APIs from the <code>keras.ops</code>
namespace (or other Keras namespaces such as
<code>keras.activations</code>, <code>keras.random</code>, or
<code>keras.layers</code>), then it can be used with any backend –
TensorFlow, JAX, or PyTorch.</p>
<p>All layers you’ve seen so far in this guide work with all Keras
backends.</p>
<p>The <code>keras.ops</code> namespace gives you access to:</p>
<ul>
<li>The NumPy API, e.g. <code>ops.matmul</code>, <code>ops.sum</code>,
<code>ops.reshape</code>, <code>ops.stack</code>, etc.</li>
<li>Neural networks-specific APIs such as <code>ops.softmax</code>,
<code>ops.conv</code>, <code>ops.binary_crossentropy</code>,
<code>ops.relu</code>, etc.</li>
</ul>
<p>You can also use backend-native APIs in your layers (such as
<code>tf.nn</code> functions), but if you do this, then your layer will
only be usable with the backend in question. For instance, you could
write the following JAX-specific layer using <code>jax.numpy</code>:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a><span class="kw">class</span> Linear(keras.layers.Layer):</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>    ...</span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>        <span class="cf">return</span> jax.numpy.matmul(inputs, <span class="va">self</span>.w) <span class="op">+</span> <span class="va">self</span>.b</span></code></pre></div>
<p>This would be the equivalent TensorFlow-specific layer:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="kw">class</span> Linear(keras.layers.Layer):</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>    ...</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a>        <span class="cf">return</span> tf.matmul(inputs, <span class="va">self</span>.w) <span class="op">+</span> <span class="va">self</span>.b</span></code></pre></div>
<p>And this would be the equivalent PyTorch-specific layer:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a><span class="kw">class</span> Linear(keras.layers.Layer):</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>    ...</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a>        <span class="cf">return</span> torch.matmul(inputs, <span class="va">self</span>.w) <span class="op">+</span> <span class="va">self</span>.b</span></code></pre></div>
<p>Because cross-backend compatibility is a tremendously useful
property, we strongly recommend that you seek to always make your layers
backend-agnostic by leveraging only Keras APIs.</p>
</div>
<div class="section level2">
<h2 id="the-add_loss-method">The <code>add_loss()</code> method<a class="anchor" aria-label="anchor" href="#the-add_loss-method"></a>
</h2>
<p>When writing the <code><a href="https://rdrr.io/r/base/call.html" class="external-link">call()</a></code> method of a layer, you can
create loss tensors that you will want to use later, when writing your
training loop. This is doable by calling
<code>self.add_loss(value)</code>:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="co"># A layer that creates an activity regularization loss</span></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="kw">class</span> ActivityRegularizationLayer(keras.layers.Layer):</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, rate<span class="op">=</span><span class="fl">1e-2</span>):</span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>        <span class="va">self</span>.rate <span class="op">=</span> rate</span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a>        <span class="va">self</span>.add_loss(<span class="va">self</span>.rate <span class="op">*</span> ops.mean(inputs))</span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a>        <span class="cf">return</span> inputs</span></code></pre></div>
<p>These losses (including those created by any inner layer) can be
retrieved via <code>layer.losses</code>. This property is reset at the
start of every <code>__call__()</code> to the top-level layer, so that
<code>layer.losses</code> always contains the loss values created during
the last forward pass.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="kw">class</span> OuterLayer(keras.layers.Layer):</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>        <span class="va">self</span>.activity_reg <span class="op">=</span> ActivityRegularizationLayer(<span class="fl">1e-2</span>)</span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.activity_reg(inputs)</span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" tabindex="-1"></a>layer <span class="op">=</span> OuterLayer()</span>
<span id="cb15-11"><a href="#cb15-11" tabindex="-1"></a><span class="cf">assert</span> (</span>
<span id="cb15-12"><a href="#cb15-12" tabindex="-1"></a>    <span class="bu">len</span>(layer.losses) <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb15-13"><a href="#cb15-13" tabindex="-1"></a>)  <span class="co"># No losses yet since the layer has never been called</span></span>
<span id="cb15-14"><a href="#cb15-14" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" tabindex="-1"></a>_ <span class="op">=</span> layer(ops.zeros((<span class="dv">1</span>, <span class="dv">1</span>)))</span>
<span id="cb15-16"><a href="#cb15-16" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">len</span>(layer.losses) <span class="op">==</span> <span class="dv">1</span>  <span class="co"># We created one loss value</span></span>
<span id="cb15-17"><a href="#cb15-17" tabindex="-1"></a></span>
<span id="cb15-18"><a href="#cb15-18" tabindex="-1"></a><span class="co"># `layer.losses` gets reset at the start of each __call__</span></span>
<span id="cb15-19"><a href="#cb15-19" tabindex="-1"></a>_ <span class="op">=</span> layer(ops.zeros((<span class="dv">1</span>, <span class="dv">1</span>)))</span>
<span id="cb15-20"><a href="#cb15-20" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">len</span>(layer.losses) <span class="op">==</span> <span class="dv">1</span>  <span class="co"># This is the loss created during the call above</span></span></code></pre></div>
<p>In addition, the <code>loss</code> property also contains
regularization losses created for the weights of any inner layer:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="kw">class</span> OuterLayerWithKernelRegularizer(keras.layers.Layer):</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>        <span class="va">self</span>.dense <span class="op">=</span> keras.layers.Dense(</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>            <span class="dv">32</span>, kernel_regularizer<span class="op">=</span>keras.regularizers.l2(<span class="fl">1e-3</span>)</span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a>        )</span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb16-9"><a href="#cb16-9" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.dense(inputs)</span>
<span id="cb16-10"><a href="#cb16-10" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" tabindex="-1"></a>layer <span class="op">=</span> OuterLayerWithKernelRegularizer()</span>
<span id="cb16-13"><a href="#cb16-13" tabindex="-1"></a>_ <span class="op">=</span> layer(ops.zeros((<span class="dv">1</span>, <span class="dv">1</span>)))</span>
<span id="cb16-14"><a href="#cb16-14" tabindex="-1"></a></span>
<span id="cb16-15"><a href="#cb16-15" tabindex="-1"></a><span class="co"># This is `1e-3 * sum(layer.dense.kernel ** 2)`,</span></span>
<span id="cb16-16"><a href="#cb16-16" tabindex="-1"></a><span class="co"># created by the `kernel_regularizer` above.</span></span>
<span id="cb16-17"><a href="#cb16-17" tabindex="-1"></a><span class="bu">print</span>(layer.losses)</span></code></pre></div>
<p>These losses are meant to be taken into account when writing custom
training loops.</p>
<p>They also work seamlessly with <code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code> (they get
automatically summed and added to the main loss, if any):</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">3</span>,))</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>outputs <span class="op">=</span> ActivityRegularizationLayer()(inputs)</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a>model <span class="op">=</span> keras.Model(inputs, outputs)</span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a><span class="co"># If there is a loss passed in `compile`, the regularization</span></span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a><span class="co"># losses get added to it</span></span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"adam"</span>, loss<span class="op">=</span><span class="st">"mse"</span>)</span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a>model.fit(np.random.random((<span class="dv">2</span>, <span class="dv">3</span>)), np.random.random((<span class="dv">2</span>, <span class="dv">3</span>)))</span>
<span id="cb17-9"><a href="#cb17-9" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" tabindex="-1"></a><span class="co"># It's also possible not to pass any loss in `compile`,</span></span>
<span id="cb17-11"><a href="#cb17-11" tabindex="-1"></a><span class="co"># since the model already has a loss to minimize, via the `add_loss`</span></span>
<span id="cb17-12"><a href="#cb17-12" tabindex="-1"></a><span class="co"># call during the forward pass!</span></span>
<span id="cb17-13"><a href="#cb17-13" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"adam"</span>)</span>
<span id="cb17-14"><a href="#cb17-14" tabindex="-1"></a>model.fit(np.random.random((<span class="dv">2</span>, <span class="dv">3</span>)), np.random.random((<span class="dv">2</span>, <span class="dv">3</span>)))</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="you-can-optionally-enable-serialization-on-your-layers">You can optionally enable serialization on your layers<a class="anchor" aria-label="anchor" href="#you-can-optionally-enable-serialization-on-your-layers"></a>
</h2>
<p>If you need your custom layers to be serializable as part of a <a href="/guides/functional_api/">Functional model</a>, you can optionally
implement a <code><a href="../../reference/get_config.html">get_config()</a></code> method:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="kw">class</span> Linear(keras.layers.Layer):</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, units<span class="op">=</span><span class="dv">32</span>):</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>        <span class="va">self</span>.units <span class="op">=</span> units</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a>        <span class="va">self</span>.w <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb18-8"><a href="#cb18-8" tabindex="-1"></a>            shape<span class="op">=</span>(input_shape[<span class="op">-</span><span class="dv">1</span>], <span class="va">self</span>.units),</span>
<span id="cb18-9"><a href="#cb18-9" tabindex="-1"></a>            initializer<span class="op">=</span><span class="st">"random_normal"</span>,</span>
<span id="cb18-10"><a href="#cb18-10" tabindex="-1"></a>            trainable<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb18-11"><a href="#cb18-11" tabindex="-1"></a>        )</span>
<span id="cb18-12"><a href="#cb18-12" tabindex="-1"></a>        <span class="va">self</span>.b <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb18-13"><a href="#cb18-13" tabindex="-1"></a>            shape<span class="op">=</span>(<span class="va">self</span>.units,), initializer<span class="op">=</span><span class="st">"random_normal"</span>, trainable<span class="op">=</span><span class="va">True</span></span>
<span id="cb18-14"><a href="#cb18-14" tabindex="-1"></a>        )</span>
<span id="cb18-15"><a href="#cb18-15" tabindex="-1"></a></span>
<span id="cb18-16"><a href="#cb18-16" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb18-17"><a href="#cb18-17" tabindex="-1"></a>        <span class="cf">return</span> ops.matmul(inputs, <span class="va">self</span>.w) <span class="op">+</span> <span class="va">self</span>.b</span>
<span id="cb18-18"><a href="#cb18-18" tabindex="-1"></a></span>
<span id="cb18-19"><a href="#cb18-19" tabindex="-1"></a>    <span class="kw">def</span> get_config(<span class="va">self</span>):</span>
<span id="cb18-20"><a href="#cb18-20" tabindex="-1"></a>        <span class="cf">return</span> {<span class="st">"units"</span>: <span class="va">self</span>.units}</span>
<span id="cb18-21"><a href="#cb18-21" tabindex="-1"></a></span>
<span id="cb18-22"><a href="#cb18-22" tabindex="-1"></a></span>
<span id="cb18-23"><a href="#cb18-23" tabindex="-1"></a><span class="co"># Now you can recreate the layer from its config:</span></span>
<span id="cb18-24"><a href="#cb18-24" tabindex="-1"></a>layer <span class="op">=</span> Linear(<span class="dv">64</span>)</span>
<span id="cb18-25"><a href="#cb18-25" tabindex="-1"></a>config <span class="op">=</span> layer.get_config()</span>
<span id="cb18-26"><a href="#cb18-26" tabindex="-1"></a><span class="bu">print</span>(config)</span>
<span id="cb18-27"><a href="#cb18-27" tabindex="-1"></a>new_layer <span class="op">=</span> Linear.from_config(config)</span></code></pre></div>
<p>Note that the <code>__init__()</code> method of the base
<code>Layer</code> class takes some keyword arguments, in particular a
<code>name</code> and a <code>dtype</code>. It’s good practice to pass
these arguments to the parent class in <code>__init__()</code> and to
include them in the layer config:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a><span class="kw">class</span> Linear(keras.layers.Layer):</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, units<span class="op">=</span><span class="dv">32</span>, <span class="op">**</span>kwargs):</span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a>        <span class="va">self</span>.units <span class="op">=</span> units</span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb19-7"><a href="#cb19-7" tabindex="-1"></a>        <span class="va">self</span>.w <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb19-8"><a href="#cb19-8" tabindex="-1"></a>            shape<span class="op">=</span>(input_shape[<span class="op">-</span><span class="dv">1</span>], <span class="va">self</span>.units),</span>
<span id="cb19-9"><a href="#cb19-9" tabindex="-1"></a>            initializer<span class="op">=</span><span class="st">"random_normal"</span>,</span>
<span id="cb19-10"><a href="#cb19-10" tabindex="-1"></a>            trainable<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb19-11"><a href="#cb19-11" tabindex="-1"></a>        )</span>
<span id="cb19-12"><a href="#cb19-12" tabindex="-1"></a>        <span class="va">self</span>.b <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb19-13"><a href="#cb19-13" tabindex="-1"></a>            shape<span class="op">=</span>(<span class="va">self</span>.units,), initializer<span class="op">=</span><span class="st">"random_normal"</span>, trainable<span class="op">=</span><span class="va">True</span></span>
<span id="cb19-14"><a href="#cb19-14" tabindex="-1"></a>        )</span>
<span id="cb19-15"><a href="#cb19-15" tabindex="-1"></a></span>
<span id="cb19-16"><a href="#cb19-16" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb19-17"><a href="#cb19-17" tabindex="-1"></a>        <span class="cf">return</span> ops.matmul(inputs, <span class="va">self</span>.w) <span class="op">+</span> <span class="va">self</span>.b</span>
<span id="cb19-18"><a href="#cb19-18" tabindex="-1"></a></span>
<span id="cb19-19"><a href="#cb19-19" tabindex="-1"></a>    <span class="kw">def</span> get_config(<span class="va">self</span>):</span>
<span id="cb19-20"><a href="#cb19-20" tabindex="-1"></a>        config <span class="op">=</span> <span class="bu">super</span>().get_config()</span>
<span id="cb19-21"><a href="#cb19-21" tabindex="-1"></a>        config.update({<span class="st">"units"</span>: <span class="va">self</span>.units})</span>
<span id="cb19-22"><a href="#cb19-22" tabindex="-1"></a>        <span class="cf">return</span> config</span>
<span id="cb19-23"><a href="#cb19-23" tabindex="-1"></a></span>
<span id="cb19-24"><a href="#cb19-24" tabindex="-1"></a></span>
<span id="cb19-25"><a href="#cb19-25" tabindex="-1"></a>layer <span class="op">=</span> Linear(<span class="dv">64</span>)</span>
<span id="cb19-26"><a href="#cb19-26" tabindex="-1"></a>config <span class="op">=</span> layer.get_config()</span>
<span id="cb19-27"><a href="#cb19-27" tabindex="-1"></a><span class="bu">print</span>(config)</span>
<span id="cb19-28"><a href="#cb19-28" tabindex="-1"></a>new_layer <span class="op">=</span> Linear.from_config(config)</span></code></pre></div>
<p>If you need more flexibility when deserializing the layer from its
config, you can also override the <code><a href="../../reference/get_config.html">from_config()</a></code> class
method. This is the base implementation of
<code><a href="../../reference/get_config.html">from_config()</a></code>:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="kw">def</span> from_config(cls, config):</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a>    <span class="cf">return</span> cls(<span class="op">**</span>config)</span></code></pre></div>
<p>To learn more about serialization and saving, see the complete <a href="/guides/serialization_and_saving/">guide to saving and serializing
models</a>.</p>
</div>
<div class="section level2">
<h2 id="privileged-training-argument-in-the-call-method">Privileged <code>training</code> argument in the <code>call()</code>
method<a class="anchor" aria-label="anchor" href="#privileged-training-argument-in-the-call-method"></a>
</h2>
<p>Some layers, in particular the <code>BatchNormalization</code> layer
and the <code>Dropout</code> layer, have different behaviors during
training and inference. For such layers, it is standard practice to
expose a <code>training</code> (boolean) argument in the
<code><a href="https://rdrr.io/r/base/call.html" class="external-link">call()</a></code> method.</p>
<p>By exposing this argument in <code><a href="https://rdrr.io/r/base/call.html" class="external-link">call()</a></code>, you enable the
built-in training and evaluation loops (e.g. <code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code>) to
correctly use the layer in training and inference.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a><span class="kw">class</span> CustomDropout(keras.layers.Layer):</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, rate, <span class="op">**</span>kwargs):</span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb21-4"><a href="#cb21-4" tabindex="-1"></a>        <span class="va">self</span>.rate <span class="op">=</span> rate</span>
<span id="cb21-5"><a href="#cb21-5" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs, training<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb21-7"><a href="#cb21-7" tabindex="-1"></a>        <span class="cf">if</span> training:</span>
<span id="cb21-8"><a href="#cb21-8" tabindex="-1"></a>            <span class="cf">return</span> keras.random.dropout(inputs, rate<span class="op">=</span><span class="va">self</span>.rate)</span>
<span id="cb21-9"><a href="#cb21-9" tabindex="-1"></a>        <span class="cf">return</span> inputs</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="privileged-mask-argument-in-the-call-method">Privileged <code>mask</code> argument in the <code>call()</code>
method<a class="anchor" aria-label="anchor" href="#privileged-mask-argument-in-the-call-method"></a>
</h2>
<p>The other privileged argument supported by <code><a href="https://rdrr.io/r/base/call.html" class="external-link">call()</a></code> is the
<code>mask</code> argument.</p>
<p>You will find it in all Keras RNN layers. A mask is a boolean tensor
(one boolean value per timestep in the input) used to skip certain input
timesteps when processing timeseries data.</p>
<p>Keras will automatically pass the correct <code>mask</code> argument
to <code>__call__()</code> for layers that support it, when a mask is
generated by a prior layer. Mask-generating layers are the
<code>Embedding</code> layer configured with
<code>mask_zero=True</code>, and the <code>Masking</code> layer.</p>
</div>
<div class="section level2">
<h2 id="the-model-class">The <code>Model</code> class<a class="anchor" aria-label="anchor" href="#the-model-class"></a>
</h2>
<p>In general, you will use the <code>Layer</code> class to define inner
computation blocks, and will use the <code>Model</code> class to define
the outer model – the object you will train.</p>
<p>For instance, in a ResNet50 model, you would have several ResNet
blocks subclassing <code>Layer</code>, and a single <code>Model</code>
encompassing the entire ResNet50 network.</p>
<p>The <code>Model</code> class has the same API as <code>Layer</code>,
with the following differences:</p>
<ul>
<li>It exposes built-in training, evaluation, and prediction loops
(<code>model.fit()</code>, <code>model.evaluate()</code>,
<code>model.predict()</code>).</li>
<li>It exposes the list of its inner layers, via the
<code>model.layers</code> property.</li>
<li>It exposes saving and serialization APIs (<code><a href="https://rdrr.io/r/base/save.html" class="external-link">save()</a></code>,
<code>save_weights()</code>…)</li>
</ul>
<p>Effectively, the <code>Layer</code> class corresponds to what we
refer to in the literature as a “layer” (as in “convolution layer” or
“recurrent layer”) or as a “block” (as in “ResNet block” or “Inception
block”).</p>
<p>Meanwhile, the <code>Model</code> class corresponds to what is
referred to in the literature as a “model” (as in “deep learning model”)
or as a “network” (as in “deep neural network”).</p>
<p>So if you’re wondering, “should I use the <code>Layer</code> class or
the <code>Model</code> class?”, ask yourself: will I need to call
<code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code> on it? Will I need to call <code><a href="https://rdrr.io/r/base/save.html" class="external-link">save()</a></code> on it?
If so, go with <code>Model</code>. If not (either because your class is
just a block in a bigger system, or because you are writing training
&amp; saving code yourself), use <code>Layer</code>.</p>
<p>For instance, we could take our mini-resnet example above, and use it
to build a <code>Model</code> that we could train with
<code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code>, and that we could save with
<code>save_weights()</code>:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a><span class="kw">class</span> ResNet(keras.Model):</span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_classes<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb22-4"><a href="#cb22-4" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb22-5"><a href="#cb22-5" tabindex="-1"></a>        <span class="va">self</span>.block_1 <span class="op">=</span> ResNetBlock()</span>
<span id="cb22-6"><a href="#cb22-6" tabindex="-1"></a>        <span class="va">self</span>.block_2 <span class="op">=</span> ResNetBlock()</span>
<span id="cb22-7"><a href="#cb22-7" tabindex="-1"></a>        <span class="va">self</span>.global_pool <span class="op">=</span> layers.GlobalAveragePooling2D()</span>
<span id="cb22-8"><a href="#cb22-8" tabindex="-1"></a>        <span class="va">self</span>.classifier <span class="op">=</span> Dense(num_classes)</span>
<span id="cb22-9"><a href="#cb22-9" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb22-11"><a href="#cb22-11" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.block_1(inputs)</span>
<span id="cb22-12"><a href="#cb22-12" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.block_2(x)</span>
<span id="cb22-13"><a href="#cb22-13" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.global_pool(x)</span>
<span id="cb22-14"><a href="#cb22-14" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.classifier(x)</span>
<span id="cb22-15"><a href="#cb22-15" tabindex="-1"></a></span>
<span id="cb22-16"><a href="#cb22-16" tabindex="-1"></a>resnet <span class="op">=</span> ResNet()</span>
<span id="cb22-17"><a href="#cb22-17" tabindex="-1"></a>dataset <span class="op">=</span> ...</span>
<span id="cb22-18"><a href="#cb22-18" tabindex="-1"></a>resnet.fit(dataset, epochs<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb22-19"><a href="#cb22-19" tabindex="-1"></a>resnet.save(filepath.keras)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="putting-it-all-together-an-end-to-end-example">Putting it all together: an end-to-end example<a class="anchor" aria-label="anchor" href="#putting-it-all-together-an-end-to-end-example"></a>
</h2>
<p>Here’s what you’ve learned so far:</p>
<ul>
<li>A <code>Layer</code> encapsulate a state (created in
<code>__init__()</code> or <code>build()</code>) and some computation
(defined in <code><a href="https://rdrr.io/r/base/call.html" class="external-link">call()</a></code>).</li>
<li>Layers can be recursively nested to create new, bigger computation
blocks.</li>
<li>Layers are backend-agnostic as long as they only use Keras APIs. You
can use backend-native APIs (such as <code>jax.numpy</code>,
<code>torch.nn</code> or <code>tf.nn</code>), but then your layer will
only be usable with that specific backend.</li>
<li>Layers can create and track losses (typically regularization losses)
via <code>add_loss()</code>.</li>
<li>The outer container, the thing you want to train, is a
<code>Model</code>. A <code>Model</code> is just like a
<code>Layer</code>, but with added training and serialization
utilities.</li>
</ul>
<p>Let’s put all of these things together into an end-to-end example:
we’re going to implement a Variational AutoEncoder (VAE) in a
backend-agnostic fashion – so that it runs the same with TensorFlow,
JAX, and PyTorch. We’ll train it on MNIST digits.</p>
<p>Our VAE will be a subclass of <code>Model</code>, built as a nested
composition of layers that subclass <code>Layer</code>. It will feature
a regularization loss (KL divergence).</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a><span class="kw">class</span> Sampling(layers.Layer):</span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a>    <span class="co">"""Uses (z_mean, z_log_var) to sample z, the vector encoding a digit."""</span></span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb23-5"><a href="#cb23-5" tabindex="-1"></a>        z_mean, z_log_var <span class="op">=</span> inputs</span>
<span id="cb23-6"><a href="#cb23-6" tabindex="-1"></a>        batch <span class="op">=</span> ops.shape(z_mean)[<span class="dv">0</span>]</span>
<span id="cb23-7"><a href="#cb23-7" tabindex="-1"></a>        dim <span class="op">=</span> ops.shape(z_mean)[<span class="dv">1</span>]</span>
<span id="cb23-8"><a href="#cb23-8" tabindex="-1"></a>        epsilon <span class="op">=</span> keras.random.normal(shape<span class="op">=</span>(batch, dim))</span>
<span id="cb23-9"><a href="#cb23-9" tabindex="-1"></a>        <span class="cf">return</span> z_mean <span class="op">+</span> ops.exp(<span class="fl">0.5</span> <span class="op">*</span> z_log_var) <span class="op">*</span> epsilon</span>
<span id="cb23-10"><a href="#cb23-10" tabindex="-1"></a></span>
<span id="cb23-11"><a href="#cb23-11" tabindex="-1"></a></span>
<span id="cb23-12"><a href="#cb23-12" tabindex="-1"></a><span class="kw">class</span> Encoder(layers.Layer):</span>
<span id="cb23-13"><a href="#cb23-13" tabindex="-1"></a>    <span class="co">"""Maps MNIST digits to a triplet (z_mean, z_log_var, z)."""</span></span>
<span id="cb23-14"><a href="#cb23-14" tabindex="-1"></a></span>
<span id="cb23-15"><a href="#cb23-15" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb23-16"><a href="#cb23-16" tabindex="-1"></a>        <span class="va">self</span>, latent_dim<span class="op">=</span><span class="dv">32</span>, intermediate_dim<span class="op">=</span><span class="dv">64</span>, name<span class="op">=</span><span class="st">"encoder"</span>, <span class="op">**</span>kwargs</span>
<span id="cb23-17"><a href="#cb23-17" tabindex="-1"></a>    ):</span>
<span id="cb23-18"><a href="#cb23-18" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(name<span class="op">=</span>name, <span class="op">**</span>kwargs)</span>
<span id="cb23-19"><a href="#cb23-19" tabindex="-1"></a>        <span class="va">self</span>.dense_proj <span class="op">=</span> layers.Dense(intermediate_dim, activation<span class="op">=</span><span class="st">"relu"</span>)</span>
<span id="cb23-20"><a href="#cb23-20" tabindex="-1"></a>        <span class="va">self</span>.dense_mean <span class="op">=</span> layers.Dense(latent_dim)</span>
<span id="cb23-21"><a href="#cb23-21" tabindex="-1"></a>        <span class="va">self</span>.dense_log_var <span class="op">=</span> layers.Dense(latent_dim)</span>
<span id="cb23-22"><a href="#cb23-22" tabindex="-1"></a>        <span class="va">self</span>.sampling <span class="op">=</span> Sampling()</span>
<span id="cb23-23"><a href="#cb23-23" tabindex="-1"></a></span>
<span id="cb23-24"><a href="#cb23-24" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb23-25"><a href="#cb23-25" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dense_proj(inputs)</span>
<span id="cb23-26"><a href="#cb23-26" tabindex="-1"></a>        z_mean <span class="op">=</span> <span class="va">self</span>.dense_mean(x)</span>
<span id="cb23-27"><a href="#cb23-27" tabindex="-1"></a>        z_log_var <span class="op">=</span> <span class="va">self</span>.dense_log_var(x)</span>
<span id="cb23-28"><a href="#cb23-28" tabindex="-1"></a>        z <span class="op">=</span> <span class="va">self</span>.sampling((z_mean, z_log_var))</span>
<span id="cb23-29"><a href="#cb23-29" tabindex="-1"></a>        <span class="cf">return</span> z_mean, z_log_var, z</span>
<span id="cb23-30"><a href="#cb23-30" tabindex="-1"></a></span>
<span id="cb23-31"><a href="#cb23-31" tabindex="-1"></a></span>
<span id="cb23-32"><a href="#cb23-32" tabindex="-1"></a><span class="kw">class</span> Decoder(layers.Layer):</span>
<span id="cb23-33"><a href="#cb23-33" tabindex="-1"></a>    <span class="co">"""Converts z, the encoded digit vector, back into a readable digit."""</span></span>
<span id="cb23-34"><a href="#cb23-34" tabindex="-1"></a></span>
<span id="cb23-35"><a href="#cb23-35" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb23-36"><a href="#cb23-36" tabindex="-1"></a>        <span class="va">self</span>, original_dim, intermediate_dim<span class="op">=</span><span class="dv">64</span>, name<span class="op">=</span><span class="st">"decoder"</span>, <span class="op">**</span>kwargs</span>
<span id="cb23-37"><a href="#cb23-37" tabindex="-1"></a>    ):</span>
<span id="cb23-38"><a href="#cb23-38" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(name<span class="op">=</span>name, <span class="op">**</span>kwargs)</span>
<span id="cb23-39"><a href="#cb23-39" tabindex="-1"></a>        <span class="va">self</span>.dense_proj <span class="op">=</span> layers.Dense(intermediate_dim, activation<span class="op">=</span><span class="st">"relu"</span>)</span>
<span id="cb23-40"><a href="#cb23-40" tabindex="-1"></a>        <span class="va">self</span>.dense_output <span class="op">=</span> layers.Dense(original_dim, activation<span class="op">=</span><span class="st">"sigmoid"</span>)</span>
<span id="cb23-41"><a href="#cb23-41" tabindex="-1"></a></span>
<span id="cb23-42"><a href="#cb23-42" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb23-43"><a href="#cb23-43" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dense_proj(inputs)</span>
<span id="cb23-44"><a href="#cb23-44" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.dense_output(x)</span>
<span id="cb23-45"><a href="#cb23-45" tabindex="-1"></a></span>
<span id="cb23-46"><a href="#cb23-46" tabindex="-1"></a></span>
<span id="cb23-47"><a href="#cb23-47" tabindex="-1"></a><span class="kw">class</span> VariationalAutoEncoder(keras.Model):</span>
<span id="cb23-48"><a href="#cb23-48" tabindex="-1"></a>    <span class="co">"""Combines the encoder and decoder into an end-to-end model for training."""</span></span>
<span id="cb23-49"><a href="#cb23-49" tabindex="-1"></a></span>
<span id="cb23-50"><a href="#cb23-50" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb23-51"><a href="#cb23-51" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb23-52"><a href="#cb23-52" tabindex="-1"></a>        original_dim,</span>
<span id="cb23-53"><a href="#cb23-53" tabindex="-1"></a>        intermediate_dim<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb23-54"><a href="#cb23-54" tabindex="-1"></a>        latent_dim<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb23-55"><a href="#cb23-55" tabindex="-1"></a>        name<span class="op">=</span><span class="st">"autoencoder"</span>,</span>
<span id="cb23-56"><a href="#cb23-56" tabindex="-1"></a>        <span class="op">**</span>kwargs</span>
<span id="cb23-57"><a href="#cb23-57" tabindex="-1"></a>    ):</span>
<span id="cb23-58"><a href="#cb23-58" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(name<span class="op">=</span>name, <span class="op">**</span>kwargs)</span>
<span id="cb23-59"><a href="#cb23-59" tabindex="-1"></a>        <span class="va">self</span>.original_dim <span class="op">=</span> original_dim</span>
<span id="cb23-60"><a href="#cb23-60" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> Encoder(</span>
<span id="cb23-61"><a href="#cb23-61" tabindex="-1"></a>            latent_dim<span class="op">=</span>latent_dim, intermediate_dim<span class="op">=</span>intermediate_dim</span>
<span id="cb23-62"><a href="#cb23-62" tabindex="-1"></a>        )</span>
<span id="cb23-63"><a href="#cb23-63" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> Decoder(original_dim, intermediate_dim<span class="op">=</span>intermediate_dim)</span>
<span id="cb23-64"><a href="#cb23-64" tabindex="-1"></a></span>
<span id="cb23-65"><a href="#cb23-65" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb23-66"><a href="#cb23-66" tabindex="-1"></a>        z_mean, z_log_var, z <span class="op">=</span> <span class="va">self</span>.encoder(inputs)</span>
<span id="cb23-67"><a href="#cb23-67" tabindex="-1"></a>        reconstructed <span class="op">=</span> <span class="va">self</span>.decoder(z)</span>
<span id="cb23-68"><a href="#cb23-68" tabindex="-1"></a>        <span class="co"># Add KL divergence regularization loss.</span></span>
<span id="cb23-69"><a href="#cb23-69" tabindex="-1"></a>        kl_loss <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> ops.mean(</span>
<span id="cb23-70"><a href="#cb23-70" tabindex="-1"></a>            z_log_var <span class="op">-</span> ops.square(z_mean) <span class="op">-</span> ops.exp(z_log_var) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb23-71"><a href="#cb23-71" tabindex="-1"></a>        )</span>
<span id="cb23-72"><a href="#cb23-72" tabindex="-1"></a>        <span class="va">self</span>.add_loss(kl_loss)</span>
<span id="cb23-73"><a href="#cb23-73" tabindex="-1"></a>        <span class="cf">return</span> reconstructed</span></code></pre></div>
<p>Let’s train it on MNIST using the <code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code> API:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a>(x_train, _), _ <span class="op">=</span> keras.datasets.mnist.load_data()</span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a>x_train <span class="op">=</span> x_train.reshape(<span class="dv">60000</span>, <span class="dv">784</span>).astype(<span class="st">"float32"</span>) <span class="op">/</span> <span class="dv">255</span></span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" tabindex="-1"></a>original_dim <span class="op">=</span> <span class="dv">784</span></span>
<span id="cb24-5"><a href="#cb24-5" tabindex="-1"></a>vae <span class="op">=</span> VariationalAutoEncoder(<span class="dv">784</span>, <span class="dv">64</span>, <span class="dv">32</span>)</span>
<span id="cb24-6"><a href="#cb24-6" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.Adam(learning_rate<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb24-8"><a href="#cb24-8" tabindex="-1"></a>vae.<span class="bu">compile</span>(optimizer, loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb24-9"><a href="#cb24-9" tabindex="-1"></a></span>
<span id="cb24-10"><a href="#cb24-10" tabindex="-1"></a>vae.fit(x_train, x_train, epochs<span class="op">=</span><span class="dv">2</span>, batch_size<span class="op">=</span><span class="dv">64</span>)</span></code></pre></div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, RStudio, Google.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
