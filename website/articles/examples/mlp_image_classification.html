<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Image classification with modern MLP models â€¢ keras</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../../bootstrap-toc.css">
<script src="../../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../../pkgdown.css" rel="stylesheet">
<script src="../../pkgdown.js"></script><link href="../../extra.css" rel="stylesheet">
<script src="../../extra.js"></script><meta property="og:title" content="Image classification with modern MLP models">
<meta property="og:description" content="Implementing the MLP-Mixer, FNet, and gMLP models for CIFAR-100 image classification.">
<meta property="og:image" content="https://keras.posit.co/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../../index.html">keras</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">2.13.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../../index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Tutorials
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/getting_started.html">Getting Started</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    </li>
    <li>
      <a href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    </li>
    <li>
      <a href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Guides
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li class="dropdown-header">Guides (New for TF 2.6)</li>
    <li>
      <a href="../../articles/new-guides/python_subclasses.html">Python Subclasses</a>
    </li>
    <li>
      <a href="../../articles/new-guides/making_new_layers_and_models_via_subclassing.html">Making New Layers and Models via Subclassing</a>
    </li>
    <li>
      <a href="../../articles/new-guides/customizing_what_happens_in_fit.html">Customizing What Happens in Fit</a>
    </li>
    <li>
      <a href="../../articles/new-guides/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    </li>
    <li>
      <a href="../../articles/new-guides/preprocessing_layers.html">Working with Preprocessing Layers</a>
    </li>
    <li>
      <a href="../../argicles/new-guides/working_with_rnns.html">Working with RNNs</a>
    </li>
    <li class="divider">
    </li>
<li class="dropdown-header">Using Keras</li>
    <li>
      <a href="../../articles/guide_keras.html">Guide to Keras Basics</a>
    </li>
    <li>
      <a href="../../articles/sequential_model.html">Sequential Model in Depth</a>
    </li>
    <li>
      <a href="../../articles/functional_api.html">Functional API in Depth</a>
    </li>
    <li>
      <a href="../../articles/about_keras_models.html">About Keras Models</a>
    </li>
    <li>
      <a href="../../articles/about_keras_layers.html">About Keras Layers</a>
    </li>
    <li>
      <a href="../../articles/training_visualization.html">Training Visualization</a>
    </li>
    <li>
      <a href="../../articles/applications.html">Pre-Trained Models</a>
    </li>
    <li>
      <a href="../../articles/faq.html">Frequently Asked Questions</a>
    </li>
    <li>
      <a href="../../articles/why_use_keras.html">Why Use Keras?</a>
    </li>
    <li class="divider">
    </li>
<li class="dropdown-header">Advanced</li>
    <li>
      <a href="../../articles/eager_guide.html">Eager Execution</a>
    </li>
    <li>
      <a href="../../articles/training_callbacks.html">Training Callbacks</a>
    </li>
    <li>
      <a href="../../articles/backend.html">Keras Backend</a>
    </li>
    <li>
      <a href="../../articles/custom_layers.html">Custom Layers</a>
    </li>
    <li>
      <a href="../../articles/custom_models.html">Custom Models</a>
    </li>
    <li>
      <a href="../../articles/saving_serializing.html">Saving and serializing</a>
    </li>
  </ul>
</li>
<li>
  <a href="../../articles/learn.html">Learn</a>
</li>
<li>
  <a href="../../articles/tools.html">Tools</a>
</li>
<li>
  <a href="../../articles/examples/index.html">Examples</a>
</li>
<li>
  <a href="../../reference/index.html">Reference</a>
</li>
<li>
  <a href="../../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/rstudio/keras/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Image classification with modern MLP models</h1>
                        <h4 data-toc-skip class="author"><a href="https://www.linkedin.com/in/khalid-salama-24403144/" class="external-link">Khalid
Salama</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/mlp_image_classification.Rmd" class="external-link"><code>vignettes/examples/mlp_image_classification.Rmd</code></a></small>
      <div class="hidden name"><code>mlp_image_classification.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>This example implements three modern attention-free, multi-layer
perceptron (MLP) based models for image classification, demonstrated on
the CIFAR-100 dataset:</p>
<ol style="list-style-type: decimal">
<li>The <a href="https://arxiv.org/abs/2105.01601" class="external-link">MLP-Mixer</a> model,
by Ilya Tolstikhin et al., based on two types of MLPs.</li>
<li>The <a href="https://arxiv.org/abs/2105.03824" class="external-link">FNet</a> model, by
James Lee-Thorp et al., based on unparameterized Fourier Transform.</li>
<li>The <a href="https://arxiv.org/abs/2105.08050" class="external-link">gMLP</a> model, by
Hanxiao Liu et al., based on MLP with gating.</li>
</ol>
<p>The purpose of the example is not to compare between these models, as
they might perform differently on different datasets with well-tuned
hyperparameters. Rather, it is to show simple implementations of their
main building blocks.</p>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="prepare-the-data">Prepare the data<a class="anchor" aria-label="anchor" href="#prepare-the-data"></a>
</h2>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>num_classes <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>input_shape <span class="op">=</span> (<span class="dv">32</span>, <span class="dv">32</span>, <span class="dv">3</span>)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>(x_train, y_train), (x_test, y_test) <span class="op">=</span> keras.datasets.cifar100.load_data()</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x_train shape: </span><span class="sc">{</span>x_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> - y_train shape: </span><span class="sc">{</span>y_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x_test shape: </span><span class="sc">{</span>x_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> - y_test shape: </span><span class="sc">{</span>y_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="configure-the-hyperparameters">Configure the hyperparameters<a class="anchor" aria-label="anchor" href="#configure-the-hyperparameters"></a>
</h2>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>weight_decay <span class="op">=</span> <span class="fl">0.0001</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>dropout_rate <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>image_size <span class="op">=</span> <span class="dv">64</span>  <span class="co"># We'll resize input images to this size.</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>patch_size <span class="op">=</span> <span class="dv">8</span>  <span class="co"># Size of the patches to be extracted from the input images.</span></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>num_patches <span class="op">=</span> (image_size <span class="op">//</span> patch_size) <span class="op">**</span> <span class="dv">2</span>  <span class="co"># Size of the data array.</span></span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>embedding_dim <span class="op">=</span> <span class="dv">256</span>  <span class="co"># Number of hidden units.</span></span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>num_blocks <span class="op">=</span> <span class="dv">4</span>  <span class="co"># Number of blocks.</span></span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image size: </span><span class="sc">{</span>image_size<span class="sc">}</span><span class="ss"> X </span><span class="sc">{</span>image_size<span class="sc">}</span><span class="ss"> = </span><span class="sc">{</span>image_size <span class="op">**</span> <span class="dv">2</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Patch size: </span><span class="sc">{</span>patch_size<span class="sc">}</span><span class="ss"> X </span><span class="sc">{</span>patch_size<span class="sc">}</span><span class="ss"> = </span><span class="sc">{</span>patch_size <span class="op">**</span> <span class="dv">2</span><span class="sc">}</span><span class="ss"> "</span>)</span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Patches per image: </span><span class="sc">{</span>num_patches<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Elements per patch (3 channels): </span><span class="sc">{</span>(patch_size <span class="op">**</span> <span class="dv">2</span>) <span class="op">*</span> <span class="dv">3</span><span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="build-a-classification-model">Build a classification model<a class="anchor" aria-label="anchor" href="#build-a-classification-model"></a>
</h2>
<p>We implement a method that builds a classifier given the processing
blocks.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="kw">def</span> build_classifier(blocks, positional_encoding<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>    inputs <span class="op">=</span> layers.Input(shape<span class="op">=</span>input_shape)</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>    <span class="co"># Augment data.</span></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>    augmented <span class="op">=</span> data_augmentation(inputs)</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>    <span class="co"># Create patches.</span></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>    patches <span class="op">=</span> Patches(patch_size)(augmented)</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>    <span class="co"># Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.</span></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>    x <span class="op">=</span> layers.Dense(units<span class="op">=</span>embedding_dim)(patches)</span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>    <span class="cf">if</span> positional_encoding:</span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> PositionEmbedding(sequence_length<span class="op">=</span>num_patches)(x)</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>    <span class="co"># Process x using the module blocks.</span></span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>    x <span class="op">=</span> blocks(x)</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a>    <span class="co"># Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor.</span></span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>    representation <span class="op">=</span> layers.GlobalAveragePooling1D()(x)</span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a>    <span class="co"># Apply dropout.</span></span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a>    representation <span class="op">=</span> layers.Dropout(rate<span class="op">=</span>dropout_rate)(representation)</span>
<span id="cb4-17"><a href="#cb4-17" tabindex="-1"></a>    <span class="co"># Compute logits outputs.</span></span>
<span id="cb4-18"><a href="#cb4-18" tabindex="-1"></a>    logits <span class="op">=</span> layers.Dense(num_classes)(representation)</span>
<span id="cb4-19"><a href="#cb4-19" tabindex="-1"></a>    <span class="co"># Create the Keras model.</span></span>
<span id="cb4-20"><a href="#cb4-20" tabindex="-1"></a>    <span class="cf">return</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>logits)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="define-an-experiment">Define an experiment<a class="anchor" aria-label="anchor" href="#define-an-experiment"></a>
</h2>
<p>We implement a utility function to compile, train, and evaluate a
given model.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="kw">def</span> run_experiment(model):</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>    <span class="co"># Create Adam optimizer with weight decay.</span></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>    optimizer <span class="op">=</span> keras.optimizers.AdamW(</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>        learning_rate<span class="op">=</span>learning_rate,</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>        weight_decay<span class="op">=</span>weight_decay,</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>    )</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>    <span class="co"># Compile the model.</span></span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>    model.<span class="bu">compile</span>(</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>        loss<span class="op">=</span>keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>        metrics<span class="op">=</span>[</span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>            keras.metrics.SparseCategoricalAccuracy(name<span class="op">=</span><span class="st">"acc"</span>),</span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>            keras.metrics.SparseTopKCategoricalAccuracy(<span class="dv">5</span>, name<span class="op">=</span><span class="st">"top5-acc"</span>),</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>        ],</span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a>    )</span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>    <span class="co"># Create a learning rate scheduler callback.</span></span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>    reduce_lr <span class="op">=</span> keras.callbacks.ReduceLROnPlateau(</span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a>        monitor<span class="op">=</span><span class="st">"val_loss"</span>, factor<span class="op">=</span><span class="fl">0.5</span>, patience<span class="op">=</span><span class="dv">5</span></span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a>    )</span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a>    <span class="co"># Create an early stopping callback.</span></span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a>    early_stopping <span class="op">=</span> keras.callbacks.EarlyStopping(</span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a>        monitor<span class="op">=</span><span class="st">"val_loss"</span>, patience<span class="op">=</span><span class="dv">10</span>, restore_best_weights<span class="op">=</span><span class="va">True</span></span>
<span id="cb5-23"><a href="#cb5-23" tabindex="-1"></a>    )</span>
<span id="cb5-24"><a href="#cb5-24" tabindex="-1"></a>    <span class="co"># Fit the model.</span></span>
<span id="cb5-25"><a href="#cb5-25" tabindex="-1"></a>    history <span class="op">=</span> model.fit(</span>
<span id="cb5-26"><a href="#cb5-26" tabindex="-1"></a>        x<span class="op">=</span>x_train,</span>
<span id="cb5-27"><a href="#cb5-27" tabindex="-1"></a>        y<span class="op">=</span>y_train,</span>
<span id="cb5-28"><a href="#cb5-28" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb5-29"><a href="#cb5-29" tabindex="-1"></a>        epochs<span class="op">=</span>num_epochs,</span>
<span id="cb5-30"><a href="#cb5-30" tabindex="-1"></a>        validation_split<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb5-31"><a href="#cb5-31" tabindex="-1"></a>        callbacks<span class="op">=</span>[early_stopping, reduce_lr],</span>
<span id="cb5-32"><a href="#cb5-32" tabindex="-1"></a>    )</span>
<span id="cb5-33"><a href="#cb5-33" tabindex="-1"></a></span>
<span id="cb5-34"><a href="#cb5-34" tabindex="-1"></a>    _, accuracy, top_5_accuracy <span class="op">=</span> model.evaluate(x_test, y_test)</span>
<span id="cb5-35"><a href="#cb5-35" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Test accuracy: </span><span class="sc">{</span><span class="bu">round</span>(accuracy <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">%"</span>)</span>
<span id="cb5-36"><a href="#cb5-36" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Test top 5 accuracy: </span><span class="sc">{</span><span class="bu">round</span>(top_5_accuracy <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">%"</span>)</span>
<span id="cb5-37"><a href="#cb5-37" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" tabindex="-1"></a>    <span class="co"># Return history to plot learning curves.</span></span>
<span id="cb5-39"><a href="#cb5-39" tabindex="-1"></a>    <span class="cf">return</span> history</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="use-data-augmentation">Use data augmentation<a class="anchor" aria-label="anchor" href="#use-data-augmentation"></a>
</h2>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>data_augmentation <span class="op">=</span> keras.Sequential(</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>    [</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>        layers.Normalization(),</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>        layers.Resizing(image_size, image_size),</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>        layers.RandomFlip(<span class="st">"horizontal"</span>),</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>        layers.RandomZoom(height_factor<span class="op">=</span><span class="fl">0.2</span>, width_factor<span class="op">=</span><span class="fl">0.2</span>),</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>    ],</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"data_augmentation"</span>,</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>)</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a><span class="co"># Compute the mean and the variance of the training data for normalization.</span></span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>data_augmentation.layers[<span class="dv">0</span>].adapt(x_train)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="implement-patch-extraction-as-a-layer">Implement patch extraction as a layer<a class="anchor" aria-label="anchor" href="#implement-patch-extraction-as-a-layer"></a>
</h2>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="kw">class</span> Patches(layers.Layer):</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, patch_size, <span class="op">**</span>kwargs):</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>        <span class="va">self</span>.patch_size <span class="op">=</span> patch_size</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, x):</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>        patches <span class="op">=</span> keras.ops.image.extract_patches(x, <span class="va">self</span>.patch_size)</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>        batch_size <span class="op">=</span> keras.ops.shape(patches)[<span class="dv">0</span>]</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>        num_patches <span class="op">=</span> keras.ops.shape(patches)[<span class="dv">1</span>] <span class="op">*</span> keras.ops.shape(patches)[<span class="dv">2</span>]</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>        patch_dim <span class="op">=</span> keras.ops.shape(patches)[<span class="dv">3</span>]</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>        out <span class="op">=</span> keras.ops.reshape(patches, (batch_size, num_patches, patch_dim))</span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>        <span class="cf">return</span> out</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="implement-position-embedding-as-a-layer">Implement position embedding as a layer<a class="anchor" aria-label="anchor" href="#implement-position-embedding-as-a-layer"></a>
</h2>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="kw">class</span> PositionEmbedding(keras.layers.Layer):</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>        sequence_length,</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>        initializer<span class="op">=</span><span class="st">"glorot_uniform"</span>,</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>        <span class="op">**</span>kwargs,</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>    ):</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>        <span class="cf">if</span> sequence_length <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>                <span class="st">"`sequence_length` must be an Integer, received `None`."</span></span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>            )</span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>        <span class="va">self</span>.sequence_length <span class="op">=</span> <span class="bu">int</span>(sequence_length)</span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>        <span class="va">self</span>.initializer <span class="op">=</span> keras.initializers.get(initializer)</span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a>    <span class="kw">def</span> get_config(<span class="va">self</span>):</span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a>        config <span class="op">=</span> <span class="bu">super</span>().get_config()</span>
<span id="cb8-18"><a href="#cb8-18" tabindex="-1"></a>        config.update(</span>
<span id="cb8-19"><a href="#cb8-19" tabindex="-1"></a>            {</span>
<span id="cb8-20"><a href="#cb8-20" tabindex="-1"></a>                <span class="st">"sequence_length"</span>: <span class="va">self</span>.sequence_length,</span>
<span id="cb8-21"><a href="#cb8-21" tabindex="-1"></a>                <span class="st">"initializer"</span>: keras.initializers.serialize(<span class="va">self</span>.initializer),</span>
<span id="cb8-22"><a href="#cb8-22" tabindex="-1"></a>            }</span>
<span id="cb8-23"><a href="#cb8-23" tabindex="-1"></a>        )</span>
<span id="cb8-24"><a href="#cb8-24" tabindex="-1"></a>        <span class="cf">return</span> config</span>
<span id="cb8-25"><a href="#cb8-25" tabindex="-1"></a></span>
<span id="cb8-26"><a href="#cb8-26" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb8-27"><a href="#cb8-27" tabindex="-1"></a>        feature_size <span class="op">=</span> input_shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb8-28"><a href="#cb8-28" tabindex="-1"></a>        <span class="va">self</span>.position_embeddings <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb8-29"><a href="#cb8-29" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"embeddings"</span>,</span>
<span id="cb8-30"><a href="#cb8-30" tabindex="-1"></a>            shape<span class="op">=</span>[<span class="va">self</span>.sequence_length, feature_size],</span>
<span id="cb8-31"><a href="#cb8-31" tabindex="-1"></a>            initializer<span class="op">=</span><span class="va">self</span>.initializer,</span>
<span id="cb8-32"><a href="#cb8-32" tabindex="-1"></a>            trainable<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb8-33"><a href="#cb8-33" tabindex="-1"></a>        )</span>
<span id="cb8-34"><a href="#cb8-34" tabindex="-1"></a></span>
<span id="cb8-35"><a href="#cb8-35" tabindex="-1"></a>        <span class="bu">super</span>().build(input_shape)</span>
<span id="cb8-36"><a href="#cb8-36" tabindex="-1"></a></span>
<span id="cb8-37"><a href="#cb8-37" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs, start_index<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb8-38"><a href="#cb8-38" tabindex="-1"></a>        shape <span class="op">=</span> keras.ops.shape(inputs)</span>
<span id="cb8-39"><a href="#cb8-39" tabindex="-1"></a>        feature_length <span class="op">=</span> shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb8-40"><a href="#cb8-40" tabindex="-1"></a>        sequence_length <span class="op">=</span> shape[<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb8-41"><a href="#cb8-41" tabindex="-1"></a>        <span class="co"># trim to match the length of the input sequence, which might be less</span></span>
<span id="cb8-42"><a href="#cb8-42" tabindex="-1"></a>        <span class="co"># than the sequence_length of the layer.</span></span>
<span id="cb8-43"><a href="#cb8-43" tabindex="-1"></a>        position_embeddings <span class="op">=</span> keras.ops.convert_to_tensor(<span class="va">self</span>.position_embeddings)</span>
<span id="cb8-44"><a href="#cb8-44" tabindex="-1"></a>        position_embeddings <span class="op">=</span> keras.ops.<span class="bu">slice</span>(</span>
<span id="cb8-45"><a href="#cb8-45" tabindex="-1"></a>            position_embeddings,</span>
<span id="cb8-46"><a href="#cb8-46" tabindex="-1"></a>            (start_index, <span class="dv">0</span>),</span>
<span id="cb8-47"><a href="#cb8-47" tabindex="-1"></a>            (sequence_length, feature_length),</span>
<span id="cb8-48"><a href="#cb8-48" tabindex="-1"></a>        )</span>
<span id="cb8-49"><a href="#cb8-49" tabindex="-1"></a>        <span class="cf">return</span> keras.ops.broadcast_to(position_embeddings, shape)</span>
<span id="cb8-50"><a href="#cb8-50" tabindex="-1"></a></span>
<span id="cb8-51"><a href="#cb8-51" tabindex="-1"></a>    <span class="kw">def</span> compute_output_shape(<span class="va">self</span>, input_shape):</span>
<span id="cb8-52"><a href="#cb8-52" tabindex="-1"></a>        <span class="cf">return</span> input_shape</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="the-mlp-mixer-model">The MLP-Mixer model<a class="anchor" aria-label="anchor" href="#the-mlp-mixer-model"></a>
</h2>
<p>The MLP-Mixer is an architecture based exclusively on multi-layer
perceptrons (MLPs), that contains two types of MLP layers:</p>
<ol style="list-style-type: decimal">
<li>One applied independently to image patches, which mixes the
per-location features.</li>
<li>The other applied across patches (along channels), which mixes
spatial information.</li>
</ol>
<p>This is similar to a <a href="https://arxiv.org/pdf/1610.02357.pdf" class="external-link">depthwise separable
convolution based model</a> such as the Xception model, but with two
chained dense transforms, no max pooling, and layer normalization
instead of batch normalization.</p>
<div class="section level3">
<h3 id="implement-the-mlp-mixer-module">Implement the MLP-Mixer module<a class="anchor" aria-label="anchor" href="#implement-the-mlp-mixer-module"></a>
</h3>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="kw">class</span> MLPMixerLayer(layers.Layer):</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>        <span class="va">self</span>, num_patches, hidden_units, dropout_rate, <span class="op">*</span>args, <span class="op">**</span>kwargs</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>    ):</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>        <span class="va">self</span>.mlp1 <span class="op">=</span> keras.Sequential(</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>            [</span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a>                layers.Dense(units<span class="op">=</span>num_patches, activation<span class="op">=</span><span class="st">"gelu"</span>),</span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a>                layers.Dense(units<span class="op">=</span>num_patches),</span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a>                layers.Dropout(rate<span class="op">=</span>dropout_rate),</span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a>            ]</span>
<span id="cb9-13"><a href="#cb9-13" tabindex="-1"></a>        )</span>
<span id="cb9-14"><a href="#cb9-14" tabindex="-1"></a>        <span class="va">self</span>.mlp2 <span class="op">=</span> keras.Sequential(</span>
<span id="cb9-15"><a href="#cb9-15" tabindex="-1"></a>            [</span>
<span id="cb9-16"><a href="#cb9-16" tabindex="-1"></a>                layers.Dense(units<span class="op">=</span>num_patches, activation<span class="op">=</span><span class="st">"gelu"</span>),</span>
<span id="cb9-17"><a href="#cb9-17" tabindex="-1"></a>                layers.Dense(units<span class="op">=</span>hidden_units),</span>
<span id="cb9-18"><a href="#cb9-18" tabindex="-1"></a>                layers.Dropout(rate<span class="op">=</span>dropout_rate),</span>
<span id="cb9-19"><a href="#cb9-19" tabindex="-1"></a>            ]</span>
<span id="cb9-20"><a href="#cb9-20" tabindex="-1"></a>        )</span>
<span id="cb9-21"><a href="#cb9-21" tabindex="-1"></a>        <span class="va">self</span>.normalize <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span><span class="fl">1e-6</span>)</span>
<span id="cb9-22"><a href="#cb9-22" tabindex="-1"></a></span>
<span id="cb9-23"><a href="#cb9-23" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb9-24"><a href="#cb9-24" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">super</span>().build(input_shape)</span>
<span id="cb9-25"><a href="#cb9-25" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb9-27"><a href="#cb9-27" tabindex="-1"></a>        <span class="co"># Apply layer normalization.</span></span>
<span id="cb9-28"><a href="#cb9-28" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.normalize(inputs)</span>
<span id="cb9-29"><a href="#cb9-29" tabindex="-1"></a>        <span class="co"># Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].</span></span>
<span id="cb9-30"><a href="#cb9-30" tabindex="-1"></a>        x_channels <span class="op">=</span> keras.ops.transpose(x, axes<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb9-31"><a href="#cb9-31" tabindex="-1"></a>        <span class="co"># Apply mlp1 on each channel independently.</span></span>
<span id="cb9-32"><a href="#cb9-32" tabindex="-1"></a>        mlp1_outputs <span class="op">=</span> <span class="va">self</span>.mlp1(x_channels)</span>
<span id="cb9-33"><a href="#cb9-33" tabindex="-1"></a>        <span class="co"># Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].</span></span>
<span id="cb9-34"><a href="#cb9-34" tabindex="-1"></a>        mlp1_outputs <span class="op">=</span> keras.ops.transpose(mlp1_outputs, axes<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb9-35"><a href="#cb9-35" tabindex="-1"></a>        <span class="co"># Add skip connection.</span></span>
<span id="cb9-36"><a href="#cb9-36" tabindex="-1"></a>        x <span class="op">=</span> mlp1_outputs <span class="op">+</span> inputs</span>
<span id="cb9-37"><a href="#cb9-37" tabindex="-1"></a>        <span class="co"># Apply layer normalization.</span></span>
<span id="cb9-38"><a href="#cb9-38" tabindex="-1"></a>        x_patches <span class="op">=</span> <span class="va">self</span>.normalize(x)</span>
<span id="cb9-39"><a href="#cb9-39" tabindex="-1"></a>        <span class="co"># Apply mlp2 on each patch independtenly.</span></span>
<span id="cb9-40"><a href="#cb9-40" tabindex="-1"></a>        mlp2_outputs <span class="op">=</span> <span class="va">self</span>.mlp2(x_patches)</span>
<span id="cb9-41"><a href="#cb9-41" tabindex="-1"></a>        <span class="co"># Add skip connection.</span></span>
<span id="cb9-42"><a href="#cb9-42" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> mlp2_outputs</span>
<span id="cb9-43"><a href="#cb9-43" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div>
</div>
<div class="section level3">
<h3 id="build-train-and-evaluate-the-mlp-mixer-model">Build, train, and evaluate the MLP-Mixer model<a class="anchor" aria-label="anchor" href="#build-train-and-evaluate-the-mlp-mixer-model"></a>
</h3>
<p>Note that training the model with the current settings on a V100 GPUs
takes around 8 seconds per epoch.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>mlpmixer_blocks <span class="op">=</span> keras.Sequential(</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>    [</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>        MLPMixerLayer(num_patches, embedding_dim, dropout_rate)</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_blocks)</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>    ]</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>)</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.005</span></span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>mlpmixer_classifier <span class="op">=</span> build_classifier(mlpmixer_blocks)</span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a>history <span class="op">=</span> run_experiment(mlpmixer_classifier)</span></code></pre></div>
<p>The MLP-Mixer model tends to have much less number of parameters
compared to convolutional and transformer-based models, which leads to
less training and serving computational cost.</p>
<p>As mentioned in the <a href="https://arxiv.org/abs/2105.01601" class="external-link">MLP-Mixer</a> paper, when
pre-trained on large datasets, or with modern regularization schemes,
the MLP-Mixer attains competitive scores to state-of-the-art models. You
can obtain better results by increasing the embedding dimensions,
increasing the number of mixer blocks, and training the model for
longer. You may also try to increase the size of the input images and
use different patch sizes.</p>
</div>
</div>
<div class="section level2">
<h2 id="the-fnet-model">The FNet model<a class="anchor" aria-label="anchor" href="#the-fnet-model"></a>
</h2>
<p>The FNet uses a similar block to the Transformer block. However, FNet
replaces the self-attention layer in the Transformer block with a
parameter-free 2D Fourier transformation layer:</p>
<ol style="list-style-type: decimal">
<li>One 1D Fourier Transform is applied along the patches.</li>
<li>One 1D Fourier Transform is applied along the channels.</li>
</ol>
<div class="section level3">
<h3 id="implement-the-fnet-module">Implement the FNet module<a class="anchor" aria-label="anchor" href="#implement-the-fnet-module"></a>
</h3>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="kw">class</span> FNetLayer(layers.Layer):</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>        <span class="va">self</span>, embedding_dim, dropout_rate, <span class="op">*</span>args, <span class="op">**</span>kwargs</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>    ):</span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>        <span class="va">self</span>.ffn <span class="op">=</span> keras.Sequential(</span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a>            [</span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a>                layers.Dense(units<span class="op">=</span>embedding_dim, activation<span class="op">=</span><span class="st">"gelu"</span>),</span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a>                layers.Dropout(rate<span class="op">=</span>dropout_rate),</span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a>                layers.Dense(units<span class="op">=</span>embedding_dim),</span>
<span id="cb11-12"><a href="#cb11-12" tabindex="-1"></a>            ]</span>
<span id="cb11-13"><a href="#cb11-13" tabindex="-1"></a>        )</span>
<span id="cb11-14"><a href="#cb11-14" tabindex="-1"></a></span>
<span id="cb11-15"><a href="#cb11-15" tabindex="-1"></a>        <span class="va">self</span>.normalize1 <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span><span class="fl">1e-6</span>)</span>
<span id="cb11-16"><a href="#cb11-16" tabindex="-1"></a>        <span class="va">self</span>.normalize2 <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span><span class="fl">1e-6</span>)</span>
<span id="cb11-17"><a href="#cb11-17" tabindex="-1"></a></span>
<span id="cb11-18"><a href="#cb11-18" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb11-19"><a href="#cb11-19" tabindex="-1"></a>        <span class="co"># Apply fourier transformations.</span></span>
<span id="cb11-20"><a href="#cb11-20" tabindex="-1"></a>        real_part <span class="op">=</span> inputs</span>
<span id="cb11-21"><a href="#cb11-21" tabindex="-1"></a>        im_part <span class="op">=</span> keras.ops.zeros_like(inputs)</span>
<span id="cb11-22"><a href="#cb11-22" tabindex="-1"></a>        x <span class="op">=</span> keras.ops.fft2((real_part, im_part))[<span class="dv">0</span>]</span>
<span id="cb11-23"><a href="#cb11-23" tabindex="-1"></a>        <span class="co"># Add skip connection.</span></span>
<span id="cb11-24"><a href="#cb11-24" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> inputs</span>
<span id="cb11-25"><a href="#cb11-25" tabindex="-1"></a>        <span class="co"># Apply layer normalization.</span></span>
<span id="cb11-26"><a href="#cb11-26" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.normalize1(x)</span>
<span id="cb11-27"><a href="#cb11-27" tabindex="-1"></a>        <span class="co"># Apply Feedfowrad network.</span></span>
<span id="cb11-28"><a href="#cb11-28" tabindex="-1"></a>        x_ffn <span class="op">=</span> <span class="va">self</span>.ffn(x)</span>
<span id="cb11-29"><a href="#cb11-29" tabindex="-1"></a>        <span class="co"># Add skip connection.</span></span>
<span id="cb11-30"><a href="#cb11-30" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> x_ffn</span>
<span id="cb11-31"><a href="#cb11-31" tabindex="-1"></a>        <span class="co"># Apply layer normalization.</span></span>
<span id="cb11-32"><a href="#cb11-32" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.normalize2(x)</span></code></pre></div>
</div>
<div class="section level3">
<h3 id="build-train-and-evaluate-the-fnet-model">Build, train, and evaluate the FNet model<a class="anchor" aria-label="anchor" href="#build-train-and-evaluate-the-fnet-model"></a>
</h3>
<p>Note that training the model with the current settings on a V100 GPUs
takes around 8 seconds per epoch.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>fnet_blocks <span class="op">=</span> keras.Sequential(</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>    [</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>        FNetLayer(embedding_dim, dropout_rate)</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_blocks)</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>    ]</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>)</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a>fnet_classifier <span class="op">=</span> build_classifier(fnet_blocks, positional_encoding<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a>history <span class="op">=</span> run_experiment(fnet_classifier)</span></code></pre></div>
<p>As shown in the <a href="https://arxiv.org/abs/2105.03824" class="external-link">FNet</a>
paper, better results can be achieved by increasing the embedding
dimensions, increasing the number of FNet blocks, and training the model
for longer. You may also try to increase the size of the input images
and use different patch sizes. The FNet scales very efficiently to long
inputs, runs much faster than attention-based Transformer models, and
produces competitive accuracy results.</p>
</div>
</div>
<div class="section level2">
<h2 id="the-gmlp-model">The gMLP model<a class="anchor" aria-label="anchor" href="#the-gmlp-model"></a>
</h2>
<p>The gMLP is a MLP architecture that features a Spatial Gating Unit
(SGU). The SGU enables cross-patch interactions across the spatial
(channel) dimension, by:</p>
<ol style="list-style-type: decimal">
<li>Transforming the input spatially by applying linear projection
across patches (along channels).</li>
<li>Applying element-wise multiplication of the input and its spatial
transformation.</li>
</ol>
<div class="section level3">
<h3 id="implement-the-gmlp-module">Implement the gMLP module<a class="anchor" aria-label="anchor" href="#implement-the-gmlp-module"></a>
</h3>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="kw">class</span> gMLPLayer(layers.Layer):</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>        <span class="va">self</span>, num_patches, embedding_dim, dropout_rate, <span class="op">*</span>args, <span class="op">**</span>kwargs</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>    ):</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a>        <span class="va">self</span>.channel_projection1 <span class="op">=</span> keras.Sequential(</span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a>            [</span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a>                layers.Dense(units<span class="op">=</span>embedding_dim <span class="op">*</span> <span class="dv">2</span>, activation<span class="op">=</span><span class="st">"gelu"</span>),</span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a>                layers.Dropout(rate<span class="op">=</span>dropout_rate),</span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a>            ]</span>
<span id="cb13-12"><a href="#cb13-12" tabindex="-1"></a>        )</span>
<span id="cb13-13"><a href="#cb13-13" tabindex="-1"></a></span>
<span id="cb13-14"><a href="#cb13-14" tabindex="-1"></a>        <span class="va">self</span>.channel_projection2 <span class="op">=</span> layers.Dense(units<span class="op">=</span>embedding_dim)</span>
<span id="cb13-15"><a href="#cb13-15" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" tabindex="-1"></a>        <span class="va">self</span>.spatial_projection <span class="op">=</span> layers.Dense(</span>
<span id="cb13-17"><a href="#cb13-17" tabindex="-1"></a>            units<span class="op">=</span>num_patches, bias_initializer<span class="op">=</span><span class="st">"Ones"</span></span>
<span id="cb13-18"><a href="#cb13-18" tabindex="-1"></a>        )</span>
<span id="cb13-19"><a href="#cb13-19" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" tabindex="-1"></a>        <span class="va">self</span>.normalize1 <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span><span class="fl">1e-6</span>)</span>
<span id="cb13-21"><a href="#cb13-21" tabindex="-1"></a>        <span class="va">self</span>.normalize2 <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span><span class="fl">1e-6</span>)</span>
<span id="cb13-22"><a href="#cb13-22" tabindex="-1"></a></span>
<span id="cb13-23"><a href="#cb13-23" tabindex="-1"></a>    <span class="kw">def</span> spatial_gating_unit(<span class="va">self</span>, x):</span>
<span id="cb13-24"><a href="#cb13-24" tabindex="-1"></a>        <span class="co"># Split x along the channel dimensions.</span></span>
<span id="cb13-25"><a href="#cb13-25" tabindex="-1"></a>        <span class="co"># Tensors u and v will in the shape of [batch_size, num_patchs, embedding_dim].</span></span>
<span id="cb13-26"><a href="#cb13-26" tabindex="-1"></a>        u, v <span class="op">=</span> keras.ops.split(x, indices_or_sections<span class="op">=</span><span class="dv">2</span>, axis<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb13-27"><a href="#cb13-27" tabindex="-1"></a>        <span class="co"># Apply layer normalization.</span></span>
<span id="cb13-28"><a href="#cb13-28" tabindex="-1"></a>        v <span class="op">=</span> <span class="va">self</span>.normalize2(v)</span>
<span id="cb13-29"><a href="#cb13-29" tabindex="-1"></a>        <span class="co"># Apply spatial projection.</span></span>
<span id="cb13-30"><a href="#cb13-30" tabindex="-1"></a>        v_channels <span class="op">=</span> keras.ops.transpose(v, axes<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb13-31"><a href="#cb13-31" tabindex="-1"></a>        v_projected <span class="op">=</span> <span class="va">self</span>.spatial_projection(v_channels)</span>
<span id="cb13-32"><a href="#cb13-32" tabindex="-1"></a>        v_projected <span class="op">=</span> keras.ops.transpose(v_projected, axes<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb13-33"><a href="#cb13-33" tabindex="-1"></a>        <span class="co"># Apply element-wise multiplication.</span></span>
<span id="cb13-34"><a href="#cb13-34" tabindex="-1"></a>        <span class="cf">return</span> u <span class="op">*</span> v_projected</span>
<span id="cb13-35"><a href="#cb13-35" tabindex="-1"></a></span>
<span id="cb13-36"><a href="#cb13-36" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb13-37"><a href="#cb13-37" tabindex="-1"></a>        <span class="co"># Apply layer normalization.</span></span>
<span id="cb13-38"><a href="#cb13-38" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.normalize1(inputs)</span>
<span id="cb13-39"><a href="#cb13-39" tabindex="-1"></a>        <span class="co"># Apply the first channel projection. x_projected shape: [batch_size, num_patches, embedding_dim * 2].</span></span>
<span id="cb13-40"><a href="#cb13-40" tabindex="-1"></a>        x_projected <span class="op">=</span> <span class="va">self</span>.channel_projection1(x)</span>
<span id="cb13-41"><a href="#cb13-41" tabindex="-1"></a>        <span class="co"># Apply the spatial gating unit. x_spatial shape: [batch_size, num_patches, embedding_dim].</span></span>
<span id="cb13-42"><a href="#cb13-42" tabindex="-1"></a>        x_spatial <span class="op">=</span> <span class="va">self</span>.spatial_gating_unit(x_projected)</span>
<span id="cb13-43"><a href="#cb13-43" tabindex="-1"></a>        <span class="co"># Apply the second channel projection. x_projected shape: [batch_size, num_patches, embedding_dim].</span></span>
<span id="cb13-44"><a href="#cb13-44" tabindex="-1"></a>        x_projected <span class="op">=</span> <span class="va">self</span>.channel_projection2(x_spatial)</span>
<span id="cb13-45"><a href="#cb13-45" tabindex="-1"></a>        <span class="co"># Add skip connection.</span></span>
<span id="cb13-46"><a href="#cb13-46" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">+</span> x_projected</span></code></pre></div>
</div>
<div class="section level3">
<h3 id="build-train-and-evaluate-the-gmlp-model">Build, train, and evaluate the gMLP model<a class="anchor" aria-label="anchor" href="#build-train-and-evaluate-the-gmlp-model"></a>
</h3>
<p>Note that training the model with the current settings on a V100 GPUs
takes around 9 seconds per epoch.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>gmlp_blocks <span class="op">=</span> keras.Sequential(</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>    [</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>        gMLPLayer(num_patches, embedding_dim, dropout_rate)</span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_blocks)</span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>    ]</span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a>)</span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.003</span></span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a>gmlp_classifier <span class="op">=</span> build_classifier(gmlp_blocks)</span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a>history <span class="op">=</span> run_experiment(gmlp_classifier)</span></code></pre></div>
<p>As shown in the <a href="https://arxiv.org/abs/2105.08050" class="external-link">gMLP</a>
paper, better results can be achieved by increasing the embedding
dimensions, increasing the number of gMLP blocks, and training the model
for longer. You may also try to increase the size of the input images
and use different patch sizes. Note that, the paper used advanced
regularization strategies, such as MixUp and CutMix, as well as
AutoAugment.</p>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, FranÃ§ois Chollet, RStudio, Google.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
