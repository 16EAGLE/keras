<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Text classification on the Newsgroup20 dataset using pre-trained GloVe word embeddings.">
<title>Using pre-trained word embeddings • keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../../deps/Fira_Mono-0.4.8/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><meta property="og:title" content="Using pre-trained word embeddings">
<meta property="og:description" content="Text classification on the Newsgroup20 dataset using pre-trained GloVe word embeddings.">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-getting-started">Getting Started</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-getting-started">
    <a class="dropdown-item" href="../../articles/intro_to_keras_for_engineers.html">Introduction to Keras for engineers</a>
    <h6 class="dropdown-header" data-toc-skip>Tutorials</h6>
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
  </div>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Using pre-trained word embeddings</h1>
                        <h4 data-toc-skip class="author"><a href="https://twitter.com/fchollet" class="external-link">fchollet</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/pretrained_word_embeddings.Rmd" class="external-link"><code>vignettes/examples/pretrained_word_embeddings.Rmd</code></a></small>
      <div class="d-none name"><code>pretrained_word_embeddings.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> tensorflow.data <span class="im">as</span> tf_data</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>In this example, we show how to train a text classification model
that uses pre-trained word embeddings.</p>
<p>We’ll work with the Newsgroup20 dataset, a set of 20,000 message
board messages belonging to 20 different topic categories.</p>
<p>For the pre-trained word embeddings, we’ll use <a href="http://nlp.stanford.edu/projects/glove/" class="external-link">GloVe embeddings</a>.</p>
</div>
<div class="section level2">
<h2 id="download-the-newsgroup20-data">Download the Newsgroup20 data<a class="anchor" aria-label="anchor" href="#download-the-newsgroup20-data"></a>
</h2>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>data_path <span class="op">=</span> keras.utils.get_file(</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>    <span class="st">"news20.tar.gz"</span>,</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>    <span class="st">"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz"</span>,</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>    untar<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="lets-take-a-look-at-the-data">Let’s take a look at the data<a class="anchor" aria-label="anchor" href="#lets-take-a-look-at-the-data"></a>
</h2>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="im">import</span> pathlib</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>data_dir <span class="op">=</span> pathlib.Path(data_path).parent <span class="op">/</span> <span class="st">"20_newsgroup"</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>dirnames <span class="op">=</span> os.listdir(data_dir)</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of directories:"</span>, <span class="bu">len</span>(dirnames))</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Directory names:"</span>, dirnames)</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>fnames <span class="op">=</span> os.listdir(data_dir <span class="op">/</span> <span class="st">"comp.graphics"</span>)</span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of files in comp.graphics:"</span>, <span class="bu">len</span>(fnames))</span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Some example filenames:"</span>, fnames[:<span class="dv">5</span>])</span></code></pre></div>
<p>Here’s a example of what one file contains:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">open</span>(data_dir <span class="op">/</span> <span class="st">"comp.graphics"</span> <span class="op">/</span> <span class="st">"38987"</span>).read())</span></code></pre></div>
<p>As you can see, there are header lines that are leaking the file’s
category, either explicitly (the first line is literally the category
name), or implicitly, e.g. via the <code>Organization</code> filed.
Let’s get rid of the headers:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>samples <span class="op">=</span> []</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>labels <span class="op">=</span> []</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>class_names <span class="op">=</span> []</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>class_index <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a><span class="cf">for</span> dirname <span class="kw">in</span> <span class="bu">sorted</span>(os.listdir(data_dir)):</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>    class_names.append(dirname)</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>    dirpath <span class="op">=</span> data_dir <span class="op">/</span> dirname</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>    fnames <span class="op">=</span> os.listdir(dirpath)</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Processing </span><span class="sc">%s</span><span class="st">, </span><span class="sc">%d</span><span class="st"> files found"</span> <span class="op">%</span> (dirname, <span class="bu">len</span>(fnames)))</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>    <span class="cf">for</span> fname <span class="kw">in</span> fnames:</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>        fpath <span class="op">=</span> dirpath <span class="op">/</span> fname</span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>        f <span class="op">=</span> <span class="bu">open</span>(fpath, encoding<span class="op">=</span><span class="st">"latin-1"</span>)</span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>        content <span class="op">=</span> f.read()</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>        lines <span class="op">=</span> content.split(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a>        lines <span class="op">=</span> lines[<span class="dv">10</span>:]</span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>        content <span class="op">=</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>.join(lines)</span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>        samples.append(content)</span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a>        labels.append(class_index)</span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a>    class_index <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Classes:"</span>, class_names)</span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of samples:"</span>, <span class="bu">len</span>(samples))</span></code></pre></div>
<p>There’s actually one category that doesn’t have the expected number
of files, but the difference is small enough that the problem remains a
balanced classification problem.</p>
</div>
<div class="section level2">
<h2 id="shuffle-and-split-the-data-into-training-validation-sets">Shuffle and split the data into training &amp; validation sets<a class="anchor" aria-label="anchor" href="#shuffle-and-split-the-data-into-training-validation-sets"></a>
</h2>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="co"># Shuffle the data</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">1337</span></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>rng <span class="op">=</span> np.random.RandomState(seed)</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>rng.shuffle(samples)</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>rng <span class="op">=</span> np.random.RandomState(seed)</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>rng.shuffle(labels)</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a><span class="co"># Extract a training &amp; validation split</span></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>validation_split <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>num_validation_samples <span class="op">=</span> <span class="bu">int</span>(validation_split <span class="op">*</span> <span class="bu">len</span>(samples))</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>train_samples <span class="op">=</span> samples[:<span class="op">-</span>num_validation_samples]</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>val_samples <span class="op">=</span> samples[<span class="op">-</span>num_validation_samples:]</span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a>train_labels <span class="op">=</span> labels[:<span class="op">-</span>num_validation_samples]</span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a>val_labels <span class="op">=</span> labels[<span class="op">-</span>num_validation_samples:]</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="create-a-vocabulary-index">Create a vocabulary index<a class="anchor" aria-label="anchor" href="#create-a-vocabulary-index"></a>
</h2>
<p>Let’s use the <code>TextVectorization</code> to index the vocabulary
found in the dataset. Later, we’ll use the same layer instance to
vectorize the samples.</p>
<p>Our layer will only consider the top 20,000 words, and will truncate
or pad sequences to be actually 200 tokens long.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> TextVectorization</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>vectorizer <span class="op">=</span> TextVectorization(max_tokens<span class="op">=</span><span class="dv">20000</span>, output_sequence_length<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>text_ds <span class="op">=</span> tf_data.Dataset.from_tensor_slices(train_samples).batch(<span class="dv">128</span>)</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>vectorizer.adapt(text_ds)</span></code></pre></div>
<p>You can retrieve the computed vocabulary used via
<code>vectorizer.get_vocabulary()</code>. Let’s print the top 5
words:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>vectorizer.get_vocabulary()[:<span class="dv">5</span>]</span></code></pre></div>
<p>Let’s vectorize a test sentence:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>output <span class="op">=</span> vectorizer([[<span class="st">"the cat sat on the mat"</span>]])</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>output.numpy()[<span class="dv">0</span>, :<span class="dv">6</span>]</span></code></pre></div>
<p>As you can see, “the” gets represented as “2”. Why not 0, given that
“the” was the first word in the vocabulary? That’s because index 0 is
reserved for padding and index 1 is reserved for “out of vocabulary”
tokens.</p>
<p>Here’s a dict mapping words to their indices:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>voc <span class="op">=</span> vectorizer.get_vocabulary()</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>word_index <span class="op">=</span> <span class="bu">dict</span>(<span class="bu">zip</span>(voc, <span class="bu">range</span>(<span class="bu">len</span>(voc))))</span></code></pre></div>
<p>As you can see, we obtain the same encoding as above for our test
sentence:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>test <span class="op">=</span> [<span class="st">"the"</span>, <span class="st">"cat"</span>, <span class="st">"sat"</span>, <span class="st">"on"</span>, <span class="st">"the"</span>, <span class="st">"mat"</span>]</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>[word_index[w] <span class="cf">for</span> w <span class="kw">in</span> test]</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="load-pre-trained-word-embeddings">Load pre-trained word embeddings<a class="anchor" aria-label="anchor" href="#load-pre-trained-word-embeddings"></a>
</h2>
<p>Let’s download pre-trained GloVe embeddings (a 822M zip file).</p>
<p>You’ll need to run the following commands:</p>
<pre><code>!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip -q glove.6B.zip</code></pre>
<p>The archive contains text-encoded vectors of various sizes:
50-dimensional, 100-dimensional, 200-dimensional, 300-dimensional. We’ll
use the 100D ones.</p>
<p>Let’s make a dict mapping words (strings) to their NumPy vector
representation:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>path_to_glove_file <span class="op">=</span> os.path.join(</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>    os.path.expanduser(<span class="st">"~"</span>), <span class="st">".keras/datasets/glove.6B.100d.txt"</span></span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>)</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>embeddings_index <span class="op">=</span> {}</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(path_to_glove_file) <span class="im">as</span> f:</span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a>    <span class="cf">for</span> line <span class="kw">in</span> f:</span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a>        word, coefs <span class="op">=</span> line.split(maxsplit<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a>        coefs <span class="op">=</span> np.fromstring(coefs, <span class="st">"f"</span>, sep<span class="op">=</span><span class="st">" "</span>)</span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a>        embeddings_index[word] <span class="op">=</span> coefs</span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Found </span><span class="sc">%s</span><span class="st"> word vectors."</span> <span class="op">%</span> <span class="bu">len</span>(embeddings_index))</span></code></pre></div>
<p>Now, let’s prepare a corresponding embedding matrix that we can use
in a Keras <code>Embedding</code> layer. It’s a simple NumPy matrix
where entry at index <code>i</code> is the pre-trained vector for the
word of index <code>i</code> in our <code>vectorizer</code>’s
vocabulary.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>num_tokens <span class="op">=</span> <span class="bu">len</span>(voc) <span class="op">+</span> <span class="dv">2</span></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>embedding_dim <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>hits <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>misses <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a><span class="co"># Prepare embedding matrix</span></span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a>embedding_matrix <span class="op">=</span> np.zeros((num_tokens, embedding_dim))</span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a><span class="cf">for</span> word, i <span class="kw">in</span> word_index.items():</span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a>    embedding_vector <span class="op">=</span> embeddings_index.get(word)</span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a>    <span class="cf">if</span> embedding_vector <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb14-11"><a href="#cb14-11" tabindex="-1"></a>        <span class="co"># Words not found in embedding index will be all-zeros.</span></span>
<span id="cb14-12"><a href="#cb14-12" tabindex="-1"></a>        <span class="co"># This includes the representation for "padding" and "OOV"</span></span>
<span id="cb14-13"><a href="#cb14-13" tabindex="-1"></a>        embedding_matrix[i] <span class="op">=</span> embedding_vector</span>
<span id="cb14-14"><a href="#cb14-14" tabindex="-1"></a>        hits <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb14-15"><a href="#cb14-15" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb14-16"><a href="#cb14-16" tabindex="-1"></a>        misses <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb14-17"><a href="#cb14-17" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Converted </span><span class="sc">%d</span><span class="st"> words (</span><span class="sc">%d</span><span class="st"> misses)"</span> <span class="op">%</span> (hits, misses))</span></code></pre></div>
<p>Next, we load the pre-trained word embeddings matrix into an
<code>Embedding</code> layer.</p>
<p>Note that we set <code>trainable=False</code> so as to keep the
embeddings fixed (we don’t want to update them during training).</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> Embedding</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>embedding_layer <span class="op">=</span> Embedding(</span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>    num_tokens,</span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>    embedding_dim,</span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a>    trainable<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a>)</span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a>embedding_layer.build((<span class="dv">1</span>,))</span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a>embedding_layer.set_weights([embedding_matrix])</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="build-the-model">Build the model<a class="anchor" aria-label="anchor" href="#build-the-model"></a>
</h2>
<p>A simple 1D convnet with global max pooling and a classifier at the
end.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>int_sequences_input <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="va">None</span>,), dtype<span class="op">=</span><span class="st">"int64"</span>)</span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>embedded_sequences <span class="op">=</span> embedding_layer(int_sequences_input)</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>x <span class="op">=</span> layers.Conv1D(<span class="dv">128</span>, <span class="dv">5</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(embedded_sequences)</span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a>x <span class="op">=</span> layers.MaxPooling1D(<span class="dv">5</span>)(x)</span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a>x <span class="op">=</span> layers.Conv1D(<span class="dv">128</span>, <span class="dv">5</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a>x <span class="op">=</span> layers.MaxPooling1D(<span class="dv">5</span>)(x)</span>
<span id="cb16-9"><a href="#cb16-9" tabindex="-1"></a>x <span class="op">=</span> layers.Conv1D(<span class="dv">128</span>, <span class="dv">5</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb16-10"><a href="#cb16-10" tabindex="-1"></a>x <span class="op">=</span> layers.GlobalMaxPooling1D()(x)</span>
<span id="cb16-11"><a href="#cb16-11" tabindex="-1"></a>x <span class="op">=</span> layers.Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb16-12"><a href="#cb16-12" tabindex="-1"></a>x <span class="op">=</span> layers.Dropout(<span class="fl">0.5</span>)(x)</span>
<span id="cb16-13"><a href="#cb16-13" tabindex="-1"></a>preds <span class="op">=</span> layers.Dense(<span class="bu">len</span>(class_names), activation<span class="op">=</span><span class="st">"softmax"</span>)(x)</span>
<span id="cb16-14"><a href="#cb16-14" tabindex="-1"></a>model <span class="op">=</span> keras.Model(int_sequences_input, preds)</span>
<span id="cb16-15"><a href="#cb16-15" tabindex="-1"></a>model.summary()</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="train-the-model">Train the model<a class="anchor" aria-label="anchor" href="#train-the-model"></a>
</h2>
<p>First, convert our list-of-strings data to NumPy arrays of integer
indices. The arrays are right-padded.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>x_train <span class="op">=</span> vectorizer(np.array([[s] <span class="cf">for</span> s <span class="kw">in</span> train_samples])).numpy()</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>x_val <span class="op">=</span> vectorizer(np.array([[s] <span class="cf">for</span> s <span class="kw">in</span> val_samples])).numpy()</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>y_train <span class="op">=</span> np.array(train_labels)</span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a>y_val <span class="op">=</span> np.array(val_labels)</span></code></pre></div>
<p>We use categorical crossentropy as our loss since we’re doing softmax
classification. Moreover, we use
<code>sparse_categorical_crossentropy</code> since our labels are
integers.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>, optimizer<span class="op">=</span><span class="st">"rmsprop"</span>, metrics<span class="op">=</span>[<span class="st">"acc"</span>]</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>)</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>model.fit(</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a>    x_train, y_train, batch_size<span class="op">=</span><span class="dv">128</span>, epochs<span class="op">=</span><span class="dv">20</span>, validation_data<span class="op">=</span>(x_val, y_val)</span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="export-an-end-to-end-model">Export an end-to-end model<a class="anchor" aria-label="anchor" href="#export-an-end-to-end-model"></a>
</h2>
<p>Now, we may want to export a <code>Model</code> object that takes as
input a string of arbitrary length, rather than a sequence of indices.
It would make the model much more portable, since you wouldn’t have to
worry about the input preprocessing pipeline.</p>
<p>Our <code>vectorizer</code> is actually a Keras layer, so it’s
simple:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>string_input <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">1</span>,), dtype<span class="op">=</span><span class="st">"string"</span>)</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>x <span class="op">=</span> vectorizer(string_input)</span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a>preds <span class="op">=</span> model(x)</span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a>end_to_end_model <span class="op">=</span> keras.Model(string_input, preds)</span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a>probabilities <span class="op">=</span> end_to_end_model(</span>
<span id="cb19-7"><a href="#cb19-7" tabindex="-1"></a>    keras.ops.convert_to_tensor(</span>
<span id="cb19-8"><a href="#cb19-8" tabindex="-1"></a>        [[<span class="st">"this message is about computer graphics and 3D modeling"</span>]]</span>
<span id="cb19-9"><a href="#cb19-9" tabindex="-1"></a>    )</span>
<span id="cb19-10"><a href="#cb19-10" tabindex="-1"></a>)</span>
<span id="cb19-11"><a href="#cb19-11" tabindex="-1"></a></span>
<span id="cb19-12"><a href="#cb19-12" tabindex="-1"></a><span class="bu">print</span>(class_names[np.argmax(probabilities[<span class="dv">0</span>])])</span></code></pre></div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
