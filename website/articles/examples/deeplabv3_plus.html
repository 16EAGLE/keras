<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Implement DeepLabV3+ architecture for Multi-class Semantic Segmentation.">
<title>Multiclass semantic segmentation using DeepLabV3+ • keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><meta property="og:title" content="Multiclass semantic segmentation using DeepLabV3+">
<meta property="og:description" content="Implement DeepLabV3+ architecture for Multi-class Semantic Segmentation.">
<meta property="og:image" content="https://keras.posit.co/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-getting-started">Getting Started</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-getting-started">
    <a class="dropdown-item" href="../../articles/intro_to_keras_for_engineers.html">Introduction to Keras for engineers</a>
    <h6 class="dropdown-header" data-toc-skip>Tutorials</h6>
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
  </div>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../../logo.png" class="logo" alt=""><h1>Multiclass semantic segmentation using DeepLabV3+</h1>
                        <h4 data-toc-skip class="author"><a href="http://github.com/soumik12345" class="external-link">Soumik Rakshit</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/deeplabv3_plus.Rmd" class="external-link"><code>vignettes/examples/deeplabv3_plus.Rmd</code></a></small>
      <div class="d-none name"><code>deeplabv3_plus.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>Semantic segmentation, with the goal to assign semantic labels to
every pixel in an image, is an essential computer vision task. In this
example, we implement the <strong>DeepLabV3+</strong> model for
multi-class semantic segmentation, a fully-convolutional architecture
that performs well on semantic segmentation benchmarks.</p>
<div class="section level3">
<h3 id="references">References:<a class="anchor" aria-label="anchor" href="#references"></a>
</h3>
<ul>
<li><a href="https://arxiv.org/pdf/1802.02611.pdf" class="external-link">Encoder-Decoder with
Atrous Separable Convolution for Semantic Image Segmentation</a></li>
<li><a href="https://arxiv.org/abs/1706.05587" class="external-link">Rethinking Atrous
Convolution for Semantic Image Segmentation</a></li>
<li><a href="https://arxiv.org/abs/1606.00915" class="external-link">DeepLab: Semantic Image
Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully
Connected CRFs</a></li>
</ul>
</div>
</div>
<div class="section level2">
<h2 id="downloading-the-data">Downloading the data<a class="anchor" aria-label="anchor" href="#downloading-the-data"></a>
</h2>
<p>We will use the <a href="https://arxiv.org/abs/1811.12596" class="external-link">Crowd
Instance-level Human Parsing Dataset</a> for training our model. The
Crowd Instance-level Human Parsing (CIHP) dataset has 38,280 diverse
human images. Each image in CIHP is labeled with pixel-wise annotations
for 20 categories, as well as instance-level identification. This
dataset can be used for the “human part segmentation” task.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> ops</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">import</span> cv2</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="im">from</span> glob <span class="im">import</span> glob</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="im">from</span> scipy.io <span class="im">import</span> loadmat</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> image <span class="im">as</span> tf_image</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> data <span class="im">as</span> tf_data</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> io <span class="im">as</span> tf_io</span></code></pre></div>
<p>gdown “1B9A9UCJYMwTL4oBEo4RZfbMZMaZhKJaz&amp;confirm=t” unzip -q
instance-level-human-parsing.zip</p>
</div>
<div class="section level2">
<h2 id="creating-a-tensorflow-dataset">Creating a TensorFlow Dataset<a class="anchor" aria-label="anchor" href="#creating-a-tensorflow-dataset"></a>
</h2>
<p>Training on the entire CIHP dataset with 38,280 images takes a lot of
time, hence we will be using a smaller subset of 200 images for training
our model in this example.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>IMAGE_SIZE <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>NUM_CLASSES <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>DATA_DIR <span class="op">=</span> (</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>    <span class="st">"./instance-level_human_parsing/instance-level_human_parsing/Training"</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>)</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>NUM_TRAIN_IMAGES <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>NUM_VAL_IMAGES <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>train_images <span class="op">=</span> <span class="bu">sorted</span>(glob(os.path.join(DATA_DIR, <span class="st">"Images/*"</span>)))[</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>    :NUM_TRAIN_IMAGES</span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>]</span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>train_masks <span class="op">=</span> <span class="bu">sorted</span>(glob(os.path.join(DATA_DIR, <span class="st">"Category_ids/*"</span>)))[</span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a>    :NUM_TRAIN_IMAGES</span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a>]</span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a>val_images <span class="op">=</span> <span class="bu">sorted</span>(glob(os.path.join(DATA_DIR, <span class="st">"Images/*"</span>)))[</span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a>    NUM_TRAIN_IMAGES : NUM_VAL_IMAGES <span class="op">+</span> NUM_TRAIN_IMAGES</span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a>]</span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a>val_masks <span class="op">=</span> <span class="bu">sorted</span>(glob(os.path.join(DATA_DIR, <span class="st">"Category_ids/*"</span>)))[</span>
<span id="cb2-20"><a href="#cb2-20" tabindex="-1"></a>    NUM_TRAIN_IMAGES : NUM_VAL_IMAGES <span class="op">+</span> NUM_TRAIN_IMAGES</span>
<span id="cb2-21"><a href="#cb2-21" tabindex="-1"></a>]</span>
<span id="cb2-22"><a href="#cb2-22" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" tabindex="-1"></a><span class="kw">def</span> read_image(image_path, mask<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb2-25"><a href="#cb2-25" tabindex="-1"></a>    image <span class="op">=</span> tf_io.read_file(image_path)</span>
<span id="cb2-26"><a href="#cb2-26" tabindex="-1"></a>    <span class="cf">if</span> mask:</span>
<span id="cb2-27"><a href="#cb2-27" tabindex="-1"></a>        image <span class="op">=</span> tf_image.decode_png(image, channels<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-28"><a href="#cb2-28" tabindex="-1"></a>        image.set_shape([<span class="va">None</span>, <span class="va">None</span>, <span class="dv">1</span>])</span>
<span id="cb2-29"><a href="#cb2-29" tabindex="-1"></a>        image <span class="op">=</span> tf_image.resize(images<span class="op">=</span>image, size<span class="op">=</span>[IMAGE_SIZE, IMAGE_SIZE])</span>
<span id="cb2-30"><a href="#cb2-30" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-31"><a href="#cb2-31" tabindex="-1"></a>        image <span class="op">=</span> tf_image.decode_png(image, channels<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb2-32"><a href="#cb2-32" tabindex="-1"></a>        image.set_shape([<span class="va">None</span>, <span class="va">None</span>, <span class="dv">3</span>])</span>
<span id="cb2-33"><a href="#cb2-33" tabindex="-1"></a>        image <span class="op">=</span> tf_image.resize(images<span class="op">=</span>image, size<span class="op">=</span>[IMAGE_SIZE, IMAGE_SIZE])</span>
<span id="cb2-34"><a href="#cb2-34" tabindex="-1"></a>    <span class="cf">return</span> image</span>
<span id="cb2-35"><a href="#cb2-35" tabindex="-1"></a></span>
<span id="cb2-36"><a href="#cb2-36" tabindex="-1"></a></span>
<span id="cb2-37"><a href="#cb2-37" tabindex="-1"></a><span class="kw">def</span> load_data(image_list, mask_list):</span>
<span id="cb2-38"><a href="#cb2-38" tabindex="-1"></a>    image <span class="op">=</span> read_image(image_list)</span>
<span id="cb2-39"><a href="#cb2-39" tabindex="-1"></a>    mask <span class="op">=</span> read_image(mask_list, mask<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-40"><a href="#cb2-40" tabindex="-1"></a>    <span class="cf">return</span> image, mask</span>
<span id="cb2-41"><a href="#cb2-41" tabindex="-1"></a></span>
<span id="cb2-42"><a href="#cb2-42" tabindex="-1"></a></span>
<span id="cb2-43"><a href="#cb2-43" tabindex="-1"></a><span class="kw">def</span> data_generator(image_list, mask_list):</span>
<span id="cb2-44"><a href="#cb2-44" tabindex="-1"></a>    dataset <span class="op">=</span> tf_data.Dataset.from_tensor_slices((image_list, mask_list))</span>
<span id="cb2-45"><a href="#cb2-45" tabindex="-1"></a>    dataset <span class="op">=</span> dataset.<span class="bu">map</span>(load_data, num_parallel_calls<span class="op">=</span>tf_data.AUTOTUNE)</span>
<span id="cb2-46"><a href="#cb2-46" tabindex="-1"></a>    dataset <span class="op">=</span> dataset.batch(BATCH_SIZE, drop_remainder<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-47"><a href="#cb2-47" tabindex="-1"></a>    <span class="cf">return</span> dataset</span>
<span id="cb2-48"><a href="#cb2-48" tabindex="-1"></a></span>
<span id="cb2-49"><a href="#cb2-49" tabindex="-1"></a></span>
<span id="cb2-50"><a href="#cb2-50" tabindex="-1"></a>train_dataset <span class="op">=</span> data_generator(train_images, train_masks)</span>
<span id="cb2-51"><a href="#cb2-51" tabindex="-1"></a>val_dataset <span class="op">=</span> data_generator(val_images, val_masks)</span>
<span id="cb2-52"><a href="#cb2-52" tabindex="-1"></a></span>
<span id="cb2-53"><a href="#cb2-53" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Train Dataset:"</span>, train_dataset)</span>
<span id="cb2-54"><a href="#cb2-54" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Val Dataset:"</span>, val_dataset)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="building-the-deeplabv3-model">Building the DeepLabV3+ model<a class="anchor" aria-label="anchor" href="#building-the-deeplabv3-model"></a>
</h2>
<p>DeepLabv3+ extends DeepLabv3 by adding an encoder-decoder structure.
The encoder module processes multiscale contextual information by
applying dilated convolution at multiple scales, while the decoder
module refines the segmentation results along object boundaries.</p>
<p><img src="https://github.com/lattice-ai/DeepLabV3-Plus/raw/master/assets/deeplabv3_plus_diagram.png"></p>
<p><strong>Dilated convolution:</strong> With dilated convolution, as we
go deeper in the network, we can keep the stride constant but with
larger field-of-view without increasing the number of parameters or the
amount of computation. Besides, it enables larger output feature maps,
which is useful for semantic segmentation.</p>
<p>The reason for using <strong>Dilated Spatial Pyramid Pooling</strong>
is that it was shown that as the sampling rate becomes larger, the
number of valid filter weights (i.e., weights that are applied to the
valid feature region, instead of padded zeros) becomes smaller.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="kw">def</span> convolution_block(</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>    block_input,</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>    num_filters<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>    kernel_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>    dilation_rate<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>    padding<span class="op">=</span><span class="st">"same"</span>,</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>    use_bias<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>):</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>    x <span class="op">=</span> layers.Conv2D(</span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>        num_filters,</span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>        kernel_size<span class="op">=</span>kernel_size,</span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>        dilation_rate<span class="op">=</span>dilation_rate,</span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a>        padding<span class="op">=</span><span class="st">"same"</span>,</span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a>        use_bias<span class="op">=</span>use_bias,</span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a>        kernel_initializer<span class="op">=</span>keras.initializers.HeNormal(),</span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a>    )(block_input)</span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a>    x <span class="op">=</span> layers.BatchNormalization()(x)</span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a>    <span class="cf">return</span> ops.nn.relu(x)</span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" tabindex="-1"></a><span class="kw">def</span> DilatedSpatialPyramidPooling(dspp_input):</span>
<span id="cb3-22"><a href="#cb3-22" tabindex="-1"></a>    dims <span class="op">=</span> dspp_input.shape</span>
<span id="cb3-23"><a href="#cb3-23" tabindex="-1"></a>    x <span class="op">=</span> layers.AveragePooling2D(pool_size<span class="op">=</span>(dims[<span class="op">-</span><span class="dv">3</span>], dims[<span class="op">-</span><span class="dv">2</span>]))(dspp_input)</span>
<span id="cb3-24"><a href="#cb3-24" tabindex="-1"></a>    x <span class="op">=</span> convolution_block(x, kernel_size<span class="op">=</span><span class="dv">1</span>, use_bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-25"><a href="#cb3-25" tabindex="-1"></a>    out_pool <span class="op">=</span> layers.UpSampling2D(</span>
<span id="cb3-26"><a href="#cb3-26" tabindex="-1"></a>        size<span class="op">=</span>(dims[<span class="op">-</span><span class="dv">3</span>] <span class="op">//</span> x.shape[<span class="dv">1</span>], dims[<span class="op">-</span><span class="dv">2</span>] <span class="op">//</span> x.shape[<span class="dv">2</span>]),</span>
<span id="cb3-27"><a href="#cb3-27" tabindex="-1"></a>        interpolation<span class="op">=</span><span class="st">"bilinear"</span>,</span>
<span id="cb3-28"><a href="#cb3-28" tabindex="-1"></a>    )(x)</span>
<span id="cb3-29"><a href="#cb3-29" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" tabindex="-1"></a>    out_1 <span class="op">=</span> convolution_block(dspp_input, kernel_size<span class="op">=</span><span class="dv">1</span>, dilation_rate<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-31"><a href="#cb3-31" tabindex="-1"></a>    out_6 <span class="op">=</span> convolution_block(dspp_input, kernel_size<span class="op">=</span><span class="dv">3</span>, dilation_rate<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb3-32"><a href="#cb3-32" tabindex="-1"></a>    out_12 <span class="op">=</span> convolution_block(dspp_input, kernel_size<span class="op">=</span><span class="dv">3</span>, dilation_rate<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb3-33"><a href="#cb3-33" tabindex="-1"></a>    out_18 <span class="op">=</span> convolution_block(dspp_input, kernel_size<span class="op">=</span><span class="dv">3</span>, dilation_rate<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb3-34"><a href="#cb3-34" tabindex="-1"></a></span>
<span id="cb3-35"><a href="#cb3-35" tabindex="-1"></a>    x <span class="op">=</span> layers.Concatenate(axis<span class="op">=-</span><span class="dv">1</span>)([out_pool, out_1, out_6, out_12, out_18])</span>
<span id="cb3-36"><a href="#cb3-36" tabindex="-1"></a>    output <span class="op">=</span> convolution_block(x, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-37"><a href="#cb3-37" tabindex="-1"></a>    <span class="cf">return</span> output</span></code></pre></div>
<p>The encoder features are first bilinearly upsampled by a factor 4,
and then concatenated with the corresponding low-level features from the
network backbone that have the same spatial resolution. For this
example, we use a ResNet50 pretrained on ImageNet as the backbone model,
and we use the low-level features from the
<code>conv4_block6_2_relu</code> block of the backbone.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="kw">def</span> DeeplabV3Plus(image_size, num_classes):</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>    model_input <span class="op">=</span> keras.Input(shape<span class="op">=</span>(image_size, image_size, <span class="dv">3</span>))</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>    preprocessed <span class="op">=</span> keras.applications.resnet50.preprocess_input(model_input)</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>    resnet50 <span class="op">=</span> keras.applications.ResNet50(</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>        weights<span class="op">=</span><span class="st">"imagenet"</span>, include_top<span class="op">=</span><span class="va">False</span>, input_tensor<span class="op">=</span>preprocessed</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>    )</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>    x <span class="op">=</span> resnet50.get_layer(<span class="st">"conv4_block6_2_relu"</span>).output</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>    x <span class="op">=</span> DilatedSpatialPyramidPooling(x)</span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>    input_a <span class="op">=</span> layers.UpSampling2D(</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>        size<span class="op">=</span>(image_size <span class="op">//</span> <span class="dv">4</span> <span class="op">//</span> x.shape[<span class="dv">1</span>], image_size <span class="op">//</span> <span class="dv">4</span> <span class="op">//</span> x.shape[<span class="dv">2</span>]),</span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>        interpolation<span class="op">=</span><span class="st">"bilinear"</span>,</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a>    )(x)</span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>    input_b <span class="op">=</span> resnet50.get_layer(<span class="st">"conv2_block3_2_relu"</span>).output</span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a>    input_b <span class="op">=</span> convolution_block(input_b, num_filters<span class="op">=</span><span class="dv">48</span>, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" tabindex="-1"></a>    x <span class="op">=</span> layers.Concatenate(axis<span class="op">=-</span><span class="dv">1</span>)([input_a, input_b])</span>
<span id="cb4-18"><a href="#cb4-18" tabindex="-1"></a>    x <span class="op">=</span> convolution_block(x)</span>
<span id="cb4-19"><a href="#cb4-19" tabindex="-1"></a>    x <span class="op">=</span> convolution_block(x)</span>
<span id="cb4-20"><a href="#cb4-20" tabindex="-1"></a>    x <span class="op">=</span> layers.UpSampling2D(</span>
<span id="cb4-21"><a href="#cb4-21" tabindex="-1"></a>        size<span class="op">=</span>(image_size <span class="op">//</span> x.shape[<span class="dv">1</span>], image_size <span class="op">//</span> x.shape[<span class="dv">2</span>]),</span>
<span id="cb4-22"><a href="#cb4-22" tabindex="-1"></a>        interpolation<span class="op">=</span><span class="st">"bilinear"</span>,</span>
<span id="cb4-23"><a href="#cb4-23" tabindex="-1"></a>    )(x)</span>
<span id="cb4-24"><a href="#cb4-24" tabindex="-1"></a>    model_output <span class="op">=</span> layers.Conv2D(</span>
<span id="cb4-25"><a href="#cb4-25" tabindex="-1"></a>        num_classes, kernel_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), padding<span class="op">=</span><span class="st">"same"</span></span>
<span id="cb4-26"><a href="#cb4-26" tabindex="-1"></a>    )(x)</span>
<span id="cb4-27"><a href="#cb4-27" tabindex="-1"></a>    <span class="cf">return</span> keras.Model(inputs<span class="op">=</span>model_input, outputs<span class="op">=</span>model_output)</span>
<span id="cb4-28"><a href="#cb4-28" tabindex="-1"></a></span>
<span id="cb4-29"><a href="#cb4-29" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" tabindex="-1"></a>model <span class="op">=</span> DeeplabV3Plus(image_size<span class="op">=</span>IMAGE_SIZE, num_classes<span class="op">=</span>NUM_CLASSES)</span>
<span id="cb4-31"><a href="#cb4-31" tabindex="-1"></a>model.summary()</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="training">Training<a class="anchor" aria-label="anchor" href="#training"></a>
</h2>
<p>We train the model using sparse categorical crossentropy as the loss
function, and Adam as the optimizer.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>loss <span class="op">=</span> keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>    optimizer<span class="op">=</span>keras.optimizers.Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>),</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>    loss<span class="op">=</span>loss,</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">"accuracy"</span>],</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>)</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>history <span class="op">=</span> model.fit(train_dataset, validation_data<span class="op">=</span>val_dataset, epochs<span class="op">=</span><span class="dv">25</span>)</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>plt.plot(history.history[<span class="st">"loss"</span>])</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>plt.title(<span class="st">"Training Loss"</span>)</span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>plt.ylabel(<span class="st">"loss"</span>)</span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>plt.xlabel(<span class="st">"epoch"</span>)</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>plt.show()</span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>plt.plot(history.history[<span class="st">"accuracy"</span>])</span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>plt.title(<span class="st">"Training Accuracy"</span>)</span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a>plt.ylabel(<span class="st">"accuracy"</span>)</span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a>plt.xlabel(<span class="st">"epoch"</span>)</span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a>plt.show()</span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a>plt.plot(history.history[<span class="st">"val_loss"</span>])</span>
<span id="cb5-23"><a href="#cb5-23" tabindex="-1"></a>plt.title(<span class="st">"Validation Loss"</span>)</span>
<span id="cb5-24"><a href="#cb5-24" tabindex="-1"></a>plt.ylabel(<span class="st">"val_loss"</span>)</span>
<span id="cb5-25"><a href="#cb5-25" tabindex="-1"></a>plt.xlabel(<span class="st">"epoch"</span>)</span>
<span id="cb5-26"><a href="#cb5-26" tabindex="-1"></a>plt.show()</span>
<span id="cb5-27"><a href="#cb5-27" tabindex="-1"></a></span>
<span id="cb5-28"><a href="#cb5-28" tabindex="-1"></a>plt.plot(history.history[<span class="st">"val_accuracy"</span>])</span>
<span id="cb5-29"><a href="#cb5-29" tabindex="-1"></a>plt.title(<span class="st">"Validation Accuracy"</span>)</span>
<span id="cb5-30"><a href="#cb5-30" tabindex="-1"></a>plt.ylabel(<span class="st">"val_accuracy"</span>)</span>
<span id="cb5-31"><a href="#cb5-31" tabindex="-1"></a>plt.xlabel(<span class="st">"epoch"</span>)</span>
<span id="cb5-32"><a href="#cb5-32" tabindex="-1"></a>plt.show()</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="inference-using-colormap-overlay">Inference using Colormap Overlay<a class="anchor" aria-label="anchor" href="#inference-using-colormap-overlay"></a>
</h2>
<p>The raw predictions from the model represent a one-hot encoded tensor
of shape <code>(N, 512, 512, 20)</code> where each one of the 20
channels is a binary mask corresponding to a predicted label. In order
to visualize the results, we plot them as RGB segmentation masks where
each pixel is represented by a unique color corresponding to the
particular label predicted. We can easily find the color corresponding
to each label from the <code>human_colormap.mat</code> file provided as
part of the dataset. We would also plot an overlay of the RGB
segmentation mask on the input image as this further helps us to
identify the different categories present in the image more
intuitively.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="co"># Loading the Colormap</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>colormap <span class="op">=</span> loadmat(</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>    <span class="st">"./instance-level_human_parsing/instance-level_human_parsing/human_colormap.mat"</span></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>)[<span class="st">"colormap"</span>]</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>colormap <span class="op">=</span> colormap <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>colormap <span class="op">=</span> colormap.astype(np.uint8)</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a><span class="kw">def</span> infer(model, image_tensor):</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>    predictions <span class="op">=</span> model.predict(np.expand_dims((image_tensor), axis<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>    predictions <span class="op">=</span> np.squeeze(predictions)</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>    predictions <span class="op">=</span> np.argmax(predictions, axis<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a>    <span class="cf">return</span> predictions</span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a><span class="kw">def</span> decode_segmentation_masks(mask, colormap, n_classes):</span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a>    r <span class="op">=</span> np.zeros_like(mask).astype(np.uint8)</span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a>    g <span class="op">=</span> np.zeros_like(mask).astype(np.uint8)</span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a>    b <span class="op">=</span> np.zeros_like(mask).astype(np.uint8)</span>
<span id="cb6-20"><a href="#cb6-20" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n_classes):</span>
<span id="cb6-21"><a href="#cb6-21" tabindex="-1"></a>        idx <span class="op">=</span> mask <span class="op">==</span> l</span>
<span id="cb6-22"><a href="#cb6-22" tabindex="-1"></a>        r[idx] <span class="op">=</span> colormap[l, <span class="dv">0</span>]</span>
<span id="cb6-23"><a href="#cb6-23" tabindex="-1"></a>        g[idx] <span class="op">=</span> colormap[l, <span class="dv">1</span>]</span>
<span id="cb6-24"><a href="#cb6-24" tabindex="-1"></a>        b[idx] <span class="op">=</span> colormap[l, <span class="dv">2</span>]</span>
<span id="cb6-25"><a href="#cb6-25" tabindex="-1"></a>    rgb <span class="op">=</span> np.stack([r, g, b], axis<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb6-26"><a href="#cb6-26" tabindex="-1"></a>    <span class="cf">return</span> rgb</span>
<span id="cb6-27"><a href="#cb6-27" tabindex="-1"></a></span>
<span id="cb6-28"><a href="#cb6-28" tabindex="-1"></a></span>
<span id="cb6-29"><a href="#cb6-29" tabindex="-1"></a><span class="kw">def</span> get_overlay(image, colored_mask):</span>
<span id="cb6-30"><a href="#cb6-30" tabindex="-1"></a>    image <span class="op">=</span> keras.utils.array_to_img(image)</span>
<span id="cb6-31"><a href="#cb6-31" tabindex="-1"></a>    image <span class="op">=</span> np.array(image).astype(np.uint8)</span>
<span id="cb6-32"><a href="#cb6-32" tabindex="-1"></a>    overlay <span class="op">=</span> cv2.addWeighted(image, <span class="fl">0.35</span>, colored_mask, <span class="fl">0.65</span>, <span class="dv">0</span>)</span>
<span id="cb6-33"><a href="#cb6-33" tabindex="-1"></a>    <span class="cf">return</span> overlay</span>
<span id="cb6-34"><a href="#cb6-34" tabindex="-1"></a></span>
<span id="cb6-35"><a href="#cb6-35" tabindex="-1"></a></span>
<span id="cb6-36"><a href="#cb6-36" tabindex="-1"></a><span class="kw">def</span> plot_samples_matplotlib(display_list, figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">3</span>)):</span>
<span id="cb6-37"><a href="#cb6-37" tabindex="-1"></a>    _, axes <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">1</span>, ncols<span class="op">=</span><span class="bu">len</span>(display_list), figsize<span class="op">=</span>figsize)</span>
<span id="cb6-38"><a href="#cb6-38" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(display_list)):</span>
<span id="cb6-39"><a href="#cb6-39" tabindex="-1"></a>        <span class="cf">if</span> display_list[i].shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> <span class="dv">3</span>:</span>
<span id="cb6-40"><a href="#cb6-40" tabindex="-1"></a>            axes[i].imshow(keras.utils.array_to_img(display_list[i]))</span>
<span id="cb6-41"><a href="#cb6-41" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb6-42"><a href="#cb6-42" tabindex="-1"></a>            axes[i].imshow(display_list[i])</span>
<span id="cb6-43"><a href="#cb6-43" tabindex="-1"></a>    plt.show()</span>
<span id="cb6-44"><a href="#cb6-44" tabindex="-1"></a></span>
<span id="cb6-45"><a href="#cb6-45" tabindex="-1"></a></span>
<span id="cb6-46"><a href="#cb6-46" tabindex="-1"></a><span class="kw">def</span> plot_predictions(images_list, colormap, model):</span>
<span id="cb6-47"><a href="#cb6-47" tabindex="-1"></a>    <span class="cf">for</span> image_file <span class="kw">in</span> images_list:</span>
<span id="cb6-48"><a href="#cb6-48" tabindex="-1"></a>        image_tensor <span class="op">=</span> read_image(image_file)</span>
<span id="cb6-49"><a href="#cb6-49" tabindex="-1"></a>        prediction_mask <span class="op">=</span> infer(image_tensor<span class="op">=</span>image_tensor, model<span class="op">=</span>model)</span>
<span id="cb6-50"><a href="#cb6-50" tabindex="-1"></a>        prediction_colormap <span class="op">=</span> decode_segmentation_masks(</span>
<span id="cb6-51"><a href="#cb6-51" tabindex="-1"></a>            prediction_mask, colormap, <span class="dv">20</span></span>
<span id="cb6-52"><a href="#cb6-52" tabindex="-1"></a>        )</span>
<span id="cb6-53"><a href="#cb6-53" tabindex="-1"></a>        overlay <span class="op">=</span> get_overlay(image_tensor, prediction_colormap)</span>
<span id="cb6-54"><a href="#cb6-54" tabindex="-1"></a>        plot_samples_matplotlib(</span>
<span id="cb6-55"><a href="#cb6-55" tabindex="-1"></a>            [image_tensor, overlay, prediction_colormap], figsize<span class="op">=</span>(<span class="dv">18</span>, <span class="dv">14</span>)</span>
<span id="cb6-56"><a href="#cb6-56" tabindex="-1"></a>        )</span></code></pre></div>
<div class="section level3">
<h3 id="inference-on-train-images">Inference on Train Images<a class="anchor" aria-label="anchor" href="#inference-on-train-images"></a>
</h3>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>plot_predictions(train_images[:<span class="dv">4</span>], colormap, model<span class="op">=</span>model)</span></code></pre></div>
</div>
<div class="section level3">
<h3 id="inference-on-validation-images">Inference on Validation Images<a class="anchor" aria-label="anchor" href="#inference-on-validation-images"></a>
</h3>
<p>You can use the trained model hosted on <a href="https://huggingface.co/keras-io/deeplabv3p-resnet50" class="external-link">Hugging Face
Hub</a> and try the demo on <a href="https://huggingface.co/spaces/keras-io/Human-Part-Segmentation" class="external-link">Hugging
Face Spaces</a>.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>plot_predictions(val_images[:<span class="dv">4</span>], colormap, model<span class="op">=</span>model)</span></code></pre></div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, RStudio, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.9000.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
