<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Using KerasNLP to train a mini-GPT model for text generation.">
<title>GPT text generation from scratch with KerasNLP • keras</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><link href="../../extra.css" rel="stylesheet">
<script src="../../extra.js"></script><meta property="og:title" content="GPT text generation from scratch with KerasNLP">
<meta property="og:description" content="Using KerasNLP to train a mini-GPT model for text generation.">
<meta property="og:image" content="https://keras.posit.co/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-tutorials">Tutorials</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-tutorials">
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <a class="dropdown-item" href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <h6 class="dropdown-header" data-toc-skip>Guides (New for TF 2.6)</h6>
    <a class="dropdown-item" href="../../articles/new-guides/python_subclasses.html">Python Subclasses</a>
    <a class="dropdown-item" href="../../articles/new-guides/making_new_layers_and_models_via_subclassing.html">Making New Layers and Models via Subclassing</a>
    <a class="dropdown-item" href="../../articles/new-guides/customizing_what_happens_in_fit.html">Customizing What Happens in Fit</a>
    <a class="dropdown-item" href="../../articles/new-guides/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <a class="dropdown-item" href="../../articles/new-guides/preprocessing_layers.html">Working with Preprocessing Layers</a>
    <a class="dropdown-item" href="../../argicles/new-guides/working_with_rnns.html">Working with RNNs</a>
    <div class="dropdown-divider"></div>
    <h6 class="dropdown-header" data-toc-skip>Using Keras</h6>
    <a class="dropdown-item" href="../../articles/guide_keras.html">Guide to Keras Basics</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model in Depth</a>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API in Depth</a>
    <a class="dropdown-item" href="../../articles/about_keras_models.html">About Keras Models</a>
    <a class="dropdown-item" href="../../articles/about_keras_layers.html">About Keras Layers</a>
    <a class="dropdown-item" href="../../articles/training_visualization.html">Training Visualization</a>
    <a class="dropdown-item" href="../../articles/applications.html">Pre-Trained Models</a>
    <a class="dropdown-item" href="../../articles/faq.html">Frequently Asked Questions</a>
    <a class="dropdown-item" href="../../articles/why_use_keras.html">Why Use Keras?</a>
    <div class="dropdown-divider"></div>
    <h6 class="dropdown-header" data-toc-skip>Advanced</h6>
    <a class="dropdown-item" href="../../articles/eager_guide.html">Eager Execution</a>
    <a class="dropdown-item" href="../../articles/training_callbacks.html">Training Callbacks</a>
    <a class="dropdown-item" href="../../articles/backend.html">Keras Backend</a>
    <a class="dropdown-item" href="../../articles/custom_layers.html">Custom Layers</a>
    <a class="dropdown-item" href="../../articles/custom_models.html">Custom Models</a>
    <a class="dropdown-item" href="../../articles/saving_serializing.html">Saving and serializing</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../articles/learn.html">Learn</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../articles/tools.html">Tools</a>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../../logo.png" class="logo" alt=""><h1>GPT text generation from scratch with KerasNLP</h1>
                        <h4 data-toc-skip class="author"><a href="https://github.com/jessechancy" class="external-link">Jesse Chan</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/text_generation_gpt.Rmd" class="external-link"><code>vignettes/examples/text_generation_gpt.Rmd</code></a></small>
      <div class="d-none name"><code>text_generation_gpt.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>In this example, we will use KerasNLP to build a scaled down
Generative Pre-Trained (GPT) model. GPT is a Transformer-based model
that allows you to generate sophisticated text from a prompt.</p>
<p>We will train the model on the <a href="https://arxiv.org/abs/1911.12391" class="external-link">simplebooks-92</a> corpus, which
is a dataset made from several novels. It is a good dataset for this
example since it has a small vocabulary and high word frequency, which
is beneficial when training a model with few parameters.</p>
<p>This example combines concepts from <a href="https://keras.io/examples/generative/text_generation_with_miniature_gpt/" class="external-link">Text
generation with a miniature GPT</a> with KerasNLP abstractions. We will
demonstrate how KerasNLP tokenization, layers and metrics simplify the
training process, and then show how to generate output text using the
KerasNLP sampling utilities.</p>
<p>Note: If you are running this example on a Colab, make sure to enable
GPU runtime for faster training.</p>
<p>This example requires KerasNLP. You can install it via the following
command: <code>pip install keras-nlp</code></p>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>os.environ[<span class="st">"KERAS_BACKEND"</span>] <span class="op">=</span> <span class="st">"jax"</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">import</span> keras_nlp</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="im">import</span> tensorflow.data <span class="im">as</span> tf_data</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="im">import</span> tensorflow.strings <span class="im">as</span> tf_strings</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="settings-hyperparameters">Settings &amp; hyperparameters<a class="anchor" aria-label="anchor" href="#settings-hyperparameters"></a>
</h2>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># Data</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>SEQ_LEN <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>MIN_TRAINING_SEQ_LEN <span class="op">=</span> <span class="dv">450</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="co"># Model</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>EMBED_DIM <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>FEED_FORWARD_DIM <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>NUM_HEADS <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>NUM_LAYERS <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>VOCAB_SIZE <span class="op">=</span> <span class="dv">5000</span>  <span class="co"># Limits parameters in model.</span></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a><span class="co"># Training</span></span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a>EPOCHS <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a><span class="co"># Inference</span></span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a>NUM_TOKENS_TO_GENERATE <span class="op">=</span> <span class="dv">80</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="load-the-data">Load the data<a class="anchor" aria-label="anchor" href="#load-the-data"></a>
</h2>
<p>Now, let’s download the dataset! The SimpleBooks dataset consists of
1,573 Gutenberg books, and has one of the smallest vocabulary size to
word-level tokens ratio. It has a vocabulary size of ~98k, a third of
WikiText-103’s, with around the same number of tokens (~100M). This
makes it easy to fit a small model.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>keras.utils.get_file(</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>    origin<span class="op">=</span><span class="st">"https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip"</span>,</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>    extract<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>)</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="bu">dir</span> <span class="op">=</span> os.path.expanduser(<span class="st">"~/.keras/datasets/simplebooks/"</span>)</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a><span class="co"># Load simplebooks-92 train set and filter out short lines.</span></span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>raw_train_ds <span class="op">=</span> (</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>    tf_data.TextLineDataset(<span class="bu">dir</span> <span class="op">+</span> <span class="st">"simplebooks-92-raw/train.txt"</span>)</span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>    .<span class="bu">filter</span>(<span class="kw">lambda</span> x: tf_strings.length(x) <span class="op">&gt;</span> MIN_TRAINING_SEQ_LEN)</span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>    .batch(BATCH_SIZE)</span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>    .shuffle(buffer_size<span class="op">=</span><span class="dv">256</span>)</span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a>)</span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a><span class="co"># Load simplebooks-92 validation set and filter out short lines.</span></span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a>raw_val_ds <span class="op">=</span> (</span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a>    tf_data.TextLineDataset(<span class="bu">dir</span> <span class="op">+</span> <span class="st">"simplebooks-92-raw/valid.txt"</span>)</span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a>    .<span class="bu">filter</span>(<span class="kw">lambda</span> x: tf_strings.length(x) <span class="op">&gt;</span> MIN_TRAINING_SEQ_LEN)</span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a>    .batch(BATCH_SIZE)</span>
<span id="cb3-20"><a href="#cb3-20" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="train-the-tokenizer">Train the tokenizer<a class="anchor" aria-label="anchor" href="#train-the-tokenizer"></a>
</h2>
<p>We train the tokenizer from the training dataset for a vocabulary
size of <code>VOCAB_SIZE</code>, which is a tuned hyperparameter. We
want to limit the vocabulary as much as possible, as we will see later
on that it has a large effect on the number of model parameters. We also
don’t want to include <em>too few</em> vocabulary terms, or there would
be too many out-of-vocabulary (OOV) sub-words. In addition, three tokens
are reserved in the vocabulary:</p>
<ul>
<li>
<code>"[PAD]"</code> for padding sequences to <code>SEQ_LEN</code>.
This token has index 0 in both <code>reserved_tokens</code> and
<code>vocab</code>, since <code>WordPieceTokenizer</code> (and other
layers) consider <code>0</code>/<code>vocab[0]</code> as the default
padding.</li>
<li>
<code>"[UNK]"</code> for OOV sub-words, which should match the
default <code>oov_token="[UNK]"</code> in
<code>WordPieceTokenizer</code>.</li>
<li>
<code>"[BOS]"</code> stands for beginning of sentence, but here
technically it is a token representing the beginning of each line of
training data.</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="co"># Train tokenizer vocabulary</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>vocab <span class="op">=</span> keras_nlp.tokenizers.compute_word_piece_vocabulary(</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>    raw_train_ds,</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>    vocabulary_size<span class="op">=</span>VOCAB_SIZE,</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>    lowercase<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>    reserved_tokens<span class="op">=</span>[<span class="st">"[PAD]"</span>, <span class="st">"[UNK]"</span>, <span class="st">"[BOS]"</span>],</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="load-tokenizer">Load tokenizer<a class="anchor" aria-label="anchor" href="#load-tokenizer"></a>
</h2>
<p>We use the vocabulary data to initialize
<code>keras_nlp.tokenizers.WordPieceTokenizer</code>. WordPieceTokenizer
is an efficient implementation of the WordPiece algorithm used by BERT
and other models. It will strip, lower-case and do other irreversible
preprocessing operations.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>tokenizer <span class="op">=</span> keras_nlp.tokenizers.WordPieceTokenizer(</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>    vocabulary<span class="op">=</span>vocab,</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>    sequence_length<span class="op">=</span>SEQ_LEN,</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>    lowercase<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="tokenize-data">Tokenize data<a class="anchor" aria-label="anchor" href="#tokenize-data"></a>
</h2>
<p>We preprocess the dataset by tokenizing and splitting it into
<code>features</code> and <code>labels</code>.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="co"># packer adds a start token</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>start_packer <span class="op">=</span> keras_nlp.layers.StartEndPacker(</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>    sequence_length<span class="op">=</span>SEQ_LEN,</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>    start_value<span class="op">=</span>tokenizer.token_to_id(<span class="st">"[BOS]"</span>),</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>)</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a><span class="kw">def</span> preprocess(inputs):</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>    outputs <span class="op">=</span> tokenizer(inputs)</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>    features <span class="op">=</span> start_packer(outputs)</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>    labels <span class="op">=</span> outputs</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>    <span class="cf">return</span> features, labels</span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a><span class="co"># Tokenize and split into train and label sequences.</span></span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a>train_ds <span class="op">=</span> raw_train_ds.<span class="bu">map</span>(</span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a>    preprocess, num_parallel_calls<span class="op">=</span>tf_data.AUTOTUNE</span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a>).prefetch(tf_data.AUTOTUNE)</span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a>val_ds <span class="op">=</span> raw_val_ds.<span class="bu">map</span>(</span>
<span id="cb6-20"><a href="#cb6-20" tabindex="-1"></a>    preprocess, num_parallel_calls<span class="op">=</span>tf_data.AUTOTUNE</span>
<span id="cb6-21"><a href="#cb6-21" tabindex="-1"></a>).prefetch(tf_data.AUTOTUNE)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="build-the-model">Build the model<a class="anchor" aria-label="anchor" href="#build-the-model"></a>
</h2>
<p>We create our scaled down GPT model with the following layers:</p>
<ul>
<li>One <code>keras_nlp.layers.TokenAndPositionEmbedding</code> layer,
which combines the embedding for the token and its position.</li>
<li>Multiple <code>keras_nlp.layers.TransformerDecoder</code> layers,
with the default causal masking. The layer has no cross-attention when
run with decoder sequence only.</li>
<li>One final dense linear layer</li>
</ul>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>inputs <span class="op">=</span> keras.layers.Input(shape<span class="op">=</span>(<span class="va">None</span>,), dtype<span class="op">=</span><span class="st">"int32"</span>)</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="co"># Embedding.</span></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>embedding_layer <span class="op">=</span> keras_nlp.layers.TokenAndPositionEmbedding(</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>    vocabulary_size<span class="op">=</span>VOCAB_SIZE,</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>    sequence_length<span class="op">=</span>SEQ_LEN,</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>    embedding_dim<span class="op">=</span>EMBED_DIM,</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>    mask_zero<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>)</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>x <span class="op">=</span> embedding_layer(inputs)</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a><span class="co"># Transformer decoders.</span></span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(NUM_LAYERS):</span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>    decoder_layer <span class="op">=</span> keras_nlp.layers.TransformerDecoder(</span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a>        num_heads<span class="op">=</span>NUM_HEADS,</span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a>        intermediate_dim<span class="op">=</span>FEED_FORWARD_DIM,</span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a>    )</span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a>    x <span class="op">=</span> decoder_layer(x)  <span class="co"># Giving one argument only skips cross-attention.</span></span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a><span class="co"># Output.</span></span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a>outputs <span class="op">=</span> keras.layers.Dense(VOCAB_SIZE)(x)</span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a>model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs)</span>
<span id="cb7-20"><a href="#cb7-20" tabindex="-1"></a>loss_fn <span class="op">=</span> keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-21"><a href="#cb7-21" tabindex="-1"></a>perplexity <span class="op">=</span> keras_nlp.metrics.Perplexity(from_logits<span class="op">=</span><span class="va">True</span>, mask_token_id<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-22"><a href="#cb7-22" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"adam"</span>, loss<span class="op">=</span>loss_fn, metrics<span class="op">=</span>[perplexity])</span></code></pre></div>
<p>Let’s take a look at our model summary - a large majority of the
parameters are in the <code>token_and_position_embedding</code> and the
output <code>dense</code> layer! This means that the vocabulary size
(<code>VOCAB_SIZE</code>) has a large effect on the size of the model,
while the number of Transformer decoder layers (<code>NUM_LAYERS</code>)
doesn’t affect it as much.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>model.summary()</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="training">Training<a class="anchor" aria-label="anchor" href="#training"></a>
</h2>
<p>Now that we have our model, let’s train it with the
<code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code> method.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>model.fit(train_ds, validation_data<span class="op">=</span>val_ds, verbose<span class="op">=</span><span class="dv">2</span>, epochs<span class="op">=</span>EPOCHS)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="inference">Inference<a class="anchor" aria-label="anchor" href="#inference"></a>
</h2>
<p>With our trained model, we can test it out to gauge its performance.
To do this we can seed our model with an input sequence starting with
the <code>"[BOS]"</code> token, and progressively sample the model by
making predictions for each subsequent token in a loop.</p>
<p>To start lets build a prompt with the same shape as our model inputs,
containing only the <code>"[BOS]"</code> token.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="co"># The "packer" layers adds the [BOS] token for us.</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>prompt_tokens <span class="op">=</span> start_packer(tokenizer([<span class="st">""</span>]))</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>prompt_tokens</span></code></pre></div>
<p>We will use the <code>keras_nlp.samplers</code> module for inference,
which requires a callback function wrapping the model we just trained.
This wrapper calls the model and returns the logit predictions for the
current token we are generating.</p>
<p>Note: There are two pieces of more advanced functionality available
when defining your callback. The first is the ability to take in a
<code>cache</code> of states computed in previous generation steps,
which can be used to speed up generation. The second is the ability to
output the final dense “hidden state” of each generated token. This is
used by <code>keras_nlp.samplers.ContrastiveSampler</code>, which avoids
repetition by penalizing repeated hidden states. Both are optional, and
we will ignore them for now.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="kw">def</span> <span class="bu">next</span>(prompt, cache, index):</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>    logits <span class="op">=</span> model(prompt)[:, index <span class="op">-</span> <span class="dv">1</span>, :]</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>    <span class="co"># Ignore hidden states for now; only needed for contrastive search.</span></span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>    hidden_states <span class="op">=</span> <span class="va">None</span></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>    <span class="cf">return</span> logits, hidden_states, cache</span></code></pre></div>
<p>Creating the wrapper function is the most complex part of using these
functions. Now that it’s done, let’s test out the different utilities,
starting with greedy search.</p>
<div class="section level3">
<h3 id="greedy-search">Greedy search<a class="anchor" aria-label="anchor" href="#greedy-search"></a>
</h3>
<p>We greedily pick the most probable token at each timestep. In other
words, we get the argmax of the model output.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>sampler <span class="op">=</span> keras_nlp.samplers.GreedySampler()</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>output_tokens <span class="op">=</span> sampler(</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>    <span class="bu">next</span><span class="op">=</span><span class="bu">next</span>,</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>    prompt<span class="op">=</span>prompt_tokens,</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>    index<span class="op">=</span><span class="dv">1</span>,  <span class="co"># Start sampling immediately after the [BOS] token.</span></span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>)</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a>txt <span class="op">=</span> tokenizer.detokenize(output_tokens)</span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Greedy search generated text: </span><span class="ch">\n</span><span class="sc">{</span>txt<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code></pre></div>
<p>As you can see, greedy search starts out making some sense, but
quickly starts repeating itself. This is a common problem with text
generation that can be fixed by some of the probabilistic text
generation utilities shown later on!</p>
</div>
<div class="section level3">
<h3 id="beam-search">Beam search<a class="anchor" aria-label="anchor" href="#beam-search"></a>
</h3>
<p>At a high-level, beam search keeps track of the
<code>num_beams</code> most probable sequences at each timestep, and
predicts the best next token from all sequences. It is an improvement
over greedy search since it stores more possibilities. However, it is
less efficient than greedy search since it has to compute and store
multiple potential sequences.</p>
<p><strong>Note:</strong> beam search with <code>num_beams=1</code> is
identical to greedy search.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>sampler <span class="op">=</span> keras_nlp.samplers.BeamSampler(num_beams<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>output_tokens <span class="op">=</span> sampler(</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>    <span class="bu">next</span><span class="op">=</span><span class="bu">next</span>,</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>    prompt<span class="op">=</span>prompt_tokens,</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>    index<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>)</span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a>txt <span class="op">=</span> tokenizer.detokenize(output_tokens)</span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Beam search generated text: </span><span class="ch">\n</span><span class="sc">{</span>txt<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code></pre></div>
<p>Similar to greedy search, beam search quickly starts repeating
itself, since it is still a deterministic method.</p>
</div>
<div class="section level3">
<h3 id="random-search">Random search<a class="anchor" aria-label="anchor" href="#random-search"></a>
</h3>
<p>Random search is our first probabilistic method. At each time step,
it samples the next token using the softmax probabilities provided by
the model.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>sampler <span class="op">=</span> keras_nlp.samplers.RandomSampler()</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>output_tokens <span class="op">=</span> sampler(</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>    <span class="bu">next</span><span class="op">=</span><span class="bu">next</span>,</span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>    prompt<span class="op">=</span>prompt_tokens,</span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>    index<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a>)</span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a>txt <span class="op">=</span> tokenizer.detokenize(output_tokens)</span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Random search generated text: </span><span class="ch">\n</span><span class="sc">{</span>txt<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code></pre></div>
<p>Voilà, no repetitions! However, with random search, we may see some
nonsensical words appearing since any word in the vocabulary has a
chance of appearing with this sampling method. This is fixed by our next
search utility, top-k search.</p>
</div>
<div class="section level3">
<h3 id="top-k-search">Top-K search<a class="anchor" aria-label="anchor" href="#top-k-search"></a>
</h3>
<p>Similar to random search, we sample the next token from the
probability distribution provided by the model. The only difference is
that here, we select out the top <code>k</code> most probable tokens,
and distribute the probability mass over them before sampling. This way,
we won’t be sampling from low probability tokens, and hence we would
have less nonsensical words!</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>sampler <span class="op">=</span> keras_nlp.samplers.TopKSampler(k<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>output_tokens <span class="op">=</span> sampler(</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>    <span class="bu">next</span><span class="op">=</span><span class="bu">next</span>,</span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>    prompt<span class="op">=</span>prompt_tokens,</span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>    index<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a>)</span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a>txt <span class="op">=</span> tokenizer.detokenize(output_tokens)</span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Top-K search generated text: </span><span class="ch">\n</span><span class="sc">{</span>txt<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code></pre></div>
</div>
<div class="section level3">
<h3 id="top-p-search">Top-P search<a class="anchor" aria-label="anchor" href="#top-p-search"></a>
</h3>
<p>Even with the top-k search, there is something to improve upon. With
top-k search, the number <code>k</code> is fixed, which means it selects
the same number of tokens for any probability distribution. Consider two
scenarios, one where the probability mass is concentrated over 2 words
and another where the probability mass is evenly concentrated across 10.
Should we choose <code>k=2</code> or <code>k=10</code>? There is no one
size that fits all <code>k</code> here.</p>
<p>This is where top-p search comes in! Instead of choosing a
<code>k</code>, we choose a probability <code>p</code> that we want the
probabilities of the top tokens to sum up to. This way, we can
dynamically adjust the <code>k</code> based on the probability
distribution. By setting <code>p=0.9</code>, if 90% of the probability
mass is concentrated on the top 2 tokens, we can filter out the top 2
tokens to sample from. If instead the 90% is distributed over 10 tokens,
it will similarly filter out the top 10 tokens to sample from.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>sampler <span class="op">=</span> keras_nlp.samplers.TopPSampler(p<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>output_tokens <span class="op">=</span> sampler(</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>    <span class="bu">next</span><span class="op">=</span><span class="bu">next</span>,</span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>    prompt<span class="op">=</span>prompt_tokens,</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>    index<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a>)</span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a>txt <span class="op">=</span> tokenizer.detokenize(output_tokens)</span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Top-P search generated text: </span><span class="ch">\n</span><span class="sc">{</span>txt<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code></pre></div>
</div>
<div class="section level3">
<h3 id="using-callbacks-for-text-generation">Using callbacks for text generation<a class="anchor" aria-label="anchor" href="#using-callbacks-for-text-generation"></a>
</h3>
<p>We can also wrap the utilities in a callback, which allows you to
print out a prediction sequence for every epoch of the model! Here is an
example of a callback for top-k search:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="kw">class</span> TopKTextGenerator(keras.callbacks.Callback):</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>    <span class="co">"""A callback to generate text from a trained model using top-k."""</span></span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, k):</span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a>        <span class="va">self</span>.sampler <span class="op">=</span> keras_nlp.samplers.TopKSampler(k)</span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a>    <span class="kw">def</span> on_epoch_end(<span class="va">self</span>, epoch, logs<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a>        output_tokens <span class="op">=</span> <span class="va">self</span>.sampler(</span>
<span id="cb17-9"><a href="#cb17-9" tabindex="-1"></a>            <span class="bu">next</span><span class="op">=</span><span class="bu">next</span>,</span>
<span id="cb17-10"><a href="#cb17-10" tabindex="-1"></a>            prompt<span class="op">=</span>prompt_tokens,</span>
<span id="cb17-11"><a href="#cb17-11" tabindex="-1"></a>            index<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb17-12"><a href="#cb17-12" tabindex="-1"></a>        )</span>
<span id="cb17-13"><a href="#cb17-13" tabindex="-1"></a>        txt <span class="op">=</span> tokenizer.detokenize(output_tokens)</span>
<span id="cb17-14"><a href="#cb17-14" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Top-K search generated text: </span><span class="ch">\n</span><span class="sc">{</span>txt<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb17-15"><a href="#cb17-15" tabindex="-1"></a></span>
<span id="cb17-16"><a href="#cb17-16" tabindex="-1"></a></span>
<span id="cb17-17"><a href="#cb17-17" tabindex="-1"></a>text_generation_callback <span class="op">=</span> TopKTextGenerator(k<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb17-18"><a href="#cb17-18" tabindex="-1"></a><span class="co"># Dummy training loop to demonstrate callback.</span></span>
<span id="cb17-19"><a href="#cb17-19" tabindex="-1"></a>model.fit(</span>
<span id="cb17-20"><a href="#cb17-20" tabindex="-1"></a>    train_ds.take(<span class="dv">1</span>), verbose<span class="op">=</span><span class="dv">2</span>, epochs<span class="op">=</span><span class="dv">2</span>, callbacks<span class="op">=</span>[text_generation_callback]</span>
<span id="cb17-21"><a href="#cb17-21" tabindex="-1"></a>)</span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="conclusion">Conclusion<a class="anchor" aria-label="anchor" href="#conclusion"></a>
</h2>
<p>To recap, in this example, we use KerasNLP layers to train a sub-word
vocabulary, tokenize training data, create a miniature GPT model, and
perform inference with the text generation library.</p>
<p>If you would like to understand how Transformers work, or learn more
about training the full GPT model, here are some further readings:</p>
<ul>
<li>Attention Is All You Need <a href="https://arxiv.org/abs/1706.03762" class="external-link">Vaswani et al., 2017</a>
</li>
<li>GPT-3 Paper <a href="https://arxiv.org/abs/2005.14165" class="external-link">Brown et al.,
2020</a>
</li>
</ul>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, RStudio, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
