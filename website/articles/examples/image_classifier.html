<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="keras3">
<title> • keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../../deps/JetBrains_Mono-0.4.7/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><meta property="og:title" content="">
<meta property="og:description" content="keras3">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-getting-started">Getting Started</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-getting-started">
    <a class="dropdown-item" href="../../articles/intro_to_keras_for_engineers.html">Introduction to Keras for engineers</a>
    <h6 class="dropdown-header" data-toc-skip>Tutorials</h6>
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
  </div>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1></h1>
                        <h4 data-toc-skip class="author"><a href="https://lukewood.xyz" class="external-link">lukewood</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/image_classifier.Rmd" class="external-link"><code>vignettes/examples/image_classifier.Rmd</code></a></small>
      <div class="d-none name"><code>image_classifier.Rmd</code></div>
    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co"># -*- coding: utf-8 -*-</span></span></code></pre></div>
<div class="section level3">
<h3 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h3>
<p>Classification is the process of predicting a categorical label for a
given input image. While classification is a relatively straightforward
computer vision task, modern approaches still are built of several
complex components. Luckily, KerasCV provides APIs to construct commonly
used components.</p>
<p>This guide demonstrates KerasCV’s modular approach to solving image
classification problems at three levels of complexity:</p>
<ul>
<li>Inference with a pretrained classifier</li>
<li>Fine-tuning a pretrained backbone</li>
<li>Training a image classifier from scratch</li>
</ul>
</div>
<div class="section level3">
<h3 id="multi-backend-support">Multi-Backend Support<a class="anchor" aria-label="anchor" href="#multi-backend-support"></a>
</h3>
<p>KerasCV’s <code>ImageClassifier</code> model supports several
backends like JAX, PyTorch, and TensorFlow with the help of
<code>keras</code>. To enable multi-backend support in KerasCV, set the
<code>KERAS_CV_MULTI_BACKEND</code> environment variable. We can then
switch between different backends by setting the
<code>KERAS_BACKEND</code> environment variable. Currently,
<code>"tensorflow"</code>, <code>"jax"</code>, and <code>"torch"</code>
are supported.</p>
<p>This demonstration uses the Jax backend.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>os.environ[<span class="st">"KERAS_CV_MULTI_BACKEND"</span>] <span class="op">=</span> <span class="st">"1"</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>os.environ[<span class="st">"KERAS_BACKEND"</span>] <span class="op">=</span> <span class="st">"jax"</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a><span class="im">import</span> keras_cv</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> ops</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> losses</span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> optimizers</span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a><span class="im">from</span> keras.optimizers <span class="im">import</span> schedules</span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> metrics</span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> data <span class="im">as</span> tf_data</span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a><span class="im">import</span> tensorflow_datasets <span class="im">as</span> tfds</span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<p>Let’s get started with the simplest KerasCV API: a pretrained
classifier. In this example, we will construct a classifier that was
pretrained on the ImageNet dataset. We’ll use this model to solve the
age old “Cat or Dog” problem.</p>
<p>The highest level module in KerasCV is a <em>task</em>. A
<em>task</em> is a <code>keras.Model</code> consisting of a (generally
pretrained) backbone model and task-specific layers. Here’s an example
using <code>keras_cv.models.ImageClassifier</code> with an
EfficientNetV2B0 Backbone.</p>
<p>EfficientNetV2B0 is a great starting model when constructing an image
classification pipeline. This architecture manages to achieve high
accuracy, while using a parameter count of 7M. If an EfficientNetV2B0 is
not powerful enough for the task you are hoping to solve, be sure to
check out <a href="https://github.com/keras-team/keras-cv/tree/master/keras_cv/models/backbones" class="external-link">KerasCV’s
other available Backbones</a>!</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>classifier <span class="op">=</span> keras_cv.models.ImageClassifier.from_preset(</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>    <span class="st">"efficientnetv2_b0_imagenet_classifier"</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>)</span></code></pre></div>
<p>where you would construct the class with
<code>EfficientNetV2B0(weights="imagenet")</code>. While the old API was
great for classification, it did not scale effectively to other use
cases that required complex architectures, like object deteciton and
semantic segmentation.</p>
<p>Now that our classifier is built, let’s apply it to this cute cat
picture!</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>filepath <span class="op">=</span> keras.utils.get_file(origin<span class="op">=</span><span class="st">"https://i.imgur.com/9i63gLN.jpg"</span>)</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>image <span class="op">=</span> keras.utils.load_img(filepath)</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>image <span class="op">=</span> np.array(image)</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>keras_cv.visualization.plot_image_gallery(</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>    image[<span class="va">None</span>, ...], rows<span class="op">=</span><span class="dv">1</span>, cols<span class="op">=</span><span class="dv">1</span>, value_range<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">255</span>), show<span class="op">=</span><span class="va">True</span>, scale<span class="op">=</span><span class="dv">4</span></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>)</span></code></pre></div>
<p>predictions = classifier.predict(np.expand_dims(image, axis=0))</p>
<p><code>Predictions come in the form of softmax-ed category rankings. We can find the index of the top classes using a simple argsort function:</code></p>
<p>top_classes = predictions[0].argsort(axis=-1)</p>
<p><code>In order to decode the class mappings, we can construct a mapping from category indices to ImageNet class names. For convenience, I've stored the ImageNet class mapping in a GitHub gist. Let's download and load it now.</code></p>
<p>classes = keras.utils.get_file( origin=“<a href="https://gist.githubusercontent.com/LukeWood/62eebcd5c5c4a4d0e0b7845780f76d55/raw/fde63e5e4c09e2fa0a3436680f436bdcb8325aac/ImagenetClassnames.json" class="external-link uri">https://gist.githubusercontent.com/LukeWood/62eebcd5c5c4a4d0e0b7845780f76d55/raw/fde63e5e4c09e2fa0a3436680f436bdcb8325aac/ImagenetClassnames.json</a>”
) with open(classes, “rb”) as f: classes = json.load(f)</p>
<p><code>Now we can simply look up the class names via index:""" top_two = [classes[str(i)] for i in top_classes[-2:]] print("Top two classes are:", top_two)</code></p>
<p>However, one of the classes is “Velvet”. We’re trying to classify
Cats VS Dogs. We don’t care about the velvet blanket!</p>
<p>Ideally, we’d have a classifier that only performs computation to
determine if an image is a cat or a dog, and has all of its resources
dedicated to this task. This can be solved by fine tuning our own
classifier.</p>
</div>
<div class="section level2">
<h2 id="fine-tuning-a-pretrained-classifier">Fine tuning a pretrained classifier<a class="anchor" aria-label="anchor" href="#fine-tuning-a-pretrained-classifier"></a>
</h2>
<p>When labeled images specific to our task are available, fine-tuning a
custom classifier can improve performance. If we want to train a Cats vs
Dogs Classifier, using explicitly labeled Cat vs Dog data should perform
better than the generic classifier! For many tasks, no relevant
pretrained model will be available (e.g., categorizing images specific
to your application).</p>
<p>First, let’s get started by loading some data:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>IMAGE_SIZE <span class="op">=</span> (<span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>AUTOTUNE <span class="op">=</span> tf_data.AUTOTUNE</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>tfds.disable_progress_bar()</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>data, dataset_info <span class="op">=</span> tfds.load(</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>    <span class="st">"cats_vs_dogs"</span>,</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>    with_info<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>    as_supervised<span class="op">=</span><span class="va">True</span></span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>)</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>train_steps_per_epoch <span class="op">=</span> (</span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>    dataset_info.splits[<span class="st">"train"</span>].num_examples <span class="op">//</span> BATCH_SIZE</span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>)</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>train_dataset <span class="op">=</span> data[<span class="st">"train"</span>]</span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>num_classes <span class="op">=</span> dataset_info.features[<span class="st">"label"</span>].num_classes</span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a>resizing <span class="op">=</span> keras_cv.layers.Resizing(</span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a>    IMAGE_SIZE[<span class="dv">0</span>], IMAGE_SIZE[<span class="dv">1</span>], crop_to_aspect_ratio<span class="op">=</span><span class="va">True</span></span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a>)</span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a>encoder <span class="op">=</span> keras.layers.CategoryEncoding(num_classes, <span class="st">"one_hot"</span>, dtype<span class="op">=</span><span class="st">"int32"</span>)</span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" tabindex="-1"></a><span class="kw">def</span> preprocess_inputs(image, label):</span>
<span id="cb5-25"><a href="#cb5-25" tabindex="-1"></a>    <span class="co"># Staticly resize images as we only iterate the dataset once.</span></span>
<span id="cb5-26"><a href="#cb5-26" tabindex="-1"></a>    <span class="cf">return</span> resizing(image), encoder(label)</span>
<span id="cb5-27"><a href="#cb5-27" tabindex="-1"></a></span>
<span id="cb5-28"><a href="#cb5-28" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" tabindex="-1"></a><span class="co"># Shuffle the dataset to increase diversity of batches.</span></span>
<span id="cb5-30"><a href="#cb5-30" tabindex="-1"></a><span class="co"># 10*BATCH_SIZE follows the assumption that bigger machines can handle bigger</span></span>
<span id="cb5-31"><a href="#cb5-31" tabindex="-1"></a><span class="co"># shuffle buffers.</span></span>
<span id="cb5-32"><a href="#cb5-32" tabindex="-1"></a>train_dataset <span class="op">=</span> train_dataset.shuffle(</span>
<span id="cb5-33"><a href="#cb5-33" tabindex="-1"></a>    <span class="dv">10</span> <span class="op">*</span> BATCH_SIZE, reshuffle_each_iteration<span class="op">=</span><span class="va">True</span></span>
<span id="cb5-34"><a href="#cb5-34" tabindex="-1"></a>).<span class="bu">map</span>(preprocess_inputs, num_parallel_calls<span class="op">=</span>AUTOTUNE)</span>
<span id="cb5-35"><a href="#cb5-35" tabindex="-1"></a>train_dataset <span class="op">=</span> train_dataset.batch(BATCH_SIZE)</span>
<span id="cb5-36"><a href="#cb5-36" tabindex="-1"></a></span>
<span id="cb5-37"><a href="#cb5-37" tabindex="-1"></a>images <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_dataset.take(<span class="dv">1</span>)))[<span class="dv">0</span>]</span>
<span id="cb5-38"><a href="#cb5-38" tabindex="-1"></a>keras_cv.visualization.plot_image_gallery(images, value_range<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">255</span>))</span></code></pre></div>
<p>Next let’s construct our model. The use of imagenet in the preset
name indicates that the backbone was pretrained on the ImageNet dataset.
Pretrained backbones extract more information from our labeled examples
by leveraging patterns extracted from potentially much larger
datasets.</p>
<p>Next lets put together our classifier:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>model <span class="op">=</span> keras_cv.models.ImageClassifier.from_preset(</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>    <span class="st">"efficientnetv2_b0_imagenet"</span>, num_classes<span class="op">=</span><span class="dv">2</span></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>)</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">"categorical_crossentropy"</span>,</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>    optimizer<span class="op">=</span>keras.optimizers.SGD(learning_rate<span class="op">=</span><span class="fl">0.01</span>),</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">"accuracy"</span>],</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>)</span></code></pre></div>
<p>All that is left to do is call <code>model.fit()</code>:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>model.fit(train_dataset)</span></code></pre></div>
<p>predictions = model.predict(np.expand_dims(image, axis=0))</p>
<p>classes = {0: “cat”, 1: “dog”} print(“Top class is:”,
classes[predictions[0].argmax()])</p>
<p>```Awesome - looks like the model correctly classified the image. #
Train a Classifier from Scratch</p>
<p>Now that we’ve gotten our hands dirty with classification, let’s take
on one last task: training a classification model from scratch! A
standard benchmark for image classification is the ImageNet dataset,
however due to licensing constraints we will use the CalTech 101 image
classification dataset in this tutorial. While we use the simpler
CalTech 101 dataset in this guide, the same training template may be
used on ImageNet to achieve near state-of-the-art scores.</p>
<p>Let’s start out by tackling data loading:</p>
<pre><code>
NUM_CLASSES = 101
# Change epochs to 100~ to fully train.
EPOCHS = 1

encoder = keras.layers.CategoryEncoding(NUM_CLASSES, "one_hot", dtype="int32")

def package_inputs(image, label):
    return {"images": image, "labels": encoder(label)}

train_ds, eval_ds = tfds.load(
    "caltech101", split=["train", "test"], as_supervised="true"
)
train_ds = train_ds.map(package_inputs, num_parallel_calls=tf_data.AUTOTUNE)
eval_ds = eval_ds.map(package_inputs, num_parallel_calls=tf_data.AUTOTUNE)

train_ds = train_ds.shuffle(BATCH_SIZE * 16)

```The CalTech101 dataset has different sizes for every image, so we use the
`ragged_batch()` API to batch them together while maintaining each individual
image's shape information.</code></pre>
<p>train_ds = train_ds.ragged_batch(BATCH_SIZE) eval_ds =
eval_ds.ragged_batch(BATCH_SIZE)</p>
<p>batch = next(iter(train_ds.take(1))) image_batch = batch[“images”]
label_batch = batch[“labels”]</p>
<p>keras_cv.visualization.plot_image_gallery( image_batch.to_tensor(),
rows=3, cols=3, value_range=(0, 255), show=True, )</p>
<p>```## Data Augmentation In our previous finetuning exmaple, we
performed a static resizing operation and did not utilize any image
augmentation. This is because a single pass over the training set was
sufficient to achieve decent results. When training to solve a more
difficult task, you’ll want to include data augmentation in your data
pipeline.</p>
<p>Data augmentation is a technique to make your model robust to changes
in input data such as lighting, cropping, and orientation. KerasCV
includes some of the most useful augmentations in the
<code>keras_cv.layers</code> API. Creating an optimal pipeline of
augmentations is an art, but in this section of the guide we’ll offer
some tips on best practices for classification.</p>
<p>One caveat to be aware of with image data augmentation is that you
must be careful to not shift your augmented data distribution too far
from the original data distribution. The goal is to prevent overfitting
and increase generalization, but samples that lie completely out of the
data distribution simply add noise to the training process.</p>
<p>The first augmentation we’ll use is <code>RandomFlip</code>. This
augmentation behaves more or less how you’d expect: it either flips the
image or not. While this augmentation is useful in CalTech101 and
ImageNet, it should be noted that it should not be used on tasks where
the data distribution is not vertical mirror invariant. An example of a
dataset where this occurs is MNIST hand written digits. Flipping a
<code>6</code> over the vertical axis will make the digit appear more
like a <code>7</code> than a <code>6</code>, but the label will still
show a <code>6</code>.</p>
<pre><code>
random_flip = keras_cv.layers.RandomFlip()
augmenters = [random_flip]

image_batch = random_flip(image_batch)
keras_cv.visualization.plot_image_gallery(
    image_batch.to_tensor(),
    rows=3,
    cols=3,
    value_range=(0, 255),
    show=True,
)

```Half of the images have been flipped!
The next augmentation we'll use is `RandomCropAndResize`.
This operation selects a random subset of the image, then resizes it to the
provided target size.
By using this augmentation, we force our classifier to become spatially
invariant.
Additionally, this layer accepts an `aspect_ratio_factor` which can be used to
distort the aspect ratio of the image.
While this can improve model performance, it should be used with caution.
It is very easy for an aspect ratio distortion to shift a sample too far from
the original training set's data distribution.
Remember - the goal of data augmentation is to produce more training samples
that align with the data distribution of your training set!

`RandomCropAndResize` also can handle `tf.RaggedTensor` inputs.  In the
CalTech101 image dataset images come in a wide variety of sizes.
As such they cannot easily be batched together into a dense training batch.
Luckily, `RandomCropAndResize` handles the Ragged -&gt; Dense conversion process
for you!

Let's add a `RandomCropAndResize` to our set of augmentations:</code></pre>
<p>crop_and_resize = keras_cv.layers.RandomCropAndResize(
target_size=IMAGE_SIZE, crop_area_factor=(0.8, 1.0),
aspect_ratio_factor=(0.9, 1.1), ) augmenters += [crop_and_resize]</p>
<p>image_batch = crop_and_resize(image_batch)
keras_cv.visualization.plot_image_gallery( image_batch, rows=3, cols=3,
value_range=(0, 255), show=True, )</p>
<p>```Great! We are now working with a batch of dense images. Next up,
lets include some spatial and color-based jitter to our training set.
This will allow us to produce a classifier that is robust to lighting
flickers, shadows, and more.</p>
<p>There are limitless ways to augment an image by altering color and
spatial features, but perhaps the most battle tested technique is <a href="https://arxiv.org/abs/1909.13719" class="external-link"><code>RandAugment</code></a>.
<code>RandAugment</code> is actually a set of 10 different
augmentations: <code>AutoContrast</code>, <code>Equalize</code>,
<code>Solarize</code>, <code>RandomColorJitter</code>,
<code>RandomContrast</code>, <code>RandomBrightness</code>,
<code>ShearX</code>, <code>ShearY</code>, <code>TranslateX</code> and
<code>TranslateY</code>. At inference time,
<code>num_augmentations</code> augmenters are sampled for each image,
and random magnitude factors are sampled for each. These augmentations
are then applied sequentially.</p>
<p>KerasCV makes tuning these parameters easy using the
<code>augmentations_per_image</code> and <code>magnitude</code>
parameters! Let’s take it for a spin:</p>
<pre><code>
rand_augment = keras_cv.layers.RandAugment(
    augmentations_per_image=3,
    magnitude=0.3,
    value_range=(0, 255),
)
augmenters += [rand_augment]

image_batch = rand_augment(image_batch)
keras_cv.visualization.plot_image_gallery(
    image_batch,
    rows=3,
    cols=3,
    value_range=(0, 255),
    show=True,
)

```Looks great; but we're not done yet!
What if an image is missing one critical feature of a class?  For example,
what if a leaf is blocking the view of a cat's ear, but our classifier
learned to classify cats simply by observing their ears?

One easy approach to tackling this is to use `RandomCutout`, which randomly
strips out a sub-section of the image:</code></pre>
<p>random_cutout = keras_cv.layers.RandomCutout( width_factor=0.4,
height_factor=0.4 ) keras_cv.visualization.plot_image_gallery(
random_cutout(image_batch), rows=3, cols=3, value_range=(0, 255),
show=True, )</p>
<p>```While this tackles the problem reasonably well, it can cause the
classifier to develop responses to borders between features and black
pixel areas caused by the cutout.</p>
<p><a href="https://arxiv.org/abs/1905.04899" class="external-link"><code>CutMix</code></a>
solves the same issue by using a more complex (and more effective)
technique. Instead of replacing the cut-out areas with black pixels,
<code>CutMix</code> replaces these regions with regions of other images
sampled from within your training set! Following this replacement, the
image’s classification label is updated to be a blend of the original
and mixed image’s class label.</p>
<p>What does this look like in practice? Let’s check it out:</p>
<pre><code>
cut_mix = keras_cv.layers.CutMix()
# CutMix needs to modify both images and labels
inputs = {"images": image_batch, "labels": tf.cast(label_batch, "float32")}

keras_cv.visualization.plot_image_gallery(
    cut_mix(inputs)["images"],
    rows=3,
    cols=3,
    value_range=(0, 255),
    show=True,
)

```Let's hold off from adding it to our augmenter for a minute - more on that
soon!

Next, let's look into `MixUp()`.
Unfortunately, while `MixUp()` has been empirically shown to *substantially*
improve both the robustness and the generalization of the trained model,
it is not well-understood why such improvement occurs... but
a little alchemy never hurt anyone!

`MixUp()` works by sampling two images from a batch, then proceeding to
literally blend together their pixel intensities as well as their
classification labels.

Let's see it in action:</code></pre>
<p>mix_up = keras_cv.layers.MixUp() # MixUp needs to modify both images
and labels inputs = {“images”: image_batch, “labels”:
tf.cast(label_batch, “float32”)}</p>
<p>keras_cv.visualization.plot_image_gallery( mix_up(inputs)[“images”],
rows=3, cols=3, value_range=(0, 255), show=True, )</p>
<p><code>If you look closely, you'll see that the images have been blended together. Instead of applying `CutMix()` and `MixUp()` to every image, we instead pick one or the other to apply to each batch. This can be expressed using `keras_cv.layers.RandomChoice()`</code></p>
<p>cut_mix_or_mix_up = keras_cv.layers.RandomChoice( [cut_mix, mix_up],
batchwise=True ) augmenters += [cut_mix_or_mix_up]</p>
<p>```Now let’s apply our final augmenter to the training data:““”
augmenter = keras_cv.layers.Augmenter(augmenters) train_ds =
train_ds.map(augmenter, num_parallel_calls=tf_data.AUTOTUNE)</p>
<p>image_batch = next(iter(train_ds.take(1)))[“images”]
keras_cv.visualization.plot_image_gallery( image_batch, rows=3, cols=3,
value_range=(0, 255), show=True, )</p>
<pre><code>
size expected by our model. We use the deterministic
`keras_cv.layers.Resizing` in this case to avoid adding noise to our
evaluation metric.

```python
inference_resizing = keras_cv.layers.Resizing(
    IMAGE_SIZE[0], IMAGE_SIZE[1], crop_to_aspect_ratio=True
)
eval_ds = eval_ds.map(inference_resizing, num_parallel_calls=tf_data.AUTOTUNE)

inference_resizing = keras_cv.layers.Resizing(
    IMAGE_SIZE[0], IMAGE_SIZE[1], crop_to_aspect_ratio=True
)
eval_ds = eval_ds.map(inference_resizing, num_parallel_calls=tf_data.AUTOTUNE)

image_batch = next(iter(eval_ds.take(1)))["images"]
keras_cv.visualization.plot_image_gallery(
    image_batch,
    rows=3,
    cols=3,
    value_range=(0, 255),
    show=True,
)</code></pre>
<p><code>model.fit()</code>, which accepts a tuple of
<code>(images, labels)</code>.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="kw">def</span> unpackage_dict(inputs):</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>    <span class="cf">return</span> inputs[<span class="st">"images"</span>], inputs[<span class="st">"labels"</span>]</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>train_ds <span class="op">=</span> train_ds.<span class="bu">map</span>(unpackage_dict, num_parallel_calls<span class="op">=</span>tf_data.AUTOTUNE)</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>eval_ds <span class="op">=</span> eval_ds.<span class="bu">map</span>(unpackage_dict, num_parallel_calls<span class="op">=</span>tf_data.AUTOTUNE)</span></code></pre></div>
<p>classifier. Congratulations on making it this far!</p>
<div class="section level3">
<h3 id="optimizer-tuning">Optimizer Tuning<a class="anchor" aria-label="anchor" href="#optimizer-tuning"></a>
</h3>
<p>To achieve optimal performance, we need to use a learning rate
schedule instead of a single learning rate. While we won’t go into
detail on the Cosine decay with warmup schedule used here, <a href="https://scorrea92.medium.com/cosine-learning-rate-decay-e8b50aa455b" class="external-link">you
can read more about it here</a>.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="kw">def</span> lr_warmup_cosine_decay(</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>    global_step,</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>    warmup_steps,</span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>    hold<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>    total_steps<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a>    start_lr<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a>    target_lr<span class="op">=</span><span class="fl">1e-2</span>,</span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a>):</span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a>    <span class="co"># Cosine decay</span></span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a>    learning_rate <span class="op">=</span> (</span>
<span id="cb14-11"><a href="#cb14-11" tabindex="-1"></a>        <span class="fl">0.5</span></span>
<span id="cb14-12"><a href="#cb14-12" tabindex="-1"></a>        <span class="op">*</span> target_lr</span>
<span id="cb14-13"><a href="#cb14-13" tabindex="-1"></a>        <span class="op">*</span> (</span>
<span id="cb14-14"><a href="#cb14-14" tabindex="-1"></a>            <span class="dv">1</span></span>
<span id="cb14-15"><a href="#cb14-15" tabindex="-1"></a>            <span class="op">+</span> ops.cos(</span>
<span id="cb14-16"><a href="#cb14-16" tabindex="-1"></a>                math.pi</span>
<span id="cb14-17"><a href="#cb14-17" tabindex="-1"></a>                <span class="op">*</span> ops.convert_to_tensor(</span>
<span id="cb14-18"><a href="#cb14-18" tabindex="-1"></a>                    global_step <span class="op">-</span> warmup_steps <span class="op">-</span> hold, dtype<span class="op">=</span><span class="st">"float32"</span></span>
<span id="cb14-19"><a href="#cb14-19" tabindex="-1"></a>                )</span>
<span id="cb14-20"><a href="#cb14-20" tabindex="-1"></a>                <span class="op">/</span> ops.convert_to_tensor(</span>
<span id="cb14-21"><a href="#cb14-21" tabindex="-1"></a>                    total_steps <span class="op">-</span> warmup_steps <span class="op">-</span> hold, dtype<span class="op">=</span><span class="st">"float32"</span></span>
<span id="cb14-22"><a href="#cb14-22" tabindex="-1"></a>                )</span>
<span id="cb14-23"><a href="#cb14-23" tabindex="-1"></a>            )</span>
<span id="cb14-24"><a href="#cb14-24" tabindex="-1"></a>        )</span>
<span id="cb14-25"><a href="#cb14-25" tabindex="-1"></a>    )</span>
<span id="cb14-26"><a href="#cb14-26" tabindex="-1"></a></span>
<span id="cb14-27"><a href="#cb14-27" tabindex="-1"></a>    warmup_lr <span class="op">=</span> (target_lr <span class="op">*</span> (global_step <span class="op">/</span> warmup_steps))</span>
<span id="cb14-28"><a href="#cb14-28" tabindex="-1"></a></span>
<span id="cb14-29"><a href="#cb14-29" tabindex="-1"></a>    <span class="cf">if</span> hold <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb14-30"><a href="#cb14-30" tabindex="-1"></a>        learning_rate <span class="op">=</span> ops.where(</span>
<span id="cb14-31"><a href="#cb14-31" tabindex="-1"></a>            global_step <span class="op">&gt;</span> warmup_steps <span class="op">+</span> hold, learning_rate, target_lr</span>
<span id="cb14-32"><a href="#cb14-32" tabindex="-1"></a>        )</span>
<span id="cb14-33"><a href="#cb14-33" tabindex="-1"></a></span>
<span id="cb14-34"><a href="#cb14-34" tabindex="-1"></a>    learning_rate <span class="op">=</span> ops.where(</span>
<span id="cb14-35"><a href="#cb14-35" tabindex="-1"></a>        global_step <span class="op">&lt;</span> warmup_steps, warmup_lr, learning_rate</span>
<span id="cb14-36"><a href="#cb14-36" tabindex="-1"></a>    )</span>
<span id="cb14-37"><a href="#cb14-37" tabindex="-1"></a>    <span class="cf">return</span> learning_rate</span>
<span id="cb14-38"><a href="#cb14-38" tabindex="-1"></a></span>
<span id="cb14-39"><a href="#cb14-39" tabindex="-1"></a></span>
<span id="cb14-40"><a href="#cb14-40" tabindex="-1"></a><span class="kw">class</span> WarmUpCosineDecay(</span>
<span id="cb14-41"><a href="#cb14-41" tabindex="-1"></a>    schedules.LearningRateSchedule</span>
<span id="cb14-42"><a href="#cb14-42" tabindex="-1"></a>):</span>
<span id="cb14-43"><a href="#cb14-43" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb14-44"><a href="#cb14-44" tabindex="-1"></a>        <span class="va">self</span>, warmup_steps, total_steps, hold, start_lr<span class="op">=</span><span class="fl">0.0</span>, target_lr<span class="op">=</span><span class="fl">1e-2</span></span>
<span id="cb14-45"><a href="#cb14-45" tabindex="-1"></a>    ):</span>
<span id="cb14-46"><a href="#cb14-46" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb14-47"><a href="#cb14-47" tabindex="-1"></a>        <span class="va">self</span>.start_lr <span class="op">=</span> start_lr</span>
<span id="cb14-48"><a href="#cb14-48" tabindex="-1"></a>        <span class="va">self</span>.target_lr <span class="op">=</span> target_lr</span>
<span id="cb14-49"><a href="#cb14-49" tabindex="-1"></a>        <span class="va">self</span>.warmup_steps <span class="op">=</span> warmup_steps</span>
<span id="cb14-50"><a href="#cb14-50" tabindex="-1"></a>        <span class="va">self</span>.total_steps <span class="op">=</span> total_steps</span>
<span id="cb14-51"><a href="#cb14-51" tabindex="-1"></a>        <span class="va">self</span>.hold <span class="op">=</span> hold</span>
<span id="cb14-52"><a href="#cb14-52" tabindex="-1"></a></span>
<span id="cb14-53"><a href="#cb14-53" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, step):</span>
<span id="cb14-54"><a href="#cb14-54" tabindex="-1"></a>        lr <span class="op">=</span> lr_warmup_cosine_decay(</span>
<span id="cb14-55"><a href="#cb14-55" tabindex="-1"></a>            global_step<span class="op">=</span>step,</span>
<span id="cb14-56"><a href="#cb14-56" tabindex="-1"></a>            total_steps<span class="op">=</span><span class="va">self</span>.total_steps,</span>
<span id="cb14-57"><a href="#cb14-57" tabindex="-1"></a>            warmup_steps<span class="op">=</span><span class="va">self</span>.warmup_steps,</span>
<span id="cb14-58"><a href="#cb14-58" tabindex="-1"></a>            start_lr<span class="op">=</span><span class="va">self</span>.start_lr,</span>
<span id="cb14-59"><a href="#cb14-59" tabindex="-1"></a>            target_lr<span class="op">=</span><span class="va">self</span>.target_lr,</span>
<span id="cb14-60"><a href="#cb14-60" tabindex="-1"></a>            hold<span class="op">=</span><span class="va">self</span>.hold,</span>
<span id="cb14-61"><a href="#cb14-61" tabindex="-1"></a>        )</span>
<span id="cb14-62"><a href="#cb14-62" tabindex="-1"></a></span>
<span id="cb14-63"><a href="#cb14-63" tabindex="-1"></a>        <span class="cf">return</span> ops.where(step <span class="op">&gt;</span> <span class="va">self</span>.total_steps, <span class="fl">0.0</span>, lr)</span></code></pre></div>
<p>The schedule looks a as we expect.</p>
<p>Next let’s construct this optimizer:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>total_images <span class="op">=</span> <span class="dv">9000</span></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>total_steps <span class="op">=</span> (total_images <span class="op">//</span> BATCH_SIZE) <span class="op">*</span> EPOCHS</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>warmup_steps <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.1</span> <span class="op">*</span> total_steps)</span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>hold_steps <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.45</span> <span class="op">*</span> total_steps)</span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>schedule <span class="op">=</span> WarmUpCosineDecay(</span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a>    start_lr<span class="op">=</span><span class="fl">0.05</span>,</span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a>    target_lr<span class="op">=</span><span class="fl">1e-2</span>,</span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a>    warmup_steps<span class="op">=</span>warmup_steps,</span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a>    total_steps<span class="op">=</span>total_steps,</span>
<span id="cb15-10"><a href="#cb15-10" tabindex="-1"></a>    hold<span class="op">=</span>hold_steps,</span>
<span id="cb15-11"><a href="#cb15-11" tabindex="-1"></a>)</span>
<span id="cb15-12"><a href="#cb15-12" tabindex="-1"></a>optimizer <span class="op">=</span> optimizers.SGD(</span>
<span id="cb15-13"><a href="#cb15-13" tabindex="-1"></a>    weight_decay<span class="op">=</span><span class="fl">5e-4</span>,</span>
<span id="cb15-14"><a href="#cb15-14" tabindex="-1"></a>    learning_rate<span class="op">=</span>schedule,</span>
<span id="cb15-15"><a href="#cb15-15" tabindex="-1"></a>    momentum<span class="op">=</span><span class="fl">0.9</span>,</span>
<span id="cb15-16"><a href="#cb15-16" tabindex="-1"></a>)</span></code></pre></div>
<p><code>keras_cv.models.EfficientNetV2B0Backbone()</code> is a
convenience alias for
<code>keras_cv.models.EfficientNetV2Backbone.from_preset('efficientnetv2_b0')</code>.
Note that this preset does not come with any pretrained weights.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>backbone <span class="op">=</span> keras_cv.models.ResNet18V2Backbone()</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>model <span class="op">=</span> keras.Sequential(</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>    [</span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>        backbone,</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>        keras.layers.GlobalMaxPooling2D(),</span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a>        keras.layers.Dropout(rate<span class="op">=</span><span class="fl">0.5</span>),</span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a>        keras.layers.Dense(<span class="dv">101</span>, activation<span class="op">=</span><span class="st">"softmax"</span>),</span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a>    ]</span>
<span id="cb16-9"><a href="#cb16-9" tabindex="-1"></a>)</span></code></pre></div>
<p>we employ label smoothing to prevent the model from overfitting to
artifacts of this augmentation process.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>loss <span class="op">=</span> losses.CategoricalCrossentropy(label_smoothing<span class="op">=</span><span class="fl">0.1</span>)</span></code></pre></div>
<p>model.compile( loss=loss, optimizer=optimizer, metrics=[
metrics.CategoricalAccuracy(), metrics.TopKCategoricalAccuracy(k=5), ],
)</p>
<p><code>and finally call fit().""" model.fit(     train_ds,     epochs=EPOCHS,     validation_data=eval_ds, )</code></p>
<p>from scratch in KerasCV. Depending on the availability of labeled
data for your application, training from scratch may or may not be more
powerful than using transfer learning in addition to the data
augmentations discussed above. For smaller datasets, pretrained models
generally produce high accuracy and faster convergence.</p>
</div>
<div class="section level3">
<h3 id="conclusions">Conclusions<a class="anchor" aria-label="anchor" href="#conclusions"></a>
</h3>
<p>While image classification is perhaps the simplest problem in
computer vision, the modern landscape has numerous complex components.
Luckily, KerasCV offers robust, production-grade APIs to make assembling
most of these components possible in one line of code. Through the use
of KerasCV’s <code>ImageClassifier</code> API, pretrained weights, and
KerasCV data augmentations you can assemble everything you need to train
a powerful classifier in a few hundred lines of code!</p>
<p>As a follow up exercise, give the following a try:</p>
<ul>
<li>Fine tune a KerasCV classifier on your own dataset</li>
<li>Learn more about <a href="https://keras.io/guides/keras_cv/cut_mix_mix_up_and_rand_augment/" class="external-link">KerasCV’s
data augmentations</a>
</li>
<li>Check out how we train our models on <a href="https://github.com/keras-team/keras-cv/blob/master/examples/training/classification/imagenet/basic_training.py" class="external-link">ImageNet</a>
</li>
</ul>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
