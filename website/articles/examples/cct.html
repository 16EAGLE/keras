<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Compact Convolutional Transformers for efficient image classification.">
<title>Compact Convolutional Transformers • keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../../deps/JetBrains_Mono-0.4.7/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><meta property="og:title" content="Compact Convolutional Transformers">
<meta property="og:description" content="Compact Convolutional Transformers for efficient image classification.">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-getting-started">Getting Started</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-getting-started">
    <a class="dropdown-item" href="../../articles/intro_to_keras_for_engineers.html">Introduction to Keras for engineers</a>
    <h6 class="dropdown-header" data-toc-skip>Tutorials</h6>
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
  </div>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Compact Convolutional Transformers</h1>
                        <h4 data-toc-skip class="author"><a href="https://twitter.com/RisingSayak" class="external-link">Sayak Paul</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/cct.Rmd" class="external-link"><code>vignettes/examples/cct.Rmd</code></a></small>
      <div class="d-none name"><code>cct.Rmd</code></div>
    </div>

    
    
<p>As discussed in the <a href="https://arxiv.org/abs/2010.11929" class="external-link">Vision
Transformers (ViT)</a> paper, a Transformer-based architecture for
vision typically requires a larger dataset than usual, as well as a
longer pre-training schedule. <a href="http://imagenet.org/" class="external-link">ImageNet-1k</a> (which has about a million
images) is considered to fall under the medium-sized data regime with
respect to ViTs. This is primarily because, unlike CNNs, ViTs (or a
typical Transformer-based architecture) do not have well-informed
inductive biases (such as convolutions for processing images). This begs
the question: can’t we combine the benefits of convolution and the
benefits of Transformers in a single network architecture? These
benefits include parameter-efficiency, and self-attention to process
long-range and global dependencies (interactions between different
regions in an image).</p>
<p>In <a href="https://arxiv.org/abs/2104.05704" class="external-link">Escaping the Big Data
Paradigm with Compact Transformers</a>, Hassani et al. present an
approach for doing exactly this. They proposed the <strong>Compact
Convolutional Transformer</strong> (CCT) architecture. In this example,
we will work on an implementation of CCT and we will see how well it
performs on the CIFAR-10 dataset.</p>
<p>If you are unfamiliar with the concept of self-attention or
Transformers, you can read <a href="https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-11/r-3/312" class="external-link">this
chapter</a> from François Chollet’s book <em>Deep Learning with
Python</em>. This example uses code snippets from another example, <a href="https://keras.io/examples/vision/image_classification_with_vision_transformer/" class="external-link">Image
classification with Vision Transformer</a>.</p>
<div class="section level2">
<h2 id="imports">Imports<a class="anchor" aria-label="anchor" href="#imports"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="hyperparameters-and-constants">Hyperparameters and constants<a class="anchor" aria-label="anchor" href="#hyperparameters-and-constants"></a>
</h2>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>positional_emb <span class="op">=</span> <span class="va">True</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>conv_layers <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>projection_dim <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>num_heads <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>transformer_units <span class="op">=</span> [</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>    projection_dim,</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>    projection_dim,</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>]</span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>transformer_layers <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>stochastic_depth_rate <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a>weight_decay <span class="op">=</span> <span class="fl">0.0001</span></span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a>image_size <span class="op">=</span> <span class="dv">32</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="load-cifar-10-dataset">Load CIFAR-10 dataset<a class="anchor" aria-label="anchor" href="#load-cifar-10-dataset"></a>
</h2>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>num_classes <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>input_shape <span class="op">=</span> (<span class="dv">32</span>, <span class="dv">32</span>, <span class="dv">3</span>)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>(x_train, y_train), (x_test, y_test) <span class="op">=</span> keras.datasets.cifar10.load_data()</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>y_train <span class="op">=</span> keras.utils.to_categorical(y_train, num_classes)</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>y_test <span class="op">=</span> keras.utils.to_categorical(y_test, num_classes)</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x_train shape: </span><span class="sc">{</span>x_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> - y_train shape: </span><span class="sc">{</span>y_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x_test shape: </span><span class="sc">{</span>x_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> - y_test shape: </span><span class="sc">{</span>y_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="the-cct-tokenizer">The CCT tokenizer<a class="anchor" aria-label="anchor" href="#the-cct-tokenizer"></a>
</h2>
<p>The first recipe introduced by the CCT authors is the tokenizer for
processing the images. In a standard ViT, images are organized into
uniform <em>non-overlapping</em> patches. This eliminates the
boundary-level information present in between different patches. This is
important for a neural network to effectively exploit the locality
information. The figure below presents an illustration of how images are
organized into patches.</p>
<p><img src="https://i.imgur.com/IkBK9oY.png"></p>
<p>We already know that convolutions are quite good at exploiting
locality information. So, based on this, the authors introduce an
all-convolution mini-network to produce image patches.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="kw">class</span> CCTTokenizer(layers.Layer):</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>        kernel_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>        stride<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>        padding<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>        pooling_kernel_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>        pooling_stride<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>        num_conv_layers<span class="op">=</span>conv_layers,</span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>        num_output_channels<span class="op">=</span>[<span class="dv">64</span>, <span class="dv">128</span>],</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>        positional_emb<span class="op">=</span>positional_emb,</span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>        <span class="op">**</span>kwargs,</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a>    ):</span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a>        <span class="co"># This is our tokenizer.</span></span>
<span id="cb4-17"><a href="#cb4-17" tabindex="-1"></a>        <span class="va">self</span>.conv_model <span class="op">=</span> keras.Sequential()</span>
<span id="cb4-18"><a href="#cb4-18" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_conv_layers):</span>
<span id="cb4-19"><a href="#cb4-19" tabindex="-1"></a>            <span class="va">self</span>.conv_model.add(</span>
<span id="cb4-20"><a href="#cb4-20" tabindex="-1"></a>                layers.Conv2D(</span>
<span id="cb4-21"><a href="#cb4-21" tabindex="-1"></a>                    num_output_channels[i],</span>
<span id="cb4-22"><a href="#cb4-22" tabindex="-1"></a>                    kernel_size,</span>
<span id="cb4-23"><a href="#cb4-23" tabindex="-1"></a>                    stride,</span>
<span id="cb4-24"><a href="#cb4-24" tabindex="-1"></a>                    padding<span class="op">=</span><span class="st">"valid"</span>,</span>
<span id="cb4-25"><a href="#cb4-25" tabindex="-1"></a>                    use_bias<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb4-26"><a href="#cb4-26" tabindex="-1"></a>                    activation<span class="op">=</span><span class="st">"relu"</span>,</span>
<span id="cb4-27"><a href="#cb4-27" tabindex="-1"></a>                    kernel_initializer<span class="op">=</span><span class="st">"he_normal"</span>,</span>
<span id="cb4-28"><a href="#cb4-28" tabindex="-1"></a>                )</span>
<span id="cb4-29"><a href="#cb4-29" tabindex="-1"></a>            )</span>
<span id="cb4-30"><a href="#cb4-30" tabindex="-1"></a>            <span class="va">self</span>.conv_model.add(layers.ZeroPadding2D(padding))</span>
<span id="cb4-31"><a href="#cb4-31" tabindex="-1"></a>            <span class="va">self</span>.conv_model.add(</span>
<span id="cb4-32"><a href="#cb4-32" tabindex="-1"></a>                layers.MaxPooling2D(pooling_kernel_size, pooling_stride, <span class="st">"same"</span>)</span>
<span id="cb4-33"><a href="#cb4-33" tabindex="-1"></a>            )</span>
<span id="cb4-34"><a href="#cb4-34" tabindex="-1"></a></span>
<span id="cb4-35"><a href="#cb4-35" tabindex="-1"></a>        <span class="va">self</span>.positional_emb <span class="op">=</span> positional_emb</span>
<span id="cb4-36"><a href="#cb4-36" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, images):</span>
<span id="cb4-38"><a href="#cb4-38" tabindex="-1"></a>        outputs <span class="op">=</span> <span class="va">self</span>.conv_model(images)</span>
<span id="cb4-39"><a href="#cb4-39" tabindex="-1"></a>        <span class="co"># After passing the images through our mini-network the spatial dimensions</span></span>
<span id="cb4-40"><a href="#cb4-40" tabindex="-1"></a>        <span class="co"># are flattened to form sequences.</span></span>
<span id="cb4-41"><a href="#cb4-41" tabindex="-1"></a>        reshaped <span class="op">=</span> keras.ops.reshape(</span>
<span id="cb4-42"><a href="#cb4-42" tabindex="-1"></a>            outputs,</span>
<span id="cb4-43"><a href="#cb4-43" tabindex="-1"></a>            (</span>
<span id="cb4-44"><a href="#cb4-44" tabindex="-1"></a>                <span class="op">-</span><span class="dv">1</span>,</span>
<span id="cb4-45"><a href="#cb4-45" tabindex="-1"></a>                keras.ops.shape(outputs)[<span class="dv">1</span>] <span class="op">*</span> keras.ops.shape(outputs)[<span class="dv">2</span>],</span>
<span id="cb4-46"><a href="#cb4-46" tabindex="-1"></a>                keras.ops.shape(outputs)[<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb4-47"><a href="#cb4-47" tabindex="-1"></a>            ),</span>
<span id="cb4-48"><a href="#cb4-48" tabindex="-1"></a>        )</span>
<span id="cb4-49"><a href="#cb4-49" tabindex="-1"></a>        <span class="cf">return</span> reshaped</span></code></pre></div>
<p>Positional embeddings are optional in CCT. If we want to use them, we
can use the Layer defined below.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="kw">class</span> PositionEmbedding(keras.layers.Layer):</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>        sequence_length,</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>        initializer<span class="op">=</span><span class="st">"glorot_uniform"</span>,</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>        <span class="op">**</span>kwargs,</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>    ):</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>        <span class="cf">if</span> sequence_length <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>                <span class="st">"`sequence_length` must be an Integer, received `None`."</span></span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>            )</span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>        <span class="va">self</span>.sequence_length <span class="op">=</span> <span class="bu">int</span>(sequence_length)</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>        <span class="va">self</span>.initializer <span class="op">=</span> keras.initializers.get(initializer)</span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>    <span class="kw">def</span> get_config(<span class="va">self</span>):</span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>        config <span class="op">=</span> <span class="bu">super</span>().get_config()</span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a>        config.update(</span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a>            {</span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a>                <span class="st">"sequence_length"</span>: <span class="va">self</span>.sequence_length,</span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a>                <span class="st">"initializer"</span>: keras.initializers.serialize(<span class="va">self</span>.initializer),</span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a>            }</span>
<span id="cb5-23"><a href="#cb5-23" tabindex="-1"></a>        )</span>
<span id="cb5-24"><a href="#cb5-24" tabindex="-1"></a>        <span class="cf">return</span> config</span>
<span id="cb5-25"><a href="#cb5-25" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb5-27"><a href="#cb5-27" tabindex="-1"></a>        feature_size <span class="op">=</span> input_shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb5-28"><a href="#cb5-28" tabindex="-1"></a>        <span class="va">self</span>.position_embeddings <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb5-29"><a href="#cb5-29" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"embeddings"</span>,</span>
<span id="cb5-30"><a href="#cb5-30" tabindex="-1"></a>            shape<span class="op">=</span>[<span class="va">self</span>.sequence_length, feature_size],</span>
<span id="cb5-31"><a href="#cb5-31" tabindex="-1"></a>            initializer<span class="op">=</span><span class="va">self</span>.initializer,</span>
<span id="cb5-32"><a href="#cb5-32" tabindex="-1"></a>            trainable<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb5-33"><a href="#cb5-33" tabindex="-1"></a>        )</span>
<span id="cb5-34"><a href="#cb5-34" tabindex="-1"></a></span>
<span id="cb5-35"><a href="#cb5-35" tabindex="-1"></a>        <span class="bu">super</span>().build(input_shape)</span>
<span id="cb5-36"><a href="#cb5-36" tabindex="-1"></a></span>
<span id="cb5-37"><a href="#cb5-37" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs, start_index<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb5-38"><a href="#cb5-38" tabindex="-1"></a>        shape <span class="op">=</span> keras.ops.shape(inputs)</span>
<span id="cb5-39"><a href="#cb5-39" tabindex="-1"></a>        feature_length <span class="op">=</span> shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb5-40"><a href="#cb5-40" tabindex="-1"></a>        sequence_length <span class="op">=</span> shape[<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb5-41"><a href="#cb5-41" tabindex="-1"></a>        <span class="co"># trim to match the length of the input sequence, which might be less</span></span>
<span id="cb5-42"><a href="#cb5-42" tabindex="-1"></a>        <span class="co"># than the sequence_length of the layer.</span></span>
<span id="cb5-43"><a href="#cb5-43" tabindex="-1"></a>        position_embeddings <span class="op">=</span> keras.ops.convert_to_tensor(</span>
<span id="cb5-44"><a href="#cb5-44" tabindex="-1"></a>            <span class="va">self</span>.position_embeddings</span>
<span id="cb5-45"><a href="#cb5-45" tabindex="-1"></a>        )</span>
<span id="cb5-46"><a href="#cb5-46" tabindex="-1"></a>        position_embeddings <span class="op">=</span> keras.ops.<span class="bu">slice</span>(</span>
<span id="cb5-47"><a href="#cb5-47" tabindex="-1"></a>            position_embeddings,</span>
<span id="cb5-48"><a href="#cb5-48" tabindex="-1"></a>            (start_index, <span class="dv">0</span>),</span>
<span id="cb5-49"><a href="#cb5-49" tabindex="-1"></a>            (sequence_length, feature_length),</span>
<span id="cb5-50"><a href="#cb5-50" tabindex="-1"></a>        )</span>
<span id="cb5-51"><a href="#cb5-51" tabindex="-1"></a>        <span class="cf">return</span> keras.ops.broadcast_to(position_embeddings, shape)</span>
<span id="cb5-52"><a href="#cb5-52" tabindex="-1"></a></span>
<span id="cb5-53"><a href="#cb5-53" tabindex="-1"></a>    <span class="kw">def</span> compute_output_shape(<span class="va">self</span>, input_shape):</span>
<span id="cb5-54"><a href="#cb5-54" tabindex="-1"></a>        <span class="cf">return</span> input_shape</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="sequence-pooling">Sequence Pooling<a class="anchor" aria-label="anchor" href="#sequence-pooling"></a>
</h2>
<p>Another recipe introduced in CCT is attention pooling or sequence
pooling. In ViT, only the feature map corresponding to the class token
is pooled and is then used for the subsequent classification task (or
any other downstream task).</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="kw">class</span> SequencePooling(layers.Layer):</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> layers.Dense(<span class="dv">1</span>)</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, x):</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>        attention_weights <span class="op">=</span> keras.ops.softmax(<span class="va">self</span>.attention(x), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>        attention_weights <span class="op">=</span> keras.ops.transpose(</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>            attention_weights, axes<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>        )</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>        weighted_representation <span class="op">=</span> keras.ops.matmul(attention_weights, x)</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>        <span class="cf">return</span> keras.ops.squeeze(weighted_representation, <span class="op">-</span><span class="dv">2</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="stochastic-depth-for-regularization">Stochastic depth for regularization<a class="anchor" aria-label="anchor" href="#stochastic-depth-for-regularization"></a>
</h2>
<p><a href="https://arxiv.org/abs/1603.09382" class="external-link">Stochastic depth</a> is a
regularization technique that randomly drops a set of layers. During
inference, the layers are kept as they are. It is very much similar to
<a href="https://jmlr.org/papers/v15/srivastava14a.html" class="external-link">Dropout</a> but
only that it operates on a block of layers rather than individual nodes
present inside a layer. In CCT, stochastic depth is used just before the
residual blocks of a Transformers encoder.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="co"># Referred from: github.com:rwightman/pytorch-image-models.</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="kw">class</span> StochasticDepth(layers.Layer):</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, drop_prop, <span class="op">**</span>kwargs):</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>        <span class="va">self</span>.drop_prob <span class="op">=</span> drop_prop</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, x, training<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>        <span class="cf">if</span> training:</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>            keep_prob <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.drop_prob</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>            shape <span class="op">=</span> (keras.ops.shape(x)[<span class="dv">0</span>],) <span class="op">+</span> (<span class="dv">1</span>,) <span class="op">*</span> (<span class="bu">len</span>(x.shape) <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>            random_tensor <span class="op">=</span> keep_prob <span class="op">+</span> keras.random.uniform(shape, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>            random_tensor <span class="op">=</span> keras.ops.floor(random_tensor)</span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a>            <span class="cf">return</span> (x <span class="op">/</span> keep_prob) <span class="op">*</span> random_tensor</span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="mlp-for-the-transformers-encoder">MLP for the Transformers encoder<a class="anchor" aria-label="anchor" href="#mlp-for-the-transformers-encoder"></a>
</h2>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="kw">def</span> mlp(x, hidden_units, dropout_rate):</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>    <span class="cf">for</span> units <span class="kw">in</span> hidden_units:</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>        x <span class="op">=</span> layers.Dense(units, activation<span class="op">=</span>keras.ops.gelu)(x)</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>        x <span class="op">=</span> layers.Dropout(dropout_rate)(x)</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>    <span class="cf">return</span> x</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="data-augmentation">Data augmentation<a class="anchor" aria-label="anchor" href="#data-augmentation"></a>
</h2>
<p>In the <a href="https://arxiv.org/abs/2104.05704" class="external-link">original paper</a>,
the authors use <a href="https://arxiv.org/abs/1805.09501" class="external-link">AutoAugment</a> to induce
stronger regularization. For this example, we will be using the standard
geometric augmentations like random cropping and flipping.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="co"># Note the rescaling layer. These layers have pre-defined inference behavior.</span></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>data_augmentation <span class="op">=</span> keras.Sequential(</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>    [</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>        layers.Rescaling(scale<span class="op">=</span><span class="fl">1.0</span> <span class="op">/</span> <span class="dv">255</span>),</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>        layers.RandomCrop(image_size, image_size),</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>        layers.RandomFlip(<span class="st">"horizontal"</span>),</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>    ],</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"data_augmentation"</span>,</span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="the-final-cct-model">The final CCT model<a class="anchor" aria-label="anchor" href="#the-final-cct-model"></a>
</h2>
<p>In CCT, outputs from the Transformers encoder are weighted and then
passed on to the final task-specific layer (in this example, we do
classification).</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="kw">def</span> create_cct_model(</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>    image_size<span class="op">=</span>image_size,</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>    input_shape<span class="op">=</span>input_shape,</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>    num_heads<span class="op">=</span>num_heads,</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>    projection_dim<span class="op">=</span>projection_dim,</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>    transformer_units<span class="op">=</span>transformer_units,</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>):</span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>    inputs <span class="op">=</span> layers.Input(input_shape)</span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a>    <span class="co"># Augment data.</span></span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a>    augmented <span class="op">=</span> data_augmentation(inputs)</span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a>    <span class="co"># Encode patches.</span></span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a>    cct_tokenizer <span class="op">=</span> CCTTokenizer()</span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a>    encoded_patches <span class="op">=</span> cct_tokenizer(augmented)</span>
<span id="cb10-16"><a href="#cb10-16" tabindex="-1"></a></span>
<span id="cb10-17"><a href="#cb10-17" tabindex="-1"></a>    <span class="co"># Apply positional embedding.</span></span>
<span id="cb10-18"><a href="#cb10-18" tabindex="-1"></a>    <span class="cf">if</span> positional_emb:</span>
<span id="cb10-19"><a href="#cb10-19" tabindex="-1"></a>        sequence_length <span class="op">=</span> encoded_patches.shape[<span class="dv">1</span>]</span>
<span id="cb10-20"><a href="#cb10-20" tabindex="-1"></a>        encoded_patches <span class="op">+=</span> PositionEmbedding(sequence_length<span class="op">=</span>sequence_length)(</span>
<span id="cb10-21"><a href="#cb10-21" tabindex="-1"></a>            encoded_patches</span>
<span id="cb10-22"><a href="#cb10-22" tabindex="-1"></a>        )</span>
<span id="cb10-23"><a href="#cb10-23" tabindex="-1"></a></span>
<span id="cb10-24"><a href="#cb10-24" tabindex="-1"></a>    <span class="co"># Calculate Stochastic Depth probabilities.</span></span>
<span id="cb10-25"><a href="#cb10-25" tabindex="-1"></a>    dpr <span class="op">=</span> [x <span class="cf">for</span> x <span class="kw">in</span> np.linspace(<span class="dv">0</span>, stochastic_depth_rate, transformer_layers)]</span>
<span id="cb10-26"><a href="#cb10-26" tabindex="-1"></a></span>
<span id="cb10-27"><a href="#cb10-27" tabindex="-1"></a>    <span class="co"># Create multiple layers of the Transformer block.</span></span>
<span id="cb10-28"><a href="#cb10-28" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(transformer_layers):</span>
<span id="cb10-29"><a href="#cb10-29" tabindex="-1"></a>        <span class="co"># Layer normalization 1.</span></span>
<span id="cb10-30"><a href="#cb10-30" tabindex="-1"></a>        x1 <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span><span class="fl">1e-5</span>)(encoded_patches)</span>
<span id="cb10-31"><a href="#cb10-31" tabindex="-1"></a></span>
<span id="cb10-32"><a href="#cb10-32" tabindex="-1"></a>        <span class="co"># Create a multi-head attention layer.</span></span>
<span id="cb10-33"><a href="#cb10-33" tabindex="-1"></a>        attention_output <span class="op">=</span> layers.MultiHeadAttention(</span>
<span id="cb10-34"><a href="#cb10-34" tabindex="-1"></a>            num_heads<span class="op">=</span>num_heads, key_dim<span class="op">=</span>projection_dim, dropout<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb10-35"><a href="#cb10-35" tabindex="-1"></a>        )(x1, x1)</span>
<span id="cb10-36"><a href="#cb10-36" tabindex="-1"></a></span>
<span id="cb10-37"><a href="#cb10-37" tabindex="-1"></a>        <span class="co"># Skip connection 1.</span></span>
<span id="cb10-38"><a href="#cb10-38" tabindex="-1"></a>        attention_output <span class="op">=</span> StochasticDepth(dpr[i])(attention_output)</span>
<span id="cb10-39"><a href="#cb10-39" tabindex="-1"></a>        x2 <span class="op">=</span> layers.Add()([attention_output, encoded_patches])</span>
<span id="cb10-40"><a href="#cb10-40" tabindex="-1"></a></span>
<span id="cb10-41"><a href="#cb10-41" tabindex="-1"></a>        <span class="co"># Layer normalization 2.</span></span>
<span id="cb10-42"><a href="#cb10-42" tabindex="-1"></a>        x3 <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span><span class="fl">1e-5</span>)(x2)</span>
<span id="cb10-43"><a href="#cb10-43" tabindex="-1"></a></span>
<span id="cb10-44"><a href="#cb10-44" tabindex="-1"></a>        <span class="co"># MLP.</span></span>
<span id="cb10-45"><a href="#cb10-45" tabindex="-1"></a>        x3 <span class="op">=</span> mlp(x3, hidden_units<span class="op">=</span>transformer_units, dropout_rate<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb10-46"><a href="#cb10-46" tabindex="-1"></a></span>
<span id="cb10-47"><a href="#cb10-47" tabindex="-1"></a>        <span class="co"># Skip connection 2.</span></span>
<span id="cb10-48"><a href="#cb10-48" tabindex="-1"></a>        x3 <span class="op">=</span> StochasticDepth(dpr[i])(x3)</span>
<span id="cb10-49"><a href="#cb10-49" tabindex="-1"></a>        encoded_patches <span class="op">=</span> layers.Add()([x3, x2])</span>
<span id="cb10-50"><a href="#cb10-50" tabindex="-1"></a></span>
<span id="cb10-51"><a href="#cb10-51" tabindex="-1"></a>    <span class="co"># Apply sequence pooling.</span></span>
<span id="cb10-52"><a href="#cb10-52" tabindex="-1"></a>    representation <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span><span class="fl">1e-5</span>)(encoded_patches)</span>
<span id="cb10-53"><a href="#cb10-53" tabindex="-1"></a>    weighted_representation <span class="op">=</span> SequencePooling()(representation)</span>
<span id="cb10-54"><a href="#cb10-54" tabindex="-1"></a></span>
<span id="cb10-55"><a href="#cb10-55" tabindex="-1"></a>    <span class="co"># Classify outputs.</span></span>
<span id="cb10-56"><a href="#cb10-56" tabindex="-1"></a>    logits <span class="op">=</span> layers.Dense(num_classes)(weighted_representation)</span>
<span id="cb10-57"><a href="#cb10-57" tabindex="-1"></a>    <span class="co"># Create the Keras model.</span></span>
<span id="cb10-58"><a href="#cb10-58" tabindex="-1"></a>    model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>logits)</span>
<span id="cb10-59"><a href="#cb10-59" tabindex="-1"></a>    <span class="cf">return</span> model</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="model-training-and-evaluation">Model training and evaluation<a class="anchor" aria-label="anchor" href="#model-training-and-evaluation"></a>
</h2>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="kw">def</span> run_experiment(model):</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>    optimizer <span class="op">=</span> keras.optimizers.AdamW(learning_rate<span class="op">=</span><span class="fl">0.001</span>, weight_decay<span class="op">=</span><span class="fl">0.0001</span>)</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>    model.<span class="bu">compile</span>(</span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>        loss<span class="op">=</span>keras.losses.CategoricalCrossentropy(</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>            from_logits<span class="op">=</span><span class="va">True</span>, label_smoothing<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a>        ),</span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a>        metrics<span class="op">=</span>[</span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a>            keras.metrics.CategoricalAccuracy(name<span class="op">=</span><span class="st">"accuracy"</span>),</span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a>            keras.metrics.TopKCategoricalAccuracy(<span class="dv">5</span>, name<span class="op">=</span><span class="st">"top-5-accuracy"</span>),</span>
<span id="cb11-12"><a href="#cb11-12" tabindex="-1"></a>        ],</span>
<span id="cb11-13"><a href="#cb11-13" tabindex="-1"></a>    )</span>
<span id="cb11-14"><a href="#cb11-14" tabindex="-1"></a></span>
<span id="cb11-15"><a href="#cb11-15" tabindex="-1"></a>    checkpoint_filepath <span class="op">=</span> <span class="st">"/tmp/checkpoint.weights.h5"</span></span>
<span id="cb11-16"><a href="#cb11-16" tabindex="-1"></a>    checkpoint_callback <span class="op">=</span> keras.callbacks.ModelCheckpoint(</span>
<span id="cb11-17"><a href="#cb11-17" tabindex="-1"></a>        checkpoint_filepath,</span>
<span id="cb11-18"><a href="#cb11-18" tabindex="-1"></a>        monitor<span class="op">=</span><span class="st">"val_accuracy"</span>,</span>
<span id="cb11-19"><a href="#cb11-19" tabindex="-1"></a>        save_best_only<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb11-20"><a href="#cb11-20" tabindex="-1"></a>        save_weights_only<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb11-21"><a href="#cb11-21" tabindex="-1"></a>    )</span>
<span id="cb11-22"><a href="#cb11-22" tabindex="-1"></a></span>
<span id="cb11-23"><a href="#cb11-23" tabindex="-1"></a>    history <span class="op">=</span> model.fit(</span>
<span id="cb11-24"><a href="#cb11-24" tabindex="-1"></a>        x<span class="op">=</span>x_train,</span>
<span id="cb11-25"><a href="#cb11-25" tabindex="-1"></a>        y<span class="op">=</span>y_train,</span>
<span id="cb11-26"><a href="#cb11-26" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb11-27"><a href="#cb11-27" tabindex="-1"></a>        epochs<span class="op">=</span>num_epochs,</span>
<span id="cb11-28"><a href="#cb11-28" tabindex="-1"></a>        validation_split<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb11-29"><a href="#cb11-29" tabindex="-1"></a>        callbacks<span class="op">=</span>[checkpoint_callback],</span>
<span id="cb11-30"><a href="#cb11-30" tabindex="-1"></a>    )</span>
<span id="cb11-31"><a href="#cb11-31" tabindex="-1"></a></span>
<span id="cb11-32"><a href="#cb11-32" tabindex="-1"></a>    model.load_weights(checkpoint_filepath)</span>
<span id="cb11-33"><a href="#cb11-33" tabindex="-1"></a>    _, accuracy, top_5_accuracy <span class="op">=</span> model.evaluate(x_test, y_test)</span>
<span id="cb11-34"><a href="#cb11-34" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Test accuracy: </span><span class="sc">{</span><span class="bu">round</span>(accuracy <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">%"</span>)</span>
<span id="cb11-35"><a href="#cb11-35" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Test top 5 accuracy: </span><span class="sc">{</span><span class="bu">round</span>(top_5_accuracy <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">%"</span>)</span>
<span id="cb11-36"><a href="#cb11-36" tabindex="-1"></a></span>
<span id="cb11-37"><a href="#cb11-37" tabindex="-1"></a>    <span class="cf">return</span> history</span>
<span id="cb11-38"><a href="#cb11-38" tabindex="-1"></a></span>
<span id="cb11-39"><a href="#cb11-39" tabindex="-1"></a></span>
<span id="cb11-40"><a href="#cb11-40" tabindex="-1"></a>cct_model <span class="op">=</span> create_cct_model()</span>
<span id="cb11-41"><a href="#cb11-41" tabindex="-1"></a>history <span class="op">=</span> run_experiment(cct_model)</span></code></pre></div>
<p>Let’s now visualize the training progress of the model.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>plt.plot(history.history[<span class="st">"loss"</span>], label<span class="op">=</span><span class="st">"train_loss"</span>)</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>plt.plot(history.history[<span class="st">"val_loss"</span>], label<span class="op">=</span><span class="st">"val_loss"</span>)</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>plt.xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>plt.ylabel(<span class="st">"Loss"</span>)</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>plt.title(<span class="st">"Train and Validation Losses Over Epochs"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>plt.legend()</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a>plt.grid()</span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p>The CCT model we just trained has just <strong>0.4 million</strong>
parameters, and it gets us to ~79% top-1 accuracy within 30 epochs. The
plot above shows no signs of overfitting as well. This means we can
train this network for longer (perhaps with a bit more regularization)
and may obtain even better performance. This performance can further be
improved by additional recipes like cosine decay learning rate schedule,
other data augmentation techniques like <a href="https://arxiv.org/abs/1805.09501" class="external-link">AutoAugment</a>, <a href="https://arxiv.org/abs/1710.09412" class="external-link">MixUp</a> or <a href="https://arxiv.org/abs/1905.04899" class="external-link">Cutmix</a>. With these
modifications, the authors present 95.1% top-1 accuracy on the CIFAR-10
dataset. The authors also present a number of experiments to study how
the number of convolution blocks, Transformers layers, etc. affect the
final performance of CCTs.</p>
<p>For a comparison, a ViT model takes about <strong>4.7
million</strong> parameters and <strong>100 epochs</strong> of training
to reach a top-1 accuracy of 78.22% on the CIFAR-10 dataset. You can
refer to <a href="https://colab.research.google.com/gist/sayakpaul/1a80d9f582b044354a1a26c5cb3d69e5/image_classification_with_vision_transformer.ipynb" class="external-link">this
notebook</a> to know about the experimental setup.</p>
<p>The authors also demonstrate the performance of Compact Convolutional
Transformers on NLP tasks and they report competitive results there.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
