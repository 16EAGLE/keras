<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Adaptively generating a smaller number of tokens for Vision Transformers.">
<title>Learning to tokenize in Vision Transformers â€¢ keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../../deps/Fira_Code-0.4.7/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><meta property="og:title" content="Learning to tokenize in Vision Transformers">
<meta property="og:description" content="Adaptively generating a smaller number of tokens for Vision Transformers.">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-getting-started">Getting Started</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-getting-started">
    <a class="dropdown-item" href="../../articles/intro_to_keras_for_engineers.html">Introduction to Keras for engineers</a>
    <h6 class="dropdown-header" data-toc-skip>Tutorials</h6>
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
  </div>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Learning to tokenize in Vision Transformers</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/token_learner.Rmd" class="external-link"><code>vignettes/examples/token_learner.Rmd</code></a></small>
      <div class="d-none name"><code>token_learner.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>Vision Transformers (<a href="https://arxiv.org/abs/2010.11929" class="external-link">Dosovitskiy et al.</a>) and many
other Transformer-based architectures (<a href="https://arxiv.org/abs/2103.14030" class="external-link">Liu et al.</a>, <a href="https://arxiv.org/abs/2101.11986" class="external-link">Yuan et al.</a>, etc.) have
shown strong results in image recognition. The following provides a
brief overview of the components involved in the Vision Transformer
architecture for image classification:</p>
<ul>
<li>Extract small patches from input images.</li>
<li>Linearly project those patches.</li>
<li>Add positional embeddings to these linear projections.</li>
<li>Run these projections through a series of Transformer (<a href="https://arxiv.org/abs/1706.03762" class="external-link">Vaswani et al.</a>) blocks.</li>
<li>Finally, take the representation from the final Transformer block
and add a classification head.</li>
</ul>
<p>If we take 224x224 images and extract 16x16 patches, we get a total
of 196 patches (also called tokens) for each image. The number of
patches increases as we increase the resolution, leading to higher
memory footprint. Could we use a reduced number of patches without
having to compromise performance? Ryoo et al.Â investigate this question
in <a href="https://openreview.net/forum?id=z-l1kpDXs88" class="external-link">TokenLearner:
Adaptive Space-Time Tokenization for Videos</a>. They introduce a novel
module called <strong>TokenLearner</strong> that can help reduce the
number of patches used by a Vision Transformer (ViT) in an adaptive
manner. With TokenLearner incorporated in the standard ViT architecture,
they are able to reduce the amount of compute (measured in FLOPS) used
by the model.</p>
<p>In this example, we implement the TokenLearner module and demonstrate
its performance with a mini ViT and the CIFAR-10 dataset. We make use of
the following references:</p>
<ul>
<li><a href="https://github.com/google-research/scenic/blob/main/scenic/projects/token_learner/model.py" class="external-link">Official
TokenLearner code</a></li>
<li><a href="https://keras.io/examples/vision/image_classification_with_vision_transformer/" class="external-link">Image
Classification with ViTs on keras.io</a></li>
<li><a href="https://nips.cc/media/neurips-2021/Slides/26578.pdf" class="external-link">TokenLearner
slides from NeurIPS 2021</a></li>
</ul>
</div>
<div class="section level2">
<h2 id="imports">Imports<a class="anchor" aria-label="anchor" href="#imports"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> ops</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> data <span class="im">as</span> tf_data</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="im">from</span> datetime <span class="im">import</span> datetime</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="im">import</span> math</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="hyperparameters">Hyperparameters<a class="anchor" aria-label="anchor" href="#hyperparameters"></a>
</h2>
<p>Please feel free to change the hyperparameters and check your
results. The best way to develop intuition about the architecture is to
experiment with it.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># DATA</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>AUTO <span class="op">=</span> tf_data.AUTOTUNE</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>INPUT_SHAPE <span class="op">=</span> (<span class="dv">32</span>, <span class="dv">32</span>, <span class="dv">3</span>)</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>NUM_CLASSES <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a><span class="co"># OPTIMIZER</span></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>LEARNING_RATE <span class="op">=</span> <span class="fl">1e-3</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>WEIGHT_DECAY <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a><span class="co"># TRAINING</span></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>EPOCHS <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a><span class="co"># AUGMENTATION</span></span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a>IMAGE_SIZE <span class="op">=</span> <span class="dv">48</span>  <span class="co"># We will resize input images to this size.</span></span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a>PATCH_SIZE <span class="op">=</span> <span class="dv">6</span>  <span class="co"># Size of the patches to be extracted from the input images.</span></span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a>NUM_PATCHES <span class="op">=</span> (IMAGE_SIZE <span class="op">//</span> PATCH_SIZE) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a><span class="co"># ViT ARCHITECTURE</span></span>
<span id="cb2-20"><a href="#cb2-20" tabindex="-1"></a>LAYER_NORM_EPS <span class="op">=</span> <span class="fl">1e-6</span></span>
<span id="cb2-21"><a href="#cb2-21" tabindex="-1"></a>PROJECTION_DIM <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb2-22"><a href="#cb2-22" tabindex="-1"></a>NUM_HEADS <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb2-23"><a href="#cb2-23" tabindex="-1"></a>NUM_LAYERS <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb2-24"><a href="#cb2-24" tabindex="-1"></a>MLP_UNITS <span class="op">=</span> [</span>
<span id="cb2-25"><a href="#cb2-25" tabindex="-1"></a>    PROJECTION_DIM <span class="op">*</span> <span class="dv">2</span>,</span>
<span id="cb2-26"><a href="#cb2-26" tabindex="-1"></a>    PROJECTION_DIM,</span>
<span id="cb2-27"><a href="#cb2-27" tabindex="-1"></a>]</span>
<span id="cb2-28"><a href="#cb2-28" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" tabindex="-1"></a><span class="co"># TOKENLEARNER</span></span>
<span id="cb2-30"><a href="#cb2-30" tabindex="-1"></a>NUM_TOKENS <span class="op">=</span> <span class="dv">4</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="load-and-prepare-the-cifar-10-dataset">Load and prepare the CIFAR-10 dataset<a class="anchor" aria-label="anchor" href="#load-and-prepare-the-cifar-10-dataset"></a>
</h2>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="co"># Load the CIFAR-10 dataset.</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>(x_train, y_train), (x_test, y_test) <span class="op">=</span> keras.datasets.cifar10.load_data()</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>(x_train, y_train), (x_val, y_val) <span class="op">=</span> (</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>    (x_train[:<span class="dv">40000</span>], y_train[:<span class="dv">40000</span>]),</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>    (x_train[<span class="dv">40000</span>:], y_train[<span class="dv">40000</span>:]),</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>)</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training samples: </span><span class="sc">{</span><span class="bu">len</span>(x_train)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Validation samples: </span><span class="sc">{</span><span class="bu">len</span>(x_val)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Testing samples: </span><span class="sc">{</span><span class="bu">len</span>(x_test)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a><span class="co"># Convert to tf.data.Dataset objects.</span></span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>train_ds <span class="op">=</span> tf_data.Dataset.from_tensor_slices((x_train, y_train))</span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a>train_ds <span class="op">=</span> train_ds.shuffle(BATCH_SIZE <span class="op">*</span> <span class="dv">100</span>).batch(BATCH_SIZE).prefetch(AUTO)</span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a>val_ds <span class="op">=</span> tf_data.Dataset.from_tensor_slices((x_val, y_val))</span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a>val_ds <span class="op">=</span> val_ds.batch(BATCH_SIZE).prefetch(AUTO)</span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a>test_ds <span class="op">=</span> tf_data.Dataset.from_tensor_slices((x_test, y_test))</span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a>test_ds <span class="op">=</span> test_ds.batch(BATCH_SIZE).prefetch(AUTO)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="data-augmentation">Data augmentation<a class="anchor" aria-label="anchor" href="#data-augmentation"></a>
</h2>
<p>The augmentation pipeline consists of:</p>
<ul>
<li>Rescaling</li>
<li>Resizing</li>
<li>Random cropping (fixed-sized or random sized)</li>
<li>Random horizontal flipping</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>data_augmentation <span class="op">=</span> keras.Sequential(</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>    [</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>        layers.Rescaling(<span class="dv">1</span> <span class="op">/</span> <span class="fl">255.0</span>),</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>        layers.Resizing(INPUT_SHAPE[<span class="dv">0</span>] <span class="op">+</span> <span class="dv">20</span>, INPUT_SHAPE[<span class="dv">0</span>] <span class="op">+</span> <span class="dv">20</span>),</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>        layers.RandomCrop(IMAGE_SIZE, IMAGE_SIZE),</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>        layers.RandomFlip(<span class="st">"horizontal"</span>),</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>    ],</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"data_augmentation"</span>,</span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>)</span></code></pre></div>
<p>Note that image data augmentation layers do not apply data
transformations at inference time. This means that when these layers are
called with <code>training=False</code> they behave differently. Refer
<a href="https://keras.io/api/layers/preprocessing_layers/image_augmentation/" class="external-link">to
the documentation</a> for more details.</p>
</div>
<div class="section level2">
<h2 id="positional-embedding-module">Positional embedding module<a class="anchor" aria-label="anchor" href="#positional-embedding-module"></a>
</h2>
<p>A <a href="https://arxiv.org/abs/1706.03762" class="external-link">Transformer</a>
architecture consists of <strong>multi-head self attention</strong>
layers and <strong>fully-connected feed forward</strong> networks (MLP)
as the main components. Both these components are <em>permutation
invariant</em>: theyâ€™re not aware of feature order.</p>
<p>To overcome this problem we inject tokens with positional
information. The <code>position_embedding</code> function adds this
positional information to the linearly projected tokens.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="kw">class</span> PatchEncoder(layers.Layer):</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_patches, projection_dim):</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>        <span class="va">self</span>.num_patches <span class="op">=</span> num_patches</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>        <span class="va">self</span>.position_embedding <span class="op">=</span> layers.Embedding(</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>            input_dim<span class="op">=</span>num_patches, output_dim<span class="op">=</span>projection_dim</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>        )</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, patch):</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>        positions <span class="op">=</span> ops.expand_dims(</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>            ops.arange(start<span class="op">=</span><span class="dv">0</span>, stop<span class="op">=</span><span class="va">self</span>.num_patches, step<span class="op">=</span><span class="dv">1</span>), axis<span class="op">=</span><span class="dv">0</span></span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>        )</span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>        encoded <span class="op">=</span> patch <span class="op">+</span> <span class="va">self</span>.position_embedding(positions)</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>        <span class="cf">return</span> encoded</span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>    <span class="kw">def</span> get_config(<span class="va">self</span>):</span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>        config <span class="op">=</span> <span class="bu">super</span>().get_config()</span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a>        config.update({<span class="st">"num_patches"</span>: <span class="va">self</span>.num_patches})</span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a>        <span class="cf">return</span> config</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="mlp-block-for-transformer">MLP block for Transformer<a class="anchor" aria-label="anchor" href="#mlp-block-for-transformer"></a>
</h2>
<p>This serves as the Fully Connected Feed Forward block for our
Transformer.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="kw">def</span> mlp(x, dropout_rate, hidden_units):</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>    <span class="co"># Iterate over the hidden units and</span></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>    <span class="co"># add Dense =&gt; Dropout.</span></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>    <span class="cf">for</span> units <span class="kw">in</span> hidden_units:</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>        x <span class="op">=</span> layers.Dense(units, activation<span class="op">=</span>ops.gelu)(x)</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>        x <span class="op">=</span> layers.Dropout(dropout_rate)(x)</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>    <span class="cf">return</span> x</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="tokenlearner-module">TokenLearner module<a class="anchor" aria-label="anchor" href="#tokenlearner-module"></a>
</h2>
<p>The following figure presents a pictorial overview of the module (<a href="https://ai.googleblog.com/2021/12/improving-vision-transformer-efficiency.html" class="external-link">source</a>).</p>
<div class="float">
<img src="https://blogger.googleusercontent.com/img/a/AVvXsEiylT3_nmd9-tzTnz3g3Vb4eTn-L5sOwtGJOad6t2we7FsjXSpbLDpuPrlInAhtE5hGCA_PfYTJtrIOKfLYLYGcYXVh1Ksfh_C1ZC-C8gw6GKtvrQesKoMrEA_LU_Gd5srl5-3iZDgJc1iyCELoXtfuIXKJ2ADDHOBaUjhU8lXTVdr2E7bCVaFgVHHkmA=w640-h208" alt="TokenLearner module GIF"><div class="figcaption">TokenLearner module GIF</div>
</div>
<p>The TokenLearner module takes as input an image-shaped tensor. It
then passes it through multiple single-channel convolutional layers
extracting different spatial attention maps focusing on different parts
of the input. These attention maps are then element-wise multiplied to
the input and result is aggregated with pooling. This pooled output can
be trated as a summary of the input and has much lesser number of
patches (8, for example) than the original one (196, for example).</p>
<p>Using multiple convolution layers helps with expressivity. Imposing a
form of spatial attention helps retain relevant information from the
inputs. Both of these components are crucial to make TokenLearner work,
especially when we are significantly reducing the number of patches.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="kw">def</span> token_learner(inputs, number_of_tokens<span class="op">=</span>NUM_TOKENS):</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>    <span class="co"># Layer normalize the inputs.</span></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>    x <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span>LAYER_NORM_EPS)(</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>        inputs</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>    )  <span class="co"># (B, H, W, C)</span></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>    <span class="co"># Applying Conv2D =&gt; Reshape =&gt; Permute</span></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>    <span class="co"># The reshape and permute is done to help with the next steps of</span></span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>    <span class="co"># multiplication and Global Average Pooling.</span></span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>    attention_maps <span class="op">=</span> keras.Sequential(</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>        [</span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>            <span class="co"># 3 layers of conv with gelu activation as suggested</span></span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a>            <span class="co"># in the paper.</span></span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a>            layers.Conv2D(</span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a>                filters<span class="op">=</span>number_of_tokens,</span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a>                kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>),</span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a>                activation<span class="op">=</span>ops.gelu,</span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a>                padding<span class="op">=</span><span class="st">"same"</span>,</span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a>                use_bias<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb7-20"><a href="#cb7-20" tabindex="-1"></a>            ),</span>
<span id="cb7-21"><a href="#cb7-21" tabindex="-1"></a>            layers.Conv2D(</span>
<span id="cb7-22"><a href="#cb7-22" tabindex="-1"></a>                filters<span class="op">=</span>number_of_tokens,</span>
<span id="cb7-23"><a href="#cb7-23" tabindex="-1"></a>                kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>),</span>
<span id="cb7-24"><a href="#cb7-24" tabindex="-1"></a>                activation<span class="op">=</span>ops.gelu,</span>
<span id="cb7-25"><a href="#cb7-25" tabindex="-1"></a>                padding<span class="op">=</span><span class="st">"same"</span>,</span>
<span id="cb7-26"><a href="#cb7-26" tabindex="-1"></a>                use_bias<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb7-27"><a href="#cb7-27" tabindex="-1"></a>            ),</span>
<span id="cb7-28"><a href="#cb7-28" tabindex="-1"></a>            layers.Conv2D(</span>
<span id="cb7-29"><a href="#cb7-29" tabindex="-1"></a>                filters<span class="op">=</span>number_of_tokens,</span>
<span id="cb7-30"><a href="#cb7-30" tabindex="-1"></a>                kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>),</span>
<span id="cb7-31"><a href="#cb7-31" tabindex="-1"></a>                activation<span class="op">=</span>ops.gelu,</span>
<span id="cb7-32"><a href="#cb7-32" tabindex="-1"></a>                padding<span class="op">=</span><span class="st">"same"</span>,</span>
<span id="cb7-33"><a href="#cb7-33" tabindex="-1"></a>                use_bias<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb7-34"><a href="#cb7-34" tabindex="-1"></a>            ),</span>
<span id="cb7-35"><a href="#cb7-35" tabindex="-1"></a>            <span class="co"># This conv layer will generate the attention maps</span></span>
<span id="cb7-36"><a href="#cb7-36" tabindex="-1"></a>            layers.Conv2D(</span>
<span id="cb7-37"><a href="#cb7-37" tabindex="-1"></a>                filters<span class="op">=</span>number_of_tokens,</span>
<span id="cb7-38"><a href="#cb7-38" tabindex="-1"></a>                kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>),</span>
<span id="cb7-39"><a href="#cb7-39" tabindex="-1"></a>                activation<span class="op">=</span><span class="st">"sigmoid"</span>,  <span class="co"># Note sigmoid for [0, 1] output</span></span>
<span id="cb7-40"><a href="#cb7-40" tabindex="-1"></a>                padding<span class="op">=</span><span class="st">"same"</span>,</span>
<span id="cb7-41"><a href="#cb7-41" tabindex="-1"></a>                use_bias<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb7-42"><a href="#cb7-42" tabindex="-1"></a>            ),</span>
<span id="cb7-43"><a href="#cb7-43" tabindex="-1"></a>            <span class="co"># Reshape and Permute</span></span>
<span id="cb7-44"><a href="#cb7-44" tabindex="-1"></a>            layers.Reshape((<span class="op">-</span><span class="dv">1</span>, number_of_tokens)),  <span class="co"># (B, H*W, num_of_tokens)</span></span>
<span id="cb7-45"><a href="#cb7-45" tabindex="-1"></a>            layers.Permute((<span class="dv">2</span>, <span class="dv">1</span>)),</span>
<span id="cb7-46"><a href="#cb7-46" tabindex="-1"></a>        ]</span>
<span id="cb7-47"><a href="#cb7-47" tabindex="-1"></a>    )(</span>
<span id="cb7-48"><a href="#cb7-48" tabindex="-1"></a>        x</span>
<span id="cb7-49"><a href="#cb7-49" tabindex="-1"></a>    )  <span class="co"># (B, num_of_tokens, H*W)</span></span>
<span id="cb7-50"><a href="#cb7-50" tabindex="-1"></a></span>
<span id="cb7-51"><a href="#cb7-51" tabindex="-1"></a>    <span class="co"># Reshape the input to align it with the output of the conv block.</span></span>
<span id="cb7-52"><a href="#cb7-52" tabindex="-1"></a>    num_filters <span class="op">=</span> inputs.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb7-53"><a href="#cb7-53" tabindex="-1"></a>    inputs <span class="op">=</span> layers.Reshape((<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, num_filters))(</span>
<span id="cb7-54"><a href="#cb7-54" tabindex="-1"></a>        inputs</span>
<span id="cb7-55"><a href="#cb7-55" tabindex="-1"></a>    )  <span class="co"># inputs == (B, 1, H*W, C)</span></span>
<span id="cb7-56"><a href="#cb7-56" tabindex="-1"></a></span>
<span id="cb7-57"><a href="#cb7-57" tabindex="-1"></a>    <span class="co"># Element-Wise multiplication of the attention maps and the inputs</span></span>
<span id="cb7-58"><a href="#cb7-58" tabindex="-1"></a>    attended_inputs <span class="op">=</span> (</span>
<span id="cb7-59"><a href="#cb7-59" tabindex="-1"></a>        ops.expand_dims(attention_maps, axis<span class="op">=-</span><span class="dv">1</span>) <span class="op">*</span> inputs</span>
<span id="cb7-60"><a href="#cb7-60" tabindex="-1"></a>    )  <span class="co"># (B, num_tokens, H*W, C)</span></span>
<span id="cb7-61"><a href="#cb7-61" tabindex="-1"></a></span>
<span id="cb7-62"><a href="#cb7-62" tabindex="-1"></a>    <span class="co"># Global average pooling the element wise multiplication result.</span></span>
<span id="cb7-63"><a href="#cb7-63" tabindex="-1"></a>    outputs <span class="op">=</span> ops.mean(attended_inputs, axis<span class="op">=</span><span class="dv">2</span>)  <span class="co"># (B, num_tokens, C)</span></span>
<span id="cb7-64"><a href="#cb7-64" tabindex="-1"></a>    <span class="cf">return</span> outputs</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="transformer-block">Transformer block<a class="anchor" aria-label="anchor" href="#transformer-block"></a>
</h2>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="kw">def</span> transformer(encoded_patches):</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>    <span class="co"># Layer normalization 1.</span></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>    x1 <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span>LAYER_NORM_EPS)(encoded_patches)</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>    <span class="co"># Multi Head Self Attention layer 1.</span></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>    attention_output <span class="op">=</span> layers.MultiHeadAttention(</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>        num_heads<span class="op">=</span>NUM_HEADS, key_dim<span class="op">=</span>PROJECTION_DIM, dropout<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>    )(x1, x1)</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>    <span class="co"># Skip connection 1.</span></span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>    x2 <span class="op">=</span> layers.Add()([attention_output, encoded_patches])</span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>    <span class="co"># Layer normalization 2.</span></span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>    x3 <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span>LAYER_NORM_EPS)(x2)</span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a>    <span class="co"># MLP layer 1.</span></span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a>    x4 <span class="op">=</span> mlp(x3, hidden_units<span class="op">=</span>MLP_UNITS, dropout_rate<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb8-18"><a href="#cb8-18" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" tabindex="-1"></a>    <span class="co"># Skip connection 2.</span></span>
<span id="cb8-20"><a href="#cb8-20" tabindex="-1"></a>    encoded_patches <span class="op">=</span> layers.Add()([x4, x2])</span>
<span id="cb8-21"><a href="#cb8-21" tabindex="-1"></a>    <span class="cf">return</span> encoded_patches</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="vit-model-with-the-tokenlearner-module">ViT model with the TokenLearner module<a class="anchor" aria-label="anchor" href="#vit-model-with-the-tokenlearner-module"></a>
</h2>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="kw">def</span> create_vit_classifier(</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>    use_token_learner<span class="op">=</span><span class="va">True</span>, token_learner_units<span class="op">=</span>NUM_TOKENS</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>):</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>    inputs <span class="op">=</span> layers.Input(shape<span class="op">=</span>INPUT_SHAPE)  <span class="co"># (B, H, W, C)</span></span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>    <span class="co"># Augment data.</span></span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>    augmented <span class="op">=</span> data_augmentation(inputs)</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a>    <span class="co"># Create patches and project the pathces.</span></span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a>    projected_patches <span class="op">=</span> layers.Conv2D(</span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a>        filters<span class="op">=</span>PROJECTION_DIM,</span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a>        kernel_size<span class="op">=</span>(PATCH_SIZE, PATCH_SIZE),</span>
<span id="cb9-13"><a href="#cb9-13" tabindex="-1"></a>        strides<span class="op">=</span>(PATCH_SIZE, PATCH_SIZE),</span>
<span id="cb9-14"><a href="#cb9-14" tabindex="-1"></a>        padding<span class="op">=</span><span class="st">"VALID"</span>,</span>
<span id="cb9-15"><a href="#cb9-15" tabindex="-1"></a>    )(augmented)</span>
<span id="cb9-16"><a href="#cb9-16" tabindex="-1"></a>    _, h, w, c <span class="op">=</span> projected_patches.shape</span>
<span id="cb9-17"><a href="#cb9-17" tabindex="-1"></a>    projected_patches <span class="op">=</span> layers.Reshape((h <span class="op">*</span> w, c))(</span>
<span id="cb9-18"><a href="#cb9-18" tabindex="-1"></a>        projected_patches</span>
<span id="cb9-19"><a href="#cb9-19" tabindex="-1"></a>    )  <span class="co"># (B, number_patches, projection_dim)</span></span>
<span id="cb9-20"><a href="#cb9-20" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" tabindex="-1"></a>    <span class="co"># Add positional embeddings to the projected patches.</span></span>
<span id="cb9-22"><a href="#cb9-22" tabindex="-1"></a>    encoded_patches <span class="op">=</span> PatchEncoder(</span>
<span id="cb9-23"><a href="#cb9-23" tabindex="-1"></a>        num_patches<span class="op">=</span>NUM_PATCHES, projection_dim<span class="op">=</span>PROJECTION_DIM</span>
<span id="cb9-24"><a href="#cb9-24" tabindex="-1"></a>    )(</span>
<span id="cb9-25"><a href="#cb9-25" tabindex="-1"></a>        projected_patches</span>
<span id="cb9-26"><a href="#cb9-26" tabindex="-1"></a>    )  <span class="co"># (B, number_patches, projection_dim)</span></span>
<span id="cb9-27"><a href="#cb9-27" tabindex="-1"></a>    encoded_patches <span class="op">=</span> layers.Dropout(<span class="fl">0.1</span>)(encoded_patches)</span>
<span id="cb9-28"><a href="#cb9-28" tabindex="-1"></a></span>
<span id="cb9-29"><a href="#cb9-29" tabindex="-1"></a>    <span class="co"># Iterate over the number of layers and stack up blocks of</span></span>
<span id="cb9-30"><a href="#cb9-30" tabindex="-1"></a>    <span class="co"># Transformer.</span></span>
<span id="cb9-31"><a href="#cb9-31" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(NUM_LAYERS):</span>
<span id="cb9-32"><a href="#cb9-32" tabindex="-1"></a>        <span class="co"># Add a Transformer block.</span></span>
<span id="cb9-33"><a href="#cb9-33" tabindex="-1"></a>        encoded_patches <span class="op">=</span> transformer(encoded_patches)</span>
<span id="cb9-34"><a href="#cb9-34" tabindex="-1"></a></span>
<span id="cb9-35"><a href="#cb9-35" tabindex="-1"></a>        <span class="co"># Add TokenLearner layer in the middle of the</span></span>
<span id="cb9-36"><a href="#cb9-36" tabindex="-1"></a>        <span class="co"># architecture. The paper suggests that anywhere</span></span>
<span id="cb9-37"><a href="#cb9-37" tabindex="-1"></a>        <span class="co"># between 1/2 or 3/4 will work well.</span></span>
<span id="cb9-38"><a href="#cb9-38" tabindex="-1"></a>        <span class="cf">if</span> use_token_learner <span class="kw">and</span> i <span class="op">==</span> NUM_LAYERS <span class="op">//</span> <span class="dv">2</span>:</span>
<span id="cb9-39"><a href="#cb9-39" tabindex="-1"></a>            _, hh, c <span class="op">=</span> encoded_patches.shape</span>
<span id="cb9-40"><a href="#cb9-40" tabindex="-1"></a>            h <span class="op">=</span> <span class="bu">int</span>(math.sqrt(hh))</span>
<span id="cb9-41"><a href="#cb9-41" tabindex="-1"></a>            encoded_patches <span class="op">=</span> layers.Reshape((h, h, c))(</span>
<span id="cb9-42"><a href="#cb9-42" tabindex="-1"></a>                encoded_patches</span>
<span id="cb9-43"><a href="#cb9-43" tabindex="-1"></a>            )  <span class="co"># (B, h, h, projection_dim)</span></span>
<span id="cb9-44"><a href="#cb9-44" tabindex="-1"></a>            encoded_patches <span class="op">=</span> token_learner(</span>
<span id="cb9-45"><a href="#cb9-45" tabindex="-1"></a>                encoded_patches, token_learner_units</span>
<span id="cb9-46"><a href="#cb9-46" tabindex="-1"></a>            )  <span class="co"># (B, num_tokens, c)</span></span>
<span id="cb9-47"><a href="#cb9-47" tabindex="-1"></a></span>
<span id="cb9-48"><a href="#cb9-48" tabindex="-1"></a>    <span class="co"># Layer normalization and Global average pooling.</span></span>
<span id="cb9-49"><a href="#cb9-49" tabindex="-1"></a>    representation <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span>LAYER_NORM_EPS)(</span>
<span id="cb9-50"><a href="#cb9-50" tabindex="-1"></a>        encoded_patches</span>
<span id="cb9-51"><a href="#cb9-51" tabindex="-1"></a>    )</span>
<span id="cb9-52"><a href="#cb9-52" tabindex="-1"></a>    representation <span class="op">=</span> layers.GlobalAvgPool1D()(representation)</span>
<span id="cb9-53"><a href="#cb9-53" tabindex="-1"></a></span>
<span id="cb9-54"><a href="#cb9-54" tabindex="-1"></a>    <span class="co"># Classify outputs.</span></span>
<span id="cb9-55"><a href="#cb9-55" tabindex="-1"></a>    outputs <span class="op">=</span> layers.Dense(NUM_CLASSES, activation<span class="op">=</span><span class="st">"softmax"</span>)(representation)</span>
<span id="cb9-56"><a href="#cb9-56" tabindex="-1"></a></span>
<span id="cb9-57"><a href="#cb9-57" tabindex="-1"></a>    <span class="co"># Create the Keras model.</span></span>
<span id="cb9-58"><a href="#cb9-58" tabindex="-1"></a>    model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs)</span>
<span id="cb9-59"><a href="#cb9-59" tabindex="-1"></a>    <span class="cf">return</span> model</span></code></pre></div>
<p>As shown in the <a href="https://openreview.net/forum?id=z-l1kpDXs88" class="external-link">TokenLearner
paper</a>, it is almost always advantageous to include the TokenLearner
module in the middle of the network.</p>
</div>
<div class="section level2">
<h2 id="training-utility">Training utility<a class="anchor" aria-label="anchor" href="#training-utility"></a>
</h2>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="kw">def</span> run_experiment(model):</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>    <span class="co"># Initialize the AdamW optimizer.</span></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>    optimizer <span class="op">=</span> keras.optimizers.AdamW(</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>        learning_rate<span class="op">=</span>LEARNING_RATE, weight_decay<span class="op">=</span>WEIGHT_DECAY</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>    )</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>    <span class="co"># Compile the model with the optimizer, loss function</span></span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>    <span class="co"># and the metrics.</span></span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a>    model.<span class="bu">compile</span>(</span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a>        loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>,</span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a>        metrics<span class="op">=</span>[</span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a>            keras.metrics.SparseCategoricalAccuracy(name<span class="op">=</span><span class="st">"accuracy"</span>),</span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a>            keras.metrics.SparseTopKCategoricalAccuracy(</span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a>                <span class="dv">5</span>, name<span class="op">=</span><span class="st">"top-5-accuracy"</span></span>
<span id="cb10-16"><a href="#cb10-16" tabindex="-1"></a>            ),</span>
<span id="cb10-17"><a href="#cb10-17" tabindex="-1"></a>        ],</span>
<span id="cb10-18"><a href="#cb10-18" tabindex="-1"></a>    )</span>
<span id="cb10-19"><a href="#cb10-19" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" tabindex="-1"></a>    <span class="co"># Define callbacks</span></span>
<span id="cb10-21"><a href="#cb10-21" tabindex="-1"></a>    checkpoint_filepath <span class="op">=</span> <span class="st">"/tmp/checkpoint.weights.h5"</span></span>
<span id="cb10-22"><a href="#cb10-22" tabindex="-1"></a>    checkpoint_callback <span class="op">=</span> keras.callbacks.ModelCheckpoint(</span>
<span id="cb10-23"><a href="#cb10-23" tabindex="-1"></a>        checkpoint_filepath,</span>
<span id="cb10-24"><a href="#cb10-24" tabindex="-1"></a>        monitor<span class="op">=</span><span class="st">"val_accuracy"</span>,</span>
<span id="cb10-25"><a href="#cb10-25" tabindex="-1"></a>        save_best_only<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb10-26"><a href="#cb10-26" tabindex="-1"></a>        save_weights_only<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb10-27"><a href="#cb10-27" tabindex="-1"></a>    )</span>
<span id="cb10-28"><a href="#cb10-28" tabindex="-1"></a></span>
<span id="cb10-29"><a href="#cb10-29" tabindex="-1"></a>    <span class="co"># Train the model.</span></span>
<span id="cb10-30"><a href="#cb10-30" tabindex="-1"></a>    _ <span class="op">=</span> model.fit(</span>
<span id="cb10-31"><a href="#cb10-31" tabindex="-1"></a>        train_ds,</span>
<span id="cb10-32"><a href="#cb10-32" tabindex="-1"></a>        epochs<span class="op">=</span>EPOCHS,</span>
<span id="cb10-33"><a href="#cb10-33" tabindex="-1"></a>        validation_data<span class="op">=</span>val_ds,</span>
<span id="cb10-34"><a href="#cb10-34" tabindex="-1"></a>        callbacks<span class="op">=</span>[checkpoint_callback],</span>
<span id="cb10-35"><a href="#cb10-35" tabindex="-1"></a>    )</span>
<span id="cb10-36"><a href="#cb10-36" tabindex="-1"></a></span>
<span id="cb10-37"><a href="#cb10-37" tabindex="-1"></a>    model.load_weights(checkpoint_filepath)</span>
<span id="cb10-38"><a href="#cb10-38" tabindex="-1"></a>    _, accuracy, top_5_accuracy <span class="op">=</span> model.evaluate(test_ds)</span>
<span id="cb10-39"><a href="#cb10-39" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Test accuracy: </span><span class="sc">{</span><span class="bu">round</span>(accuracy <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">%"</span>)</span>
<span id="cb10-40"><a href="#cb10-40" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Test top 5 accuracy: </span><span class="sc">{</span><span class="bu">round</span>(top_5_accuracy <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">%"</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="train-and-evaluate-a-vit-with-tokenlearner">Train and evaluate a ViT with TokenLearner<a class="anchor" aria-label="anchor" href="#train-and-evaluate-a-vit-with-tokenlearner"></a>
</h2>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>vit_token_learner <span class="op">=</span> create_vit_classifier()</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>run_experiment(vit_token_learner)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="results">Results<a class="anchor" aria-label="anchor" href="#results"></a>
</h2>
<p>We experimented with and without the TokenLearner inside the mini ViT
we implemented (with the same hyperparameters presented in this
example). Here are our results:</p>
<table class="table">
<colgroup>
<col width="20%">
<col width="20%">
<col width="20%">
<col width="20%">
<col width="20%">
</colgroup>
<thead><tr class="header">
<th align="center"><strong>TokenLearner</strong></th>
<th align="center"><strong># tokens in<br> TokenLearner</strong></th>
<th align="center"><strong>Top-1 Acc<br>(Averaged across 5
runs)</strong></th>
<th align="center"><strong>GFLOPs</strong></th>
<th align="center"><strong>TensorBoard</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="center">N</td>
<td align="center">-</td>
<td align="center">56.112%</td>
<td align="center">0.0184</td>
<td align="center"><a href="https://tensorboard.dev/experiment/vkCwM49dQZ2RiK0ZT4mj7w/" class="external-link">Link</a></td>
</tr>
<tr class="even">
<td align="center">Y</td>
<td align="center">8</td>
<td align="center"><strong>56.55%</strong></td>
<td align="center"><strong>0.0153</strong></td>
<td align="center"><a href="https://tensorboard.dev/experiment/vkCwM49dQZ2RiK0ZT4mj7w/" class="external-link">Link</a></td>
</tr>
<tr class="odd">
<td align="center">N</td>
<td align="center">-</td>
<td align="center">56.37%</td>
<td align="center">0.0184</td>
<td align="center"><a href="https://tensorboard.dev/experiment/hdyJ4wznQROwqZTgbtmztQ/" class="external-link">Link</a></td>
</tr>
<tr class="even">
<td align="center">Y</td>
<td align="center">4</td>
<td align="center"><strong>56.4980%</strong></td>
<td align="center"><strong>0.0147</strong></td>
<td align="center"><a href="https://tensorboard.dev/experiment/hdyJ4wznQROwqZTgbtmztQ/" class="external-link">Link</a></td>
</tr>
<tr class="odd">
<td align="center">N</td>
<td align="center">- (# Transformer layers: 8)</td>
<td align="center">55.36%</td>
<td align="center">0.0359</td>
<td align="center"><a href="https://tensorboard.dev/experiment/sepBK5zNSaOtdCeEG6SV9w/" class="external-link">Link</a></td>
</tr>
</tbody>
</table>
<p>TokenLearner is able to consistently outperform our mini ViT without
the module. It is also interesting to notice that it was also able to
outperform a deeper version of our mini ViT (with 8 layers). The authors
also report similar observations in the paper and they attribute this to
the adaptiveness of TokenLearner.</p>
<p>One should also note that the FLOPs count <strong>decreases</strong>
considerably with the addition of the TokenLearner module. With less
FLOPs count the TokenLearner module is able to deliver better results.
This aligns very well with the authorsâ€™ findings.</p>
<p>Additionally, the authors <a href="https://github.com/google-research/scenic/blob/main/scenic/projects/token_learner/model.py#L104" class="external-link">introduced</a>
a newer version of the TokenLearner for smaller training data regimes.
Quoting the authors:</p>
<blockquote>
<p>Instead of using 4 conv. layers with small channels to implement
spatial attention, this version uses 2 grouped conv. layers with more
channels. It also uses softmax instead of sigmoid. We confirmed that
this version works better when having limited training data, such as
training with ImageNet1K from scratch.</p>
</blockquote>
<p>We experimented with this module and in the following table we
summarize the results:</p>
<table class="table">
<colgroup>
<col width="20%">
<col width="20%">
<col width="20%">
<col width="20%">
<col width="20%">
</colgroup>
<thead><tr class="header">
<th align="center"><strong># Groups</strong></th>
<th align="center"><strong># Tokens</strong></th>
<th align="center"><strong>Top-1 Acc</strong></th>
<th align="center"><strong>GFLOPs</strong></th>
<th align="center"><strong>TensorBoard</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="center">4</td>
<td align="center">4</td>
<td align="center">54.638%</td>
<td align="center">0.0149</td>
<td align="center"><a href="https://tensorboard.dev/experiment/KmfkGqAGQjikEw85phySmw/" class="external-link">Link</a></td>
</tr>
<tr class="even">
<td align="center">8</td>
<td align="center">8</td>
<td align="center">54.898%</td>
<td align="center">0.0146</td>
<td align="center"><a href="https://tensorboard.dev/experiment/0PpgYOq9RFWV9njX6NJQ2w/" class="external-link">Link</a></td>
</tr>
<tr class="odd">
<td align="center">4</td>
<td align="center">8</td>
<td align="center">55.196%</td>
<td align="center">0.0149</td>
<td align="center"><a href="https://tensorboard.dev/experiment/WUkrHbZASdu3zrfmY4ETZg/" class="external-link">Link</a></td>
</tr>
</tbody>
</table>
<p>Please note that we used the same hyperparameters presented in this
example. Our implementation is available <a href="https://github.com/ariG23498/TokenLearner/blob/master/TokenLearner-V1.1.ipynb" class="external-link">in
this notebook</a>. We acknowledge that the results with this new
TokenLearner module are slightly off than expected and this might
mitigate with hyperparameter tuning.</p>
<p><em>Note</em>: To compute the FLOPs of our models we used <a href="https://github.com/AdityaKane2001/regnety/blob/main/regnety/utils/model_utils.py#L27" class="external-link">this
utility</a> from <a href="https://github.com/AdityaKane2001/regnety" class="external-link">this
repository</a>.</p>
</div>
<div class="section level2">
<h2 id="number-of-parameters">Number of parameters<a class="anchor" aria-label="anchor" href="#number-of-parameters"></a>
</h2>
<p>You may have noticed that adding the TokenLearner module increases
the number of parameters of the base network. But that does not mean it
is less efficient as shown by <a href="https://arxiv.org/abs/2110.12894" class="external-link">Dehghani et al.</a>. Similar
findings were reported by <a href="https://arxiv.org/abs/2103.07579" class="external-link">Bello et al.</a> as well. The
TokenLearner module helps reducing the FLOPS in the overall network
thereby helping to reduce the memory footprint.</p>
</div>
<div class="section level2">
<h2 id="final-notes">Final notes<a class="anchor" aria-label="anchor" href="#final-notes"></a>
</h2>
<ul>
<li>TokenFuser: The authors of the paper also propose another module
named TokenFuser. This module helps in remapping the representation of
the TokenLearner output back to its original spatial resolution. To
reuse the TokenLearner in the ViT architecture, the TokenFuser is a
must. We first learn the tokens from the TokenLearner, build a
representation of the tokens from a Transformer layer and then remap the
representation into the original spatial resolution, so that it can
again be consumed by a TokenLearner. Note here that you can only use the
TokenLearner module once in entire ViT model if not paired with the
TokenFuser.</li>
<li>Use of these modules for video: The authors also suggest that
TokenFuser goes really well with Vision Transformers for Videos (<a href="https://arxiv.org/abs/2103.15691" class="external-link">Arnab et al.</a>).</li>
</ul>
<p>We are grateful to <a href="https://jarvislabs.ai/" class="external-link">JarvisLabs</a>
and <a href="https://developers.google.com/programs/experts/" class="external-link">Google
Developers Experts</a> program for helping with GPU credits. Also, we
are thankful to Michael Ryoo (first author of TokenLearner) for fruitful
discussions.</p>
<table class="table">
<colgroup>
<col width="50%">
<col width="50%">
</colgroup>
<thead><tr class="header">
<th align="center">Trained Model</th>
<th align="center">Demo</th>
</tr></thead>
<tbody><tr class="odd">
<td align="center"><a href="https://huggingface.co/keras-io/learning_to_tokenize_in_ViT" class="external-link"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Model-TokenLearner-black.svg" alt="Generic badge"></a></td>
<td align="center"><a href="https://huggingface.co/spaces/keras-io/token_learner" class="external-link"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Spaces-TokenLearner-black.svg" alt="Generic badge"></a></td>
</tr></tbody>
</table>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, FranÃ§ois Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
