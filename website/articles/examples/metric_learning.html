<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Example of using similarity metric learning on CIFAR-10 images.">
<title>Metric learning for image similarity search • keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><meta property="og:title" content="Metric learning for image similarity search">
<meta property="og:description" content="Example of using similarity metric learning on CIFAR-10 images.">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-getting-started">Getting Started</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-getting-started">
    <a class="dropdown-item" href="../../articles/intro_to_keras_for_engineers.html">Introduction to Keras for engineers</a>
    <h6 class="dropdown-header" data-toc-skip>Tutorials</h6>
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
  </div>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Metric learning for image similarity search</h1>
                        <h4 data-toc-skip class="author"><a href="https://twitter.com/mat_kelcey" class="external-link">Mat Kelcey</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/metric_learning.Rmd" class="external-link"><code>vignettes/examples/metric_learning.Rmd</code></a></small>
      <div class="d-none name"><code>metric_learning.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="overview">Overview<a class="anchor" aria-label="anchor" href="#overview"></a>
</h2>
<p>Metric learning aims to train models that can embed inputs into a
high-dimensional space such that “similar” inputs, as defined by the
training scheme, are located close to each other. These models once
trained can produce embeddings for downstream systems where such
similarity is useful; examples include as a ranking signal for search or
as a form of pretrained embedding model for another supervised
problem.</p>
<p>For a more detailed overview of metric learning see:</p>
<ul>
<li><a href="http://contrib.scikit-learn.org/metric-learn/introduction.html" class="external-link">What
is metric learning?</a></li>
<li><a href="https://www.youtube.com/watch?v=Jb4Ewl5RzkI" class="external-link">“Using
crossentropy for metric learning” tutorial</a></li>
</ul>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> ConfusionMatrixDisplay</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="im">from</span> keras.datasets <span class="im">import</span> cifar10</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="dataset">Dataset<a class="anchor" aria-label="anchor" href="#dataset"></a>
</h2>
<p>For this example we will be using the <a href="https://www.cs.toronto.edu/~kriz/cifar.html" class="external-link">CIFAR-10</a>
dataset.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>(x_train, y_train), (x_test, y_test) <span class="op">=</span> cifar10.load_data()</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>x_train <span class="op">=</span> x_train.astype(<span class="st">"float32"</span>) <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>y_train <span class="op">=</span> np.squeeze(y_train)</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>x_test <span class="op">=</span> x_test.astype(<span class="st">"float32"</span>) <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>y_test <span class="op">=</span> np.squeeze(y_test)</span></code></pre></div>
<p>To get a sense of the dataset we can visualise a grid of 25 random
examples.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>height_width <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="kw">def</span> show_collage(examples):</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>    box_size <span class="op">=</span> height_width <span class="op">+</span> <span class="dv">2</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>    num_rows, num_cols <span class="op">=</span> examples.shape[:<span class="dv">2</span>]</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>    collage <span class="op">=</span> Image.new(</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>        mode<span class="op">=</span><span class="st">"RGB"</span>,</span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>        size<span class="op">=</span>(num_cols <span class="op">*</span> box_size, num_rows <span class="op">*</span> box_size),</span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>        color<span class="op">=</span>(<span class="dv">250</span>, <span class="dv">250</span>, <span class="dv">250</span>),</span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>    )</span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a>    <span class="cf">for</span> row_idx <span class="kw">in</span> <span class="bu">range</span>(num_rows):</span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a>        <span class="cf">for</span> col_idx <span class="kw">in</span> <span class="bu">range</span>(num_cols):</span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a>            array <span class="op">=</span> (np.array(examples[row_idx, col_idx]) <span class="op">*</span> <span class="dv">255</span>).astype(</span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a>                np.uint8</span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a>            )</span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a>            collage.paste(</span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a>                Image.fromarray(array), (col_idx <span class="op">*</span> box_size, row_idx <span class="op">*</span> box_size)</span>
<span id="cb3-20"><a href="#cb3-20" tabindex="-1"></a>            )</span>
<span id="cb3-21"><a href="#cb3-21" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" tabindex="-1"></a>    <span class="co"># Double size for visualisation.</span></span>
<span id="cb3-23"><a href="#cb3-23" tabindex="-1"></a>    collage <span class="op">=</span> collage.resize((<span class="dv">2</span> <span class="op">*</span> num_cols <span class="op">*</span> box_size, <span class="dv">2</span> <span class="op">*</span> num_rows <span class="op">*</span> box_size))</span>
<span id="cb3-24"><a href="#cb3-24" tabindex="-1"></a>    <span class="cf">return</span> collage</span>
<span id="cb3-25"><a href="#cb3-25" tabindex="-1"></a></span>
<span id="cb3-26"><a href="#cb3-26" tabindex="-1"></a></span>
<span id="cb3-27"><a href="#cb3-27" tabindex="-1"></a><span class="co"># Show a collage of 5x5 random images.</span></span>
<span id="cb3-28"><a href="#cb3-28" tabindex="-1"></a>sample_idxs <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">50000</span>, size<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">5</span>))</span>
<span id="cb3-29"><a href="#cb3-29" tabindex="-1"></a>examples <span class="op">=</span> x_train[sample_idxs]</span>
<span id="cb3-30"><a href="#cb3-30" tabindex="-1"></a>show_collage(examples)</span></code></pre></div>
<p>Metric learning provides training data not as explicit
<code>(X, y)</code> pairs but instead uses multiple instances that are
related in the way we want to express similarity. In our example we will
use instances of the same class to represent similarity; a single
training instance will not be one image, but a pair of images of the
same class. When referring to the images in this pair we’ll use the
common metric learning names of the <code>anchor</code> (a randomly
chosen image) and the <code>positive</code> (another randomly chosen
image of the same class).</p>
<p>To facilitate this we need to build a form of lookup that maps from
classes to the instances of that class. When generating data for
training we will sample from this lookup.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>class_idx_to_train_idxs <span class="op">=</span> defaultdict(<span class="bu">list</span>)</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="cf">for</span> y_train_idx, y <span class="kw">in</span> <span class="bu">enumerate</span>(y_train):</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>    class_idx_to_train_idxs[y].append(y_train_idx)</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>class_idx_to_test_idxs <span class="op">=</span> defaultdict(<span class="bu">list</span>)</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a><span class="cf">for</span> y_test_idx, y <span class="kw">in</span> <span class="bu">enumerate</span>(y_test):</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>    class_idx_to_test_idxs[y].append(y_test_idx)</span></code></pre></div>
<p>For this example we are using the simplest approach to training; a
batch will consist of <code>(anchor, positive)</code> pairs spread
across the classes. The goal of learning will be to move the anchor and
positive pairs closer together and further away from other instances in
the batch. In this case the batch size will be dictated by the number of
classes; for CIFAR-10 this is 10.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>num_classes <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="kw">class</span> AnchorPositivePairs(keras.utils.PyDataset):</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_batchs):</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>        <span class="va">self</span>.num_batchs <span class="op">=</span> num_batchs</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.num_batchs</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, _idx):</span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>        x <span class="op">=</span> np.empty(</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>            (<span class="dv">2</span>, num_classes, height_width, height_width, <span class="dv">3</span>), dtype<span class="op">=</span>np.float32</span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a>        )</span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>        <span class="cf">for</span> class_idx <span class="kw">in</span> <span class="bu">range</span>(num_classes):</span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>            examples_for_class <span class="op">=</span> class_idx_to_train_idxs[class_idx]</span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a>            anchor_idx <span class="op">=</span> random.choice(examples_for_class)</span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a>            positive_idx <span class="op">=</span> random.choice(examples_for_class)</span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a>            <span class="cf">while</span> positive_idx <span class="op">==</span> anchor_idx:</span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a>                positive_idx <span class="op">=</span> random.choice(examples_for_class)</span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a>            x[<span class="dv">0</span>, class_idx] <span class="op">=</span> x_train[anchor_idx]</span>
<span id="cb5-23"><a href="#cb5-23" tabindex="-1"></a>            x[<span class="dv">1</span>, class_idx] <span class="op">=</span> x_train[positive_idx]</span>
<span id="cb5-24"><a href="#cb5-24" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div>
<p>We can visualise a batch in another collage. The top row shows
randomly chosen anchors from the 10 classes, the bottom row shows the
corresponding 10 positives.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>examples <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(AnchorPositivePairs(num_batchs<span class="op">=</span><span class="dv">1</span>)))</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>show_collage(examples)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="embedding-model">Embedding model<a class="anchor" aria-label="anchor" href="#embedding-model"></a>
</h2>
<p>We define a custom model with a <code>train_step</code> that first
embeds both anchors and positives and then uses their pairwise dot
products as logits for a softmax.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="kw">class</span> EmbeddingModel(keras.Model):</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>    <span class="kw">def</span> train_step(<span class="va">self</span>, data):</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>        <span class="co"># Note: Workaround for open issue, to be removed.</span></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(data, <span class="bu">tuple</span>):</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>            data <span class="op">=</span> data[<span class="dv">0</span>]</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>        anchors, positives <span class="op">=</span> data[<span class="dv">0</span>], data[<span class="dv">1</span>]</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>        <span class="cf">with</span> tf.GradientTape() <span class="im">as</span> tape:</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>            <span class="co"># Run both anchors and positives through model.</span></span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>            anchor_embeddings <span class="op">=</span> <span class="va">self</span>(anchors, training<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>            positive_embeddings <span class="op">=</span> <span class="va">self</span>(positives, training<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a>            <span class="co"># Calculate cosine similarity between anchors and positives. As they have</span></span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a>            <span class="co"># been normalised this is just the pair wise dot products.</span></span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a>            similarities <span class="op">=</span> tf.einsum(</span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a>                <span class="st">"ae,pe-&gt;ap"</span>, anchor_embeddings, positive_embeddings</span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a>            )</span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a>            <span class="co"># Since we intend to use these as logits we scale them by a temperature.</span></span>
<span id="cb7-20"><a href="#cb7-20" tabindex="-1"></a>            <span class="co"># This value would normally be chosen as a hyper parameter.</span></span>
<span id="cb7-21"><a href="#cb7-21" tabindex="-1"></a>            temperature <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb7-22"><a href="#cb7-22" tabindex="-1"></a>            similarities <span class="op">/=</span> temperature</span>
<span id="cb7-23"><a href="#cb7-23" tabindex="-1"></a></span>
<span id="cb7-24"><a href="#cb7-24" tabindex="-1"></a>            <span class="co"># We use these similarities as logits for a softmax. The labels for</span></span>
<span id="cb7-25"><a href="#cb7-25" tabindex="-1"></a>            <span class="co"># this call are just the sequence [0, 1, 2, ..., num_classes] since we</span></span>
<span id="cb7-26"><a href="#cb7-26" tabindex="-1"></a>            <span class="co"># want the main diagonal values, which correspond to the anchor/positive</span></span>
<span id="cb7-27"><a href="#cb7-27" tabindex="-1"></a>            <span class="co"># pairs, to be high. This loss will move embeddings for the</span></span>
<span id="cb7-28"><a href="#cb7-28" tabindex="-1"></a>            <span class="co"># anchor/positive pairs together and move all other pairs apart.</span></span>
<span id="cb7-29"><a href="#cb7-29" tabindex="-1"></a>            sparse_labels <span class="op">=</span> tf.<span class="bu">range</span>(num_classes)</span>
<span id="cb7-30"><a href="#cb7-30" tabindex="-1"></a>            loss <span class="op">=</span> <span class="va">self</span>.compute_loss(y<span class="op">=</span>sparse_labels, y_pred<span class="op">=</span>similarities)</span>
<span id="cb7-31"><a href="#cb7-31" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" tabindex="-1"></a>        <span class="co"># Calculate gradients and apply via optimizer.</span></span>
<span id="cb7-33"><a href="#cb7-33" tabindex="-1"></a>        gradients <span class="op">=</span> tape.gradient(loss, <span class="va">self</span>.trainable_variables)</span>
<span id="cb7-34"><a href="#cb7-34" tabindex="-1"></a>        <span class="va">self</span>.optimizer.apply_gradients(<span class="bu">zip</span>(gradients, <span class="va">self</span>.trainable_variables))</span>
<span id="cb7-35"><a href="#cb7-35" tabindex="-1"></a></span>
<span id="cb7-36"><a href="#cb7-36" tabindex="-1"></a>        <span class="co"># Update and return metrics (specifically the one for the loss value).</span></span>
<span id="cb7-37"><a href="#cb7-37" tabindex="-1"></a>        <span class="cf">for</span> metric <span class="kw">in</span> <span class="va">self</span>.metrics:</span>
<span id="cb7-38"><a href="#cb7-38" tabindex="-1"></a>            metric.update_state(sparse_labels, similarities)</span>
<span id="cb7-39"><a href="#cb7-39" tabindex="-1"></a>        <span class="cf">return</span> {m.name: m.result() <span class="cf">for</span> m <span class="kw">in</span> <span class="va">self</span>.metrics}</span></code></pre></div>
<p>Next we describe the architecture that maps from an image to an
embedding. This model simply consists of a sequence of 2d convolutions
followed by global pooling with a final linear projection to an
embedding space. As is common in metric learning we normalise the
embeddings so that we can use simple dot products to measure similarity.
For simplicity this model is intentionally small.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>inputs <span class="op">=</span> layers.Input(shape<span class="op">=</span>(height_width, height_width, <span class="dv">3</span>))</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(filters<span class="op">=</span><span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, strides<span class="op">=</span><span class="dv">2</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>    inputs</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>)</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(filters<span class="op">=</span><span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, strides<span class="op">=</span><span class="dv">2</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>x <span class="op">=</span> layers.Conv2D(filters<span class="op">=</span><span class="dv">128</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, strides<span class="op">=</span><span class="dv">2</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>x <span class="op">=</span> layers.GlobalAveragePooling2D()(x)</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>embeddings <span class="op">=</span> layers.Dense(units<span class="op">=</span><span class="dv">8</span>, activation<span class="op">=</span><span class="va">None</span>)(x)</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>embeddings <span class="op">=</span> keras.layers.UnitNormalization()(embeddings)</span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>model <span class="op">=</span> EmbeddingModel(inputs, embeddings)</span></code></pre></div>
<p>Finally we run the training. On a Google Colab GPU instance this
takes about a minute.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>    optimizer<span class="op">=</span>keras.optimizers.Adam(learning_rate<span class="op">=</span><span class="fl">1e-3</span>),</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>    loss<span class="op">=</span>keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>)</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>history <span class="op">=</span> model.fit(AnchorPositivePairs(num_batchs<span class="op">=</span><span class="dv">1000</span>), epochs<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>plt.plot(history.history[<span class="st">"loss"</span>])</span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a>plt.show()</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="testing">Testing<a class="anchor" aria-label="anchor" href="#testing"></a>
</h2>
<p>We can review the quality of this model by applying it to the test
set and considering near neighbours in the embedding space.</p>
<p>First we embed the test set and calculate all near neighbours. Recall
that since the embeddings are unit length we can calculate cosine
similarity via dot products.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>near_neighbours_per_example <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>embeddings <span class="op">=</span> model.predict(x_test)</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>gram_matrix <span class="op">=</span> np.einsum(<span class="st">"ae,be-&gt;ab"</span>, embeddings, embeddings)</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>near_neighbours <span class="op">=</span> np.argsort(gram_matrix.T)[</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>    :, <span class="op">-</span>(near_neighbours_per_example <span class="op">+</span> <span class="dv">1</span>) :</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>]</span></code></pre></div>
<p>As a visual check of these embeddings we can build a collage of the
near neighbours for 5 random examples. The first column of the image
below is a randomly selected image, the following 10 columns show the
nearest neighbours in order of similarity.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>num_collage_examples <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>examples <span class="op">=</span> np.empty(</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>    (</span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>        num_collage_examples,</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>        near_neighbours_per_example <span class="op">+</span> <span class="dv">1</span>,</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>        height_width,</span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a>        height_width,</span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a>        <span class="dv">3</span>,</span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a>    ),</span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a>    dtype<span class="op">=</span>np.float32,</span>
<span id="cb11-12"><a href="#cb11-12" tabindex="-1"></a>)</span>
<span id="cb11-13"><a href="#cb11-13" tabindex="-1"></a><span class="cf">for</span> row_idx <span class="kw">in</span> <span class="bu">range</span>(num_collage_examples):</span>
<span id="cb11-14"><a href="#cb11-14" tabindex="-1"></a>    examples[row_idx, <span class="dv">0</span>] <span class="op">=</span> x_test[row_idx]</span>
<span id="cb11-15"><a href="#cb11-15" tabindex="-1"></a>    anchor_near_neighbours <span class="op">=</span> <span class="bu">reversed</span>(near_neighbours[row_idx][:<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb11-16"><a href="#cb11-16" tabindex="-1"></a>    <span class="cf">for</span> col_idx, nn_idx <span class="kw">in</span> <span class="bu">enumerate</span>(anchor_near_neighbours):</span>
<span id="cb11-17"><a href="#cb11-17" tabindex="-1"></a>        examples[row_idx, col_idx <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> x_test[nn_idx]</span>
<span id="cb11-18"><a href="#cb11-18" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" tabindex="-1"></a>show_collage(examples)</span></code></pre></div>
<p>We can also get a quantified view of the performance by considering
the correctness of near neighbours in terms of a confusion matrix.</p>
<p>Let us sample 10 examples from each of the 10 classes and consider
their near neighbours as a form of prediction; that is, does the example
and its near neighbours share the same class?</p>
<p>We observe that each animal class does generally well, and is
confused the most with the other animal classes. The vehicle classes
follow the same pattern.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>confusion_matrix <span class="op">=</span> np.zeros((num_classes, num_classes))</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="co"># For each class.</span></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a><span class="cf">for</span> class_idx <span class="kw">in</span> <span class="bu">range</span>(num_classes):</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>    <span class="co"># Consider 10 examples.</span></span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>    example_idxs <span class="op">=</span> class_idx_to_test_idxs[class_idx][:<span class="dv">10</span>]</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a>    <span class="cf">for</span> y_test_idx <span class="kw">in</span> example_idxs:</span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a>        <span class="co"># And count the classes of its near neighbours.</span></span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a>        <span class="cf">for</span> nn_idx <span class="kw">in</span> near_neighbours[y_test_idx][:<span class="op">-</span><span class="dv">1</span>]:</span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a>            nn_class_idx <span class="op">=</span> y_test[nn_idx]</span>
<span id="cb12-11"><a href="#cb12-11" tabindex="-1"></a>            confusion_matrix[class_idx, nn_class_idx] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb12-12"><a href="#cb12-12" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" tabindex="-1"></a><span class="co"># Display a confusion matrix.</span></span>
<span id="cb12-14"><a href="#cb12-14" tabindex="-1"></a>labels <span class="op">=</span> [</span>
<span id="cb12-15"><a href="#cb12-15" tabindex="-1"></a>    <span class="st">"Airplane"</span>,</span>
<span id="cb12-16"><a href="#cb12-16" tabindex="-1"></a>    <span class="st">"Automobile"</span>,</span>
<span id="cb12-17"><a href="#cb12-17" tabindex="-1"></a>    <span class="st">"Bird"</span>,</span>
<span id="cb12-18"><a href="#cb12-18" tabindex="-1"></a>    <span class="st">"Cat"</span>,</span>
<span id="cb12-19"><a href="#cb12-19" tabindex="-1"></a>    <span class="st">"Deer"</span>,</span>
<span id="cb12-20"><a href="#cb12-20" tabindex="-1"></a>    <span class="st">"Dog"</span>,</span>
<span id="cb12-21"><a href="#cb12-21" tabindex="-1"></a>    <span class="st">"Frog"</span>,</span>
<span id="cb12-22"><a href="#cb12-22" tabindex="-1"></a>    <span class="st">"Horse"</span>,</span>
<span id="cb12-23"><a href="#cb12-23" tabindex="-1"></a>    <span class="st">"Ship"</span>,</span>
<span id="cb12-24"><a href="#cb12-24" tabindex="-1"></a>    <span class="st">"Truck"</span>,</span>
<span id="cb12-25"><a href="#cb12-25" tabindex="-1"></a>]</span>
<span id="cb12-26"><a href="#cb12-26" tabindex="-1"></a>disp <span class="op">=</span> ConfusionMatrixDisplay(</span>
<span id="cb12-27"><a href="#cb12-27" tabindex="-1"></a>    confusion_matrix<span class="op">=</span>confusion_matrix, display_labels<span class="op">=</span>labels</span>
<span id="cb12-28"><a href="#cb12-28" tabindex="-1"></a>)</span>
<span id="cb12-29"><a href="#cb12-29" tabindex="-1"></a>disp.plot(</span>
<span id="cb12-30"><a href="#cb12-30" tabindex="-1"></a>    include_values<span class="op">=</span><span class="va">True</span>, cmap<span class="op">=</span><span class="st">"viridis"</span>, ax<span class="op">=</span><span class="va">None</span>, xticks_rotation<span class="op">=</span><span class="st">"vertical"</span></span>
<span id="cb12-31"><a href="#cb12-31" tabindex="-1"></a>)</span>
<span id="cb12-32"><a href="#cb12-32" tabindex="-1"></a>plt.show()</span></code></pre></div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, RStudio, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.9000.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
