<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Mitigating resolution discrepancy between training and test sets.">
<title>FixRes: Fixing train-test resolution discrepancy â€¢ keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../../deps/Fira_Mono-0.4.8/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><meta property="og:title" content="FixRes: Fixing train-test resolution discrepancy">
<meta property="og:description" content="Mitigating resolution discrepancy between training and test sets.">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-getting-started">Getting Started</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-getting-started">
    <a class="dropdown-item" href="../../articles/intro_to_keras_for_engineers.html">Introduction to Keras for engineers</a>
    <h6 class="dropdown-header" data-toc-skip>Tutorials</h6>
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
  </div>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>FixRes: Fixing train-test resolution discrepancy</h1>
                        <h4 data-toc-skip class="author"><a href="https://twitter.com/RisingSayak" class="external-link">Sayak Paul</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/fixres.Rmd" class="external-link"><code>vignettes/examples/fixres.Rmd</code></a></small>
      <div class="d-none name"><code>fixres.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>It is a common practice to use the same input image resolution while
training and testing vision models. However, as investigated in <a href="https://arxiv.org/abs/1906.06423" class="external-link">Fixing the train-test resolution
discrepancy</a> (Touvron et al.), this practice leads to suboptimal
performance. Data augmentation is an indispensable part of the training
process of deep neural networks. For vision models, we typically use
random resized crops during training and center crops during inference.
This introduces a discrepancy in the object sizes seen during training
and inference. As shown by Touvron et al., if we can fix this
discrepancy, we can significantly boost model performance.</p>
<p>In this example, we implement the <strong>FixRes</strong> techniques
introduced by Touvron et al. to fix this discrepancy.</p>
</div>
<div class="section level2">
<h2 id="imports">Imports<a class="anchor" aria-label="anchor" href="#imports"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf  <span class="co"># just for image processing and pipeline</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">import</span> tensorflow_datasets <span class="im">as</span> tfds</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>tfds.disable_progress_bar()</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="load-the-tf_flowers-dataset">Load the <code>tf_flowers</code> dataset<a class="anchor" aria-label="anchor" href="#load-the-tf_flowers-dataset"></a>
</h2>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>train_dataset, val_dataset <span class="op">=</span> tfds.load(</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>    <span class="st">"tf_flowers"</span>, split<span class="op">=</span>[<span class="st">"train[:90%]"</span>, <span class="st">"train[90%:]"</span>], as_supervised<span class="op">=</span><span class="va">True</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>)</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>num_train <span class="op">=</span> train_dataset.cardinality()</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>num_val <span class="op">=</span> val_dataset.cardinality()</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of training examples: </span><span class="sc">{</span>num_train<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of validation examples: </span><span class="sc">{</span>num_val<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="data-preprocessing-utilities">Data preprocessing utilities<a class="anchor" aria-label="anchor" href="#data-preprocessing-utilities"></a>
</h2>
<p>We create three datasets:</p>
<ol style="list-style-type: decimal">
<li>A dataset with a smaller resolution - 128x128.</li>
<li>Two datasets with a larger resolution - 224x224.</li>
</ol>
<p>We will apply different augmentation transforms to the
larger-resolution datasets.</p>
<p>The idea of FixRes is to first train a model on a smaller resolution
dataset and then fine-tune it on a larger resolution dataset. This
simple yet effective recipe leads to non-trivial performance
improvements. Please refer to the <a href="https://arxiv.org/abs/1906.06423" class="external-link">original paper</a> for
results.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="co"># Reference: https://github.com/facebookresearch/FixRes/blob/main/transforms_v2.py.</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>auto <span class="op">=</span> tf.data.AUTOTUNE</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>smaller_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>bigger_size <span class="op">=</span> <span class="dv">224</span></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>size_for_resizing <span class="op">=</span> <span class="bu">int</span>((bigger_size <span class="op">/</span> smaller_size) <span class="op">*</span> bigger_size)</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>central_crop_layer <span class="op">=</span> layers.CenterCrop(bigger_size, bigger_size)</span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a><span class="kw">def</span> preprocess_initial(train, image_size):</span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a>    <span class="co">"""Initial preprocessing function for training on smaller resolution.</span></span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a><span class="co">    For training, do random_horizontal_flip -&gt; random_crop.</span></span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a><span class="co">    For validation, just resize.</span></span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a><span class="co">    No color-jittering has been used.</span></span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" tabindex="-1"></a>    <span class="kw">def</span> _pp(image, label, train):</span>
<span id="cb3-21"><a href="#cb3-21" tabindex="-1"></a>        <span class="cf">if</span> train:</span>
<span id="cb3-22"><a href="#cb3-22" tabindex="-1"></a>            channels <span class="op">=</span> image.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb3-23"><a href="#cb3-23" tabindex="-1"></a>            begin, size, _ <span class="op">=</span> tf.image.sample_distorted_bounding_box(</span>
<span id="cb3-24"><a href="#cb3-24" tabindex="-1"></a>                tf.shape(image),</span>
<span id="cb3-25"><a href="#cb3-25" tabindex="-1"></a>                tf.zeros([<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">4</span>], tf.float32),</span>
<span id="cb3-26"><a href="#cb3-26" tabindex="-1"></a>                area_range<span class="op">=</span>(<span class="fl">0.05</span>, <span class="fl">1.0</span>),</span>
<span id="cb3-27"><a href="#cb3-27" tabindex="-1"></a>                min_object_covered<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb3-28"><a href="#cb3-28" tabindex="-1"></a>                use_image_if_no_bounding_boxes<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-29"><a href="#cb3-29" tabindex="-1"></a>            )</span>
<span id="cb3-30"><a href="#cb3-30" tabindex="-1"></a>            image <span class="op">=</span> tf.<span class="bu">slice</span>(image, begin, size)</span>
<span id="cb3-31"><a href="#cb3-31" tabindex="-1"></a></span>
<span id="cb3-32"><a href="#cb3-32" tabindex="-1"></a>            image.set_shape([<span class="va">None</span>, <span class="va">None</span>, channels])</span>
<span id="cb3-33"><a href="#cb3-33" tabindex="-1"></a>            image <span class="op">=</span> tf.image.resize(image, [image_size, image_size])</span>
<span id="cb3-34"><a href="#cb3-34" tabindex="-1"></a>            image <span class="op">=</span> tf.image.random_flip_left_right(image)</span>
<span id="cb3-35"><a href="#cb3-35" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb3-36"><a href="#cb3-36" tabindex="-1"></a>            image <span class="op">=</span> tf.image.resize(image, [image_size, image_size])</span>
<span id="cb3-37"><a href="#cb3-37" tabindex="-1"></a></span>
<span id="cb3-38"><a href="#cb3-38" tabindex="-1"></a>        <span class="cf">return</span> image, label</span>
<span id="cb3-39"><a href="#cb3-39" tabindex="-1"></a></span>
<span id="cb3-40"><a href="#cb3-40" tabindex="-1"></a>    <span class="cf">return</span> _pp</span>
<span id="cb3-41"><a href="#cb3-41" tabindex="-1"></a></span>
<span id="cb3-42"><a href="#cb3-42" tabindex="-1"></a></span>
<span id="cb3-43"><a href="#cb3-43" tabindex="-1"></a><span class="kw">def</span> preprocess_finetune(image, label, train):</span>
<span id="cb3-44"><a href="#cb3-44" tabindex="-1"></a>    <span class="co">"""Preprocessing function for fine-tuning on a higher resolution.</span></span>
<span id="cb3-45"><a href="#cb3-45" tabindex="-1"></a></span>
<span id="cb3-46"><a href="#cb3-46" tabindex="-1"></a><span class="co">    For training, resize to a bigger resolution to maintain the ratio -&gt;</span></span>
<span id="cb3-47"><a href="#cb3-47" tabindex="-1"></a><span class="co">        random_horizontal_flip -&gt; center_crop.</span></span>
<span id="cb3-48"><a href="#cb3-48" tabindex="-1"></a><span class="co">    For validation, do the same without any horizontal flipping.</span></span>
<span id="cb3-49"><a href="#cb3-49" tabindex="-1"></a><span class="co">    No color-jittering has been used.</span></span>
<span id="cb3-50"><a href="#cb3-50" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-51"><a href="#cb3-51" tabindex="-1"></a>    image <span class="op">=</span> tf.image.resize(image, [size_for_resizing, size_for_resizing])</span>
<span id="cb3-52"><a href="#cb3-52" tabindex="-1"></a>    <span class="cf">if</span> train:</span>
<span id="cb3-53"><a href="#cb3-53" tabindex="-1"></a>        image <span class="op">=</span> tf.image.random_flip_left_right(image)</span>
<span id="cb3-54"><a href="#cb3-54" tabindex="-1"></a>    image <span class="op">=</span> central_crop_layer(image[<span class="va">None</span>, ...])[<span class="dv">0</span>]</span>
<span id="cb3-55"><a href="#cb3-55" tabindex="-1"></a></span>
<span id="cb3-56"><a href="#cb3-56" tabindex="-1"></a>    <span class="cf">return</span> image, label</span>
<span id="cb3-57"><a href="#cb3-57" tabindex="-1"></a></span>
<span id="cb3-58"><a href="#cb3-58" tabindex="-1"></a></span>
<span id="cb3-59"><a href="#cb3-59" tabindex="-1"></a><span class="kw">def</span> make_dataset(</span>
<span id="cb3-60"><a href="#cb3-60" tabindex="-1"></a>    dataset: tf.data.Dataset,</span>
<span id="cb3-61"><a href="#cb3-61" tabindex="-1"></a>    train: <span class="bu">bool</span>,</span>
<span id="cb3-62"><a href="#cb3-62" tabindex="-1"></a>    image_size: <span class="bu">int</span> <span class="op">=</span> smaller_size,</span>
<span id="cb3-63"><a href="#cb3-63" tabindex="-1"></a>    fixres: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb3-64"><a href="#cb3-64" tabindex="-1"></a>    num_parallel_calls<span class="op">=</span>auto,</span>
<span id="cb3-65"><a href="#cb3-65" tabindex="-1"></a>):</span>
<span id="cb3-66"><a href="#cb3-66" tabindex="-1"></a>    <span class="cf">if</span> image_size <span class="kw">not</span> <span class="kw">in</span> [smaller_size, bigger_size]:</span>
<span id="cb3-67"><a href="#cb3-67" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"</span><span class="sc">{</span>image_size<span class="sc">}</span><span class="ss"> resolution is not supported."</span>)</span>
<span id="cb3-68"><a href="#cb3-68" tabindex="-1"></a></span>
<span id="cb3-69"><a href="#cb3-69" tabindex="-1"></a>    <span class="co"># Determine which preprocessing function we are using.</span></span>
<span id="cb3-70"><a href="#cb3-70" tabindex="-1"></a>    <span class="cf">if</span> image_size <span class="op">==</span> smaller_size:</span>
<span id="cb3-71"><a href="#cb3-71" tabindex="-1"></a>        preprocess_func <span class="op">=</span> preprocess_initial(train, image_size)</span>
<span id="cb3-72"><a href="#cb3-72" tabindex="-1"></a>    <span class="cf">elif</span> <span class="kw">not</span> fixres <span class="kw">and</span> image_size <span class="op">==</span> bigger_size:</span>
<span id="cb3-73"><a href="#cb3-73" tabindex="-1"></a>        preprocess_func <span class="op">=</span> preprocess_initial(train, image_size)</span>
<span id="cb3-74"><a href="#cb3-74" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb3-75"><a href="#cb3-75" tabindex="-1"></a>        preprocess_func <span class="op">=</span> preprocess_finetune</span>
<span id="cb3-76"><a href="#cb3-76" tabindex="-1"></a></span>
<span id="cb3-77"><a href="#cb3-77" tabindex="-1"></a>    <span class="cf">if</span> train:</span>
<span id="cb3-78"><a href="#cb3-78" tabindex="-1"></a>        dataset <span class="op">=</span> dataset.shuffle(batch_size <span class="op">*</span> <span class="dv">10</span>)</span>
<span id="cb3-79"><a href="#cb3-79" tabindex="-1"></a></span>
<span id="cb3-80"><a href="#cb3-80" tabindex="-1"></a>    <span class="cf">return</span> (</span>
<span id="cb3-81"><a href="#cb3-81" tabindex="-1"></a>        dataset.<span class="bu">map</span>(</span>
<span id="cb3-82"><a href="#cb3-82" tabindex="-1"></a>            <span class="kw">lambda</span> x, y: preprocess_func(x, y, train),</span>
<span id="cb3-83"><a href="#cb3-83" tabindex="-1"></a>            num_parallel_calls<span class="op">=</span>num_parallel_calls,</span>
<span id="cb3-84"><a href="#cb3-84" tabindex="-1"></a>        )</span>
<span id="cb3-85"><a href="#cb3-85" tabindex="-1"></a>        .batch(batch_size)</span>
<span id="cb3-86"><a href="#cb3-86" tabindex="-1"></a>        .prefetch(num_parallel_calls)</span>
<span id="cb3-87"><a href="#cb3-87" tabindex="-1"></a>    )</span></code></pre></div>
<p>Notice how the augmentation transforms vary for the kind of dataset
we are preparing.</p>
</div>
<div class="section level2">
<h2 id="prepare-datasets">Prepare datasets<a class="anchor" aria-label="anchor" href="#prepare-datasets"></a>
</h2>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>initial_train_dataset <span class="op">=</span> make_dataset(</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>    train_dataset, train<span class="op">=</span><span class="va">True</span>, image_size<span class="op">=</span>smaller_size</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>)</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>initial_val_dataset <span class="op">=</span> make_dataset(</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>    val_dataset, train<span class="op">=</span><span class="va">False</span>, image_size<span class="op">=</span>smaller_size</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>)</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>finetune_train_dataset <span class="op">=</span> make_dataset(</span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>    train_dataset, train<span class="op">=</span><span class="va">True</span>, image_size<span class="op">=</span>bigger_size</span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>)</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>finetune_val_dataset <span class="op">=</span> make_dataset(</span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>    val_dataset, train<span class="op">=</span><span class="va">False</span>, image_size<span class="op">=</span>bigger_size</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a>)</span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a>vanilla_train_dataset <span class="op">=</span> make_dataset(</span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a>    train_dataset, train<span class="op">=</span><span class="va">True</span>, image_size<span class="op">=</span>bigger_size, fixres<span class="op">=</span><span class="va">False</span></span>
<span id="cb4-17"><a href="#cb4-17" tabindex="-1"></a>)</span>
<span id="cb4-18"><a href="#cb4-18" tabindex="-1"></a>vanilla_val_dataset <span class="op">=</span> make_dataset(</span>
<span id="cb4-19"><a href="#cb4-19" tabindex="-1"></a>    val_dataset, train<span class="op">=</span><span class="va">False</span>, image_size<span class="op">=</span>bigger_size, fixres<span class="op">=</span><span class="va">False</span></span>
<span id="cb4-20"><a href="#cb4-20" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="visualize-the-datasets">Visualize the datasets<a class="anchor" aria-label="anchor" href="#visualize-the-datasets"></a>
</h2>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="kw">def</span> visualize_dataset(batch_images):</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>    <span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">25</span>):</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>        ax <span class="op">=</span> plt.subplot(<span class="dv">5</span>, <span class="dv">5</span>, n <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>        plt.imshow(batch_images[n].numpy().astype(<span class="st">"int"</span>))</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>        plt.axis(<span class="st">"off"</span>)</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>    plt.show()</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Batch shape: </span><span class="sc">{</span>batch_images<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">."</span>)</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a><span class="co"># Smaller resolution.</span></span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>initial_sample_images, _ <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(initial_train_dataset))</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>visualize_dataset(initial_sample_images)</span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a><span class="co"># Bigger resolution, only for fine-tuning.</span></span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>finetune_sample_images, _ <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(finetune_train_dataset))</span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a>visualize_dataset(finetune_sample_images)</span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a><span class="co"># Bigger resolution, with the same augmentation transforms as</span></span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a><span class="co"># the smaller resolution dataset.</span></span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a>vanilla_sample_images, _ <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(vanilla_train_dataset))</span>
<span id="cb5-23"><a href="#cb5-23" tabindex="-1"></a>visualize_dataset(vanilla_sample_images)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="model-training-utilities">Model training utilities<a class="anchor" aria-label="anchor" href="#model-training-utilities"></a>
</h2>
<p>We train multiple variants of ResNet50V2 (<a href="https://arxiv.org/abs/1603.05027" class="external-link">He et al.</a>):</p>
<ol style="list-style-type: decimal">
<li>On the smaller resolution dataset (128x128). It will be trained from
scratch.</li>
<li>Then fine-tune the model from 1 on the larger resolution (224x224)
dataset.</li>
<li>Train another ResNet50V2 from scratch on the larger resolution
dataset.</li>
</ol>
<p>As a reminder, the larger resolution datasets differ in terms of
their augmentation transforms.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="kw">def</span> get_training_model(num_classes<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>    inputs <span class="op">=</span> layers.Input((<span class="va">None</span>, <span class="va">None</span>, <span class="dv">3</span>))</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>    resnet_base <span class="op">=</span> keras.applications.ResNet50V2(</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>        include_top<span class="op">=</span><span class="va">False</span>, weights<span class="op">=</span><span class="va">None</span>, pooling<span class="op">=</span><span class="st">"avg"</span></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>    )</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>    resnet_base.trainable <span class="op">=</span> <span class="va">True</span></span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>    x <span class="op">=</span> layers.Rescaling(scale<span class="op">=</span><span class="fl">1.0</span> <span class="op">/</span> <span class="fl">127.5</span>, offset<span class="op">=-</span><span class="dv">1</span>)(inputs)</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>    x <span class="op">=</span> resnet_base(x)</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>    outputs <span class="op">=</span> layers.Dense(num_classes, activation<span class="op">=</span><span class="st">"softmax"</span>)(x)</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>    <span class="cf">return</span> keras.Model(inputs, outputs)</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a><span class="kw">def</span> train_and_evaluate(</span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a>    model,</span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a>    train_ds,</span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a>    val_ds,</span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a>    epochs,</span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">1e-3</span>,</span>
<span id="cb6-20"><a href="#cb6-20" tabindex="-1"></a>    use_early_stopping<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb6-21"><a href="#cb6-21" tabindex="-1"></a>):</span>
<span id="cb6-22"><a href="#cb6-22" tabindex="-1"></a>    optimizer <span class="op">=</span> keras.optimizers.Adam(learning_rate<span class="op">=</span>learning_rate)</span>
<span id="cb6-23"><a href="#cb6-23" tabindex="-1"></a>    model.<span class="bu">compile</span>(</span>
<span id="cb6-24"><a href="#cb6-24" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb6-25"><a href="#cb6-25" tabindex="-1"></a>        loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>,</span>
<span id="cb6-26"><a href="#cb6-26" tabindex="-1"></a>        metrics<span class="op">=</span>[<span class="st">"accuracy"</span>],</span>
<span id="cb6-27"><a href="#cb6-27" tabindex="-1"></a>    )</span>
<span id="cb6-28"><a href="#cb6-28" tabindex="-1"></a></span>
<span id="cb6-29"><a href="#cb6-29" tabindex="-1"></a>    <span class="cf">if</span> use_early_stopping:</span>
<span id="cb6-30"><a href="#cb6-30" tabindex="-1"></a>        es_callback <span class="op">=</span> keras.callbacks.EarlyStopping(patience<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb6-31"><a href="#cb6-31" tabindex="-1"></a>        callbacks <span class="op">=</span> [es_callback]</span>
<span id="cb6-32"><a href="#cb6-32" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb6-33"><a href="#cb6-33" tabindex="-1"></a>        callbacks <span class="op">=</span> <span class="va">None</span></span>
<span id="cb6-34"><a href="#cb6-34" tabindex="-1"></a></span>
<span id="cb6-35"><a href="#cb6-35" tabindex="-1"></a>    model.fit(</span>
<span id="cb6-36"><a href="#cb6-36" tabindex="-1"></a>        train_ds,</span>
<span id="cb6-37"><a href="#cb6-37" tabindex="-1"></a>        validation_data<span class="op">=</span>val_ds,</span>
<span id="cb6-38"><a href="#cb6-38" tabindex="-1"></a>        epochs<span class="op">=</span>epochs,</span>
<span id="cb6-39"><a href="#cb6-39" tabindex="-1"></a>        callbacks<span class="op">=</span>callbacks,</span>
<span id="cb6-40"><a href="#cb6-40" tabindex="-1"></a>    )</span>
<span id="cb6-41"><a href="#cb6-41" tabindex="-1"></a></span>
<span id="cb6-42"><a href="#cb6-42" tabindex="-1"></a>    _, accuracy <span class="op">=</span> model.evaluate(val_ds)</span>
<span id="cb6-43"><a href="#cb6-43" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Top-1 accuracy on the validation set: </span><span class="sc">{</span>accuracy<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%."</span>)</span>
<span id="cb6-44"><a href="#cb6-44" tabindex="-1"></a>    <span class="cf">return</span> model</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="experiment-1-train-on-128x128-and-then-fine-tune-on-224x224">Experiment 1: Train on 128x128 and then fine-tune on 224x224<a class="anchor" aria-label="anchor" href="#experiment-1-train-on-128x128-and-then-fine-tune-on-224x224"></a>
</h2>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>smaller_res_model <span class="op">=</span> get_training_model()</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>smaller_res_model <span class="op">=</span> train_and_evaluate(</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>    smaller_res_model, initial_train_dataset, initial_val_dataset, epochs</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>)</span></code></pre></div>
<div class="section level3">
<h3 id="freeze-all-the-layers-except-for-the-final-batch-normalization-layer">Freeze all the layers except for the final Batch Normalization
layer<a class="anchor" aria-label="anchor" href="#freeze-all-the-layers-except-for-the-final-batch-normalization-layer"></a>
</h3>
<p>For fine-tuning, we train only two layers:</p>
<ul>
<li>The final Batch Normalization (<a href="https://arxiv.org/abs/1502.03167" class="external-link">Ioffe et al.</a>) layer.</li>
<li>The classification layer.</li>
</ul>
<p>We are unfreezing the final Batch Normalization layer to compensate
for the change in activation statistics before the global average
pooling layer. As shown in <a href="https://arxiv.org/abs/1906.06423" class="external-link">the paper</a>, unfreezing the
final Batch Normalization layer is enough.</p>
<p>For a comprehensive guide on fine-tuning models in Keras, refer to <a href="https://keras.io/guides/transfer_learning/" class="external-link">this tutorial</a>.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> smaller_res_model.layers[<span class="dv">2</span>].layers:</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>    layer.trainable <span class="op">=</span> <span class="va">False</span></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>smaller_res_model.layers[<span class="dv">2</span>].get_layer(<span class="st">"post_bn"</span>).trainable <span class="op">=</span> <span class="va">True</span></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a><span class="co"># Use a lower learning rate during fine-tuning.</span></span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>bigger_res_model <span class="op">=</span> train_and_evaluate(</span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>    smaller_res_model,</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>    finetune_train_dataset,</span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>    finetune_val_dataset,</span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>    epochs,</span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">1e-4</span>,</span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a>)</span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="experiment-2-train-a-model-on-224x224-resolution-from-scratch">Experiment 2: Train a model on 224x224 resolution from scratch<a class="anchor" aria-label="anchor" href="#experiment-2-train-a-model-on-224x224-resolution-from-scratch"></a>
</h2>
<p>Now, we train another model from scratch on the larger resolution
dataset. Recall that the augmentation transforms used in this dataset
are different from before.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>vanilla_bigger_res_model <span class="op">=</span> get_training_model()</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>vanilla_bigger_res_model <span class="op">=</span> train_and_evaluate(</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>    vanilla_bigger_res_model, vanilla_train_dataset, vanilla_val_dataset, epochs</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>)</span></code></pre></div>
<p>As we can notice from the above cells, FixRes leads to a better
performance. Another advantage of FixRes is the improved total training
time and reduction in GPU memory usage. FixRes is model-agnostic, you
can use it on any image classification model to potentially boost
performance.</p>
<p>You can find more results <a href="https://tensorboard.dev/experiment/BQOg28w0TlmvuJYeqsVntw" class="external-link">here</a>
that were gathered by running the same code with different random
seeds.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, FranÃ§ois Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
