<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Image classification with Swin Transformers â€¢ keras</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../../bootstrap-toc.css">
<script src="../../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../../pkgdown.css" rel="stylesheet">
<script src="../../pkgdown.js"></script><link href="../../extra.css" rel="stylesheet">
<script src="../../extra.js"></script><meta property="og:title" content="Image classification with Swin Transformers">
<meta property="og:description" content="Image classification using Swin Transformers, a general-purpose backbone for computer vision.">
<meta property="og:image" content="https://keras.posit.co/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../../index.html">keras</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">2.13.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../../index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Tutorials
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/getting_started.html">Getting Started</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    </li>
    <li>
      <a href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    </li>
    <li>
      <a href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Guides
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li class="dropdown-header">Guides (New for TF 2.6)</li>
    <li>
      <a href="../../articles/new-guides/python_subclasses.html">Python Subclasses</a>
    </li>
    <li>
      <a href="../../articles/new-guides/making_new_layers_and_models_via_subclassing.html">Making New Layers and Models via Subclassing</a>
    </li>
    <li>
      <a href="../../articles/new-guides/customizing_what_happens_in_fit.html">Customizing What Happens in Fit</a>
    </li>
    <li>
      <a href="../../articles/new-guides/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    </li>
    <li>
      <a href="../../articles/new-guides/preprocessing_layers.html">Working with Preprocessing Layers</a>
    </li>
    <li>
      <a href="../../argicles/new-guides/working_with_rnns.html">Working with RNNs</a>
    </li>
    <li class="divider">
    </li>
<li class="dropdown-header">Using Keras</li>
    <li>
      <a href="../../articles/guide_keras.html">Guide to Keras Basics</a>
    </li>
    <li>
      <a href="../../articles/sequential_model.html">Sequential Model in Depth</a>
    </li>
    <li>
      <a href="../../articles/functional_api.html">Functional API in Depth</a>
    </li>
    <li>
      <a href="../../articles/about_keras_models.html">About Keras Models</a>
    </li>
    <li>
      <a href="../../articles/about_keras_layers.html">About Keras Layers</a>
    </li>
    <li>
      <a href="../../articles/training_visualization.html">Training Visualization</a>
    </li>
    <li>
      <a href="../../articles/applications.html">Pre-Trained Models</a>
    </li>
    <li>
      <a href="../../articles/faq.html">Frequently Asked Questions</a>
    </li>
    <li>
      <a href="../../articles/why_use_keras.html">Why Use Keras?</a>
    </li>
    <li class="divider">
    </li>
<li class="dropdown-header">Advanced</li>
    <li>
      <a href="../../articles/eager_guide.html">Eager Execution</a>
    </li>
    <li>
      <a href="../../articles/training_callbacks.html">Training Callbacks</a>
    </li>
    <li>
      <a href="../../articles/backend.html">Keras Backend</a>
    </li>
    <li>
      <a href="../../articles/custom_layers.html">Custom Layers</a>
    </li>
    <li>
      <a href="../../articles/custom_models.html">Custom Models</a>
    </li>
    <li>
      <a href="../../articles/saving_serializing.html">Saving and serializing</a>
    </li>
  </ul>
</li>
<li>
  <a href="../../articles/learn.html">Learn</a>
</li>
<li>
  <a href="../../articles/tools.html">Tools</a>
</li>
<li>
  <a href="../../articles/examples/index.html">Examples</a>
</li>
<li>
  <a href="../../reference/index.html">Reference</a>
</li>
<li>
  <a href="../../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/rstudio/keras/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Image classification with Swin Transformers</h1>
                        <h4 data-toc-skip class="author"><a href="https://twitter.com/rishit_dagli" class="external-link">Rishit Dagli</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/swim_transformers.Rmd" class="external-link"><code>vignettes/examples/swim_transformers.Rmd</code></a></small>
      <div class="hidden name"><code>swim_transformers.Rmd</code></div>

    </div>

    
    
<p>This example implements <a href="https://arxiv.org/abs/2103.14030" class="external-link">Swin Transformer: Hierarchical
Vision Transformer using Shifted Windows</a> by Liu et al.Â for image
classification, and demonstrates it on the <a href="https://www.cs.toronto.edu/~kriz/cifar.html" class="external-link">CIFAR-100
dataset</a>.</p>
<p>Swin Transformer (<strong>S</strong>hifted <strong>Win</strong>dow
Transformer) can serve as a general-purpose backbone for computer
vision. Swin Transformer is a hierarchical Transformer whose
representations are computed with <em>shifted windows</em>. The shifted
window scheme brings greater efficiency by limiting self-attention
computation to non-overlapping local windows while also allowing for
cross-window connections. This architecture has the flexibility to model
information at various scales and has a linear computational complexity
with respect to image size.</p>
<p>This example requires TensorFlow 2.5 or higher.</p>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="prepare-the-data">Prepare the data<a class="anchor" aria-label="anchor" href="#prepare-the-data"></a>
</h2>
<p>We load the CIFAR-100 dataset through <code>tf.keras.datasets</code>,
normalize the images, and convert the integer labels to one-hot encoded
vectors.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>num_classes <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>input_shape <span class="op">=</span> (<span class="dv">32</span>, <span class="dv">32</span>, <span class="dv">3</span>)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>(x_train, y_train), (x_test, y_test) <span class="op">=</span> keras.datasets.cifar100.load_data()</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>x_train, x_test <span class="op">=</span> x_train <span class="op">/</span> <span class="fl">255.0</span>, x_test <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>y_train <span class="op">=</span> keras.utils.numerical_utils.to_categorical(y_train, num_classes)</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>y_test <span class="op">=</span> keras.utils.numerical_utils.to_categorical(y_test, num_classes)</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x_train shape: </span><span class="sc">{</span>x_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> - y_train shape: </span><span class="sc">{</span>y_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x_test shape: </span><span class="sc">{</span>x_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> - y_test shape: </span><span class="sc">{</span>y_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">25</span>):</span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>    plt.subplot(<span class="dv">5</span>, <span class="dv">5</span>, i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a>    plt.xticks([])</span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a>    plt.yticks([])</span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a>    plt.grid(<span class="va">False</span>)</span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a>    plt.imshow(x_train[i])</span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a>plt.show()</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="configure-the-hyperparameters">Configure the hyperparameters<a class="anchor" aria-label="anchor" href="#configure-the-hyperparameters"></a>
</h2>
<p>A key parameter to pick is the <code>patch_size</code>, the size of
the input patches. In order to use each pixel as an individual input,
you can set <code>patch_size</code> to <code>(1, 1)</code>. Below, we
take inspiration from the original paper settings for training on
ImageNet-1K, keeping most of the original settings for this example.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>patch_size <span class="op">=</span> (<span class="dv">2</span>, <span class="dv">2</span>)  <span class="co"># 2-by-2 sized patches</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>dropout_rate <span class="op">=</span> <span class="fl">0.03</span>  <span class="co"># Dropout rate</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>num_heads <span class="op">=</span> <span class="dv">8</span>  <span class="co"># Attention heads</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>embed_dim <span class="op">=</span> <span class="dv">64</span>  <span class="co"># Embedding dimension</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>num_mlp <span class="op">=</span> <span class="dv">256</span>  <span class="co"># MLP layer size</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>qkv_bias <span class="op">=</span> <span class="va">True</span>  <span class="co"># Convert embedded patches to query, key, and values with a learnable additive value</span></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>window_size <span class="op">=</span> <span class="dv">2</span>  <span class="co"># Size of attention window</span></span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>shift_size <span class="op">=</span> <span class="dv">1</span>  <span class="co"># Size of shifting window</span></span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>image_dimension <span class="op">=</span> <span class="dv">32</span>  <span class="co"># Initial image size</span></span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>num_patch_x <span class="op">=</span> input_shape[<span class="dv">0</span>] <span class="op">//</span> patch_size[<span class="dv">0</span>]</span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>num_patch_y <span class="op">=</span> input_shape[<span class="dv">1</span>] <span class="op">//</span> patch_size[<span class="dv">1</span>]</span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-3</span></span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a>validation_split <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a>weight_decay <span class="op">=</span> <span class="fl">0.0001</span></span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a>label_smoothing <span class="op">=</span> <span class="fl">0.1</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="helper-functions">Helper functions<a class="anchor" aria-label="anchor" href="#helper-functions"></a>
</h2>
<p>We create two helper functions to help us get a sequence of patches
from the image, merge patches, and apply dropout.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="kw">def</span> window_partition(x, window_size):</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>    _, height, width, channels <span class="op">=</span> x.shape</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>    patch_num_y <span class="op">=</span> height <span class="op">//</span> window_size</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>    patch_num_x <span class="op">=</span> width <span class="op">//</span> window_size</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>    x <span class="op">=</span> tf.reshape(</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>        x,</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>        shape<span class="op">=</span>(</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>            <span class="op">-</span><span class="dv">1</span>,</span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>            patch_num_y,</span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>            window_size,</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>            patch_num_x,</span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>            window_size,</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a>            channels,</span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>        ),</span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a>    )</span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a>    x <span class="op">=</span> tf.transpose(x, (<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">5</span>))</span>
<span id="cb4-17"><a href="#cb4-17" tabindex="-1"></a>    windows <span class="op">=</span> tf.reshape(x, shape<span class="op">=</span>(<span class="op">-</span><span class="dv">1</span>, window_size, window_size, channels))</span>
<span id="cb4-18"><a href="#cb4-18" tabindex="-1"></a>    <span class="cf">return</span> windows</span>
<span id="cb4-19"><a href="#cb4-19" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" tabindex="-1"></a><span class="kw">def</span> window_reverse(windows, window_size, height, width, channels):</span>
<span id="cb4-22"><a href="#cb4-22" tabindex="-1"></a>    patch_num_y <span class="op">=</span> height <span class="op">//</span> window_size</span>
<span id="cb4-23"><a href="#cb4-23" tabindex="-1"></a>    patch_num_x <span class="op">=</span> width <span class="op">//</span> window_size</span>
<span id="cb4-24"><a href="#cb4-24" tabindex="-1"></a>    x <span class="op">=</span> tf.reshape(</span>
<span id="cb4-25"><a href="#cb4-25" tabindex="-1"></a>        windows,</span>
<span id="cb4-26"><a href="#cb4-26" tabindex="-1"></a>        shape<span class="op">=</span>(</span>
<span id="cb4-27"><a href="#cb4-27" tabindex="-1"></a>            <span class="op">-</span><span class="dv">1</span>,</span>
<span id="cb4-28"><a href="#cb4-28" tabindex="-1"></a>            patch_num_y,</span>
<span id="cb4-29"><a href="#cb4-29" tabindex="-1"></a>            patch_num_x,</span>
<span id="cb4-30"><a href="#cb4-30" tabindex="-1"></a>            window_size,</span>
<span id="cb4-31"><a href="#cb4-31" tabindex="-1"></a>            window_size,</span>
<span id="cb4-32"><a href="#cb4-32" tabindex="-1"></a>            channels,</span>
<span id="cb4-33"><a href="#cb4-33" tabindex="-1"></a>        ),</span>
<span id="cb4-34"><a href="#cb4-34" tabindex="-1"></a>    )</span>
<span id="cb4-35"><a href="#cb4-35" tabindex="-1"></a>    x <span class="op">=</span> tf.transpose(x, perm<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">5</span>))</span>
<span id="cb4-36"><a href="#cb4-36" tabindex="-1"></a>    x <span class="op">=</span> tf.reshape(x, shape<span class="op">=</span>(<span class="op">-</span><span class="dv">1</span>, height, width, channels))</span>
<span id="cb4-37"><a href="#cb4-37" tabindex="-1"></a>    <span class="cf">return</span> x</span>
<span id="cb4-38"><a href="#cb4-38" tabindex="-1"></a></span>
<span id="cb4-39"><a href="#cb4-39" tabindex="-1"></a></span>
<span id="cb4-40"><a href="#cb4-40" tabindex="-1"></a><span class="kw">class</span> DropPath(layers.Layer):</span>
<span id="cb4-41"><a href="#cb4-41" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, drop_prob<span class="op">=</span><span class="va">None</span>, <span class="op">**</span>kwargs):</span>
<span id="cb4-42"><a href="#cb4-42" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb4-43"><a href="#cb4-43" tabindex="-1"></a>        <span class="va">self</span>.drop_prob <span class="op">=</span> drop_prob</span>
<span id="cb4-44"><a href="#cb4-44" tabindex="-1"></a></span>
<span id="cb4-45"><a href="#cb4-45" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, x):</span>
<span id="cb4-46"><a href="#cb4-46" tabindex="-1"></a>        input_shape <span class="op">=</span> tf.shape(x)</span>
<span id="cb4-47"><a href="#cb4-47" tabindex="-1"></a>        batch_size <span class="op">=</span> input_shape[<span class="dv">0</span>]</span>
<span id="cb4-48"><a href="#cb4-48" tabindex="-1"></a>        rank <span class="op">=</span> x.shape.rank</span>
<span id="cb4-49"><a href="#cb4-49" tabindex="-1"></a>        shape <span class="op">=</span> (batch_size,) <span class="op">+</span> (<span class="dv">1</span>,) <span class="op">*</span> (rank <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb4-50"><a href="#cb4-50" tabindex="-1"></a>        random_tensor <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.drop_prob) <span class="op">+</span> tf.random.uniform(</span>
<span id="cb4-51"><a href="#cb4-51" tabindex="-1"></a>            shape, dtype<span class="op">=</span>x.dtype</span>
<span id="cb4-52"><a href="#cb4-52" tabindex="-1"></a>        )</span>
<span id="cb4-53"><a href="#cb4-53" tabindex="-1"></a>        path_mask <span class="op">=</span> tf.floor(random_tensor)</span>
<span id="cb4-54"><a href="#cb4-54" tabindex="-1"></a>        output <span class="op">=</span> tf.math.divide(x, <span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.drop_prob) <span class="op">*</span> path_mask</span>
<span id="cb4-55"><a href="#cb4-55" tabindex="-1"></a>        <span class="cf">return</span> output</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="window-based-multi-head-self-attention">Window based multi-head self-attention<a class="anchor" aria-label="anchor" href="#window-based-multi-head-self-attention"></a>
</h2>
<p>Usually Transformers perform global self-attention, where the
relationships between a token and all other tokens are computed. The
global computation leads to quadratic complexity with respect to the
number of tokens. Here, as the <a href="https://arxiv.org/abs/2103.14030" class="external-link">original paper</a> suggests, we
compute self-attention within local windows, in a non-overlapping
manner. Global self-attention leads to quadratic computational
complexity in the number of patches, whereas window-based self-attention
leads to linear complexity and is easily scalable.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="kw">class</span> WindowAttention(layers.Layer):</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>        dim,</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>        window_size,</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>        num_heads,</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>        qkv_bias<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>        dropout_rate<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>        <span class="op">**</span>kwargs,</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>    ):</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>        <span class="va">self</span>.dim <span class="op">=</span> dim</span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>        <span class="va">self</span>.window_size <span class="op">=</span> window_size</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> (dim <span class="op">//</span> num_heads) <span class="op">**</span> <span class="op">-</span><span class="fl">0.5</span></span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>        <span class="va">self</span>.qkv <span class="op">=</span> layers.Dense(dim <span class="op">*</span> <span class="dv">3</span>, use_bias<span class="op">=</span>qkv_bias)</span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> layers.Dropout(dropout_rate)</span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a>        <span class="va">self</span>.proj <span class="op">=</span> layers.Dense(dim)</span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a>        num_window_elements <span class="op">=</span> (<span class="dv">2</span> <span class="op">*</span> <span class="va">self</span>.window_size[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> (</span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a>            <span class="dv">2</span> <span class="op">*</span> <span class="va">self</span>.window_size[<span class="dv">1</span>] <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb5-23"><a href="#cb5-23" tabindex="-1"></a>        )</span>
<span id="cb5-24"><a href="#cb5-24" tabindex="-1"></a>        <span class="va">self</span>.relative_position_bias_table <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb5-25"><a href="#cb5-25" tabindex="-1"></a>            shape<span class="op">=</span>(num_window_elements, <span class="va">self</span>.num_heads),</span>
<span id="cb5-26"><a href="#cb5-26" tabindex="-1"></a>            initializer<span class="op">=</span>tf.initializers.Zeros(),</span>
<span id="cb5-27"><a href="#cb5-27" tabindex="-1"></a>            trainable<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb5-28"><a href="#cb5-28" tabindex="-1"></a>        )</span>
<span id="cb5-29"><a href="#cb5-29" tabindex="-1"></a>        coords_h <span class="op">=</span> np.arange(<span class="va">self</span>.window_size[<span class="dv">0</span>])</span>
<span id="cb5-30"><a href="#cb5-30" tabindex="-1"></a>        coords_w <span class="op">=</span> np.arange(<span class="va">self</span>.window_size[<span class="dv">1</span>])</span>
<span id="cb5-31"><a href="#cb5-31" tabindex="-1"></a>        coords_matrix <span class="op">=</span> np.meshgrid(coords_h, coords_w, indexing<span class="op">=</span><span class="st">"ij"</span>)</span>
<span id="cb5-32"><a href="#cb5-32" tabindex="-1"></a>        coords <span class="op">=</span> np.stack(coords_matrix)</span>
<span id="cb5-33"><a href="#cb5-33" tabindex="-1"></a>        coords_flatten <span class="op">=</span> coords.reshape(<span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb5-34"><a href="#cb5-34" tabindex="-1"></a>        relative_coords <span class="op">=</span> (</span>
<span id="cb5-35"><a href="#cb5-35" tabindex="-1"></a>            coords_flatten[:, :, <span class="va">None</span>] <span class="op">-</span> coords_flatten[:, <span class="va">None</span>, :]</span>
<span id="cb5-36"><a href="#cb5-36" tabindex="-1"></a>        )</span>
<span id="cb5-37"><a href="#cb5-37" tabindex="-1"></a>        relative_coords <span class="op">=</span> relative_coords.transpose([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>])</span>
<span id="cb5-38"><a href="#cb5-38" tabindex="-1"></a>        relative_coords[:, :, <span class="dv">0</span>] <span class="op">+=</span> <span class="va">self</span>.window_size[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb5-39"><a href="#cb5-39" tabindex="-1"></a>        relative_coords[:, :, <span class="dv">1</span>] <span class="op">+=</span> <span class="va">self</span>.window_size[<span class="dv">1</span>] <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb5-40"><a href="#cb5-40" tabindex="-1"></a>        relative_coords[:, :, <span class="dv">0</span>] <span class="op">*=</span> <span class="dv">2</span> <span class="op">*</span> <span class="va">self</span>.window_size[<span class="dv">1</span>] <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb5-41"><a href="#cb5-41" tabindex="-1"></a>        relative_position_index <span class="op">=</span> relative_coords.<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb5-42"><a href="#cb5-42" tabindex="-1"></a></span>
<span id="cb5-43"><a href="#cb5-43" tabindex="-1"></a>        <span class="va">self</span>.relative_position_index <span class="op">=</span> tf.Variable(</span>
<span id="cb5-44"><a href="#cb5-44" tabindex="-1"></a>            initial_value<span class="op">=</span><span class="kw">lambda</span>: tf.convert_to_tensor(relative_position_index),</span>
<span id="cb5-45"><a href="#cb5-45" tabindex="-1"></a>            trainable<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb5-46"><a href="#cb5-46" tabindex="-1"></a>        )</span>
<span id="cb5-47"><a href="#cb5-47" tabindex="-1"></a></span>
<span id="cb5-48"><a href="#cb5-48" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, x, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb5-49"><a href="#cb5-49" tabindex="-1"></a>        _, size, channels <span class="op">=</span> x.shape</span>
<span id="cb5-50"><a href="#cb5-50" tabindex="-1"></a>        head_dim <span class="op">=</span> channels <span class="op">//</span> <span class="va">self</span>.num_heads</span>
<span id="cb5-51"><a href="#cb5-51" tabindex="-1"></a>        x_qkv <span class="op">=</span> <span class="va">self</span>.qkv(x)</span>
<span id="cb5-52"><a href="#cb5-52" tabindex="-1"></a>        x_qkv <span class="op">=</span> tf.reshape(x_qkv, shape<span class="op">=</span>(<span class="op">-</span><span class="dv">1</span>, size, <span class="dv">3</span>, <span class="va">self</span>.num_heads, head_dim))</span>
<span id="cb5-53"><a href="#cb5-53" tabindex="-1"></a>        x_qkv <span class="op">=</span> tf.transpose(x_qkv, perm<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">4</span>))</span>
<span id="cb5-54"><a href="#cb5-54" tabindex="-1"></a>        q, k, v <span class="op">=</span> x_qkv[<span class="dv">0</span>], x_qkv[<span class="dv">1</span>], x_qkv[<span class="dv">2</span>]</span>
<span id="cb5-55"><a href="#cb5-55" tabindex="-1"></a>        q <span class="op">=</span> q <span class="op">*</span> <span class="va">self</span>.scale</span>
<span id="cb5-56"><a href="#cb5-56" tabindex="-1"></a>        k <span class="op">=</span> tf.transpose(k, perm<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">2</span>))</span>
<span id="cb5-57"><a href="#cb5-57" tabindex="-1"></a>        attn <span class="op">=</span> q <span class="op">@</span> k</span>
<span id="cb5-58"><a href="#cb5-58" tabindex="-1"></a></span>
<span id="cb5-59"><a href="#cb5-59" tabindex="-1"></a>        num_window_elements <span class="op">=</span> <span class="va">self</span>.window_size[<span class="dv">0</span>] <span class="op">*</span> <span class="va">self</span>.window_size[<span class="dv">1</span>]</span>
<span id="cb5-60"><a href="#cb5-60" tabindex="-1"></a>        relative_position_index_flat <span class="op">=</span> tf.reshape(</span>
<span id="cb5-61"><a href="#cb5-61" tabindex="-1"></a>            <span class="va">self</span>.relative_position_index, shape<span class="op">=</span>(<span class="op">-</span><span class="dv">1</span>,)</span>
<span id="cb5-62"><a href="#cb5-62" tabindex="-1"></a>        )</span>
<span id="cb5-63"><a href="#cb5-63" tabindex="-1"></a>        relative_position_bias <span class="op">=</span> tf.gather(</span>
<span id="cb5-64"><a href="#cb5-64" tabindex="-1"></a>            <span class="va">self</span>.relative_position_bias_table, relative_position_index_flat</span>
<span id="cb5-65"><a href="#cb5-65" tabindex="-1"></a>        )</span>
<span id="cb5-66"><a href="#cb5-66" tabindex="-1"></a>        relative_position_bias <span class="op">=</span> tf.reshape(</span>
<span id="cb5-67"><a href="#cb5-67" tabindex="-1"></a>            relative_position_bias,</span>
<span id="cb5-68"><a href="#cb5-68" tabindex="-1"></a>            shape<span class="op">=</span>(num_window_elements, num_window_elements, <span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb5-69"><a href="#cb5-69" tabindex="-1"></a>        )</span>
<span id="cb5-70"><a href="#cb5-70" tabindex="-1"></a>        relative_position_bias <span class="op">=</span> tf.transpose(</span>
<span id="cb5-71"><a href="#cb5-71" tabindex="-1"></a>            relative_position_bias, perm<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb5-72"><a href="#cb5-72" tabindex="-1"></a>        )</span>
<span id="cb5-73"><a href="#cb5-73" tabindex="-1"></a>        attn <span class="op">=</span> attn <span class="op">+</span> tf.expand_dims(relative_position_bias, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-74"><a href="#cb5-74" tabindex="-1"></a></span>
<span id="cb5-75"><a href="#cb5-75" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb5-76"><a href="#cb5-76" tabindex="-1"></a>            nW <span class="op">=</span> mask.shape[<span class="dv">0</span>]</span>
<span id="cb5-77"><a href="#cb5-77" tabindex="-1"></a>            mask_float <span class="op">=</span> tf.cast(</span>
<span id="cb5-78"><a href="#cb5-78" tabindex="-1"></a>                tf.expand_dims(tf.expand_dims(mask, axis<span class="op">=</span><span class="dv">1</span>), axis<span class="op">=</span><span class="dv">0</span>), tf.float32</span>
<span id="cb5-79"><a href="#cb5-79" tabindex="-1"></a>            )</span>
<span id="cb5-80"><a href="#cb5-80" tabindex="-1"></a>            attn <span class="op">=</span> (</span>
<span id="cb5-81"><a href="#cb5-81" tabindex="-1"></a>                tf.reshape(attn, shape<span class="op">=</span>(<span class="op">-</span><span class="dv">1</span>, nW, <span class="va">self</span>.num_heads, size, size))</span>
<span id="cb5-82"><a href="#cb5-82" tabindex="-1"></a>                <span class="op">+</span> mask_float</span>
<span id="cb5-83"><a href="#cb5-83" tabindex="-1"></a>            )</span>
<span id="cb5-84"><a href="#cb5-84" tabindex="-1"></a>            attn <span class="op">=</span> tf.reshape(attn, shape<span class="op">=</span>(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.num_heads, size, size))</span>
<span id="cb5-85"><a href="#cb5-85" tabindex="-1"></a>            attn <span class="op">=</span> keras.activations.softmax(attn, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb5-86"><a href="#cb5-86" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb5-87"><a href="#cb5-87" tabindex="-1"></a>            attn <span class="op">=</span> keras.activations.softmax(attn, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb5-88"><a href="#cb5-88" tabindex="-1"></a>        attn <span class="op">=</span> <span class="va">self</span>.dropout(attn)</span>
<span id="cb5-89"><a href="#cb5-89" tabindex="-1"></a></span>
<span id="cb5-90"><a href="#cb5-90" tabindex="-1"></a>        x_qkv <span class="op">=</span> attn <span class="op">@</span> v</span>
<span id="cb5-91"><a href="#cb5-91" tabindex="-1"></a>        x_qkv <span class="op">=</span> tf.transpose(x_qkv, perm<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb5-92"><a href="#cb5-92" tabindex="-1"></a>        x_qkv <span class="op">=</span> tf.reshape(x_qkv, shape<span class="op">=</span>(<span class="op">-</span><span class="dv">1</span>, size, channels))</span>
<span id="cb5-93"><a href="#cb5-93" tabindex="-1"></a>        x_qkv <span class="op">=</span> <span class="va">self</span>.proj(x_qkv)</span>
<span id="cb5-94"><a href="#cb5-94" tabindex="-1"></a>        x_qkv <span class="op">=</span> <span class="va">self</span>.dropout(x_qkv)</span>
<span id="cb5-95"><a href="#cb5-95" tabindex="-1"></a>        <span class="cf">return</span> x_qkv</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="the-complete-swin-transformer-model">The complete Swin Transformer model<a class="anchor" aria-label="anchor" href="#the-complete-swin-transformer-model"></a>
</h2>
<p>Finally, we put together the complete Swin Transformer by replacing
the standard multi-head attention (MHA) with shifted windows attention.
As suggested in the original paper, we create a model comprising of a
shifted window-based MHA layer, followed by a 2-layer MLP with GELU
nonlinearity in between, applying <code>LayerNormalization</code> before
each MSA layer and each MLP, and a residual connection after each of
these layers.</p>
<p>Notice that we only create a simple MLP with 2 Dense and 2 Dropout
layers. Often you will see models using ResNet-50 as the MLP which is
quite standard in the literature. However in this paper the authors use
a 2-layer MLP with GELU nonlinearity in between.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="kw">class</span> SwinTransformer(layers.Layer):</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>        dim,</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>        num_patch,</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>        num_heads,</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>        window_size<span class="op">=</span><span class="dv">7</span>,</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>        shift_size<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>        num_mlp<span class="op">=</span><span class="dv">1024</span>,</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>        qkv_bias<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>        dropout_rate<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>        <span class="op">**</span>kwargs,</span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a>    ):</span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a>        <span class="va">self</span>.dim <span class="op">=</span> dim  <span class="co"># number of input dimensions</span></span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a>        <span class="va">self</span>.num_patch <span class="op">=</span> num_patch  <span class="co"># number of embedded patches</span></span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads  <span class="co"># number of attention heads</span></span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a>        <span class="va">self</span>.window_size <span class="op">=</span> window_size  <span class="co"># size of window</span></span>
<span id="cb6-20"><a href="#cb6-20" tabindex="-1"></a>        <span class="va">self</span>.shift_size <span class="op">=</span> shift_size  <span class="co"># size of window shift</span></span>
<span id="cb6-21"><a href="#cb6-21" tabindex="-1"></a>        <span class="va">self</span>.num_mlp <span class="op">=</span> num_mlp  <span class="co"># number of MLP nodes</span></span>
<span id="cb6-22"><a href="#cb6-22" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" tabindex="-1"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span><span class="fl">1e-5</span>)</span>
<span id="cb6-24"><a href="#cb6-24" tabindex="-1"></a>        <span class="va">self</span>.attn <span class="op">=</span> WindowAttention(</span>
<span id="cb6-25"><a href="#cb6-25" tabindex="-1"></a>            dim,</span>
<span id="cb6-26"><a href="#cb6-26" tabindex="-1"></a>            window_size<span class="op">=</span>(<span class="va">self</span>.window_size, <span class="va">self</span>.window_size),</span>
<span id="cb6-27"><a href="#cb6-27" tabindex="-1"></a>            num_heads<span class="op">=</span>num_heads,</span>
<span id="cb6-28"><a href="#cb6-28" tabindex="-1"></a>            qkv_bias<span class="op">=</span>qkv_bias,</span>
<span id="cb6-29"><a href="#cb6-29" tabindex="-1"></a>            dropout_rate<span class="op">=</span>dropout_rate,</span>
<span id="cb6-30"><a href="#cb6-30" tabindex="-1"></a>        )</span>
<span id="cb6-31"><a href="#cb6-31" tabindex="-1"></a>        <span class="va">self</span>.drop_path <span class="op">=</span> DropPath(dropout_rate)</span>
<span id="cb6-32"><a href="#cb6-32" tabindex="-1"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span><span class="fl">1e-5</span>)</span>
<span id="cb6-33"><a href="#cb6-33" tabindex="-1"></a></span>
<span id="cb6-34"><a href="#cb6-34" tabindex="-1"></a>        <span class="va">self</span>.mlp <span class="op">=</span> keras.Sequential(</span>
<span id="cb6-35"><a href="#cb6-35" tabindex="-1"></a>            [</span>
<span id="cb6-36"><a href="#cb6-36" tabindex="-1"></a>                layers.Dense(num_mlp),</span>
<span id="cb6-37"><a href="#cb6-37" tabindex="-1"></a>                layers.Activation(keras.activations.gelu),</span>
<span id="cb6-38"><a href="#cb6-38" tabindex="-1"></a>                layers.Dropout(dropout_rate),</span>
<span id="cb6-39"><a href="#cb6-39" tabindex="-1"></a>                layers.Dense(dim),</span>
<span id="cb6-40"><a href="#cb6-40" tabindex="-1"></a>                layers.Dropout(dropout_rate),</span>
<span id="cb6-41"><a href="#cb6-41" tabindex="-1"></a>            ]</span>
<span id="cb6-42"><a href="#cb6-42" tabindex="-1"></a>        )</span>
<span id="cb6-43"><a href="#cb6-43" tabindex="-1"></a></span>
<span id="cb6-44"><a href="#cb6-44" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">min</span>(<span class="va">self</span>.num_patch) <span class="op">&lt;</span> <span class="va">self</span>.window_size:</span>
<span id="cb6-45"><a href="#cb6-45" tabindex="-1"></a>            <span class="va">self</span>.shift_size <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-46"><a href="#cb6-46" tabindex="-1"></a>            <span class="va">self</span>.window_size <span class="op">=</span> <span class="bu">min</span>(<span class="va">self</span>.num_patch)</span>
<span id="cb6-47"><a href="#cb6-47" tabindex="-1"></a></span>
<span id="cb6-48"><a href="#cb6-48" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb6-49"><a href="#cb6-49" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.shift_size <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb6-50"><a href="#cb6-50" tabindex="-1"></a>            <span class="va">self</span>.attn_mask <span class="op">=</span> <span class="va">None</span></span>
<span id="cb6-51"><a href="#cb6-51" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb6-52"><a href="#cb6-52" tabindex="-1"></a>            height, width <span class="op">=</span> <span class="va">self</span>.num_patch</span>
<span id="cb6-53"><a href="#cb6-53" tabindex="-1"></a>            h_slices <span class="op">=</span> (</span>
<span id="cb6-54"><a href="#cb6-54" tabindex="-1"></a>                <span class="bu">slice</span>(<span class="dv">0</span>, <span class="op">-</span><span class="va">self</span>.window_size),</span>
<span id="cb6-55"><a href="#cb6-55" tabindex="-1"></a>                <span class="bu">slice</span>(<span class="op">-</span><span class="va">self</span>.window_size, <span class="op">-</span><span class="va">self</span>.shift_size),</span>
<span id="cb6-56"><a href="#cb6-56" tabindex="-1"></a>                <span class="bu">slice</span>(<span class="op">-</span><span class="va">self</span>.shift_size, <span class="va">None</span>),</span>
<span id="cb6-57"><a href="#cb6-57" tabindex="-1"></a>            )</span>
<span id="cb6-58"><a href="#cb6-58" tabindex="-1"></a>            w_slices <span class="op">=</span> (</span>
<span id="cb6-59"><a href="#cb6-59" tabindex="-1"></a>                <span class="bu">slice</span>(<span class="dv">0</span>, <span class="op">-</span><span class="va">self</span>.window_size),</span>
<span id="cb6-60"><a href="#cb6-60" tabindex="-1"></a>                <span class="bu">slice</span>(<span class="op">-</span><span class="va">self</span>.window_size, <span class="op">-</span><span class="va">self</span>.shift_size),</span>
<span id="cb6-61"><a href="#cb6-61" tabindex="-1"></a>                <span class="bu">slice</span>(<span class="op">-</span><span class="va">self</span>.shift_size, <span class="va">None</span>),</span>
<span id="cb6-62"><a href="#cb6-62" tabindex="-1"></a>            )</span>
<span id="cb6-63"><a href="#cb6-63" tabindex="-1"></a>            mask_array <span class="op">=</span> np.zeros((<span class="dv">1</span>, height, width, <span class="dv">1</span>))</span>
<span id="cb6-64"><a href="#cb6-64" tabindex="-1"></a>            count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-65"><a href="#cb6-65" tabindex="-1"></a>            <span class="cf">for</span> h <span class="kw">in</span> h_slices:</span>
<span id="cb6-66"><a href="#cb6-66" tabindex="-1"></a>                <span class="cf">for</span> w <span class="kw">in</span> w_slices:</span>
<span id="cb6-67"><a href="#cb6-67" tabindex="-1"></a>                    mask_array[:, h, w, :] <span class="op">=</span> count</span>
<span id="cb6-68"><a href="#cb6-68" tabindex="-1"></a>                    count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb6-69"><a href="#cb6-69" tabindex="-1"></a>            mask_array <span class="op">=</span> tf.convert_to_tensor(mask_array)</span>
<span id="cb6-70"><a href="#cb6-70" tabindex="-1"></a></span>
<span id="cb6-71"><a href="#cb6-71" tabindex="-1"></a>            <span class="co"># mask array to windows</span></span>
<span id="cb6-72"><a href="#cb6-72" tabindex="-1"></a>            mask_windows <span class="op">=</span> window_partition(mask_array, <span class="va">self</span>.window_size)</span>
<span id="cb6-73"><a href="#cb6-73" tabindex="-1"></a>            mask_windows <span class="op">=</span> tf.reshape(</span>
<span id="cb6-74"><a href="#cb6-74" tabindex="-1"></a>                mask_windows, shape<span class="op">=</span>[<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.window_size <span class="op">*</span> <span class="va">self</span>.window_size]</span>
<span id="cb6-75"><a href="#cb6-75" tabindex="-1"></a>            )</span>
<span id="cb6-76"><a href="#cb6-76" tabindex="-1"></a>            attn_mask <span class="op">=</span> tf.expand_dims(mask_windows, axis<span class="op">=</span><span class="dv">1</span>) <span class="op">-</span> tf.expand_dims(</span>
<span id="cb6-77"><a href="#cb6-77" tabindex="-1"></a>                mask_windows, axis<span class="op">=</span><span class="dv">2</span></span>
<span id="cb6-78"><a href="#cb6-78" tabindex="-1"></a>            )</span>
<span id="cb6-79"><a href="#cb6-79" tabindex="-1"></a>            attn_mask <span class="op">=</span> tf.where(attn_mask <span class="op">!=</span> <span class="dv">0</span>, <span class="op">-</span><span class="fl">100.0</span>, attn_mask)</span>
<span id="cb6-80"><a href="#cb6-80" tabindex="-1"></a>            attn_mask <span class="op">=</span> tf.where(attn_mask <span class="op">==</span> <span class="dv">0</span>, <span class="fl">0.0</span>, attn_mask)</span>
<span id="cb6-81"><a href="#cb6-81" tabindex="-1"></a>            <span class="va">self</span>.attn_mask <span class="op">=</span> tf.Variable(</span>
<span id="cb6-82"><a href="#cb6-82" tabindex="-1"></a>                initial_value<span class="op">=</span>attn_mask, trainable<span class="op">=</span><span class="va">False</span></span>
<span id="cb6-83"><a href="#cb6-83" tabindex="-1"></a>            )</span>
<span id="cb6-84"><a href="#cb6-84" tabindex="-1"></a></span>
<span id="cb6-85"><a href="#cb6-85" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, x):</span>
<span id="cb6-86"><a href="#cb6-86" tabindex="-1"></a>        height, width <span class="op">=</span> <span class="va">self</span>.num_patch</span>
<span id="cb6-87"><a href="#cb6-87" tabindex="-1"></a>        _, num_patches_before, channels <span class="op">=</span> x.shape</span>
<span id="cb6-88"><a href="#cb6-88" tabindex="-1"></a>        x_skip <span class="op">=</span> x</span>
<span id="cb6-89"><a href="#cb6-89" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm1(x)</span>
<span id="cb6-90"><a href="#cb6-90" tabindex="-1"></a>        x <span class="op">=</span> tf.reshape(x, shape<span class="op">=</span>(<span class="op">-</span><span class="dv">1</span>, height, width, channels))</span>
<span id="cb6-91"><a href="#cb6-91" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.shift_size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb6-92"><a href="#cb6-92" tabindex="-1"></a>            shifted_x <span class="op">=</span> tf.roll(</span>
<span id="cb6-93"><a href="#cb6-93" tabindex="-1"></a>                x, shift<span class="op">=</span>[<span class="op">-</span><span class="va">self</span>.shift_size, <span class="op">-</span><span class="va">self</span>.shift_size], axis<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">2</span>]</span>
<span id="cb6-94"><a href="#cb6-94" tabindex="-1"></a>            )</span>
<span id="cb6-95"><a href="#cb6-95" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb6-96"><a href="#cb6-96" tabindex="-1"></a>            shifted_x <span class="op">=</span> x</span>
<span id="cb6-97"><a href="#cb6-97" tabindex="-1"></a></span>
<span id="cb6-98"><a href="#cb6-98" tabindex="-1"></a>        x_windows <span class="op">=</span> window_partition(shifted_x, <span class="va">self</span>.window_size)</span>
<span id="cb6-99"><a href="#cb6-99" tabindex="-1"></a>        x_windows <span class="op">=</span> tf.reshape(</span>
<span id="cb6-100"><a href="#cb6-100" tabindex="-1"></a>            x_windows, shape<span class="op">=</span>(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.window_size <span class="op">*</span> <span class="va">self</span>.window_size, channels)</span>
<span id="cb6-101"><a href="#cb6-101" tabindex="-1"></a>        )</span>
<span id="cb6-102"><a href="#cb6-102" tabindex="-1"></a>        attn_windows <span class="op">=</span> <span class="va">self</span>.attn(x_windows, mask<span class="op">=</span><span class="va">self</span>.attn_mask)</span>
<span id="cb6-103"><a href="#cb6-103" tabindex="-1"></a></span>
<span id="cb6-104"><a href="#cb6-104" tabindex="-1"></a>        attn_windows <span class="op">=</span> tf.reshape(</span>
<span id="cb6-105"><a href="#cb6-105" tabindex="-1"></a>            attn_windows,</span>
<span id="cb6-106"><a href="#cb6-106" tabindex="-1"></a>            shape<span class="op">=</span>(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.window_size, <span class="va">self</span>.window_size, channels),</span>
<span id="cb6-107"><a href="#cb6-107" tabindex="-1"></a>        )</span>
<span id="cb6-108"><a href="#cb6-108" tabindex="-1"></a>        shifted_x <span class="op">=</span> window_reverse(</span>
<span id="cb6-109"><a href="#cb6-109" tabindex="-1"></a>            attn_windows, <span class="va">self</span>.window_size, height, width, channels</span>
<span id="cb6-110"><a href="#cb6-110" tabindex="-1"></a>        )</span>
<span id="cb6-111"><a href="#cb6-111" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.shift_size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb6-112"><a href="#cb6-112" tabindex="-1"></a>            x <span class="op">=</span> tf.roll(</span>
<span id="cb6-113"><a href="#cb6-113" tabindex="-1"></a>                shifted_x, shift<span class="op">=</span>[<span class="va">self</span>.shift_size, <span class="va">self</span>.shift_size], axis<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">2</span>]</span>
<span id="cb6-114"><a href="#cb6-114" tabindex="-1"></a>            )</span>
<span id="cb6-115"><a href="#cb6-115" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb6-116"><a href="#cb6-116" tabindex="-1"></a>            x <span class="op">=</span> shifted_x</span>
<span id="cb6-117"><a href="#cb6-117" tabindex="-1"></a></span>
<span id="cb6-118"><a href="#cb6-118" tabindex="-1"></a>        x <span class="op">=</span> tf.reshape(x, shape<span class="op">=</span>(<span class="op">-</span><span class="dv">1</span>, height <span class="op">*</span> width, channels))</span>
<span id="cb6-119"><a href="#cb6-119" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.drop_path(x)</span>
<span id="cb6-120"><a href="#cb6-120" tabindex="-1"></a>        x <span class="op">=</span> x_skip <span class="op">+</span> x</span>
<span id="cb6-121"><a href="#cb6-121" tabindex="-1"></a>        x_skip <span class="op">=</span> x</span>
<span id="cb6-122"><a href="#cb6-122" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm2(x)</span>
<span id="cb6-123"><a href="#cb6-123" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.mlp(x)</span>
<span id="cb6-124"><a href="#cb6-124" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.drop_path(x)</span>
<span id="cb6-125"><a href="#cb6-125" tabindex="-1"></a>        x <span class="op">=</span> x_skip <span class="op">+</span> x</span>
<span id="cb6-126"><a href="#cb6-126" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="model-training-and-evaluation">Model training and evaluation<a class="anchor" aria-label="anchor" href="#model-training-and-evaluation"></a>
</h2>
<div class="section level3">
<h3 id="extract-and-embed-patches">Extract and embed patches<a class="anchor" aria-label="anchor" href="#extract-and-embed-patches"></a>
</h3>
<p>We first create 3 layers to help us extract, embed and merge patches
from the images on top of which we will later use the Swin Transformer
class we built.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="kw">class</span> PatchExtract(layers.Layer):</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, patch_size, <span class="op">**</span>kwargs):</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>        <span class="va">self</span>.patch_size_x <span class="op">=</span> patch_size[<span class="dv">0</span>]</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>        <span class="va">self</span>.patch_size_y <span class="op">=</span> patch_size[<span class="dv">0</span>]</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, images):</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>        batch_size <span class="op">=</span> tf.shape(images)[<span class="dv">0</span>]</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>        patches <span class="op">=</span> tf.image.extract_patches(</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>            images<span class="op">=</span>images,</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>            sizes<span class="op">=</span>(<span class="dv">1</span>, <span class="va">self</span>.patch_size_x, <span class="va">self</span>.patch_size_y, <span class="dv">1</span>),</span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>            strides<span class="op">=</span>(<span class="dv">1</span>, <span class="va">self</span>.patch_size_x, <span class="va">self</span>.patch_size_y, <span class="dv">1</span>),</span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a>            rates<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a>            padding<span class="op">=</span><span class="st">"VALID"</span>,</span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a>        )</span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a>        patch_dim <span class="op">=</span> patches.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a>        patch_num <span class="op">=</span> patches.shape[<span class="dv">1</span>]</span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a>        <span class="cf">return</span> tf.reshape(</span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a>            patches, (batch_size, patch_num <span class="op">*</span> patch_num, patch_dim)</span>
<span id="cb7-20"><a href="#cb7-20" tabindex="-1"></a>        )</span>
<span id="cb7-21"><a href="#cb7-21" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" tabindex="-1"></a></span>
<span id="cb7-23"><a href="#cb7-23" tabindex="-1"></a><span class="kw">class</span> PatchEmbedding(layers.Layer):</span>
<span id="cb7-24"><a href="#cb7-24" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_patch, embed_dim, <span class="op">**</span>kwargs):</span>
<span id="cb7-25"><a href="#cb7-25" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb7-26"><a href="#cb7-26" tabindex="-1"></a>        <span class="va">self</span>.num_patch <span class="op">=</span> num_patch</span>
<span id="cb7-27"><a href="#cb7-27" tabindex="-1"></a>        <span class="va">self</span>.proj <span class="op">=</span> layers.Dense(embed_dim)</span>
<span id="cb7-28"><a href="#cb7-28" tabindex="-1"></a>        <span class="va">self</span>.pos_embed <span class="op">=</span> layers.Embedding(</span>
<span id="cb7-29"><a href="#cb7-29" tabindex="-1"></a>            input_dim<span class="op">=</span>num_patch, output_dim<span class="op">=</span>embed_dim</span>
<span id="cb7-30"><a href="#cb7-30" tabindex="-1"></a>        )</span>
<span id="cb7-31"><a href="#cb7-31" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, patch):</span>
<span id="cb7-33"><a href="#cb7-33" tabindex="-1"></a>        pos <span class="op">=</span> tf.<span class="bu">range</span>(start<span class="op">=</span><span class="dv">0</span>, limit<span class="op">=</span><span class="va">self</span>.num_patch, delta<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-34"><a href="#cb7-34" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.proj(patch) <span class="op">+</span> <span class="va">self</span>.pos_embed(pos)</span>
<span id="cb7-35"><a href="#cb7-35" tabindex="-1"></a></span>
<span id="cb7-36"><a href="#cb7-36" tabindex="-1"></a></span>
<span id="cb7-37"><a href="#cb7-37" tabindex="-1"></a><span class="kw">class</span> PatchMerging(keras.layers.Layer):</span>
<span id="cb7-38"><a href="#cb7-38" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_patch, embed_dim):</span>
<span id="cb7-39"><a href="#cb7-39" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-40"><a href="#cb7-40" tabindex="-1"></a>        <span class="va">self</span>.num_patch <span class="op">=</span> num_patch</span>
<span id="cb7-41"><a href="#cb7-41" tabindex="-1"></a>        <span class="va">self</span>.embed_dim <span class="op">=</span> embed_dim</span>
<span id="cb7-42"><a href="#cb7-42" tabindex="-1"></a>        <span class="va">self</span>.linear_trans <span class="op">=</span> layers.Dense(<span class="dv">2</span> <span class="op">*</span> embed_dim, use_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-43"><a href="#cb7-43" tabindex="-1"></a></span>
<span id="cb7-44"><a href="#cb7-44" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, x):</span>
<span id="cb7-45"><a href="#cb7-45" tabindex="-1"></a>        height, width <span class="op">=</span> <span class="va">self</span>.num_patch</span>
<span id="cb7-46"><a href="#cb7-46" tabindex="-1"></a>        _, _, C <span class="op">=</span> x.shape</span>
<span id="cb7-47"><a href="#cb7-47" tabindex="-1"></a>        x <span class="op">=</span> tf.reshape(x, shape<span class="op">=</span>(<span class="op">-</span><span class="dv">1</span>, height, width, C))</span>
<span id="cb7-48"><a href="#cb7-48" tabindex="-1"></a>        x0 <span class="op">=</span> x[:, <span class="dv">0</span>::<span class="dv">2</span>, <span class="dv">0</span>::<span class="dv">2</span>, :]</span>
<span id="cb7-49"><a href="#cb7-49" tabindex="-1"></a>        x1 <span class="op">=</span> x[:, <span class="dv">1</span>::<span class="dv">2</span>, <span class="dv">0</span>::<span class="dv">2</span>, :]</span>
<span id="cb7-50"><a href="#cb7-50" tabindex="-1"></a>        x2 <span class="op">=</span> x[:, <span class="dv">0</span>::<span class="dv">2</span>, <span class="dv">1</span>::<span class="dv">2</span>, :]</span>
<span id="cb7-51"><a href="#cb7-51" tabindex="-1"></a>        x3 <span class="op">=</span> x[:, <span class="dv">1</span>::<span class="dv">2</span>, <span class="dv">1</span>::<span class="dv">2</span>, :]</span>
<span id="cb7-52"><a href="#cb7-52" tabindex="-1"></a>        x <span class="op">=</span> tf.concat((x0, x1, x2, x3), axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb7-53"><a href="#cb7-53" tabindex="-1"></a>        x <span class="op">=</span> tf.reshape(x, shape<span class="op">=</span>(<span class="op">-</span><span class="dv">1</span>, (height <span class="op">//</span> <span class="dv">2</span>) <span class="op">*</span> (width <span class="op">//</span> <span class="dv">2</span>), <span class="dv">4</span> <span class="op">*</span> C))</span>
<span id="cb7-54"><a href="#cb7-54" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.linear_trans(x)</span></code></pre></div>
</div>
<div class="section level3">
<h3 id="build-the-model">Build the model<a class="anchor" aria-label="anchor" href="#build-the-model"></a>
</h3>
<p>We put together the Swin Transformer model.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> layers.Input(input_shape)</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>x <span class="op">=</span> layers.RandomCrop(image_dimension, image_dimension)(<span class="bu">input</span>)</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>x <span class="op">=</span> layers.RandomFlip(<span class="st">"horizontal"</span>)(x)</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>x <span class="op">=</span> PatchExtract(patch_size)(x)</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>x <span class="op">=</span> PatchEmbedding(num_patch_x <span class="op">*</span> num_patch_y, embed_dim)(x)</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>x <span class="op">=</span> SwinTransformer(</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>    dim<span class="op">=</span>embed_dim,</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>    num_patch<span class="op">=</span>(num_patch_x, num_patch_y),</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>    num_heads<span class="op">=</span>num_heads,</span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>    window_size<span class="op">=</span>window_size,</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>    shift_size<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>    num_mlp<span class="op">=</span>num_mlp,</span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>    qkv_bias<span class="op">=</span>qkv_bias,</span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>    dropout_rate<span class="op">=</span>dropout_rate,</span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a>)(x)</span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a>x <span class="op">=</span> SwinTransformer(</span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a>    dim<span class="op">=</span>embed_dim,</span>
<span id="cb8-18"><a href="#cb8-18" tabindex="-1"></a>    num_patch<span class="op">=</span>(num_patch_x, num_patch_y),</span>
<span id="cb8-19"><a href="#cb8-19" tabindex="-1"></a>    num_heads<span class="op">=</span>num_heads,</span>
<span id="cb8-20"><a href="#cb8-20" tabindex="-1"></a>    window_size<span class="op">=</span>window_size,</span>
<span id="cb8-21"><a href="#cb8-21" tabindex="-1"></a>    shift_size<span class="op">=</span>shift_size,</span>
<span id="cb8-22"><a href="#cb8-22" tabindex="-1"></a>    num_mlp<span class="op">=</span>num_mlp,</span>
<span id="cb8-23"><a href="#cb8-23" tabindex="-1"></a>    qkv_bias<span class="op">=</span>qkv_bias,</span>
<span id="cb8-24"><a href="#cb8-24" tabindex="-1"></a>    dropout_rate<span class="op">=</span>dropout_rate,</span>
<span id="cb8-25"><a href="#cb8-25" tabindex="-1"></a>)(x)</span>
<span id="cb8-26"><a href="#cb8-26" tabindex="-1"></a>x <span class="op">=</span> PatchMerging((num_patch_x, num_patch_y), embed_dim<span class="op">=</span>embed_dim)(x)</span>
<span id="cb8-27"><a href="#cb8-27" tabindex="-1"></a>x <span class="op">=</span> layers.GlobalAveragePooling1D()(x)</span>
<span id="cb8-28"><a href="#cb8-28" tabindex="-1"></a>output <span class="op">=</span> layers.Dense(num_classes, activation<span class="op">=</span><span class="st">"softmax"</span>)(x)</span></code></pre></div>
</div>
<div class="section level3">
<h3 id="train-on-cifar-100">Train on CIFAR-100<a class="anchor" aria-label="anchor" href="#train-on-cifar-100"></a>
</h3>
<p>We train the model on CIFAR-100. Here, we only train the model for 40
epochs to keep the training time short in this example. In practice, you
should train for 150 epochs to reach convergence.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>model <span class="op">=</span> keras.Model(<span class="bu">input</span>, output)</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>    loss<span class="op">=</span>keras.losses.CategoricalCrossentropy(label_smoothing<span class="op">=</span>label_smoothing),</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>    optimizer<span class="op">=</span>keras.optimizers.AdamW(</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>        learning_rate<span class="op">=</span>learning_rate, weight_decay<span class="op">=</span>weight_decay</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>    ),</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>    metrics<span class="op">=</span>[</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>        keras.metrics.CategoricalAccuracy(name<span class="op">=</span><span class="st">"accuracy"</span>),</span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a>        keras.metrics.TopKCategoricalAccuracy(<span class="dv">5</span>, name<span class="op">=</span><span class="st">"top-5-accuracy"</span>),</span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a>    ],</span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a>)</span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" tabindex="-1"></a>history <span class="op">=</span> model.fit(</span>
<span id="cb9-14"><a href="#cb9-14" tabindex="-1"></a>    x_train,</span>
<span id="cb9-15"><a href="#cb9-15" tabindex="-1"></a>    y_train,</span>
<span id="cb9-16"><a href="#cb9-16" tabindex="-1"></a>    batch_size<span class="op">=</span>batch_size,</span>
<span id="cb9-17"><a href="#cb9-17" tabindex="-1"></a>    epochs<span class="op">=</span>num_epochs,</span>
<span id="cb9-18"><a href="#cb9-18" tabindex="-1"></a>    validation_split<span class="op">=</span>validation_split,</span>
<span id="cb9-19"><a href="#cb9-19" tabindex="-1"></a>)</span></code></pre></div>
<p>Letâ€™s visualize the training progress of the model.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>plt.plot(history.history[<span class="st">"loss"</span>], label<span class="op">=</span><span class="st">"train_loss"</span>)</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>plt.plot(history.history[<span class="st">"val_loss"</span>], label<span class="op">=</span><span class="st">"val_loss"</span>)</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>plt.xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>plt.ylabel(<span class="st">"Loss"</span>)</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>plt.title(<span class="st">"Train and Validation Losses Over Epochs"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>plt.legend()</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>plt.grid()</span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p>Letâ€™s display the final results of the training on CIFAR-100.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>loss, accuracy, top_5_accuracy <span class="op">=</span> model.evaluate(x_test, y_test)</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test loss: </span><span class="sc">{</span><span class="bu">round</span>(loss, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test accuracy: </span><span class="sc">{</span><span class="bu">round</span>(accuracy <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">%"</span>)</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test top 5 accuracy: </span><span class="sc">{</span><span class="bu">round</span>(top_5_accuracy <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">%"</span>)</span></code></pre></div>
<p>The Swin Transformer model we just trained has just 152K parameters,
and it gets us to ~75% test top-5 accuracy within just 40 epochs without
any signs of overfitting as well as seen in above graph. This means we
can train this network for longer (perhaps with a bit more
regularization) and obtain even better performance. This performance can
further be improved by additional techniques like cosine decay learning
rate schedule, other data augmentation techniques. While experimenting,
I tried training the model for 150 epochs with a slightly higher dropout
and greater embedding dimensions which pushes the performance to ~72%
test accuracy on CIFAR-100 as you can see in the screenshot.</p>
<div class="float">
<img src="https://i.imgur.com/9vnQesZ.png" alt="Results of training for longer"><div class="figcaption">Results of training for longer</div>
</div>
<p>The authors present a top-1 accuracy of 87.3% on ImageNet. The
authors also present a number of experiments to study how input sizes,
optimizers etc. affect the final performance of this model. The authors
further present using this model for object detection, semantic
segmentation and instance segmentation as well and report competitive
results for these. You are strongly advised to also check out the <a href="https://arxiv.org/abs/2103.14030" class="external-link">original paper</a>.</p>
<p>This example takes inspiration from the official <a href="https://github.com/microsoft/Swin-Transformer" class="external-link">PyTorch</a> and <a href="https://github.com/VcampSoldiers/Swin-Transformer-Tensorflow" class="external-link">TensorFlow</a>
implementations.</p>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, FranÃ§ois Chollet, RStudio, Google.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
