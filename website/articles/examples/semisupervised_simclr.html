<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Contrastive pretraining with SimCLR for semi-supervised image classification on the STL-10 dataset.">
<title>Semi-supervised image classification using contrastive pretraining with SimCLR • keras</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><link href="../../extra.css" rel="stylesheet">
<script src="../../extra.js"></script><meta property="og:title" content="Semi-supervised image classification using contrastive pretraining with SimCLR">
<meta property="og:description" content="Contrastive pretraining with SimCLR for semi-supervised image classification on the STL-10 dataset.">
<meta property="og:image" content="https://keras.posit.co/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-tutorials">Tutorials</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-tutorials">
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <a class="dropdown-item" href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <h6 class="dropdown-header" data-toc-skip>Guides (New for TF 2.6)</h6>
    <a class="dropdown-item" href="../../articles/new-guides/python_subclasses.html">Python Subclasses</a>
    <a class="dropdown-item" href="../../articles/new-guides/making_new_layers_and_models_via_subclassing.html">Making New Layers and Models via Subclassing</a>
    <a class="dropdown-item" href="../../articles/new-guides/customizing_what_happens_in_fit.html">Customizing What Happens in Fit</a>
    <a class="dropdown-item" href="../../articles/new-guides/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <a class="dropdown-item" href="../../articles/new-guides/preprocessing_layers.html">Working with Preprocessing Layers</a>
    <a class="dropdown-item" href="../../argicles/new-guides/working_with_rnns.html">Working with RNNs</a>
    <div class="dropdown-divider"></div>
    <h6 class="dropdown-header" data-toc-skip>Using Keras</h6>
    <a class="dropdown-item" href="../../articles/guide_keras.html">Guide to Keras Basics</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model in Depth</a>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API in Depth</a>
    <a class="dropdown-item" href="../../articles/about_keras_models.html">About Keras Models</a>
    <a class="dropdown-item" href="../../articles/about_keras_layers.html">About Keras Layers</a>
    <a class="dropdown-item" href="../../articles/training_visualization.html">Training Visualization</a>
    <a class="dropdown-item" href="../../articles/applications.html">Pre-Trained Models</a>
    <a class="dropdown-item" href="../../articles/faq.html">Frequently Asked Questions</a>
    <a class="dropdown-item" href="../../articles/why_use_keras.html">Why Use Keras?</a>
    <div class="dropdown-divider"></div>
    <h6 class="dropdown-header" data-toc-skip>Advanced</h6>
    <a class="dropdown-item" href="../../articles/eager_guide.html">Eager Execution</a>
    <a class="dropdown-item" href="../../articles/training_callbacks.html">Training Callbacks</a>
    <a class="dropdown-item" href="../../articles/backend.html">Keras Backend</a>
    <a class="dropdown-item" href="../../articles/custom_layers.html">Custom Layers</a>
    <a class="dropdown-item" href="../../articles/custom_models.html">Custom Models</a>
    <a class="dropdown-item" href="../../articles/saving_serializing.html">Saving and serializing</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../articles/learn.html">Learn</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../articles/tools.html">Tools</a>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../../logo.png" class="logo" alt=""><h1>Semi-supervised image classification using contrastive pretraining with SimCLR</h1>
                        <h4 data-toc-skip class="author"><a href="https://www.linkedin.com/in/andras-beres-789190210" class="external-link">András
Béres</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/semisupervised_simclr.Rmd" class="external-link"><code>vignettes/examples/semisupervised_simclr.Rmd</code></a></small>
      <div class="d-none name"><code>semisupervised_simclr.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<div class="section level3">
<h3 id="semi-supervised-learning">Semi-supervised learning<a class="anchor" aria-label="anchor" href="#semi-supervised-learning"></a>
</h3>
<p>Semi-supervised learning is a machine learning paradigm that deals
with <strong>partially labeled datasets</strong>. When applying deep
learning in the real world, one usually has to gather a large dataset to
make it work well. However, while the cost of labeling scales linearly
with the dataset size (labeling each example takes a constant time),
model performance only scales <a href="https://arxiv.org/abs/2001.08361" class="external-link">sublinearly</a> with it. This
means that labeling more and more samples becomes less and less
cost-efficient, while gathering unlabeled data is generally cheap, as it
is usually readily available in large quantities.</p>
<p>Semi-supervised learning offers to solve this problem by only
requiring a partially labeled dataset, and by being label-efficient by
utilizing the unlabeled examples for learning as well.</p>
<p>In this example, we will pretrain an encoder with contrastive
learning on the <a href="https://ai.stanford.edu/~acoates/stl10/" class="external-link">STL-10</a>
semi-supervised dataset using no labels at all, and then fine-tune it
using only its labeled subset.</p>
</div>
<div class="section level3">
<h3 id="contrastive-learning">Contrastive learning<a class="anchor" aria-label="anchor" href="#contrastive-learning"></a>
</h3>
<p>On the highest level, the main idea behind contrastive learning is to
<strong>learn representations that are invariant to image
augmentations</strong> in a self-supervised manner. One problem with
this objective is that it has a trivial degenerate solution: the case
where the representations are constant, and do not depend at all on the
input images.</p>
<p>Contrastive learning avoids this trap by modifying the objective in
the following way: it pulls representations of augmented versions/views
of the same image closer to each other (contracting positives), while
simultaneously pushing different images away from each other
(contrasting negatives) in representation space.</p>
<p>One such contrastive approach is <a href="https://arxiv.org/abs/2002.05709" class="external-link">SimCLR</a>, which essentially
identifies the core components needed to optimize this objective, and
can achieve high performance by scaling this simple approach.</p>
<p>Another approach is <a href="https://arxiv.org/abs/2011.10566" class="external-link">SimSiam</a> (<a href="https://keras.io/examples/vision/simsiam/" class="external-link">Keras example</a>),
whose main difference from SimCLR is that the former does not use any
negatives in its loss. Therefore, it does not explicitly prevent the
trivial solution, and, instead, avoids it implicitly by architecture
design (asymmetric encoding paths using a predictor network and batch
normalization (BatchNorm) are applied in the final layers).</p>
<p>For further reading about SimCLR, check out <a href="https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html" class="external-link">the
official Google AI blog post</a>, and for an overview of self-supervised
learning across both vision and language check out <a href="https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/" class="external-link">this
blog post</a>.</p>
</div>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co"># Make sure we are able to handle large datasets</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> resource</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>low, high <span class="op">=</span> resource.getrlimit(resource.RLIMIT_NOFILE)</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>resource.setrlimit(resource.RLIMIT_NOFILE, (high, high))</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="im">import</span> tensorflow_datasets <span class="im">as</span> tfds</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="hyperparameter-setup">Hyperparameter setup<a class="anchor" aria-label="anchor" href="#hyperparameter-setup"></a>
</h2>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># Dataset hyperparameters</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>unlabeled_dataset_size <span class="op">=</span> <span class="dv">100000</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>labeled_dataset_size <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>image_size <span class="op">=</span> <span class="dv">96</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>image_channels <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a><span class="co"># Algorithm hyperparameters</span></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">525</span>  <span class="co"># Corresponds to 200 steps per epoch</span></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>width <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>temperature <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a><span class="co"># Stronger augmentations for contrastive, weaker ones for supervised training</span></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>contrastive_augmentation <span class="op">=</span> {<span class="st">"min_area"</span>: <span class="fl">0.25</span>, <span class="st">"brightness"</span>: <span class="fl">0.6</span>, <span class="st">"jitter"</span>: <span class="fl">0.2</span>}</span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a>classification_augmentation <span class="op">=</span> {</span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a>    <span class="st">"min_area"</span>: <span class="fl">0.75</span>,</span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a>    <span class="st">"brightness"</span>: <span class="fl">0.3</span>,</span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a>    <span class="st">"jitter"</span>: <span class="fl">0.1</span>,</span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="dataset">Dataset<a class="anchor" aria-label="anchor" href="#dataset"></a>
</h2>
<p>During training we will simultaneously load a large batch of
unlabeled images along with a smaller batch of labeled images.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="kw">def</span> prepare_dataset():</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>    <span class="co"># Labeled and unlabeled samples are loaded synchronously</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>    <span class="co"># with batch sizes selected accordingly</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>    steps_per_epoch <span class="op">=</span> (</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>        unlabeled_dataset_size <span class="op">+</span> labeled_dataset_size</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>    ) <span class="op">//</span> batch_size</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>    unlabeled_batch_size <span class="op">=</span> unlabeled_dataset_size <span class="op">//</span> steps_per_epoch</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>    labeled_batch_size <span class="op">=</span> labeled_dataset_size <span class="op">//</span> steps_per_epoch</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>    <span class="bu">print</span>(</span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>        <span class="ss">f"batch size is </span><span class="sc">{</span>unlabeled_batch_size<span class="sc">}</span><span class="ss"> (unlabeled) + </span><span class="sc">{</span>labeled_batch_size<span class="sc">}</span><span class="ss"> (labeled)"</span></span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>    )</span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a>    <span class="co"># Turning off shuffle to lower resource usage</span></span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a>    unlabeled_train_dataset <span class="op">=</span> (</span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a>        tfds.load(</span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a>            <span class="st">"stl10"</span>, split<span class="op">=</span><span class="st">"unlabelled"</span>, as_supervised<span class="op">=</span><span class="va">True</span>, shuffle_files<span class="op">=</span><span class="va">False</span></span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a>        )</span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a>        .shuffle(buffer_size<span class="op">=</span><span class="dv">10</span> <span class="op">*</span> unlabeled_batch_size)</span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a>        .batch(unlabeled_batch_size)</span>
<span id="cb3-20"><a href="#cb3-20" tabindex="-1"></a>    )</span>
<span id="cb3-21"><a href="#cb3-21" tabindex="-1"></a>    labeled_train_dataset <span class="op">=</span> (</span>
<span id="cb3-22"><a href="#cb3-22" tabindex="-1"></a>        tfds.load(</span>
<span id="cb3-23"><a href="#cb3-23" tabindex="-1"></a>            <span class="st">"stl10"</span>, split<span class="op">=</span><span class="st">"train"</span>, as_supervised<span class="op">=</span><span class="va">True</span>, shuffle_files<span class="op">=</span><span class="va">False</span></span>
<span id="cb3-24"><a href="#cb3-24" tabindex="-1"></a>        )</span>
<span id="cb3-25"><a href="#cb3-25" tabindex="-1"></a>        .shuffle(buffer_size<span class="op">=</span><span class="dv">10</span> <span class="op">*</span> labeled_batch_size)</span>
<span id="cb3-26"><a href="#cb3-26" tabindex="-1"></a>        .batch(labeled_batch_size)</span>
<span id="cb3-27"><a href="#cb3-27" tabindex="-1"></a>    )</span>
<span id="cb3-28"><a href="#cb3-28" tabindex="-1"></a>    test_dataset <span class="op">=</span> (</span>
<span id="cb3-29"><a href="#cb3-29" tabindex="-1"></a>        tfds.load(<span class="st">"stl10"</span>, split<span class="op">=</span><span class="st">"test"</span>, as_supervised<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-30"><a href="#cb3-30" tabindex="-1"></a>        .batch(batch_size)</span>
<span id="cb3-31"><a href="#cb3-31" tabindex="-1"></a>        .prefetch(buffer_size<span class="op">=</span>tf.data.AUTOTUNE)</span>
<span id="cb3-32"><a href="#cb3-32" tabindex="-1"></a>    )</span>
<span id="cb3-33"><a href="#cb3-33" tabindex="-1"></a></span>
<span id="cb3-34"><a href="#cb3-34" tabindex="-1"></a>    <span class="co"># Labeled and unlabeled datasets are zipped together</span></span>
<span id="cb3-35"><a href="#cb3-35" tabindex="-1"></a>    train_dataset <span class="op">=</span> tf.data.Dataset.<span class="bu">zip</span>(</span>
<span id="cb3-36"><a href="#cb3-36" tabindex="-1"></a>        (unlabeled_train_dataset, labeled_train_dataset)</span>
<span id="cb3-37"><a href="#cb3-37" tabindex="-1"></a>    ).prefetch(buffer_size<span class="op">=</span>tf.data.AUTOTUNE)</span>
<span id="cb3-38"><a href="#cb3-38" tabindex="-1"></a></span>
<span id="cb3-39"><a href="#cb3-39" tabindex="-1"></a>    <span class="cf">return</span> train_dataset, labeled_train_dataset, test_dataset</span>
<span id="cb3-40"><a href="#cb3-40" tabindex="-1"></a></span>
<span id="cb3-41"><a href="#cb3-41" tabindex="-1"></a></span>
<span id="cb3-42"><a href="#cb3-42" tabindex="-1"></a><span class="co"># Load STL10 dataset</span></span>
<span id="cb3-43"><a href="#cb3-43" tabindex="-1"></a>train_dataset, labeled_train_dataset, test_dataset <span class="op">=</span> prepare_dataset()</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="image-augmentations">Image augmentations<a class="anchor" aria-label="anchor" href="#image-augmentations"></a>
</h2>
<p>The two most important image augmentations for contrastive learning
are the following:</p>
<ul>
<li>Cropping: forces the model to encode different parts of the same
image similarly, we implement it with the <a href="https://keras.io/api/layers/preprocessing_layers/image_preprocessing/random_translation/" class="external-link">RandomTranslation</a>
and <a href="https://keras.io/api/layers/preprocessing_layers/image_preprocessing/random_zoom/" class="external-link">RandomZoom</a>
layers</li>
<li>Color jitter: prevents a trivial color histogram-based solution to
the task by distorting color histograms. A principled way to implement
that is by affine transformations in color space.</li>
</ul>
<p>In this example we use random horizontal flips as well. Stronger
augmentations are applied for contrastive learning, along with weaker
ones for supervised classification to avoid overfitting on the few
labeled examples.</p>
<p>We implement random color jitter as a custom preprocessing layer.
Using preprocessing layers for data augmentation has the following two
advantages:</p>
<ul>
<li>The data augmentation will run on GPU in batches, so the training
will not be bottlenecked by the data pipeline in environments with
constrained CPU resources (such as a Colab Notebook, or a personal
machine)</li>
<li>Deployment is easier as the data preprocessing pipeline is
encapsulated in the model, and does not have to be reimplemented when
deploying it</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="co"># Distorts the color distibutions of images</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="kw">class</span> RandomColorAffine(layers.Layer):</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, brightness<span class="op">=</span><span class="dv">0</span>, jitter<span class="op">=</span><span class="dv">0</span>, <span class="op">**</span>kwargs):</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>        <span class="va">self</span>.brightness <span class="op">=</span> brightness</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>        <span class="va">self</span>.jitter <span class="op">=</span> jitter</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>    <span class="kw">def</span> get_config(<span class="va">self</span>):</span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>        config <span class="op">=</span> <span class="bu">super</span>().get_config()</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>        config.update({<span class="st">"brightness"</span>: <span class="va">self</span>.brightness, <span class="st">"jitter"</span>: <span class="va">self</span>.jitter})</span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>        <span class="cf">return</span> config</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, images, training<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a>        <span class="cf">if</span> training:</span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a>            batch_size <span class="op">=</span> tf.shape(images)[<span class="dv">0</span>]</span>
<span id="cb4-17"><a href="#cb4-17" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" tabindex="-1"></a>            <span class="co"># Same for all colors</span></span>
<span id="cb4-19"><a href="#cb4-19" tabindex="-1"></a>            brightness_scales <span class="op">=</span> <span class="dv">1</span> <span class="op">+</span> tf.random.uniform(</span>
<span id="cb4-20"><a href="#cb4-20" tabindex="-1"></a>                (batch_size, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb4-21"><a href="#cb4-21" tabindex="-1"></a>                minval<span class="op">=-</span><span class="va">self</span>.brightness,</span>
<span id="cb4-22"><a href="#cb4-22" tabindex="-1"></a>                maxval<span class="op">=</span><span class="va">self</span>.brightness,</span>
<span id="cb4-23"><a href="#cb4-23" tabindex="-1"></a>            )</span>
<span id="cb4-24"><a href="#cb4-24" tabindex="-1"></a>            <span class="co"># Different for all colors</span></span>
<span id="cb4-25"><a href="#cb4-25" tabindex="-1"></a>            jitter_matrices <span class="op">=</span> tf.random.uniform(</span>
<span id="cb4-26"><a href="#cb4-26" tabindex="-1"></a>                (batch_size, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>), minval<span class="op">=-</span><span class="va">self</span>.jitter, maxval<span class="op">=</span><span class="va">self</span>.jitter</span>
<span id="cb4-27"><a href="#cb4-27" tabindex="-1"></a>            )</span>
<span id="cb4-28"><a href="#cb4-28" tabindex="-1"></a></span>
<span id="cb4-29"><a href="#cb4-29" tabindex="-1"></a>            color_transforms <span class="op">=</span> (</span>
<span id="cb4-30"><a href="#cb4-30" tabindex="-1"></a>                tf.eye(<span class="dv">3</span>, batch_shape<span class="op">=</span>[batch_size, <span class="dv">1</span>]) <span class="op">*</span> brightness_scales</span>
<span id="cb4-31"><a href="#cb4-31" tabindex="-1"></a>                <span class="op">+</span> jitter_matrices</span>
<span id="cb4-32"><a href="#cb4-32" tabindex="-1"></a>            )</span>
<span id="cb4-33"><a href="#cb4-33" tabindex="-1"></a>            images <span class="op">=</span> tf.clip_by_value(tf.matmul(images, color_transforms), <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb4-34"><a href="#cb4-34" tabindex="-1"></a>        <span class="cf">return</span> images</span>
<span id="cb4-35"><a href="#cb4-35" tabindex="-1"></a></span>
<span id="cb4-36"><a href="#cb4-36" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" tabindex="-1"></a><span class="co"># Image augmentation module</span></span>
<span id="cb4-38"><a href="#cb4-38" tabindex="-1"></a><span class="kw">def</span> get_augmenter(min_area, brightness, jitter):</span>
<span id="cb4-39"><a href="#cb4-39" tabindex="-1"></a>    zoom_factor <span class="op">=</span> <span class="fl">1.0</span> <span class="op">-</span> math.sqrt(min_area)</span>
<span id="cb4-40"><a href="#cb4-40" tabindex="-1"></a>    <span class="cf">return</span> keras.Sequential(</span>
<span id="cb4-41"><a href="#cb4-41" tabindex="-1"></a>        [</span>
<span id="cb4-42"><a href="#cb4-42" tabindex="-1"></a>            keras.Input(shape<span class="op">=</span>(image_size, image_size, image_channels)),</span>
<span id="cb4-43"><a href="#cb4-43" tabindex="-1"></a>            layers.Rescaling(<span class="dv">1</span> <span class="op">/</span> <span class="dv">255</span>, dtype<span class="op">=</span><span class="st">"uint8"</span>),</span>
<span id="cb4-44"><a href="#cb4-44" tabindex="-1"></a>            layers.RandomFlip(<span class="st">"horizontal"</span>),</span>
<span id="cb4-45"><a href="#cb4-45" tabindex="-1"></a>            layers.RandomTranslation(zoom_factor <span class="op">/</span> <span class="dv">2</span>, zoom_factor <span class="op">/</span> <span class="dv">2</span>),</span>
<span id="cb4-46"><a href="#cb4-46" tabindex="-1"></a>            layers.RandomZoom((<span class="op">-</span>zoom_factor, <span class="fl">0.0</span>), (<span class="op">-</span>zoom_factor, <span class="fl">0.0</span>)),</span>
<span id="cb4-47"><a href="#cb4-47" tabindex="-1"></a>            RandomColorAffine(brightness, jitter),</span>
<span id="cb4-48"><a href="#cb4-48" tabindex="-1"></a>        ]</span>
<span id="cb4-49"><a href="#cb4-49" tabindex="-1"></a>    )</span>
<span id="cb4-50"><a href="#cb4-50" tabindex="-1"></a></span>
<span id="cb4-51"><a href="#cb4-51" tabindex="-1"></a></span>
<span id="cb4-52"><a href="#cb4-52" tabindex="-1"></a><span class="kw">def</span> visualize_augmentations(num_images):</span>
<span id="cb4-53"><a href="#cb4-53" tabindex="-1"></a>    <span class="co"># Sample a batch from a dataset</span></span>
<span id="cb4-54"><a href="#cb4-54" tabindex="-1"></a>    images <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_dataset))[<span class="dv">0</span>][<span class="dv">0</span>][:num_images]</span>
<span id="cb4-55"><a href="#cb4-55" tabindex="-1"></a></span>
<span id="cb4-56"><a href="#cb4-56" tabindex="-1"></a>    <span class="co"># Apply augmentations</span></span>
<span id="cb4-57"><a href="#cb4-57" tabindex="-1"></a>    augmented_images <span class="op">=</span> <span class="bu">zip</span>(</span>
<span id="cb4-58"><a href="#cb4-58" tabindex="-1"></a>        images,</span>
<span id="cb4-59"><a href="#cb4-59" tabindex="-1"></a>        get_augmenter(<span class="op">**</span>classification_augmentation)(images),</span>
<span id="cb4-60"><a href="#cb4-60" tabindex="-1"></a>        get_augmenter(<span class="op">**</span>contrastive_augmentation)(images),</span>
<span id="cb4-61"><a href="#cb4-61" tabindex="-1"></a>        get_augmenter(<span class="op">**</span>contrastive_augmentation)(images),</span>
<span id="cb4-62"><a href="#cb4-62" tabindex="-1"></a>    )</span>
<span id="cb4-63"><a href="#cb4-63" tabindex="-1"></a>    row_titles <span class="op">=</span> [</span>
<span id="cb4-64"><a href="#cb4-64" tabindex="-1"></a>        <span class="st">"Original:"</span>,</span>
<span id="cb4-65"><a href="#cb4-65" tabindex="-1"></a>        <span class="st">"Weakly augmented:"</span>,</span>
<span id="cb4-66"><a href="#cb4-66" tabindex="-1"></a>        <span class="st">"Strongly augmented:"</span>,</span>
<span id="cb4-67"><a href="#cb4-67" tabindex="-1"></a>        <span class="st">"Strongly augmented:"</span>,</span>
<span id="cb4-68"><a href="#cb4-68" tabindex="-1"></a>    ]</span>
<span id="cb4-69"><a href="#cb4-69" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(num_images <span class="op">*</span> <span class="fl">2.2</span>, <span class="dv">4</span> <span class="op">*</span> <span class="fl">2.2</span>), dpi<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb4-70"><a href="#cb4-70" tabindex="-1"></a>    <span class="cf">for</span> column, image_row <span class="kw">in</span> <span class="bu">enumerate</span>(augmented_images):</span>
<span id="cb4-71"><a href="#cb4-71" tabindex="-1"></a>        <span class="cf">for</span> row, image <span class="kw">in</span> <span class="bu">enumerate</span>(image_row):</span>
<span id="cb4-72"><a href="#cb4-72" tabindex="-1"></a>            plt.subplot(<span class="dv">4</span>, num_images, row <span class="op">*</span> num_images <span class="op">+</span> column <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-73"><a href="#cb4-73" tabindex="-1"></a>            plt.imshow(image)</span>
<span id="cb4-74"><a href="#cb4-74" tabindex="-1"></a>            <span class="cf">if</span> column <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb4-75"><a href="#cb4-75" tabindex="-1"></a>                plt.title(row_titles[row], loc<span class="op">=</span><span class="st">"left"</span>)</span>
<span id="cb4-76"><a href="#cb4-76" tabindex="-1"></a>            plt.axis(<span class="st">"off"</span>)</span>
<span id="cb4-77"><a href="#cb4-77" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb4-78"><a href="#cb4-78" tabindex="-1"></a></span>
<span id="cb4-79"><a href="#cb4-79" tabindex="-1"></a></span>
<span id="cb4-80"><a href="#cb4-80" tabindex="-1"></a>visualize_augmentations(num_images<span class="op">=</span><span class="dv">8</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="encoder-architecture">Encoder architecture<a class="anchor" aria-label="anchor" href="#encoder-architecture"></a>
</h2>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># Define the encoder architecture</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a><span class="kw">def</span> get_encoder():</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>    <span class="cf">return</span> keras.Sequential(</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>        [</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>            keras.Input(shape<span class="op">=</span>(image_size, image_size, image_channels)),</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>            layers.Conv2D(width, kernel_size<span class="op">=</span><span class="dv">3</span>, strides<span class="op">=</span><span class="dv">2</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>            layers.Conv2D(width, kernel_size<span class="op">=</span><span class="dv">3</span>, strides<span class="op">=</span><span class="dv">2</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>            layers.Conv2D(width, kernel_size<span class="op">=</span><span class="dv">3</span>, strides<span class="op">=</span><span class="dv">2</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>            layers.Conv2D(width, kernel_size<span class="op">=</span><span class="dv">3</span>, strides<span class="op">=</span><span class="dv">2</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>            layers.Flatten(),</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>            layers.Dense(width, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>        ],</span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>        name<span class="op">=</span><span class="st">"encoder"</span>,</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>    )</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="supervised-baseline-model">Supervised baseline model<a class="anchor" aria-label="anchor" href="#supervised-baseline-model"></a>
</h2>
<p>A baseline supervised model is trained using random
initialization.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="co"># Baseline supervised training with random initialization</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>baseline_model <span class="op">=</span> keras.Sequential(</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>    [</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>        keras.Input(shape<span class="op">=</span>(image_size, image_size, image_channels)),</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>        get_augmenter(<span class="op">**</span>classification_augmentation),</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>        get_encoder(),</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>        layers.Dense(<span class="dv">10</span>),</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>    ],</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"baseline_model"</span>,</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>)</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>baseline_model.<span class="bu">compile</span>(</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>    optimizer<span class="op">=</span>keras.optimizers.Adam(),</span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a>    loss<span class="op">=</span>keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a>    metrics<span class="op">=</span>[keras.metrics.SparseCategoricalAccuracy(name<span class="op">=</span><span class="st">"acc"</span>)],</span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a>)</span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a>baseline_history <span class="op">=</span> baseline_model.fit(</span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a>    labeled_train_dataset, epochs<span class="op">=</span>num_epochs, validation_data<span class="op">=</span>test_dataset</span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a>)</span>
<span id="cb6-20"><a href="#cb6-20" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb6-22"><a href="#cb6-22" tabindex="-1"></a>    <span class="st">"Maximal validation accuracy: </span><span class="sc">{:.2f}</span><span class="st">%"</span>.<span class="bu">format</span>(</span>
<span id="cb6-23"><a href="#cb6-23" tabindex="-1"></a>        <span class="bu">max</span>(baseline_history.history[<span class="st">"val_acc"</span>]) <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb6-24"><a href="#cb6-24" tabindex="-1"></a>    )</span>
<span id="cb6-25"><a href="#cb6-25" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="self-supervised-model-for-contrastive-pretraining">Self-supervised model for contrastive pretraining<a class="anchor" aria-label="anchor" href="#self-supervised-model-for-contrastive-pretraining"></a>
</h2>
<p>We pretrain an encoder on unlabeled images with a contrastive loss. A
nonlinear projection head is attached to the top of the encoder, as it
improves the quality of representations of the encoder.</p>
<p>We use the InfoNCE/NT-Xent/N-pairs loss, which can be interpreted in
the following way:</p>
<ol style="list-style-type: decimal">
<li>We treat each image in the batch as if it had its own class.</li>
<li>Then, we have two examples (a pair of augmented views) for each
“class”.</li>
<li>Each view’s representation is compared to every possible pair’s one
(for both augmented versions).</li>
<li>We use the temperature-scaled cosine similarity of compared
representations as logits.</li>
<li>Finally, we use categorical cross-entropy as the “classification”
loss</li>
</ol>
<p>The following two metrics are used for monitoring the pretraining
performance:</p>
<ul>
<li>
<a href="https://arxiv.org/abs/2002.05709" class="external-link">Contrastive accuracy
(SimCLR Table 5)</a>: Self-supervised metric, the ratio of cases in
which the representation of an image is more similar to its differently
augmented version’s one, than to the representation of any other image
in the current batch. Self-supervised metrics can be used for
hyperparameter tuning even in the case when there are no labeled
examples.</li>
<li>
<a href="https://arxiv.org/abs/1603.08511" class="external-link">Linear probing
accuracy</a>: Linear probing is a popular metric to evaluate
self-supervised classifiers. It is computed as the accuracy of a
logistic regression classifier trained on top of the encoder’s features.
In our case, this is done by training a single dense layer on top of the
frozen encoder. Note that contrary to traditional approach where the
classifier is trained after the pretraining phase, in this example we
train it during pretraining. This might slightly decrease its accuracy,
but that way we can monitor its value during training, which helps with
experimentation and debugging.</li>
</ul>
<p>Another widely used supervised metric is the <a href="https://arxiv.org/abs/1805.01978" class="external-link">KNN accuracy</a>, which is the
accuracy of a KNN classifier trained on top of the encoder’s features,
which is not implemented in this example.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="co"># Define the contrastive model with model-subclassing</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="kw">class</span> ContrastiveModel(keras.Model):</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>        <span class="va">self</span>.temperature <span class="op">=</span> temperature</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>        <span class="va">self</span>.contrastive_augmenter <span class="op">=</span> get_augmenter(<span class="op">**</span>contrastive_augmentation)</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>        <span class="va">self</span>.classification_augmenter <span class="op">=</span> get_augmenter(</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>            <span class="op">**</span>classification_augmentation</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>        )</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> get_encoder()</span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>        <span class="co"># Non-linear MLP as projection head</span></span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a>        <span class="va">self</span>.projection_head <span class="op">=</span> keras.Sequential(</span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a>            [</span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a>                keras.Input(shape<span class="op">=</span>(width,)),</span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a>                layers.Dense(width, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a>                layers.Dense(width),</span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a>            ],</span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"projection_head"</span>,</span>
<span id="cb7-20"><a href="#cb7-20" tabindex="-1"></a>        )</span>
<span id="cb7-21"><a href="#cb7-21" tabindex="-1"></a>        <span class="co"># Single dense layer for linear probing</span></span>
<span id="cb7-22"><a href="#cb7-22" tabindex="-1"></a>        <span class="va">self</span>.linear_probe <span class="op">=</span> keras.Sequential(</span>
<span id="cb7-23"><a href="#cb7-23" tabindex="-1"></a>            [layers.Input(shape<span class="op">=</span>(width,)), layers.Dense(<span class="dv">10</span>)],</span>
<span id="cb7-24"><a href="#cb7-24" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"linear_probe"</span>,</span>
<span id="cb7-25"><a href="#cb7-25" tabindex="-1"></a>        )</span>
<span id="cb7-26"><a href="#cb7-26" tabindex="-1"></a></span>
<span id="cb7-27"><a href="#cb7-27" tabindex="-1"></a>        <span class="va">self</span>.encoder.summary()</span>
<span id="cb7-28"><a href="#cb7-28" tabindex="-1"></a>        <span class="va">self</span>.projection_head.summary()</span>
<span id="cb7-29"><a href="#cb7-29" tabindex="-1"></a>        <span class="va">self</span>.linear_probe.summary()</span>
<span id="cb7-30"><a href="#cb7-30" tabindex="-1"></a></span>
<span id="cb7-31"><a href="#cb7-31" tabindex="-1"></a>    <span class="kw">def</span> <span class="bu">compile</span>(<span class="va">self</span>, contrastive_optimizer, probe_optimizer, <span class="op">**</span>kwargs):</span>
<span id="cb7-32"><a href="#cb7-32" tabindex="-1"></a>        <span class="bu">super</span>().<span class="bu">compile</span>(<span class="op">**</span>kwargs)</span>
<span id="cb7-33"><a href="#cb7-33" tabindex="-1"></a></span>
<span id="cb7-34"><a href="#cb7-34" tabindex="-1"></a>        <span class="va">self</span>.contrastive_optimizer <span class="op">=</span> contrastive_optimizer</span>
<span id="cb7-35"><a href="#cb7-35" tabindex="-1"></a>        <span class="va">self</span>.probe_optimizer <span class="op">=</span> probe_optimizer</span>
<span id="cb7-36"><a href="#cb7-36" tabindex="-1"></a></span>
<span id="cb7-37"><a href="#cb7-37" tabindex="-1"></a>        <span class="co"># self.contrastive_loss will be defined as a method</span></span>
<span id="cb7-38"><a href="#cb7-38" tabindex="-1"></a>        <span class="va">self</span>.probe_loss <span class="op">=</span> keras.losses.SparseCategoricalCrossentropy(</span>
<span id="cb7-39"><a href="#cb7-39" tabindex="-1"></a>            from_logits<span class="op">=</span><span class="va">True</span></span>
<span id="cb7-40"><a href="#cb7-40" tabindex="-1"></a>        )</span>
<span id="cb7-41"><a href="#cb7-41" tabindex="-1"></a></span>
<span id="cb7-42"><a href="#cb7-42" tabindex="-1"></a>        <span class="va">self</span>.contrastive_loss_tracker <span class="op">=</span> keras.metrics.Mean(name<span class="op">=</span><span class="st">"c_loss"</span>)</span>
<span id="cb7-43"><a href="#cb7-43" tabindex="-1"></a>        <span class="va">self</span>.contrastive_accuracy <span class="op">=</span> keras.metrics.SparseCategoricalAccuracy(</span>
<span id="cb7-44"><a href="#cb7-44" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"c_acc"</span></span>
<span id="cb7-45"><a href="#cb7-45" tabindex="-1"></a>        )</span>
<span id="cb7-46"><a href="#cb7-46" tabindex="-1"></a>        <span class="va">self</span>.probe_loss_tracker <span class="op">=</span> keras.metrics.Mean(name<span class="op">=</span><span class="st">"p_loss"</span>)</span>
<span id="cb7-47"><a href="#cb7-47" tabindex="-1"></a>        <span class="va">self</span>.probe_accuracy <span class="op">=</span> keras.metrics.SparseCategoricalAccuracy(</span>
<span id="cb7-48"><a href="#cb7-48" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"p_acc"</span></span>
<span id="cb7-49"><a href="#cb7-49" tabindex="-1"></a>        )</span>
<span id="cb7-50"><a href="#cb7-50" tabindex="-1"></a></span>
<span id="cb7-51"><a href="#cb7-51" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb7-52"><a href="#cb7-52" tabindex="-1"></a>    <span class="kw">def</span> metrics(<span class="va">self</span>):</span>
<span id="cb7-53"><a href="#cb7-53" tabindex="-1"></a>        <span class="cf">return</span> [</span>
<span id="cb7-54"><a href="#cb7-54" tabindex="-1"></a>            <span class="va">self</span>.contrastive_loss_tracker,</span>
<span id="cb7-55"><a href="#cb7-55" tabindex="-1"></a>            <span class="va">self</span>.contrastive_accuracy,</span>
<span id="cb7-56"><a href="#cb7-56" tabindex="-1"></a>            <span class="va">self</span>.probe_loss_tracker,</span>
<span id="cb7-57"><a href="#cb7-57" tabindex="-1"></a>            <span class="va">self</span>.probe_accuracy,</span>
<span id="cb7-58"><a href="#cb7-58" tabindex="-1"></a>        ]</span>
<span id="cb7-59"><a href="#cb7-59" tabindex="-1"></a></span>
<span id="cb7-60"><a href="#cb7-60" tabindex="-1"></a>    <span class="kw">def</span> contrastive_loss(<span class="va">self</span>, projections_1, projections_2):</span>
<span id="cb7-61"><a href="#cb7-61" tabindex="-1"></a>        <span class="co"># InfoNCE loss (information noise-contrastive estimation)</span></span>
<span id="cb7-62"><a href="#cb7-62" tabindex="-1"></a>        <span class="co"># NT-Xent loss (normalized temperature-scaled cross entropy)</span></span>
<span id="cb7-63"><a href="#cb7-63" tabindex="-1"></a></span>
<span id="cb7-64"><a href="#cb7-64" tabindex="-1"></a>        <span class="co"># Cosine similarity: the dot product of the l2-normalized feature vectors</span></span>
<span id="cb7-65"><a href="#cb7-65" tabindex="-1"></a>        projections_1 <span class="op">=</span> tf.math.l2_normalize(projections_1, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-66"><a href="#cb7-66" tabindex="-1"></a>        projections_2 <span class="op">=</span> tf.math.l2_normalize(projections_2, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-67"><a href="#cb7-67" tabindex="-1"></a>        similarities <span class="op">=</span> (</span>
<span id="cb7-68"><a href="#cb7-68" tabindex="-1"></a>            tf.matmul(projections_1, projections_2, transpose_b<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-69"><a href="#cb7-69" tabindex="-1"></a>            <span class="op">/</span> <span class="va">self</span>.temperature</span>
<span id="cb7-70"><a href="#cb7-70" tabindex="-1"></a>        )</span>
<span id="cb7-71"><a href="#cb7-71" tabindex="-1"></a></span>
<span id="cb7-72"><a href="#cb7-72" tabindex="-1"></a>        <span class="co"># The similarity between the representations of two augmented views of the</span></span>
<span id="cb7-73"><a href="#cb7-73" tabindex="-1"></a>        <span class="co"># same image should be higher than their similarity with other views</span></span>
<span id="cb7-74"><a href="#cb7-74" tabindex="-1"></a>        batch_size <span class="op">=</span> tf.shape(projections_1)[<span class="dv">0</span>]</span>
<span id="cb7-75"><a href="#cb7-75" tabindex="-1"></a>        contrastive_labels <span class="op">=</span> tf.<span class="bu">range</span>(batch_size)</span>
<span id="cb7-76"><a href="#cb7-76" tabindex="-1"></a>        <span class="va">self</span>.contrastive_accuracy.update_state(contrastive_labels, similarities)</span>
<span id="cb7-77"><a href="#cb7-77" tabindex="-1"></a>        <span class="va">self</span>.contrastive_accuracy.update_state(</span>
<span id="cb7-78"><a href="#cb7-78" tabindex="-1"></a>            contrastive_labels, tf.transpose(similarities)</span>
<span id="cb7-79"><a href="#cb7-79" tabindex="-1"></a>        )</span>
<span id="cb7-80"><a href="#cb7-80" tabindex="-1"></a></span>
<span id="cb7-81"><a href="#cb7-81" tabindex="-1"></a>        <span class="co"># The temperature-scaled similarities are used as logits for cross-entropy</span></span>
<span id="cb7-82"><a href="#cb7-82" tabindex="-1"></a>        <span class="co"># a symmetrized version of the loss is used here</span></span>
<span id="cb7-83"><a href="#cb7-83" tabindex="-1"></a>        loss_1_2 <span class="op">=</span> keras.losses.sparse_categorical_crossentropy(</span>
<span id="cb7-84"><a href="#cb7-84" tabindex="-1"></a>            contrastive_labels, similarities, from_logits<span class="op">=</span><span class="va">True</span></span>
<span id="cb7-85"><a href="#cb7-85" tabindex="-1"></a>        )</span>
<span id="cb7-86"><a href="#cb7-86" tabindex="-1"></a>        loss_2_1 <span class="op">=</span> keras.losses.sparse_categorical_crossentropy(</span>
<span id="cb7-87"><a href="#cb7-87" tabindex="-1"></a>            contrastive_labels, tf.transpose(similarities), from_logits<span class="op">=</span><span class="va">True</span></span>
<span id="cb7-88"><a href="#cb7-88" tabindex="-1"></a>        )</span>
<span id="cb7-89"><a href="#cb7-89" tabindex="-1"></a>        <span class="cf">return</span> (loss_1_2 <span class="op">+</span> loss_2_1) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb7-90"><a href="#cb7-90" tabindex="-1"></a></span>
<span id="cb7-91"><a href="#cb7-91" tabindex="-1"></a>    <span class="kw">def</span> train_step(<span class="va">self</span>, data):</span>
<span id="cb7-92"><a href="#cb7-92" tabindex="-1"></a>        (unlabeled_images, _), (labeled_images, labels) <span class="op">=</span> data</span>
<span id="cb7-93"><a href="#cb7-93" tabindex="-1"></a></span>
<span id="cb7-94"><a href="#cb7-94" tabindex="-1"></a>        <span class="co"># Both labeled and unlabeled images are used, without labels</span></span>
<span id="cb7-95"><a href="#cb7-95" tabindex="-1"></a>        images <span class="op">=</span> tf.concat((unlabeled_images, labeled_images), axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-96"><a href="#cb7-96" tabindex="-1"></a>        <span class="co"># Each image is augmented twice, differently</span></span>
<span id="cb7-97"><a href="#cb7-97" tabindex="-1"></a>        augmented_images_1 <span class="op">=</span> <span class="va">self</span>.contrastive_augmenter(images, training<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-98"><a href="#cb7-98" tabindex="-1"></a>        augmented_images_2 <span class="op">=</span> <span class="va">self</span>.contrastive_augmenter(images, training<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-99"><a href="#cb7-99" tabindex="-1"></a>        <span class="cf">with</span> tf.GradientTape() <span class="im">as</span> tape:</span>
<span id="cb7-100"><a href="#cb7-100" tabindex="-1"></a>            features_1 <span class="op">=</span> <span class="va">self</span>.encoder(augmented_images_1, training<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-101"><a href="#cb7-101" tabindex="-1"></a>            features_2 <span class="op">=</span> <span class="va">self</span>.encoder(augmented_images_2, training<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-102"><a href="#cb7-102" tabindex="-1"></a>            <span class="co"># The representations are passed through a projection mlp</span></span>
<span id="cb7-103"><a href="#cb7-103" tabindex="-1"></a>            projections_1 <span class="op">=</span> <span class="va">self</span>.projection_head(features_1, training<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-104"><a href="#cb7-104" tabindex="-1"></a>            projections_2 <span class="op">=</span> <span class="va">self</span>.projection_head(features_2, training<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-105"><a href="#cb7-105" tabindex="-1"></a>            contrastive_loss <span class="op">=</span> <span class="va">self</span>.contrastive_loss(</span>
<span id="cb7-106"><a href="#cb7-106" tabindex="-1"></a>                projections_1, projections_2</span>
<span id="cb7-107"><a href="#cb7-107" tabindex="-1"></a>            )</span>
<span id="cb7-108"><a href="#cb7-108" tabindex="-1"></a>        gradients <span class="op">=</span> tape.gradient(</span>
<span id="cb7-109"><a href="#cb7-109" tabindex="-1"></a>            contrastive_loss,</span>
<span id="cb7-110"><a href="#cb7-110" tabindex="-1"></a>            <span class="va">self</span>.encoder.trainable_weights</span>
<span id="cb7-111"><a href="#cb7-111" tabindex="-1"></a>            <span class="op">+</span> <span class="va">self</span>.projection_head.trainable_weights,</span>
<span id="cb7-112"><a href="#cb7-112" tabindex="-1"></a>        )</span>
<span id="cb7-113"><a href="#cb7-113" tabindex="-1"></a>        <span class="va">self</span>.contrastive_optimizer.apply_gradients(</span>
<span id="cb7-114"><a href="#cb7-114" tabindex="-1"></a>            <span class="bu">zip</span>(</span>
<span id="cb7-115"><a href="#cb7-115" tabindex="-1"></a>                gradients,</span>
<span id="cb7-116"><a href="#cb7-116" tabindex="-1"></a>                <span class="va">self</span>.encoder.trainable_weights</span>
<span id="cb7-117"><a href="#cb7-117" tabindex="-1"></a>                <span class="op">+</span> <span class="va">self</span>.projection_head.trainable_weights,</span>
<span id="cb7-118"><a href="#cb7-118" tabindex="-1"></a>            )</span>
<span id="cb7-119"><a href="#cb7-119" tabindex="-1"></a>        )</span>
<span id="cb7-120"><a href="#cb7-120" tabindex="-1"></a>        <span class="va">self</span>.contrastive_loss_tracker.update_state(contrastive_loss)</span>
<span id="cb7-121"><a href="#cb7-121" tabindex="-1"></a></span>
<span id="cb7-122"><a href="#cb7-122" tabindex="-1"></a>        <span class="co"># Labels are only used in evalutation for an on-the-fly logistic regression</span></span>
<span id="cb7-123"><a href="#cb7-123" tabindex="-1"></a>        preprocessed_images <span class="op">=</span> <span class="va">self</span>.classification_augmenter(</span>
<span id="cb7-124"><a href="#cb7-124" tabindex="-1"></a>            labeled_images, training<span class="op">=</span><span class="va">True</span></span>
<span id="cb7-125"><a href="#cb7-125" tabindex="-1"></a>        )</span>
<span id="cb7-126"><a href="#cb7-126" tabindex="-1"></a>        <span class="cf">with</span> tf.GradientTape() <span class="im">as</span> tape:</span>
<span id="cb7-127"><a href="#cb7-127" tabindex="-1"></a>            <span class="co"># the encoder is used in inference mode here to avoid regularization</span></span>
<span id="cb7-128"><a href="#cb7-128" tabindex="-1"></a>            <span class="co"># and updating the batch normalization paramers if they are used</span></span>
<span id="cb7-129"><a href="#cb7-129" tabindex="-1"></a>            features <span class="op">=</span> <span class="va">self</span>.encoder(preprocessed_images, training<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-130"><a href="#cb7-130" tabindex="-1"></a>            class_logits <span class="op">=</span> <span class="va">self</span>.linear_probe(features, training<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-131"><a href="#cb7-131" tabindex="-1"></a>            probe_loss <span class="op">=</span> <span class="va">self</span>.probe_loss(labels, class_logits)</span>
<span id="cb7-132"><a href="#cb7-132" tabindex="-1"></a>        gradients <span class="op">=</span> tape.gradient(</span>
<span id="cb7-133"><a href="#cb7-133" tabindex="-1"></a>            probe_loss, <span class="va">self</span>.linear_probe.trainable_weights</span>
<span id="cb7-134"><a href="#cb7-134" tabindex="-1"></a>        )</span>
<span id="cb7-135"><a href="#cb7-135" tabindex="-1"></a>        <span class="va">self</span>.probe_optimizer.apply_gradients(</span>
<span id="cb7-136"><a href="#cb7-136" tabindex="-1"></a>            <span class="bu">zip</span>(gradients, <span class="va">self</span>.linear_probe.trainable_weights)</span>
<span id="cb7-137"><a href="#cb7-137" tabindex="-1"></a>        )</span>
<span id="cb7-138"><a href="#cb7-138" tabindex="-1"></a>        <span class="va">self</span>.probe_loss_tracker.update_state(probe_loss)</span>
<span id="cb7-139"><a href="#cb7-139" tabindex="-1"></a>        <span class="va">self</span>.probe_accuracy.update_state(labels, class_logits)</span>
<span id="cb7-140"><a href="#cb7-140" tabindex="-1"></a></span>
<span id="cb7-141"><a href="#cb7-141" tabindex="-1"></a>        <span class="cf">return</span> {m.name: m.result() <span class="cf">for</span> m <span class="kw">in</span> <span class="va">self</span>.metrics}</span>
<span id="cb7-142"><a href="#cb7-142" tabindex="-1"></a></span>
<span id="cb7-143"><a href="#cb7-143" tabindex="-1"></a>    <span class="kw">def</span> test_step(<span class="va">self</span>, data):</span>
<span id="cb7-144"><a href="#cb7-144" tabindex="-1"></a>        labeled_images, labels <span class="op">=</span> data</span>
<span id="cb7-145"><a href="#cb7-145" tabindex="-1"></a></span>
<span id="cb7-146"><a href="#cb7-146" tabindex="-1"></a>        <span class="co"># For testing the components are used with a training=False flag</span></span>
<span id="cb7-147"><a href="#cb7-147" tabindex="-1"></a>        preprocessed_images <span class="op">=</span> <span class="va">self</span>.classification_augmenter(</span>
<span id="cb7-148"><a href="#cb7-148" tabindex="-1"></a>            labeled_images, training<span class="op">=</span><span class="va">False</span></span>
<span id="cb7-149"><a href="#cb7-149" tabindex="-1"></a>        )</span>
<span id="cb7-150"><a href="#cb7-150" tabindex="-1"></a>        features <span class="op">=</span> <span class="va">self</span>.encoder(preprocessed_images, training<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-151"><a href="#cb7-151" tabindex="-1"></a>        class_logits <span class="op">=</span> <span class="va">self</span>.linear_probe(features, training<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-152"><a href="#cb7-152" tabindex="-1"></a>        probe_loss <span class="op">=</span> <span class="va">self</span>.probe_loss(labels, class_logits)</span>
<span id="cb7-153"><a href="#cb7-153" tabindex="-1"></a>        <span class="va">self</span>.probe_loss_tracker.update_state(probe_loss)</span>
<span id="cb7-154"><a href="#cb7-154" tabindex="-1"></a>        <span class="va">self</span>.probe_accuracy.update_state(labels, class_logits)</span>
<span id="cb7-155"><a href="#cb7-155" tabindex="-1"></a></span>
<span id="cb7-156"><a href="#cb7-156" tabindex="-1"></a>        <span class="co"># Only the probe metrics are logged at test time</span></span>
<span id="cb7-157"><a href="#cb7-157" tabindex="-1"></a>        <span class="cf">return</span> {m.name: m.result() <span class="cf">for</span> m <span class="kw">in</span> <span class="va">self</span>.metrics[<span class="dv">2</span>:]}</span>
<span id="cb7-158"><a href="#cb7-158" tabindex="-1"></a></span>
<span id="cb7-159"><a href="#cb7-159" tabindex="-1"></a></span>
<span id="cb7-160"><a href="#cb7-160" tabindex="-1"></a><span class="co"># Contrastive pretraining</span></span>
<span id="cb7-161"><a href="#cb7-161" tabindex="-1"></a>pretraining_model <span class="op">=</span> ContrastiveModel()</span>
<span id="cb7-162"><a href="#cb7-162" tabindex="-1"></a>pretraining_model.<span class="bu">compile</span>(</span>
<span id="cb7-163"><a href="#cb7-163" tabindex="-1"></a>    contrastive_optimizer<span class="op">=</span>keras.optimizers.Adam(),</span>
<span id="cb7-164"><a href="#cb7-164" tabindex="-1"></a>    probe_optimizer<span class="op">=</span>keras.optimizers.Adam(),</span>
<span id="cb7-165"><a href="#cb7-165" tabindex="-1"></a>)</span>
<span id="cb7-166"><a href="#cb7-166" tabindex="-1"></a></span>
<span id="cb7-167"><a href="#cb7-167" tabindex="-1"></a>pretraining_history <span class="op">=</span> pretraining_model.fit(</span>
<span id="cb7-168"><a href="#cb7-168" tabindex="-1"></a>    train_dataset, epochs<span class="op">=</span>num_epochs, validation_data<span class="op">=</span>test_dataset</span>
<span id="cb7-169"><a href="#cb7-169" tabindex="-1"></a>)</span>
<span id="cb7-170"><a href="#cb7-170" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb7-171"><a href="#cb7-171" tabindex="-1"></a>    <span class="st">"Maximal validation accuracy: </span><span class="sc">{:.2f}</span><span class="st">%"</span>.<span class="bu">format</span>(</span>
<span id="cb7-172"><a href="#cb7-172" tabindex="-1"></a>        <span class="bu">max</span>(pretraining_history.history[<span class="st">"val_p_acc"</span>]) <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb7-173"><a href="#cb7-173" tabindex="-1"></a>    )</span>
<span id="cb7-174"><a href="#cb7-174" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="supervised-finetuning-of-the-pretrained-encoder">Supervised finetuning of the pretrained encoder<a class="anchor" aria-label="anchor" href="#supervised-finetuning-of-the-pretrained-encoder"></a>
</h2>
<p>We then finetune the encoder on the labeled examples, by attaching a
single randomly initalized fully connected classification layer on its
top.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="co"># Supervised finetuning of the pretrained encoder</span></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>finetuning_model <span class="op">=</span> keras.Sequential(</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>    [</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>        layers.Input(shape<span class="op">=</span>(image_size, image_size, image_channels)),</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>        get_augmenter(<span class="op">**</span>classification_augmentation),</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>        pretraining_model.encoder,</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>        layers.Dense(<span class="dv">10</span>),</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>    ],</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"finetuning_model"</span>,</span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>)</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>finetuning_model.<span class="bu">compile</span>(</span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>    optimizer<span class="op">=</span>keras.optimizers.Adam(),</span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>    loss<span class="op">=</span>keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>    metrics<span class="op">=</span>[keras.metrics.SparseCategoricalAccuracy(name<span class="op">=</span><span class="st">"acc"</span>)],</span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a>)</span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a>finetuning_history <span class="op">=</span> finetuning_model.fit(</span>
<span id="cb8-18"><a href="#cb8-18" tabindex="-1"></a>    labeled_train_dataset, epochs<span class="op">=</span>num_epochs, validation_data<span class="op">=</span>test_dataset</span>
<span id="cb8-19"><a href="#cb8-19" tabindex="-1"></a>)</span>
<span id="cb8-20"><a href="#cb8-20" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb8-21"><a href="#cb8-21" tabindex="-1"></a>    <span class="st">"Maximal validation accuracy: </span><span class="sc">{:.2f}</span><span class="st">%"</span>.<span class="bu">format</span>(</span>
<span id="cb8-22"><a href="#cb8-22" tabindex="-1"></a>        <span class="bu">max</span>(finetuning_history.history[<span class="st">"val_acc"</span>]) <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb8-23"><a href="#cb8-23" tabindex="-1"></a>    )</span>
<span id="cb8-24"><a href="#cb8-24" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="comparison-against-the-baseline">Comparison against the baseline<a class="anchor" aria-label="anchor" href="#comparison-against-the-baseline"></a>
</h2>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="co"># The classification accuracies of the baseline and the pretraining + finetuning process:</span></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="kw">def</span> plot_training_curves(</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>    pretraining_history, finetuning_history, baseline_history</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>):</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>    <span class="cf">for</span> metric_key, metric_name <span class="kw">in</span> <span class="bu">zip</span>([<span class="st">"acc"</span>, <span class="st">"loss"</span>], [<span class="st">"accuracy"</span>, <span class="st">"loss"</span>]):</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>        plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>), dpi<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>        plt.plot(</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>            baseline_history.history[<span class="ss">f"val_</span><span class="sc">{</span>metric_key<span class="sc">}</span><span class="ss">"</span>],</span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a>            label<span class="op">=</span><span class="st">"supervised baseline"</span>,</span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a>        )</span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a>        plt.plot(</span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a>            pretraining_history.history[<span class="ss">f"val_p_</span><span class="sc">{</span>metric_key<span class="sc">}</span><span class="ss">"</span>],</span>
<span id="cb9-13"><a href="#cb9-13" tabindex="-1"></a>            label<span class="op">=</span><span class="st">"self-supervised pretraining"</span>,</span>
<span id="cb9-14"><a href="#cb9-14" tabindex="-1"></a>        )</span>
<span id="cb9-15"><a href="#cb9-15" tabindex="-1"></a>        plt.plot(</span>
<span id="cb9-16"><a href="#cb9-16" tabindex="-1"></a>            finetuning_history.history[<span class="ss">f"val_</span><span class="sc">{</span>metric_key<span class="sc">}</span><span class="ss">"</span>],</span>
<span id="cb9-17"><a href="#cb9-17" tabindex="-1"></a>            label<span class="op">=</span><span class="st">"supervised finetuning"</span>,</span>
<span id="cb9-18"><a href="#cb9-18" tabindex="-1"></a>        )</span>
<span id="cb9-19"><a href="#cb9-19" tabindex="-1"></a>        plt.legend()</span>
<span id="cb9-20"><a href="#cb9-20" tabindex="-1"></a>        plt.title(<span class="ss">f"Classification </span><span class="sc">{</span>metric_name<span class="sc">}</span><span class="ss"> during training"</span>)</span>
<span id="cb9-21"><a href="#cb9-21" tabindex="-1"></a>        plt.xlabel(<span class="st">"epochs"</span>)</span>
<span id="cb9-22"><a href="#cb9-22" tabindex="-1"></a>        plt.ylabel(<span class="ss">f"validation </span><span class="sc">{</span>metric_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-23"><a href="#cb9-23" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" tabindex="-1"></a></span>
<span id="cb9-25"><a href="#cb9-25" tabindex="-1"></a>plot_training_curves(pretraining_history, finetuning_history, baseline_history)</span></code></pre></div>
<p>By comparing the training curves, we can see that when using
contrastive pretraining, a higher validation accuracy can be reached,
paired with a lower validation loss, which means that the pretrained
network was able to generalize better when seeing only a small amount of
labeled examples.</p>
</div>
<div class="section level2">
<h2 id="improving-further">Improving further<a class="anchor" aria-label="anchor" href="#improving-further"></a>
</h2>
<div class="section level3">
<h3 id="architecture">Architecture<a class="anchor" aria-label="anchor" href="#architecture"></a>
</h3>
<p>The experiment in the original paper demonstrated that increasing the
width and depth of the models improves performance at a higher rate than
for supervised learning. Also, using a <a href="https://keras.io/api/applications/resnet/#resnet50-function" class="external-link">ResNet-50</a>
encoder is quite standard in the literature. However keep in mind, that
more powerful models will not only increase training time but will also
require more memory and will limit the maximal batch size you can
use.</p>
<p>It has <a href="https://arxiv.org/abs/1905.09272" class="external-link">been</a> <a href="https://arxiv.org/abs/1911.05722" class="external-link">reported</a> that the usage of
BatchNorm layers could sometimes degrade performance, as it introduces
an intra-batch dependency between samples, which is why I did not have
used them in this example. In my experiments however, using BatchNorm,
especially in the projection head, improves performance.</p>
</div>
<div class="section level3">
<h3 id="hyperparameters">Hyperparameters<a class="anchor" aria-label="anchor" href="#hyperparameters"></a>
</h3>
<p>The hyperparameters used in this example have been tuned manually for
this task and architecture. Therefore, without changing them, only
marginal gains can be expected from further hyperparameter tuning.</p>
<p>However for a different task or model architecture these would need
tuning, so here are my notes on the most important ones:</p>
<ul>
<li>
<strong>Batch size</strong>: since the objective can be interpreted
as a classification over a batch of images (loosely speaking), the batch
size is actually a more important hyperparameter than usual. The higher,
the better.</li>
<li>
<strong>Temperature</strong>: the temperature defines the “softness”
of the softmax distribution that is used in the cross-entropy loss, and
is an important hyperparameter. Lower values generally lead to a higher
contrastive accuracy. A recent trick (in <a href="https://arxiv.org/abs/2102.05918" class="external-link">ALIGN</a>) is to learn the
temperature’s value as well (which can be done by defining it as a
tf.Variable, and applying gradients on it). Even though this provides a
good baseline value, in my experiments the learned temperature was
somewhat lower than optimal, as it is optimized with respect to the
contrastive loss, which is not a perfect proxy for representation
quality.</li>
<li>
<strong>Image augmentation strength</strong>: during pretraining
stronger augmentations increase the difficulty of the task, however
after a point too strong augmentations will degrade performance. During
finetuning stronger augmentations reduce overfitting while in my
experience too strong augmentations decrease the performance gains from
pretraining. The whole data augmentation pipeline can be seen as an
important hyperparameter of the algorithm, implementations of other
custom image augmentation layers in Keras can be found in <a href="https://github.com/beresandras/image-augmentation-layers-keras" class="external-link">this
repository</a>.</li>
<li>
<strong>Learning rate schedule</strong>: a constant schedule is used
here, but it is quite common in the literature to use a <a href="https://www.tensorflow.org/api_docs/python/tf/keras/experimental/CosineDecay" class="external-link">cosine
decay schedule</a>, which can further improve performance.</li>
<li>
<strong>Optimizer</strong>: Adam is used in this example, as it
provides good performance with default parameters. SGD with momentum
requires more tuning, however it could slightly increase
performance.</li>
</ul>
</div>
</div>
<div class="section level2">
<h2 id="related-works">Related works<a class="anchor" aria-label="anchor" href="#related-works"></a>
</h2>
<p>Other instance-level (image-level) contrastive learning methods:</p>
<ul>
<li>
<a href="https://arxiv.org/abs/1911.05722" class="external-link">MoCo</a> (<a href="https://arxiv.org/abs/2003.04297" class="external-link">v2</a>, <a href="https://arxiv.org/abs/2104.02057" class="external-link">v3</a>): uses a momentum-encoder
as well, whose weights are an exponential moving average of the target
encoder</li>
<li>
<a href="https://arxiv.org/abs/2006.09882" class="external-link">SwAV</a>: uses clustering
instead of pairwise comparison</li>
<li>
<a href="https://arxiv.org/abs/2103.03230" class="external-link">BarlowTwins</a>: uses a
cross correlation-based objective instead of pairwise comparison</li>
</ul>
<p>Keras implementations of <strong>MoCo</strong> and
<strong>BarlowTwins</strong> can be found in <a href="https://github.com/beresandras/contrastive-classification-keras" class="external-link">this
repository</a>, which includes a Colab notebook.</p>
<p>There is also a new line of works, which optimize a similar
objective, but without the use of any negatives:</p>
<ul>
<li>
<a href="https://arxiv.org/abs/2006.07733" class="external-link">BYOL</a>:
momentum-encoder + no negatives</li>
<li>
<a href="https://arxiv.org/abs/2011.10566" class="external-link">SimSiam</a> (<a href="https://keras.io/examples/vision/simsiam/" class="external-link">Keras example</a>): no
momentum-encoder + no negatives</li>
</ul>
<p>In my experience, these methods are more brittle (they can collapse
to a constant representation, I could not get them to work using this
encoder architecture). Even though they are generally more dependent on
the <a href="https://generallyintelligent.ai/understanding-self-supervised-contrastive-learning.html" class="external-link">model</a>
<a href="https://arxiv.org/abs/2010.10241" class="external-link">architecture</a>, they can
improve performance at smaller batch sizes.</p>
<p>You can use the trained model hosted on <a href="https://huggingface.co/keras-io/semi-supervised-classification-simclr" class="external-link">Hugging
Face Hub</a> and try the demo on <a href="https://huggingface.co/spaces/keras-io/semi-supervised-classification" class="external-link">Hugging
Face Spaces</a>.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, RStudio, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
