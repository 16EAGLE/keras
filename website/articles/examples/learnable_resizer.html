<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="How to optimally learn representations of images for a given resolution.">
<title>Learning to Resize in Computer Vision • keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../../deps/Fira_Mono-0.4.8/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><meta property="og:title" content="Learning to Resize in Computer Vision">
<meta property="og:description" content="How to optimally learn representations of images for a given resolution.">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-getting-started">Getting Started</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-getting-started">
    <a class="dropdown-item" href="../../articles/intro_to_keras_for_engineers.html">Introduction to Keras for engineers</a>
    <h6 class="dropdown-header" data-toc-skip>Tutorials</h6>
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
  </div>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Learning to Resize in Computer Vision</h1>
                        <h4 data-toc-skip class="author"><a href="https://twitter.com/RisingSayak" class="external-link">Sayak Paul</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/learnable_resizer.Rmd" class="external-link"><code>vignettes/examples/learnable_resizer.Rmd</code></a></small>
      <div class="d-none name"><code>learnable_resizer.Rmd</code></div>
    </div>

    
    
<p>It is a common belief that if we constrain vision models to perceive
things as humans do, their performance can be improved. For example, in
<a href="https://arxiv.org/abs/1811.12231" class="external-link">this work</a>, Geirhos et
al. showed that the vision models pre-trained on the ImageNet-1k dataset
are biased towards texture, whereas human beings mostly use the shape
descriptor to develop a common perception. But does this belief always
apply, especially when it comes to improving the performance of vision
models?</p>
<p>It turns out it may not always be the case. When training vision
models, it is common to resize images to a lower dimension ((224 x 224),
(299 x 299), etc.) to allow mini-batch learning and also to keep up the
compute limitations. We generally make use of image resizing methods
like <strong>bilinear interpolation</strong> for this step and the
resized images do not lose much of their perceptual character to the
human eyes. In <a href="https://arxiv.org/abs/2103.09950v1" class="external-link">Learning to
Resize Images for Computer Vision Tasks</a>, Talebi et al. show that if
we try to optimize the perceptual quality of the images for the vision
models rather than the human eyes, their performance can further be
improved. They investigate the following question:</p>
<p><strong>For a given image resolution and a model, how to best resize
the given images?</strong></p>
<p>As shown in the paper, this idea helps to consistently improve the
performance of the common vision models (pre-trained on ImageNet-1k)
like DenseNet-121, ResNet-50, MobileNetV2, and EfficientNets. In this
example, we will implement the learnable image resizing module as
proposed in the paper and demonstrate that on the <a href="https://www.microsoft.com/en-us/download/details.aspx?id=54765" class="external-link">Cats
and Dogs dataset</a> using the <a href="https://arxiv.org/abs/1608.06993" class="external-link">DenseNet-121</a>
architecture.</p>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> ops</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> data <span class="im">as</span> tf_data</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> image <span class="im">as</span> tf_image</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> one_hot <span class="im">as</span> tf_one_hot</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="im">import</span> tensorflow_datasets <span class="im">as</span> tfds</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>tfds.disable_progress_bar()</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="define-hyperparameters">Define hyperparameters<a class="anchor" aria-label="anchor" href="#define-hyperparameters"></a>
</h2>
<p>In order to facilitate mini-batch learning, we need to have a fixed
shape for the images inside a given batch. This is why an initial
resizing is required. We first resize all the images to (300 x 300)
shape and then learn their optimal representation for the (150 x 150)
resolution.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>INP_SIZE <span class="op">=</span> (<span class="dv">300</span>, <span class="dv">300</span>)</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>TARGET_SIZE <span class="op">=</span> (<span class="dv">150</span>, <span class="dv">150</span>)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>INTERPOLATION <span class="op">=</span> <span class="st">"bilinear"</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>AUTO <span class="op">=</span> tf_data.AUTOTUNE</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>EPOCHS <span class="op">=</span> <span class="dv">5</span></span></code></pre></div>
<p>In this example, we will use the bilinear interpolation but the
learnable image resizer module is not dependent on any specific
interpolation method. We can also use others, such as bicubic.</p>
</div>
<div class="section level2">
<h2 id="load-and-prepare-the-dataset">Load and prepare the dataset<a class="anchor" aria-label="anchor" href="#load-and-prepare-the-dataset"></a>
</h2>
<p>For this example, we will only use 40% of the total training
dataset.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>train_ds, validation_ds <span class="op">=</span> tfds.load(</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>    <span class="st">"cats_vs_dogs"</span>,</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>    <span class="co"># Reserve 10% for validation</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>    split<span class="op">=</span>[<span class="st">"train[:40%]"</span>, <span class="st">"train[40%:50%]"</span>],</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>    as_supervised<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>)</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a><span class="kw">def</span> preprocess_dataset(image, label):</span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>    image <span class="op">=</span> tf_image.resize(image, (INP_SIZE[<span class="dv">0</span>], INP_SIZE[<span class="dv">1</span>]))</span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>    label <span class="op">=</span> tf_one_hot(label, depth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>    <span class="cf">return</span> (image, label)</span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a>train_ds <span class="op">=</span> (</span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a>    train_ds.shuffle(BATCH_SIZE <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a>    .<span class="bu">map</span>(preprocess_dataset, num_parallel_calls<span class="op">=</span>AUTO)</span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a>    .batch(BATCH_SIZE)</span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a>    .prefetch(AUTO)</span>
<span id="cb3-20"><a href="#cb3-20" tabindex="-1"></a>)</span>
<span id="cb3-21"><a href="#cb3-21" tabindex="-1"></a>validation_ds <span class="op">=</span> (</span>
<span id="cb3-22"><a href="#cb3-22" tabindex="-1"></a>    validation_ds.<span class="bu">map</span>(preprocess_dataset, num_parallel_calls<span class="op">=</span>AUTO)</span>
<span id="cb3-23"><a href="#cb3-23" tabindex="-1"></a>    .batch(BATCH_SIZE)</span>
<span id="cb3-24"><a href="#cb3-24" tabindex="-1"></a>    .prefetch(AUTO)</span>
<span id="cb3-25"><a href="#cb3-25" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="define-the-learnable-resizer-utilities">Define the learnable resizer utilities<a class="anchor" aria-label="anchor" href="#define-the-learnable-resizer-utilities"></a>
</h2>
<p>The figure below (courtesy: <a href="https://arxiv.org/abs/2103.09950v1" class="external-link">Learning to Resize Images for
Computer Vision Tasks</a>) presents the structure of the learnable
resizing module:</p>
<p><img src="https://i.ibb.co/gJYtSs0/image.png"></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="kw">def</span> conv_block(</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>    x, filters, kernel_size, strides, activation<span class="op">=</span>layers.LeakyReLU(<span class="fl">0.2</span>)</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>):</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>    x <span class="op">=</span> layers.Conv2D(</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>        filters, kernel_size, strides, padding<span class="op">=</span><span class="st">"same"</span>, use_bias<span class="op">=</span><span class="va">False</span></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>    )(x)</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>    x <span class="op">=</span> layers.BatchNormalization()(x)</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>    <span class="cf">if</span> activation:</span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>        x <span class="op">=</span> activation(x)</span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>    <span class="cf">return</span> x</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a><span class="kw">def</span> res_block(x):</span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>    inputs <span class="op">=</span> x</span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a>    x <span class="op">=</span> conv_block(x, <span class="dv">16</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a>    x <span class="op">=</span> conv_block(x, <span class="dv">16</span>, <span class="dv">3</span>, <span class="dv">1</span>, activation<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb4-17"><a href="#cb4-17" tabindex="-1"></a>    <span class="cf">return</span> layers.Add()([inputs, x])</span>
<span id="cb4-18"><a href="#cb4-18" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" tabindex="-1"></a><span class="kw">def</span> get_learnable_resizer(</span>
<span id="cb4-21"><a href="#cb4-21" tabindex="-1"></a>    filters<span class="op">=</span><span class="dv">16</span>, num_res_blocks<span class="op">=</span><span class="dv">1</span>, interpolation<span class="op">=</span>INTERPOLATION</span>
<span id="cb4-22"><a href="#cb4-22" tabindex="-1"></a>):</span>
<span id="cb4-23"><a href="#cb4-23" tabindex="-1"></a>    inputs <span class="op">=</span> layers.Input(shape<span class="op">=</span>[<span class="va">None</span>, <span class="va">None</span>, <span class="dv">3</span>])</span>
<span id="cb4-24"><a href="#cb4-24" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" tabindex="-1"></a>    <span class="co"># First, perform naive resizing.</span></span>
<span id="cb4-26"><a href="#cb4-26" tabindex="-1"></a>    naive_resize <span class="op">=</span> layers.Resizing(<span class="op">*</span>TARGET_SIZE, interpolation<span class="op">=</span>interpolation)(</span>
<span id="cb4-27"><a href="#cb4-27" tabindex="-1"></a>        inputs</span>
<span id="cb4-28"><a href="#cb4-28" tabindex="-1"></a>    )</span>
<span id="cb4-29"><a href="#cb4-29" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" tabindex="-1"></a>    <span class="co"># First convolution block without batch normalization.</span></span>
<span id="cb4-31"><a href="#cb4-31" tabindex="-1"></a>    x <span class="op">=</span> layers.Conv2D(</span>
<span id="cb4-32"><a href="#cb4-32" tabindex="-1"></a>        filters<span class="op">=</span>filters, kernel_size<span class="op">=</span><span class="dv">7</span>, strides<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="st">"same"</span></span>
<span id="cb4-33"><a href="#cb4-33" tabindex="-1"></a>    )(inputs)</span>
<span id="cb4-34"><a href="#cb4-34" tabindex="-1"></a>    x <span class="op">=</span> layers.LeakyReLU(<span class="fl">0.2</span>)(x)</span>
<span id="cb4-35"><a href="#cb4-35" tabindex="-1"></a></span>
<span id="cb4-36"><a href="#cb4-36" tabindex="-1"></a>    <span class="co"># Second convolution block with batch normalization.</span></span>
<span id="cb4-37"><a href="#cb4-37" tabindex="-1"></a>    x <span class="op">=</span> layers.Conv2D(</span>
<span id="cb4-38"><a href="#cb4-38" tabindex="-1"></a>        filters<span class="op">=</span>filters, kernel_size<span class="op">=</span><span class="dv">1</span>, strides<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="st">"same"</span></span>
<span id="cb4-39"><a href="#cb4-39" tabindex="-1"></a>    )(x)</span>
<span id="cb4-40"><a href="#cb4-40" tabindex="-1"></a>    x <span class="op">=</span> layers.LeakyReLU(<span class="fl">0.2</span>)(x)</span>
<span id="cb4-41"><a href="#cb4-41" tabindex="-1"></a>    x <span class="op">=</span> layers.BatchNormalization()(x)</span>
<span id="cb4-42"><a href="#cb4-42" tabindex="-1"></a></span>
<span id="cb4-43"><a href="#cb4-43" tabindex="-1"></a>    <span class="co"># Intermediate resizing as a bottleneck.</span></span>
<span id="cb4-44"><a href="#cb4-44" tabindex="-1"></a>    bottleneck <span class="op">=</span> layers.Resizing(<span class="op">*</span>TARGET_SIZE, interpolation<span class="op">=</span>interpolation)(x)</span>
<span id="cb4-45"><a href="#cb4-45" tabindex="-1"></a></span>
<span id="cb4-46"><a href="#cb4-46" tabindex="-1"></a>    <span class="co"># Residual passes.</span></span>
<span id="cb4-47"><a href="#cb4-47" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_res_blocks):</span>
<span id="cb4-48"><a href="#cb4-48" tabindex="-1"></a>        x <span class="op">=</span> res_block(bottleneck)</span>
<span id="cb4-49"><a href="#cb4-49" tabindex="-1"></a></span>
<span id="cb4-50"><a href="#cb4-50" tabindex="-1"></a>    <span class="co"># Projection.</span></span>
<span id="cb4-51"><a href="#cb4-51" tabindex="-1"></a>    x <span class="op">=</span> layers.Conv2D(</span>
<span id="cb4-52"><a href="#cb4-52" tabindex="-1"></a>        filters<span class="op">=</span>filters,</span>
<span id="cb4-53"><a href="#cb4-53" tabindex="-1"></a>        kernel_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb4-54"><a href="#cb4-54" tabindex="-1"></a>        strides<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb4-55"><a href="#cb4-55" tabindex="-1"></a>        padding<span class="op">=</span><span class="st">"same"</span>,</span>
<span id="cb4-56"><a href="#cb4-56" tabindex="-1"></a>        use_bias<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb4-57"><a href="#cb4-57" tabindex="-1"></a>    )(x)</span>
<span id="cb4-58"><a href="#cb4-58" tabindex="-1"></a>    x <span class="op">=</span> layers.BatchNormalization()(x)</span>
<span id="cb4-59"><a href="#cb4-59" tabindex="-1"></a></span>
<span id="cb4-60"><a href="#cb4-60" tabindex="-1"></a>    <span class="co"># Skip connection.</span></span>
<span id="cb4-61"><a href="#cb4-61" tabindex="-1"></a>    x <span class="op">=</span> layers.Add()([bottleneck, x])</span>
<span id="cb4-62"><a href="#cb4-62" tabindex="-1"></a></span>
<span id="cb4-63"><a href="#cb4-63" tabindex="-1"></a>    <span class="co"># Final resized image.</span></span>
<span id="cb4-64"><a href="#cb4-64" tabindex="-1"></a>    x <span class="op">=</span> layers.Conv2D(filters<span class="op">=</span><span class="dv">3</span>, kernel_size<span class="op">=</span><span class="dv">7</span>, strides<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="st">"same"</span>)(x)</span>
<span id="cb4-65"><a href="#cb4-65" tabindex="-1"></a>    final_resize <span class="op">=</span> layers.Add()([naive_resize, x])</span>
<span id="cb4-66"><a href="#cb4-66" tabindex="-1"></a></span>
<span id="cb4-67"><a href="#cb4-67" tabindex="-1"></a>    <span class="cf">return</span> keras.Model(inputs, final_resize, name<span class="op">=</span><span class="st">"learnable_resizer"</span>)</span>
<span id="cb4-68"><a href="#cb4-68" tabindex="-1"></a></span>
<span id="cb4-69"><a href="#cb4-69" tabindex="-1"></a></span>
<span id="cb4-70"><a href="#cb4-70" tabindex="-1"></a>learnable_resizer <span class="op">=</span> get_learnable_resizer()</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="visualize-the-outputs-of-the-learnable-resizing-module">Visualize the outputs of the learnable resizing module<a class="anchor" aria-label="anchor" href="#visualize-the-outputs-of-the-learnable-resizing-module"></a>
</h2>
<p>Here, we visualize how the resized images would look like after being
passed through the random weights of the resizer.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>sample_images, _ <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_ds))</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>get_np <span class="op">=</span> <span class="kw">lambda</span> image: ops.convert_to_numpy(</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>    ops.squeeze(image)</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>)  <span class="co"># Helper to convert image from any backend to numpy</span></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">10</span>))</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a><span class="cf">for</span> i, image <span class="kw">in</span> <span class="bu">enumerate</span>(sample_images[:<span class="dv">6</span>]):</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>    image <span class="op">=</span> image <span class="op">/</span> <span class="dv">255</span></span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>    ax <span class="op">=</span> plt.subplot(<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">2</span> <span class="op">*</span> i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>    plt.title(<span class="st">"Input Image"</span>)</span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>    plt.imshow(image.numpy().squeeze())</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>    plt.axis(<span class="st">"off"</span>)</span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>    ax <span class="op">=</span> plt.subplot(<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">2</span> <span class="op">*</span> i <span class="op">+</span> <span class="dv">2</span>)</span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>    resized_image <span class="op">=</span> learnable_resizer(image[<span class="va">None</span>, ...])</span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a>    plt.title(<span class="st">"Resized Image"</span>)</span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a>    plt.imshow(get_np(resized_image))</span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a>    plt.axis(<span class="st">"off"</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="model-building-utility">Model building utility<a class="anchor" aria-label="anchor" href="#model-building-utility"></a>
</h2>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="kw">def</span> get_model():</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>    backbone <span class="op">=</span> keras.applications.DenseNet121(</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>        weights<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>        include_top<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>        classes<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>        input_shape<span class="op">=</span>((TARGET_SIZE[<span class="dv">0</span>], TARGET_SIZE[<span class="dv">1</span>], <span class="dv">3</span>)),</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>    )</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>    backbone.trainable <span class="op">=</span> <span class="va">True</span></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>    inputs <span class="op">=</span> layers.Input((INP_SIZE[<span class="dv">0</span>], INP_SIZE[<span class="dv">1</span>], <span class="dv">3</span>))</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>    x <span class="op">=</span> layers.Rescaling(scale<span class="op">=</span><span class="fl">1.0</span> <span class="op">/</span> <span class="dv">255</span>)(inputs)</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>    x <span class="op">=</span> learnable_resizer(x)</span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a>    outputs <span class="op">=</span> backbone(x)</span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a>    <span class="cf">return</span> keras.Model(inputs, outputs)</span></code></pre></div>
<p>The structure of the learnable image resizer module allows for
flexible integrations with different vision models.</p>
</div>
<div class="section level2">
<h2 id="compile-and-train-our-model-with-learnable-resizer">Compile and train our model with learnable resizer<a class="anchor" aria-label="anchor" href="#compile-and-train-our-model-with-learnable-resizer"></a>
</h2>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>model <span class="op">=</span> get_model()</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>    loss<span class="op">=</span>keras.losses.CategoricalCrossentropy(label_smoothing<span class="op">=</span><span class="fl">0.1</span>),</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>    optimizer<span class="op">=</span><span class="st">"sgd"</span>,</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">"accuracy"</span>],</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>)</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>model.fit(train_ds, validation_data<span class="op">=</span>validation_ds, epochs<span class="op">=</span>EPOCHS)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="visualize-the-outputs-of-the-trained-visualizer">Visualize the outputs of the trained visualizer<a class="anchor" aria-label="anchor" href="#visualize-the-outputs-of-the-trained-visualizer"></a>
</h2>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">10</span>))</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a><span class="cf">for</span> i, image <span class="kw">in</span> <span class="bu">enumerate</span>(sample_images[:<span class="dv">6</span>]):</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>    image <span class="op">=</span> image <span class="op">/</span> <span class="dv">255</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>    ax <span class="op">=</span> plt.subplot(<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">2</span> <span class="op">*</span> i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>    plt.title(<span class="st">"Input Image"</span>)</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>    plt.imshow(image.numpy().squeeze())</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>    plt.axis(<span class="st">"off"</span>)</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>    ax <span class="op">=</span> plt.subplot(<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">2</span> <span class="op">*</span> i <span class="op">+</span> <span class="dv">2</span>)</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>    resized_image <span class="op">=</span> learnable_resizer(image[<span class="va">None</span>, ...])</span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>    plt.title(<span class="st">"Resized Image"</span>)</span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>    plt.imshow(get_np(resized_image) <span class="op">/</span> <span class="dv">10</span>)</span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>    plt.axis(<span class="st">"off"</span>)</span></code></pre></div>
<p>The plot shows that the visuals of the images have improved with
training. The following table shows the benefits of using the resizing
module in comparison to using the bilinear interpolation:</p>
<table class="table">
<colgroup>
<col width="35%">
<col width="43%">
<col width="21%">
</colgroup>
<thead><tr class="header">
<th align="center">Model</th>
<th align="center">Number of parameters (Million)</th>
<th align="center">Top-1 accuracy</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="center">With the learnable resizer</td>
<td align="center">7.051717</td>
<td align="center">67.67%</td>
</tr>
<tr class="even">
<td align="center">Without the learnable resizer</td>
<td align="center">7.039554</td>
<td align="center">60.19%</td>
</tr>
</tbody>
</table>
<p>For more details, you can check out <a href="https://github.com/sayakpaul/Learnable-Image-Resizing" class="external-link">this
repository</a>. Note the above-reported models were trained for 10
epochs on 90% of the training set of Cats and Dogs unlike this example.
Also, note that the increase in the number of parameters due to the
resizing module is very negligible. To ensure that the improvement in
the performance is not due to stochasticity, the models were trained
using the same initial random weights.</p>
<p>Now, a question worth asking here is - <em>isn’t the improved
accuracy simply a consequence of adding more layers (the resizer is a
mini network after all) to the model, compared to the baseline?</em></p>
<p>To show that it is not the case, the authors conduct the following
experiment:</p>
<ul>
<li><p>Take a pre-trained model trained some size, say (224 x
224).</p></li>
<li><p>Now, first, use it to infer predictions on images resized to a
lower resolution. Record the performance.</p></li>
<li><p>For the second experiment, plug in the resizer module at the top
of the pre-trained model and warm-start the training. Record the
performance.</p></li>
</ul>
<p>Now, the authors argue that using the second option is better because
it helps the model learn how to adjust the representations better with
respect to the given resolution. Since the results purely are empirical,
a few more experiments such as analyzing the cross-channel interaction
would have been even better. It is worth noting that elements like <a href="https://arxiv.org/abs/1709.01507" class="external-link">Squeeze and Excitation (SE)
blocks</a>, <a href="https://arxiv.org/pdf/1904.11492" class="external-link">Global Context
(GC) blocks</a> also add a few parameters to an existing network but
they are known to help a network process information in systematic ways
to improve the overall performance.</p>
</div>
<div class="section level2">
<h2 id="notes">Notes<a class="anchor" aria-label="anchor" href="#notes"></a>
</h2>
<ul>
<li><p>To impose shape bias inside the vision models, Geirhos et
al. trained them with a combination of natural and stylized images. It
might be interesting to investigate if this learnable resizing module
could achieve something similar as the outputs seem to discard the
texture information.</p></li>
<li><p>The resizer module can handle arbitrary resolutions and aspect
ratios which is very important for tasks like object detection and
segmentation.</p></li>
<li><p>There is another closely related topic on <strong><em>adaptive
image resizing</em></strong> that attempts to resize images/feature maps
adaptively during training. <a href="https://arxiv.org/pdf/2104.00298" class="external-link">EfficientV2</a> uses this
idea.</p></li>
</ul>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
