<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Implement Gradient Centralization to improve training performance of DNNs.">
<title>Gradient Centralization for Better Training Performance • keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><meta property="og:title" content="Gradient Centralization for Better Training Performance">
<meta property="og:description" content="Implement Gradient Centralization to improve training performance of DNNs.">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-getting-started">Getting Started</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-getting-started">
    <a class="dropdown-item" href="../../articles/intro_to_keras_for_engineers.html">Introduction to Keras for engineers</a>
    <h6 class="dropdown-header" data-toc-skip>Tutorials</h6>
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
  </div>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Gradient Centralization for Better Training Performance</h1>
                        <h4 data-toc-skip class="author"><a href="https://github.com/Rishit-dagli" class="external-link">Rishit Dagli</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/gradient_centralization.Rmd" class="external-link"><code>vignettes/examples/gradient_centralization.Rmd</code></a></small>
      <div class="d-none name"><code>gradient_centralization.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>This example implements <a href="https://arxiv.org/abs/2004.01461" class="external-link">Gradient Centralization</a>, a
new optimization technique for Deep Neural Networks by Yong et al., and
demonstrates it on Laurence Moroney’s <a href="https://www.tensorflow.org/datasets/catalog/horses_or_humans" class="external-link">Horses
or Humans Dataset</a>. Gradient Centralization can both speedup training
process and improve the final generalization performance of DNNs. It
operates directly on gradients by centralizing the gradient vectors to
have zero mean. Gradient Centralization morever improves the
Lipschitzness of the loss function and its gradient so that the training
process becomes more efficient and stable.</p>
<p>This example requires <code>tensorflow_datasets</code> which can be
installed with this command:</p>
<pre><code>pip install tensorflow-datasets</code></pre>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="im">from</span> time <span class="im">import</span> time</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a><span class="im">from</span> keras.optimizers <span class="im">import</span> RMSprop</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> ops</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> data <span class="im">as</span> tf_data</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a><span class="im">import</span> tensorflow_datasets <span class="im">as</span> tfds</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="prepare-the-data">Prepare the data<a class="anchor" aria-label="anchor" href="#prepare-the-data"></a>
</h2>
<p>For this example, we will be using the <a href="https://www.tensorflow.org/datasets/catalog/horses_or_humans" class="external-link">Horses
or Humans dataset</a>.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>num_classes <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>input_shape <span class="op">=</span> (<span class="dv">300</span>, <span class="dv">300</span>, <span class="dv">3</span>)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>dataset_name <span class="op">=</span> <span class="st">"horses_or_humans"</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>AUTOTUNE <span class="op">=</span> tf_data.AUTOTUNE</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>(train_ds, test_ds), metadata <span class="op">=</span> tfds.load(</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>    name<span class="op">=</span>dataset_name,</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>    split<span class="op">=</span>[tfds.Split.TRAIN, tfds.Split.TEST],</span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>    with_info<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>    as_supervised<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>)</span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image shape: </span><span class="sc">{</span>metadata<span class="sc">.</span>features[<span class="st">'image'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training images: </span><span class="sc">{</span>metadata<span class="sc">.</span>splits[<span class="st">'train'</span>]<span class="sc">.</span>num_examples<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test images: </span><span class="sc">{</span>metadata<span class="sc">.</span>splits[<span class="st">'test'</span>]<span class="sc">.</span>num_examples<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="use-data-augmentation">Use Data Augmentation<a class="anchor" aria-label="anchor" href="#use-data-augmentation"></a>
</h2>
<p>We will rescale the data to <code>[0, 1]</code> and perform simple
augmentations to our data.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>rescale <span class="op">=</span> layers.Rescaling(<span class="fl">1.0</span> <span class="op">/</span> <span class="dv">255</span>)</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>data_augmentation <span class="op">=</span> [</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>    layers.RandomFlip(<span class="st">"horizontal_and_vertical"</span>),</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>    layers.RandomRotation(<span class="fl">0.3</span>),</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>    layers.RandomZoom(<span class="fl">0.2</span>),</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>]</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a><span class="co"># Helper to apply augmentation</span></span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a><span class="kw">def</span> apply_aug(x):</span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>    <span class="cf">for</span> aug <span class="kw">in</span> data_augmentation:</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a>        x <span class="op">=</span> aug(x)</span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>    <span class="cf">return</span> x</span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" tabindex="-1"></a><span class="kw">def</span> prepare(ds, shuffle<span class="op">=</span><span class="va">False</span>, augment<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb4-18"><a href="#cb4-18" tabindex="-1"></a>    <span class="co"># Rescale dataset</span></span>
<span id="cb4-19"><a href="#cb4-19" tabindex="-1"></a>    ds <span class="op">=</span> ds.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: (rescale(x), y), num_parallel_calls<span class="op">=</span>AUTOTUNE)</span>
<span id="cb4-20"><a href="#cb4-20" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" tabindex="-1"></a>    <span class="cf">if</span> shuffle:</span>
<span id="cb4-22"><a href="#cb4-22" tabindex="-1"></a>        ds <span class="op">=</span> ds.shuffle(<span class="dv">1024</span>)</span>
<span id="cb4-23"><a href="#cb4-23" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" tabindex="-1"></a>    <span class="co"># Batch dataset</span></span>
<span id="cb4-25"><a href="#cb4-25" tabindex="-1"></a>    ds <span class="op">=</span> ds.batch(batch_size)</span>
<span id="cb4-26"><a href="#cb4-26" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" tabindex="-1"></a>    <span class="co"># Use data augmentation only on the training set</span></span>
<span id="cb4-28"><a href="#cb4-28" tabindex="-1"></a>    <span class="cf">if</span> augment:</span>
<span id="cb4-29"><a href="#cb4-29" tabindex="-1"></a>        ds <span class="op">=</span> ds.<span class="bu">map</span>(</span>
<span id="cb4-30"><a href="#cb4-30" tabindex="-1"></a>            <span class="kw">lambda</span> x, y: (apply_aug(x), y),</span>
<span id="cb4-31"><a href="#cb4-31" tabindex="-1"></a>            num_parallel_calls<span class="op">=</span>AUTOTUNE,</span>
<span id="cb4-32"><a href="#cb4-32" tabindex="-1"></a>        )</span>
<span id="cb4-33"><a href="#cb4-33" tabindex="-1"></a></span>
<span id="cb4-34"><a href="#cb4-34" tabindex="-1"></a>    <span class="co"># Use buffered prefecting</span></span>
<span id="cb4-35"><a href="#cb4-35" tabindex="-1"></a>    <span class="cf">return</span> ds.prefetch(buffer_size<span class="op">=</span>AUTOTUNE)</span></code></pre></div>
<p>Rescale and augment the data</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>train_ds <span class="op">=</span> prepare(train_ds, shuffle<span class="op">=</span><span class="va">True</span>, augment<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>test_ds <span class="op">=</span> prepare(test_ds)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="define-a-model">Define a model<a class="anchor" aria-label="anchor" href="#define-a-model"></a>
</h2>
<p>In this section we will define a Convolutional neural network.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>model <span class="op">=</span> keras.Sequential(</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>    [</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>        layers.Conv2D(<span class="dv">16</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">"relu"</span>, input_shape<span class="op">=</span>(<span class="dv">300</span>, <span class="dv">300</span>, <span class="dv">3</span>)),</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>        layers.MaxPooling2D(<span class="dv">2</span>, <span class="dv">2</span>),</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>        layers.Conv2D(<span class="dv">32</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>        layers.Dropout(<span class="fl">0.5</span>),</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>        layers.MaxPooling2D(<span class="dv">2</span>, <span class="dv">2</span>),</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>        layers.Conv2D(<span class="dv">64</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>        layers.Dropout(<span class="fl">0.5</span>),</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>        layers.MaxPooling2D(<span class="dv">2</span>, <span class="dv">2</span>),</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>        layers.Conv2D(<span class="dv">64</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>        layers.MaxPooling2D(<span class="dv">2</span>, <span class="dv">2</span>),</span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a>        layers.Conv2D(<span class="dv">64</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a>        layers.MaxPooling2D(<span class="dv">2</span>, <span class="dv">2</span>),</span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a>        layers.Flatten(),</span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a>        layers.Dropout(<span class="fl">0.5</span>),</span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a>        layers.Dense(<span class="dv">512</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a>        layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span>),</span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a>    ]</span>
<span id="cb6-20"><a href="#cb6-20" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="implement-gradient-centralization">Implement Gradient Centralization<a class="anchor" aria-label="anchor" href="#implement-gradient-centralization"></a>
</h2>
<p>We will now subclass the <code>RMSProp</code> optimizer class
modifying the <code>keras.optimizers.Optimizer.get_gradients()</code>
method where we now implement Gradient Centralization. On a high level
the idea is that let us say we obtain our gradients through back
propogation for a Dense or Convolution layer we then compute the mean of
the column vectors of the weight matrix, and then remove the mean from
each column vector.</p>
<p>The experiments in <a href="https://arxiv.org/abs/2004.01461" class="external-link">this
paper</a> on various applications, including general image
classification, fine-grained image classification, detection and
segmentation and Person ReID demonstrate that GC can consistently
improve the performance of DNN learning.</p>
<p>Also, for simplicity at the moment we are not implementing gradient
cliiping functionality, however this quite easy to implement.</p>
<p>At the moment we are just creating a subclass for the
<code>RMSProp</code> optimizer however you could easily reproduce this
for any other optimizer or on a custom optimizer in the same way. We
will be using this class in the later section when we train a model with
Gradient Centralization.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="kw">class</span> GCRMSprop(RMSprop):</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>    <span class="kw">def</span> get_gradients(<span class="va">self</span>, loss, params):</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>        <span class="co"># We here just provide a modified get_gradients() function since we are</span></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>        <span class="co"># trying to just compute the centralized gradients.</span></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>        grads <span class="op">=</span> []</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>        gradients <span class="op">=</span> <span class="bu">super</span>().get_gradients()</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>        <span class="cf">for</span> grad <span class="kw">in</span> gradients:</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>            grad_len <span class="op">=</span> <span class="bu">len</span>(grad.shape)</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>            <span class="cf">if</span> grad_len <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>                axis <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(grad_len <span class="op">-</span> <span class="dv">1</span>))</span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>                grad <span class="op">-=</span> ops.mean(grad, axis<span class="op">=</span>axis, keep_dims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a>            grads.append(grad)</span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a>        <span class="cf">return</span> grads</span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a>optimizer <span class="op">=</span> GCRMSprop(learning_rate<span class="op">=</span><span class="fl">1e-4</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="training-utilities">Training utilities<a class="anchor" aria-label="anchor" href="#training-utilities"></a>
</h2>
<p>We will also create a callback which allows us to easily measure the
total training time and the time taken for each epoch since we are
interested in comparing the effect of Gradient Centralization on the
model we built above.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="kw">class</span> TimeHistory(keras.callbacks.Callback):</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>    <span class="kw">def</span> on_train_begin(<span class="va">self</span>, logs<span class="op">=</span>{}):</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>        <span class="va">self</span>.times <span class="op">=</span> []</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>    <span class="kw">def</span> on_epoch_begin(<span class="va">self</span>, batch, logs<span class="op">=</span>{}):</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>        <span class="va">self</span>.epoch_time_start <span class="op">=</span> time()</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>    <span class="kw">def</span> on_epoch_end(<span class="va">self</span>, batch, logs<span class="op">=</span>{}):</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>        <span class="va">self</span>.times.append(time() <span class="op">-</span> <span class="va">self</span>.epoch_time_start)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="train-the-model-without-gc">Train the model without GC<a class="anchor" aria-label="anchor" href="#train-the-model-without-gc"></a>
</h2>
<p>We now train the model we built earlier without Gradient
Centralization which we can compare to the training performance of the
model trained with Gradient Centralization.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>time_callback_no_gc <span class="op">=</span> TimeHistory()</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">"binary_crossentropy"</span>,</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>    optimizer<span class="op">=</span>RMSprop(learning_rate<span class="op">=</span><span class="fl">1e-4</span>),</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">"accuracy"</span>],</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>)</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>model.summary()</span></code></pre></div>
<p>We also save the history since we later want to compare our model
trained with and not trained with Gradient Centralization</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>history_no_gc <span class="op">=</span> model.fit(</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>    train_ds, epochs<span class="op">=</span><span class="dv">10</span>, verbose<span class="op">=</span><span class="dv">1</span>, callbacks<span class="op">=</span>[time_callback_no_gc]</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="train-the-model-with-gc">Train the model with GC<a class="anchor" aria-label="anchor" href="#train-the-model-with-gc"></a>
</h2>
<p>We will now train the same model, this time using Gradient
Centralization, notice our optimizer is the one using Gradient
Centralization this time.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>time_callback_gc <span class="op">=</span> TimeHistory()</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">"binary_crossentropy"</span>, optimizer<span class="op">=</span>optimizer, metrics<span class="op">=</span>[<span class="st">"accuracy"</span>]</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>)</span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>model.summary()</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a>history_gc <span class="op">=</span> model.fit(</span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a>    train_ds, epochs<span class="op">=</span><span class="dv">10</span>, verbose<span class="op">=</span><span class="dv">1</span>, callbacks<span class="op">=</span>[time_callback_gc]</span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="comparing-performance">Comparing performance<a class="anchor" aria-label="anchor" href="#comparing-performance"></a>
</h2>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Not using Gradient Centralization"</span>)</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Loss: </span><span class="sc">{</span>history_no_gc<span class="sc">.</span>history[<span class="st">'loss'</span>][<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>history_no_gc<span class="sc">.</span>history[<span class="st">'accuracy'</span>][<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training Time: </span><span class="sc">{</span><span class="bu">sum</span>(time_callback_no_gc.times)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Using Gradient Centralization"</span>)</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Loss: </span><span class="sc">{</span>history_gc<span class="sc">.</span>history[<span class="st">'loss'</span>][<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>history_gc<span class="sc">.</span>history[<span class="st">'accuracy'</span>][<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training Time: </span><span class="sc">{</span><span class="bu">sum</span>(time_callback_gc.times)<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
<p>Readers are encouraged to try out Gradient Centralization on
different datasets from different domains and experiment with it’s
effect. You are strongly advised to check out the <a href="https://arxiv.org/abs/2004.01461" class="external-link">original paper</a> as well - the
authors present several studies on Gradient Centralization showing how
it can improve general performance, generalization, training time as
well as more efficient.</p>
<p>Many thanks to <a href="https://github.com/ialimustufa" class="external-link">Ali Mustufa
Shaikh</a> for reviewing this implementation.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, RStudio, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.9000.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
