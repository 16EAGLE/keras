<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Training a video classifier with hybrid transformers.">
<title>Video Classification with Transformers â€¢ keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><meta property="og:title" content="Video Classification with Transformers">
<meta property="og:description" content="Training a video classifier with hybrid transformers.">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-getting-started">Getting Started</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-getting-started">
    <a class="dropdown-item" href="../../articles/intro_to_keras_for_engineers.html">Introduction to Keras for engineers</a>
    <h6 class="dropdown-header" data-toc-skip>Tutorials</h6>
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
  </div>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Video Classification with Transformers</h1>
                        <h4 data-toc-skip class="author"><a href="https://twitter.com/RisingSayak" class="external-link">Sayak Paul</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/video_transformers.Rmd" class="external-link"><code>vignettes/examples/video_transformers.Rmd</code></a></small>
      <div class="d-none name"><code>video_transformers.Rmd</code></div>
    </div>

    
    
<p>This example is a follow-up to the <a href="https://keras.io/examples/vision/video_classification/" class="external-link">Video
Classification with a CNN-RNN Architecture</a> example. This time, we
will be using a Transformer-based model (<a href="https://arxiv.org/abs/1706.03762" class="external-link">Vaswani et al.</a>) to classify
videos. You can follow <a href="https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-11" class="external-link">this
book chapter</a> in case you need an introduction to Transformers (with
code). After reading this example, you will know how to develop hybrid
Transformer-based models for video classification that operate on CNN
feature maps.</p>
<p>pip install -q git+<a href="https://github.com/keras-team/keras" class="external-link uri">https://github.com/keras-team/keras</a> pip install -q
git+<a href="https://github.com/tensorflow/docs" class="external-link uri">https://github.com/tensorflow/docs</a></p>
<div class="section level2">
<h2 id="data-collection">Data collection<a class="anchor" aria-label="anchor" href="#data-collection"></a>
</h2>
<p>As done in the <a href="https://keras.io/examples/vision/video_classification/" class="external-link">predecessor</a>
to this example, we will be using a subsampled version of the <a href="https://www.crcv.ucf.edu/data/UCF101.php" class="external-link">UCF101 dataset</a>, a
well-known benchmark dataset. In case you want to operate on a larger
subsample or even the entire dataset, please refer to <a href="https://colab.research.google.com/github/sayakpaul/Action-Recognition-in-TensorFlow/blob/main/Data_Preparation_UCF101.ipynb" class="external-link">this
notebook</a>.</p>
<p>wget -q <a href="https://github.com/sayakpaul/Action-Recognition-in-TensorFlow/releases/download/v1.0.0/ucf101_top5.tar.gz" class="external-link uri">https://github.com/sayakpaul/Action-Recognition-in-TensorFlow/releases/download/v1.0.0/ucf101_top5.tar.gz</a>
tar -xf ucf101_top5.tar.gz</p>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>os.environ[<span class="st">"KERAS_BACKEND"</span>] <span class="op">=</span> <span class="st">"jax"</span>  <span class="co"># @param ["tensorflow", "jax", "torch"]</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="im">from</span> keras.applications.densenet <span class="im">import</span> DenseNet121</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="im">from</span> tensorflow_docs.vis <span class="im">import</span> embed</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a><span class="im">import</span> imageio</span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a><span class="im">import</span> cv2</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="define-hyperparameters">Define hyperparameters<a class="anchor" aria-label="anchor" href="#define-hyperparameters"></a>
</h2>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>MAX_SEQ_LENGTH <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>NUM_FEATURES <span class="op">=</span> <span class="dv">1024</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>IMG_SIZE <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>EPOCHS <span class="op">=</span> <span class="dv">5</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="data-preparation">Data preparation<a class="anchor" aria-label="anchor" href="#data-preparation"></a>
</h2>
<p>We will mostly be following the same data preparation steps in this
example, except for the following changes:</p>
<ul>
<li>We reduce the image size to 128x128 instead of 224x224 to speed up
computation.</li>
<li>Instead of using a pre-trained <a href="https://arxiv.org/abs/1512.00567" class="external-link">InceptionV3</a> network, we use
a pre-trained <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf" class="external-link">DenseNet121</a>
for feature extraction.</li>
<li>We directly pad shorter videos to length
<code>MAX_SEQ_LENGTH</code>.</li>
</ul>
<p>First, letâ€™s load up the <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html" class="external-link">DataFrames</a>.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>train_df <span class="op">=</span> pd.read_csv(<span class="st">"train.csv"</span>)</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>test_df <span class="op">=</span> pd.read_csv(<span class="st">"test.csv"</span>)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total videos for training: </span><span class="sc">{</span><span class="bu">len</span>(train_df)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total videos for testing: </span><span class="sc">{</span><span class="bu">len</span>(test_df)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>center_crop_layer <span class="op">=</span> layers.CenterCrop(IMG_SIZE, IMG_SIZE)</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a><span class="kw">def</span> crop_center(frame):</span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>    cropped <span class="op">=</span> center_crop_layer(frame[<span class="va">None</span>, ...])</span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>    cropped <span class="op">=</span> keras.ops.convert_to_numpy(cropped)</span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a>    cropped <span class="op">=</span> keras.ops.squeeze(cropped)</span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a>    <span class="cf">return</span> cropped</span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a><span class="co"># Following method is modified from this tutorial:</span></span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a><span class="co"># https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub</span></span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a><span class="kw">def</span> load_video(path, max_frames<span class="op">=</span><span class="dv">0</span>, offload_to_cpu<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb3-20"><a href="#cb3-20" tabindex="-1"></a>    cap <span class="op">=</span> cv2.VideoCapture(path)</span>
<span id="cb3-21"><a href="#cb3-21" tabindex="-1"></a>    frames <span class="op">=</span> []</span>
<span id="cb3-22"><a href="#cb3-22" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb3-23"><a href="#cb3-23" tabindex="-1"></a>        <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb3-24"><a href="#cb3-24" tabindex="-1"></a>            ret, frame <span class="op">=</span> cap.read()</span>
<span id="cb3-25"><a href="#cb3-25" tabindex="-1"></a>            <span class="cf">if</span> <span class="kw">not</span> ret:</span>
<span id="cb3-26"><a href="#cb3-26" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb3-27"><a href="#cb3-27" tabindex="-1"></a>            frame <span class="op">=</span> frame[:, :, [<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>]]</span>
<span id="cb3-28"><a href="#cb3-28" tabindex="-1"></a>            frame <span class="op">=</span> crop_center(frame)</span>
<span id="cb3-29"><a href="#cb3-29" tabindex="-1"></a>            <span class="cf">if</span> offload_to_cpu <span class="kw">and</span> keras.backend.backend() <span class="op">==</span> <span class="st">"torch"</span>:</span>
<span id="cb3-30"><a href="#cb3-30" tabindex="-1"></a>                frame <span class="op">=</span> frame.to(<span class="st">"cpu"</span>)</span>
<span id="cb3-31"><a href="#cb3-31" tabindex="-1"></a>            frames.append(frame)</span>
<span id="cb3-32"><a href="#cb3-32" tabindex="-1"></a></span>
<span id="cb3-33"><a href="#cb3-33" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(frames) <span class="op">==</span> max_frames:</span>
<span id="cb3-34"><a href="#cb3-34" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb3-35"><a href="#cb3-35" tabindex="-1"></a>    <span class="cf">finally</span>:</span>
<span id="cb3-36"><a href="#cb3-36" tabindex="-1"></a>        cap.release()</span>
<span id="cb3-37"><a href="#cb3-37" tabindex="-1"></a>    <span class="cf">if</span> offload_to_cpu <span class="kw">and</span> keras.backend.backend() <span class="op">==</span> <span class="st">"torch"</span>:</span>
<span id="cb3-38"><a href="#cb3-38" tabindex="-1"></a>        <span class="cf">return</span> np.array([frame.to(<span class="st">"cpu"</span>).numpy() <span class="cf">for</span> frame <span class="kw">in</span> frames])</span>
<span id="cb3-39"><a href="#cb3-39" tabindex="-1"></a>    <span class="cf">return</span> np.array(frames)</span>
<span id="cb3-40"><a href="#cb3-40" tabindex="-1"></a></span>
<span id="cb3-41"><a href="#cb3-41" tabindex="-1"></a></span>
<span id="cb3-42"><a href="#cb3-42" tabindex="-1"></a><span class="kw">def</span> build_feature_extractor():</span>
<span id="cb3-43"><a href="#cb3-43" tabindex="-1"></a>    feature_extractor <span class="op">=</span> DenseNet121(</span>
<span id="cb3-44"><a href="#cb3-44" tabindex="-1"></a>        weights<span class="op">=</span><span class="st">"imagenet"</span>,</span>
<span id="cb3-45"><a href="#cb3-45" tabindex="-1"></a>        include_top<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb3-46"><a href="#cb3-46" tabindex="-1"></a>        pooling<span class="op">=</span><span class="st">"avg"</span>,</span>
<span id="cb3-47"><a href="#cb3-47" tabindex="-1"></a>        input_shape<span class="op">=</span>(IMG_SIZE, IMG_SIZE, <span class="dv">3</span>),</span>
<span id="cb3-48"><a href="#cb3-48" tabindex="-1"></a>    )</span>
<span id="cb3-49"><a href="#cb3-49" tabindex="-1"></a>    preprocess_input <span class="op">=</span> keras.applications.densenet.preprocess_input</span>
<span id="cb3-50"><a href="#cb3-50" tabindex="-1"></a></span>
<span id="cb3-51"><a href="#cb3-51" tabindex="-1"></a>    inputs <span class="op">=</span> keras.Input((IMG_SIZE, IMG_SIZE, <span class="dv">3</span>))</span>
<span id="cb3-52"><a href="#cb3-52" tabindex="-1"></a>    preprocessed <span class="op">=</span> preprocess_input(inputs)</span>
<span id="cb3-53"><a href="#cb3-53" tabindex="-1"></a></span>
<span id="cb3-54"><a href="#cb3-54" tabindex="-1"></a>    outputs <span class="op">=</span> feature_extractor(preprocessed)</span>
<span id="cb3-55"><a href="#cb3-55" tabindex="-1"></a>    <span class="cf">return</span> keras.Model(inputs, outputs, name<span class="op">=</span><span class="st">"feature_extractor"</span>)</span>
<span id="cb3-56"><a href="#cb3-56" tabindex="-1"></a></span>
<span id="cb3-57"><a href="#cb3-57" tabindex="-1"></a></span>
<span id="cb3-58"><a href="#cb3-58" tabindex="-1"></a>feature_extractor <span class="op">=</span> build_feature_extractor()</span>
<span id="cb3-59"><a href="#cb3-59" tabindex="-1"></a></span>
<span id="cb3-60"><a href="#cb3-60" tabindex="-1"></a></span>
<span id="cb3-61"><a href="#cb3-61" tabindex="-1"></a><span class="co"># Label preprocessing with StringLookup.</span></span>
<span id="cb3-62"><a href="#cb3-62" tabindex="-1"></a>label_processor <span class="op">=</span> keras.layers.StringLookup(</span>
<span id="cb3-63"><a href="#cb3-63" tabindex="-1"></a>    num_oov_indices<span class="op">=</span><span class="dv">0</span>, vocabulary<span class="op">=</span>np.unique(train_df[<span class="st">"tag"</span>]), mask_token<span class="op">=</span><span class="va">None</span></span>
<span id="cb3-64"><a href="#cb3-64" tabindex="-1"></a>)</span>
<span id="cb3-65"><a href="#cb3-65" tabindex="-1"></a><span class="bu">print</span>(label_processor.get_vocabulary())</span>
<span id="cb3-66"><a href="#cb3-66" tabindex="-1"></a></span>
<span id="cb3-67"><a href="#cb3-67" tabindex="-1"></a></span>
<span id="cb3-68"><a href="#cb3-68" tabindex="-1"></a><span class="kw">def</span> prepare_all_videos(df, root_dir):</span>
<span id="cb3-69"><a href="#cb3-69" tabindex="-1"></a>    num_samples <span class="op">=</span> <span class="bu">len</span>(df)</span>
<span id="cb3-70"><a href="#cb3-70" tabindex="-1"></a>    video_paths <span class="op">=</span> df[<span class="st">"video_name"</span>].values.tolist()</span>
<span id="cb3-71"><a href="#cb3-71" tabindex="-1"></a>    labels <span class="op">=</span> df[<span class="st">"tag"</span>].values</span>
<span id="cb3-72"><a href="#cb3-72" tabindex="-1"></a>    labels <span class="op">=</span> label_processor(labels[..., <span class="va">None</span>]).numpy()</span>
<span id="cb3-73"><a href="#cb3-73" tabindex="-1"></a></span>
<span id="cb3-74"><a href="#cb3-74" tabindex="-1"></a>    <span class="co"># `frame_features` are what we will feed to our sequence model.</span></span>
<span id="cb3-75"><a href="#cb3-75" tabindex="-1"></a>    frame_features <span class="op">=</span> np.zeros(</span>
<span id="cb3-76"><a href="#cb3-76" tabindex="-1"></a>        shape<span class="op">=</span>(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype<span class="op">=</span><span class="st">"float32"</span></span>
<span id="cb3-77"><a href="#cb3-77" tabindex="-1"></a>    )</span>
<span id="cb3-78"><a href="#cb3-78" tabindex="-1"></a></span>
<span id="cb3-79"><a href="#cb3-79" tabindex="-1"></a>    <span class="co"># For each video.</span></span>
<span id="cb3-80"><a href="#cb3-80" tabindex="-1"></a>    <span class="cf">for</span> idx, path <span class="kw">in</span> <span class="bu">enumerate</span>(video_paths):</span>
<span id="cb3-81"><a href="#cb3-81" tabindex="-1"></a>        <span class="co"># Gather all its frames and add a batch dimension.</span></span>
<span id="cb3-82"><a href="#cb3-82" tabindex="-1"></a>        frames <span class="op">=</span> load_video(os.path.join(root_dir, path))</span>
<span id="cb3-83"><a href="#cb3-83" tabindex="-1"></a></span>
<span id="cb3-84"><a href="#cb3-84" tabindex="-1"></a>        <span class="co"># Pad shorter videos.</span></span>
<span id="cb3-85"><a href="#cb3-85" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(frames) <span class="op">&lt;</span> MAX_SEQ_LENGTH:</span>
<span id="cb3-86"><a href="#cb3-86" tabindex="-1"></a>            diff <span class="op">=</span> MAX_SEQ_LENGTH <span class="op">-</span> <span class="bu">len</span>(frames)</span>
<span id="cb3-87"><a href="#cb3-87" tabindex="-1"></a>            padding <span class="op">=</span> np.zeros((diff, IMG_SIZE, IMG_SIZE, <span class="dv">3</span>))</span>
<span id="cb3-88"><a href="#cb3-88" tabindex="-1"></a>            frames <span class="op">=</span> np.concatenate(frames, padding)</span>
<span id="cb3-89"><a href="#cb3-89" tabindex="-1"></a></span>
<span id="cb3-90"><a href="#cb3-90" tabindex="-1"></a>        frames <span class="op">=</span> frames[<span class="va">None</span>, ...]</span>
<span id="cb3-91"><a href="#cb3-91" tabindex="-1"></a></span>
<span id="cb3-92"><a href="#cb3-92" tabindex="-1"></a>        <span class="co"># Initialize placeholder to store the features of the current video.</span></span>
<span id="cb3-93"><a href="#cb3-93" tabindex="-1"></a>        temp_frame_features <span class="op">=</span> np.zeros(</span>
<span id="cb3-94"><a href="#cb3-94" tabindex="-1"></a>            shape<span class="op">=</span>(<span class="dv">1</span>, MAX_SEQ_LENGTH, NUM_FEATURES), dtype<span class="op">=</span><span class="st">"float32"</span></span>
<span id="cb3-95"><a href="#cb3-95" tabindex="-1"></a>        )</span>
<span id="cb3-96"><a href="#cb3-96" tabindex="-1"></a></span>
<span id="cb3-97"><a href="#cb3-97" tabindex="-1"></a>        <span class="co"># Extract features from the frames of the current video.</span></span>
<span id="cb3-98"><a href="#cb3-98" tabindex="-1"></a>        <span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(frames):</span>
<span id="cb3-99"><a href="#cb3-99" tabindex="-1"></a>            video_length <span class="op">=</span> batch.shape[<span class="dv">0</span>]</span>
<span id="cb3-100"><a href="#cb3-100" tabindex="-1"></a>            length <span class="op">=</span> <span class="bu">min</span>(MAX_SEQ_LENGTH, video_length)</span>
<span id="cb3-101"><a href="#cb3-101" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(length):</span>
<span id="cb3-102"><a href="#cb3-102" tabindex="-1"></a>                <span class="cf">if</span> np.mean(batch[j, :]) <span class="op">&gt;</span> <span class="fl">0.0</span>:</span>
<span id="cb3-103"><a href="#cb3-103" tabindex="-1"></a>                    temp_frame_features[i, j, :] <span class="op">=</span> feature_extractor.predict(</span>
<span id="cb3-104"><a href="#cb3-104" tabindex="-1"></a>                        batch[<span class="va">None</span>, j, :]</span>
<span id="cb3-105"><a href="#cb3-105" tabindex="-1"></a>                    )</span>
<span id="cb3-106"><a href="#cb3-106" tabindex="-1"></a></span>
<span id="cb3-107"><a href="#cb3-107" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb3-108"><a href="#cb3-108" tabindex="-1"></a>                    temp_frame_features[i, j, :] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb3-109"><a href="#cb3-109" tabindex="-1"></a></span>
<span id="cb3-110"><a href="#cb3-110" tabindex="-1"></a>        frame_features[idx,] <span class="op">=</span> temp_frame_features.squeeze()</span>
<span id="cb3-111"><a href="#cb3-111" tabindex="-1"></a></span>
<span id="cb3-112"><a href="#cb3-112" tabindex="-1"></a>    <span class="cf">return</span> frame_features, labels</span></code></pre></div>
<p>Calling <code>prepare_all_videos()</code> on <code>train_df</code>
and <code>test_df</code> takes ~20 minutes to complete. For this reason,
to save time, here we download already preprocessed NumPy arrays:</p>
<p>!wget -q <a href="https://git.io/JZmf4" class="external-link uri">https://git.io/JZmf4</a> -O top5_data_prepared.tar.gz !tar
-xf top5_data_prepared.tar.gz</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>train_data, train_labels <span class="op">=</span> np.load(<span class="st">"train_data.npy"</span>), np.load(</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>    <span class="st">"train_labels.npy"</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>)</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>test_data, test_labels <span class="op">=</span> np.load(<span class="st">"test_data.npy"</span>), np.load(<span class="st">"test_labels.npy"</span>)</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Frame features in train set: </span><span class="sc">{</span>train_data<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="building-the-transformer-based-model">Building the Transformer-based model<a class="anchor" aria-label="anchor" href="#building-the-transformer-based-model"></a>
</h2>
<p>We will be building on top of the code shared in <a href="https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-11" class="external-link">this
book chapter</a> of <a href="https://www.manning.com/books/deep-learning-with-python" class="external-link">Deep
Learning with Python (Second ed.)</a> by FranÃ§ois Chollet.</p>
<p>First, self-attention layers that form the basic blocks of a
Transformer are order-agnostic. Since videos are ordered sequences of
frames, we need our Transformer model to take into account order
information. We do this via <strong>positional encoding</strong>. We
simply embed the positions of the frames present inside videos with an
<a href="https://keras.io/api/layers/core_layers/embedding" class="external-link"><code>Embedding</code>
layer</a>. We then add these positional embeddings to the precomputed
CNN feature maps.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="kw">class</span> PositionalEmbedding(layers.Layer):</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, sequence_length, output_dim, <span class="op">**</span>kwargs):</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>        <span class="va">self</span>.position_embeddings <span class="op">=</span> layers.Embedding(</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>            input_dim<span class="op">=</span>sequence_length, output_dim<span class="op">=</span>output_dim</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>        )</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>        <span class="va">self</span>.sequence_length <span class="op">=</span> sequence_length</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>        <span class="va">self</span>.output_dim <span class="op">=</span> output_dim</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>        <span class="co"># The inputs are of shape: `(batch_size, frames, num_features)`</span></span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>        inputs <span class="op">=</span> keras.backend.cast(inputs, <span class="va">self</span>.compute_dtype)</span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>        length <span class="op">=</span> keras.backend.shape(inputs)[<span class="dv">1</span>]</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>        positions <span class="op">=</span> keras.ops.numpy.arange(start<span class="op">=</span><span class="dv">0</span>, stop<span class="op">=</span>length, step<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a>        embedded_positions <span class="op">=</span> <span class="va">self</span>.position_embeddings(positions)</span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>        <span class="cf">return</span> inputs <span class="op">+</span> embedded_positions</span></code></pre></div>
<p>Now, we can create a subclassed layer for the Transformer.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="kw">class</span> TransformerEncoder(layers.Layer):</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim, dense_dim, num_heads, <span class="op">**</span>kwargs):</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>        <span class="va">self</span>.embed_dim <span class="op">=</span> embed_dim</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>        <span class="va">self</span>.dense_dim <span class="op">=</span> dense_dim</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> layers.MultiHeadAttention(</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>            num_heads<span class="op">=</span>num_heads, key_dim<span class="op">=</span>embed_dim, dropout<span class="op">=</span><span class="fl">0.3</span></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>        )</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>        <span class="va">self</span>.dense_proj <span class="op">=</span> keras.Sequential(</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>            [</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>                layers.Dense(dense_dim, activation<span class="op">=</span>keras.activations.gelu),</span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a>                layers.Dense(embed_dim),</span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a>            ]</span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a>        )</span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a>        <span class="va">self</span>.layernorm_1 <span class="op">=</span> layers.LayerNormalization()</span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a>        <span class="va">self</span>.layernorm_2 <span class="op">=</span> layers.LayerNormalization()</span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb6-20"><a href="#cb6-20" tabindex="-1"></a>        attention_output <span class="op">=</span> <span class="va">self</span>.attention(inputs, inputs, attention_mask<span class="op">=</span>mask)</span>
<span id="cb6-21"><a href="#cb6-21" tabindex="-1"></a>        proj_input <span class="op">=</span> <span class="va">self</span>.layernorm_1(inputs <span class="op">+</span> attention_output)</span>
<span id="cb6-22"><a href="#cb6-22" tabindex="-1"></a>        proj_output <span class="op">=</span> <span class="va">self</span>.dense_proj(proj_input)</span>
<span id="cb6-23"><a href="#cb6-23" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.layernorm_2(proj_input <span class="op">+</span> proj_output)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="utility-functions-for-training">Utility functions for training<a class="anchor" aria-label="anchor" href="#utility-functions-for-training"></a>
</h2>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="kw">def</span> get_compiled_model(shape):</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>    sequence_length <span class="op">=</span> MAX_SEQ_LENGTH</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>    embed_dim <span class="op">=</span> NUM_FEATURES</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>    dense_dim <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>    num_heads <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>    classes <span class="op">=</span> <span class="bu">len</span>(label_processor.get_vocabulary())</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>    inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>shape)</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>    x <span class="op">=</span> PositionalEmbedding(</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>        sequence_length, embed_dim, name<span class="op">=</span><span class="st">"frame_position_embedding"</span></span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>    )(inputs)</span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>    x <span class="op">=</span> TransformerEncoder(</span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a>        embed_dim, dense_dim, num_heads, name<span class="op">=</span><span class="st">"transformer_layer"</span></span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a>    )(x)</span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a>    x <span class="op">=</span> layers.GlobalMaxPooling1D()(x)</span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a>    x <span class="op">=</span> layers.Dropout(<span class="fl">0.5</span>)(x)</span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a>    outputs <span class="op">=</span> layers.Dense(classes, activation<span class="op">=</span><span class="st">"softmax"</span>)(x)</span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a>    model <span class="op">=</span> keras.Model(inputs, outputs)</span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" tabindex="-1"></a>    model.<span class="bu">compile</span>(</span>
<span id="cb7-21"><a href="#cb7-21" tabindex="-1"></a>        optimizer<span class="op">=</span><span class="st">"adam"</span>,</span>
<span id="cb7-22"><a href="#cb7-22" tabindex="-1"></a>        loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>,</span>
<span id="cb7-23"><a href="#cb7-23" tabindex="-1"></a>        metrics<span class="op">=</span>[<span class="st">"accuracy"</span>],</span>
<span id="cb7-24"><a href="#cb7-24" tabindex="-1"></a>    )</span>
<span id="cb7-25"><a href="#cb7-25" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb7-26"><a href="#cb7-26" tabindex="-1"></a></span>
<span id="cb7-27"><a href="#cb7-27" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" tabindex="-1"></a><span class="kw">def</span> run_experiment():</span>
<span id="cb7-29"><a href="#cb7-29" tabindex="-1"></a>    filepath <span class="op">=</span> <span class="st">"/tmp/video_classifier.weights.h5"</span></span>
<span id="cb7-30"><a href="#cb7-30" tabindex="-1"></a>    checkpoint <span class="op">=</span> keras.callbacks.ModelCheckpoint(</span>
<span id="cb7-31"><a href="#cb7-31" tabindex="-1"></a>        filepath, save_weights_only<span class="op">=</span><span class="va">True</span>, save_best_only<span class="op">=</span><span class="va">True</span>, verbose<span class="op">=</span><span class="dv">1</span></span>
<span id="cb7-32"><a href="#cb7-32" tabindex="-1"></a>    )</span>
<span id="cb7-33"><a href="#cb7-33" tabindex="-1"></a></span>
<span id="cb7-34"><a href="#cb7-34" tabindex="-1"></a>    model <span class="op">=</span> get_compiled_model(train_data.shape[<span class="dv">1</span>:])</span>
<span id="cb7-35"><a href="#cb7-35" tabindex="-1"></a>    history <span class="op">=</span> model.fit(</span>
<span id="cb7-36"><a href="#cb7-36" tabindex="-1"></a>        train_data,</span>
<span id="cb7-37"><a href="#cb7-37" tabindex="-1"></a>        train_labels,</span>
<span id="cb7-38"><a href="#cb7-38" tabindex="-1"></a>        validation_split<span class="op">=</span><span class="fl">0.15</span>,</span>
<span id="cb7-39"><a href="#cb7-39" tabindex="-1"></a>        epochs<span class="op">=</span>EPOCHS,</span>
<span id="cb7-40"><a href="#cb7-40" tabindex="-1"></a>        callbacks<span class="op">=</span>[checkpoint],</span>
<span id="cb7-41"><a href="#cb7-41" tabindex="-1"></a>    )</span>
<span id="cb7-42"><a href="#cb7-42" tabindex="-1"></a></span>
<span id="cb7-43"><a href="#cb7-43" tabindex="-1"></a>    model.load_weights(filepath)</span>
<span id="cb7-44"><a href="#cb7-44" tabindex="-1"></a>    _, accuracy <span class="op">=</span> model.evaluate(test_data, test_labels)</span>
<span id="cb7-45"><a href="#cb7-45" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Test accuracy: </span><span class="sc">{</span><span class="bu">round</span>(accuracy <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">%"</span>)</span>
<span id="cb7-46"><a href="#cb7-46" tabindex="-1"></a></span>
<span id="cb7-47"><a href="#cb7-47" tabindex="-1"></a>    <span class="cf">return</span> model</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="model-training-and-inference">Model training and inference<a class="anchor" aria-label="anchor" href="#model-training-and-inference"></a>
</h2>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>trained_model <span class="op">=</span> run_experiment()</span></code></pre></div>
<p><strong>Note</strong>: This model has ~4.23 Million parameters, which
is way more than the sequence model (99918 parameters) we used in the
prequel of this example. This kind of Transformer model works best with
a larger dataset and a longer pre-training schedule.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="kw">def</span> prepare_single_video(frames):</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>    frame_features <span class="op">=</span> np.zeros(</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>        shape<span class="op">=</span>(<span class="dv">1</span>, MAX_SEQ_LENGTH, NUM_FEATURES), dtype<span class="op">=</span><span class="st">"float32"</span></span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>    )</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>    <span class="co"># Pad shorter videos.</span></span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(frames) <span class="op">&lt;</span> MAX_SEQ_LENGTH:</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>        diff <span class="op">=</span> MAX_SEQ_LENGTH <span class="op">-</span> <span class="bu">len</span>(frames)</span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a>        padding <span class="op">=</span> np.zeros((diff, IMG_SIZE, IMG_SIZE, <span class="dv">3</span>))</span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a>        frames <span class="op">=</span> np.concatenate(frames, padding)</span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a>    frames <span class="op">=</span> frames[<span class="va">None</span>, ...]</span>
<span id="cb9-13"><a href="#cb9-13" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" tabindex="-1"></a>    <span class="co"># Extract features from the frames of the current video.</span></span>
<span id="cb9-15"><a href="#cb9-15" tabindex="-1"></a>    <span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(frames):</span>
<span id="cb9-16"><a href="#cb9-16" tabindex="-1"></a>        video_length <span class="op">=</span> batch.shape[<span class="dv">0</span>]</span>
<span id="cb9-17"><a href="#cb9-17" tabindex="-1"></a>        length <span class="op">=</span> <span class="bu">min</span>(MAX_SEQ_LENGTH, video_length)</span>
<span id="cb9-18"><a href="#cb9-18" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(length):</span>
<span id="cb9-19"><a href="#cb9-19" tabindex="-1"></a>            <span class="cf">if</span> np.mean(batch[j, :]) <span class="op">&gt;</span> <span class="fl">0.0</span>:</span>
<span id="cb9-20"><a href="#cb9-20" tabindex="-1"></a>                frame_features[i, j, :] <span class="op">=</span> feature_extractor.predict(</span>
<span id="cb9-21"><a href="#cb9-21" tabindex="-1"></a>                    batch[<span class="va">None</span>, j, :]</span>
<span id="cb9-22"><a href="#cb9-22" tabindex="-1"></a>                )</span>
<span id="cb9-23"><a href="#cb9-23" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb9-24"><a href="#cb9-24" tabindex="-1"></a>                frame_features[i, j, :] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb9-25"><a href="#cb9-25" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" tabindex="-1"></a>    <span class="cf">return</span> frame_features</span>
<span id="cb9-27"><a href="#cb9-27" tabindex="-1"></a></span>
<span id="cb9-28"><a href="#cb9-28" tabindex="-1"></a></span>
<span id="cb9-29"><a href="#cb9-29" tabindex="-1"></a><span class="kw">def</span> predict_action(path):</span>
<span id="cb9-30"><a href="#cb9-30" tabindex="-1"></a>    class_vocab <span class="op">=</span> label_processor.get_vocabulary()</span>
<span id="cb9-31"><a href="#cb9-31" tabindex="-1"></a></span>
<span id="cb9-32"><a href="#cb9-32" tabindex="-1"></a>    frames <span class="op">=</span> load_video(os.path.join(<span class="st">"test"</span>, path), offload_to_cpu<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-33"><a href="#cb9-33" tabindex="-1"></a>    frame_features <span class="op">=</span> prepare_single_video(frames)</span>
<span id="cb9-34"><a href="#cb9-34" tabindex="-1"></a>    probabilities <span class="op">=</span> trained_model.predict(frame_features)[<span class="dv">0</span>]</span>
<span id="cb9-35"><a href="#cb9-35" tabindex="-1"></a></span>
<span id="cb9-36"><a href="#cb9-36" tabindex="-1"></a>    plot_x_axis, plot_y_axis <span class="op">=</span> [], []</span>
<span id="cb9-37"><a href="#cb9-37" tabindex="-1"></a></span>
<span id="cb9-38"><a href="#cb9-38" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> np.argsort(probabilities)[::<span class="op">-</span><span class="dv">1</span>]:</span>
<span id="cb9-39"><a href="#cb9-39" tabindex="-1"></a>        plot_x_axis.append(class_vocab[i])</span>
<span id="cb9-40"><a href="#cb9-40" tabindex="-1"></a>        plot_y_axis.append(probabilities[i])</span>
<span id="cb9-41"><a href="#cb9-41" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>class_vocab[i]<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>probabilities[i] <span class="op">*</span> <span class="dv">100</span><span class="sc">:5.2f}</span><span class="ss">%"</span>)</span>
<span id="cb9-42"><a href="#cb9-42" tabindex="-1"></a></span>
<span id="cb9-43"><a href="#cb9-43" tabindex="-1"></a>    plt.bar(plot_x_axis, plot_y_axis, label<span class="op">=</span>plot_x_axis)</span>
<span id="cb9-44"><a href="#cb9-44" tabindex="-1"></a>    plt.xlabel(<span class="st">"class_label"</span>)</span>
<span id="cb9-45"><a href="#cb9-45" tabindex="-1"></a>    plt.xlabel(<span class="st">"Probability"</span>)</span>
<span id="cb9-46"><a href="#cb9-46" tabindex="-1"></a>    plt.show()</span>
<span id="cb9-47"><a href="#cb9-47" tabindex="-1"></a></span>
<span id="cb9-48"><a href="#cb9-48" tabindex="-1"></a>    <span class="cf">return</span> frames</span>
<span id="cb9-49"><a href="#cb9-49" tabindex="-1"></a></span>
<span id="cb9-50"><a href="#cb9-50" tabindex="-1"></a></span>
<span id="cb9-51"><a href="#cb9-51" tabindex="-1"></a><span class="co"># This utility is for visualization.</span></span>
<span id="cb9-52"><a href="#cb9-52" tabindex="-1"></a><span class="co"># Referenced from:</span></span>
<span id="cb9-53"><a href="#cb9-53" tabindex="-1"></a><span class="co"># https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub</span></span>
<span id="cb9-54"><a href="#cb9-54" tabindex="-1"></a><span class="kw">def</span> to_gif(images):</span>
<span id="cb9-55"><a href="#cb9-55" tabindex="-1"></a>    converted_images <span class="op">=</span> images.astype(np.uint8)</span>
<span id="cb9-56"><a href="#cb9-56" tabindex="-1"></a>    imageio.mimsave(<span class="st">"animation.gif"</span>, converted_images, fps<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb9-57"><a href="#cb9-57" tabindex="-1"></a>    <span class="cf">return</span> embed.embed_file(<span class="st">"animation.gif"</span>)</span>
<span id="cb9-58"><a href="#cb9-58" tabindex="-1"></a></span>
<span id="cb9-59"><a href="#cb9-59" tabindex="-1"></a></span>
<span id="cb9-60"><a href="#cb9-60" tabindex="-1"></a>test_video <span class="op">=</span> np.random.choice(test_df[<span class="st">"video_name"</span>].values.tolist())</span>
<span id="cb9-61"><a href="#cb9-61" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test video path: </span><span class="sc">{</span>test_video<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-62"><a href="#cb9-62" tabindex="-1"></a>test_frames <span class="op">=</span> predict_action(test_video)</span>
<span id="cb9-63"><a href="#cb9-63" tabindex="-1"></a>to_gif(test_frames[:MAX_SEQ_LENGTH])</span></code></pre></div>
<p>The performance of our model is far from optimal, because it was
trained on a small dataset.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, FranÃ§ois Chollet, RStudio, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.9000.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
