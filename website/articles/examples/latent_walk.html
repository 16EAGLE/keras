<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Explore the latent manifold of Stable Diffusion.">
<title>A walk through latent space with Stable Diffusion • keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../../deps/JetBrains_Mono-0.4.7/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><meta property="og:title" content="A walk through latent space with Stable Diffusion">
<meta property="og:description" content="Explore the latent manifold of Stable Diffusion.">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-getting-started">Getting Started</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-getting-started">
    <a class="dropdown-item" href="../../articles/intro_to_keras_for_engineers.html">Introduction to Keras for engineers</a>
    <h6 class="dropdown-header" data-toc-skip>Tutorials</h6>
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
  </div>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>A walk through latent space with Stable Diffusion</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/latent_walk.Rmd" class="external-link"><code>vignettes/examples/latent_walk.Rmd</code></a></small>
      <div class="d-none name"><code>latent_walk.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="overview">Overview<a class="anchor" aria-label="anchor" href="#overview"></a>
</h2>
<p>Generative image models learn a “latent manifold” of the visual
world: a low-dimensional vector space where each point maps to an image.
Going from such a point on the manifold back to a displayable image is
called “decoding” – in the Stable Diffusion model, this is handled by
the “decoder” model.</p>
<div class="float">
<img src="https://i.imgur.com/2uC8rYJ.png" alt="The Stable Diffusion architecture"><div class="figcaption">The Stable Diffusion architecture</div>
</div>
<p>This latent manifold of images is continuous and interpolative,
meaning that:</p>
<ol style="list-style-type: decimal">
<li>Moving a little on the manifold only changes the corresponding image
a little (continuity).</li>
<li>For any two points A and B on the manifold (i.e. any two images), it
is possible to move from A to B via a path where each intermediate point
is also on the manifold (i.e. is also a valid image). Intermediate
points would be called “interpolations” between the two starting
images.</li>
</ol>
<p>Stable Diffusion isn’t just an image model, though, it’s also a
natural language model. It has two latent spaces: the image
representation space learned by the encoder used during training, and
the prompt latent space which is learned using a combination of
pretraining and training-time fine-tuning.</p>
<p><em>Latent space walking</em>, or <em>latent space exploration</em>,
is the process of sampling a point in latent space and incrementally
changing the latent representation. Its most common application is
generating animations where each sampled point is fed to the decoder and
is stored as a frame in the final animation. For high-quality latent
representations, this produces coherent-looking animations. These
animations can provide insight into the feature map of the latent space,
and can ultimately lead to improvements in the training process. One
such GIF is displayed below:</p>
<div class="float">
<img src="../../../../../../../img/examples/generative/random_walks_with_stable_diffusion/panda2plane.gif" alt="Panda to Plane"><div class="figcaption">Panda to Plane</div>
</div>
<p>In this guide, we will show how to take advantage of the Stable
Diffusion API in KerasCV to perform prompt interpolation and circular
walks through Stable Diffusion’s visual latent manifold, as well as
through the text encoder’s latent manifold.</p>
<p>This guide assumes the reader has a high-level understanding of
Stable Diffusion. If you haven’t already, you should start by reading
the <a href="https://keras.io/guides/keras_cv/generate_images_with_stable_diffusion/" class="external-link">Stable
Diffusion Tutorial</a>.</p>
<p>To start, we import KerasCV and load up a Stable Diffusion model
using the optimizations discussed in the tutorial <a href="https://keras.io/guides/keras_cv/generate_images_with_stable_diffusion/" class="external-link">Generate
images with Stable Diffusion</a>. Note that if you are running with a M1
Mac GPU you should not enable mixed precision.</p>
<p>pip install keras-cv –upgrade –quiet</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> keras_cv</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> keras</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> ops</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="co"># Enable mixed precision</span></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="co"># (only do this if you have a recent NVIDIA GPU)</span></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>keras.mixed_precision.set_global_policy(<span class="st">"mixed_float16"</span>)</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a><span class="co"># Instantiate the Stable Diffusion model</span></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>model <span class="op">=</span> keras_cv.models.StableDiffusion(jit_compile<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="interpolating-between-text-prompts">Interpolating between text prompts<a class="anchor" aria-label="anchor" href="#interpolating-between-text-prompts"></a>
</h2>
<p>In Stable Diffusion, a text prompt is first encoded into a vector,
and that encoding is used to guide the diffusion process. The latent
encoding vector has shape 77x768 (that’s huge!), and when we give Stable
Diffusion a text prompt, we’re generating images from just one such
point on the latent manifold.</p>
<p>To explore more of this manifold, we can interpolate between two text
encodings and generate images at those interpolated points:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>prompt_1 <span class="op">=</span> <span class="st">"A watercolor painting of a Golden Retriever at the beach"</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>prompt_2 <span class="op">=</span> <span class="st">"A still life DSLR photo of a bowl of fruit"</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>interpolation_steps <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>encoding_1 <span class="op">=</span> ops.squeeze(model.encode_text(prompt_1))</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>encoding_2 <span class="op">=</span> ops.squeeze(model.encode_text(prompt_2))</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>interpolated_encodings <span class="op">=</span> ops.linspace(encoding_1, encoding_2, interpolation_steps)</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a><span class="co"># Show the size of the latent manifold</span></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Encoding shape: </span><span class="sc">{</span>encoding_1<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
<p>Once we’ve interpolated the encodings, we can generate images from
each point. Note that in order to maintain some stability between the
resulting images we keep the diffusion noise constant between
images.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">12345</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>noise <span class="op">=</span> keras.random.normal((<span class="dv">512</span> <span class="op">//</span> <span class="dv">8</span>, <span class="dv">512</span> <span class="op">//</span> <span class="dv">8</span>, <span class="dv">4</span>), seed<span class="op">=</span>seed)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>images <span class="op">=</span> model.generate_image(</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>    interpolated_encodings,</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>    batch_size<span class="op">=</span>interpolation_steps,</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>    diffusion_noise<span class="op">=</span>noise,</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>)</span></code></pre></div>
<p>Now that we’ve generated some interpolated images, let’s take a look
at them!</p>
<p>Throughout this tutorial, we’re going to export sequences of images
as gifs so that they can be easily viewed with some temporal context.
For sequences of images where the first and last images don’t match
conceptually, we rubber-band the gif.</p>
<p>If you’re running in Colab, you can view your own GIFs by
running:</p>
<pre><code>from IPython.display import Image as IImage
IImage("doggo-and-fruit-5.gif")</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="kw">def</span> export_as_gif(filename, images, frames_per_second<span class="op">=</span><span class="dv">10</span>, rubber_band<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>    <span class="cf">if</span> rubber_band:</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>        images <span class="op">+=</span> images[<span class="dv">2</span>:<span class="op">-</span><span class="dv">1</span>][::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>    images[<span class="dv">0</span>].save(</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>        filename,</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>        save_all<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>        append_images<span class="op">=</span>images[<span class="dv">1</span>:],</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>        duration<span class="op">=</span><span class="dv">1000</span> <span class="op">//</span> frames_per_second,</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>        loop<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>    )</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>export_as_gif(</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>    <span class="st">"doggo-and-fruit-5.gif"</span>,</span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a>    [Image.fromarray(img) <span class="cf">for</span> img <span class="kw">in</span> images],</span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>    frames_per_second<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>    rubber_band<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a>)</span></code></pre></div>
<div class="float">
<img src="https://i.imgur.com/4ZCxZY4.gif" alt="Dog to Fruit 5"><div class="figcaption">Dog to Fruit 5</div>
</div>
<p>The results may seem surprising. Generally, interpolating between
prompts produces coherent looking images, and often demonstrates a
progressive concept shift between the contents of the two prompts. This
is indicative of a high quality representation space, that closely
mirrors the natural structure of the visual world.</p>
<p>To best visualize this, we should do a much more fine-grained
interpolation, using hundreds of steps. In order to keep batch size
small (so that we don’t OOM our GPU), this requires manually batching
our interpolated encodings.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>interpolation_steps <span class="op">=</span> <span class="dv">150</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>batches <span class="op">=</span> interpolation_steps <span class="op">//</span> batch_size</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>interpolated_encodings <span class="op">=</span> ops.linspace(encoding_1, encoding_2, interpolation_steps)</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>batched_encodings <span class="op">=</span> ops.split(interpolated_encodings, batches)</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>images <span class="op">=</span> []</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a><span class="cf">for</span> batch <span class="kw">in</span> <span class="bu">range</span>(batches):</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>    images <span class="op">+=</span> [</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>        Image.fromarray(img)</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>        <span class="cf">for</span> img <span class="kw">in</span> model.generate_image(</span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a>            batched_encodings[batch],</span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a>            batch_size<span class="op">=</span>batch_size,</span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a>            num_steps<span class="op">=</span><span class="dv">25</span>,</span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a>            diffusion_noise<span class="op">=</span>noise,</span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a>        )</span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a>    ]</span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" tabindex="-1"></a>export_as_gif(<span class="st">"doggo-and-fruit-150.gif"</span>, images, rubber_band<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="float">
<img src="../../../../../../../img/examples/generative/random_walks_with_stable_diffusion/dog2fruit150.gif" alt="Dog to Fruit 150"><div class="figcaption">Dog to Fruit 150</div>
</div>
<p>The resulting gif shows a much clearer and more coherent shift
between the two prompts. Try out some prompts of your own and
experiment!</p>
<p>We can even extend this concept for more than one image. For example,
we can interpolate between four prompts:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>prompt_1 <span class="op">=</span> <span class="st">"A watercolor painting of a Golden Retriever at the beach"</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>prompt_2 <span class="op">=</span> <span class="st">"A still life DSLR photo of a bowl of fruit"</span></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>prompt_3 <span class="op">=</span> <span class="st">"The eiffel tower in the style of starry night"</span></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>prompt_4 <span class="op">=</span> <span class="st">"An architectural sketch of a skyscraper"</span></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>interpolation_steps <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>batches <span class="op">=</span> (interpolation_steps<span class="op">**</span><span class="dv">2</span>) <span class="op">//</span> batch_size</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>encoding_1 <span class="op">=</span> ops.squeeze(model.encode_text(prompt_1))</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>encoding_2 <span class="op">=</span> ops.squeeze(model.encode_text(prompt_2))</span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>encoding_3 <span class="op">=</span> ops.squeeze(model.encode_text(prompt_3))</span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a>encoding_4 <span class="op">=</span> ops.squeeze(model.encode_text(prompt_4))</span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a>interpolated_encodings <span class="op">=</span> ops.linspace(</span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a>    ops.linspace(encoding_1, encoding_2, interpolation_steps),</span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a>    ops.linspace(encoding_3, encoding_4, interpolation_steps),</span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a>    interpolation_steps,</span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a>)</span>
<span id="cb7-20"><a href="#cb7-20" tabindex="-1"></a>interpolated_encodings <span class="op">=</span> ops.reshape(</span>
<span id="cb7-21"><a href="#cb7-21" tabindex="-1"></a>    interpolated_encodings, (interpolation_steps<span class="op">**</span><span class="dv">2</span>, <span class="dv">77</span>, <span class="dv">768</span>)</span>
<span id="cb7-22"><a href="#cb7-22" tabindex="-1"></a>)</span>
<span id="cb7-23"><a href="#cb7-23" tabindex="-1"></a>batched_encodings <span class="op">=</span> ops.split(interpolated_encodings, batches)</span>
<span id="cb7-24"><a href="#cb7-24" tabindex="-1"></a></span>
<span id="cb7-25"><a href="#cb7-25" tabindex="-1"></a>images <span class="op">=</span> []</span>
<span id="cb7-26"><a href="#cb7-26" tabindex="-1"></a><span class="cf">for</span> batch <span class="kw">in</span> <span class="bu">range</span>(batches):</span>
<span id="cb7-27"><a href="#cb7-27" tabindex="-1"></a>    images.append(</span>
<span id="cb7-28"><a href="#cb7-28" tabindex="-1"></a>        model.generate_image(</span>
<span id="cb7-29"><a href="#cb7-29" tabindex="-1"></a>            batched_encodings[batch],</span>
<span id="cb7-30"><a href="#cb7-30" tabindex="-1"></a>            batch_size<span class="op">=</span>batch_size,</span>
<span id="cb7-31"><a href="#cb7-31" tabindex="-1"></a>            diffusion_noise<span class="op">=</span>noise,</span>
<span id="cb7-32"><a href="#cb7-32" tabindex="-1"></a>        )</span>
<span id="cb7-33"><a href="#cb7-33" tabindex="-1"></a>    )</span>
<span id="cb7-34"><a href="#cb7-34" tabindex="-1"></a></span>
<span id="cb7-35"><a href="#cb7-35" tabindex="-1"></a></span>
<span id="cb7-36"><a href="#cb7-36" tabindex="-1"></a><span class="kw">def</span> plot_grid(</span>
<span id="cb7-37"><a href="#cb7-37" tabindex="-1"></a>    images,</span>
<span id="cb7-38"><a href="#cb7-38" tabindex="-1"></a>    path,</span>
<span id="cb7-39"><a href="#cb7-39" tabindex="-1"></a>    grid_size,</span>
<span id="cb7-40"><a href="#cb7-40" tabindex="-1"></a>    scale<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb7-41"><a href="#cb7-41" tabindex="-1"></a>):</span>
<span id="cb7-42"><a href="#cb7-42" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(grid_size <span class="op">*</span> scale, grid_size <span class="op">*</span> scale))</span>
<span id="cb7-43"><a href="#cb7-43" tabindex="-1"></a>    fig.tight_layout()</span>
<span id="cb7-44"><a href="#cb7-44" tabindex="-1"></a>    plt.subplots_adjust(wspace<span class="op">=</span><span class="dv">0</span>, hspace<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-45"><a href="#cb7-45" tabindex="-1"></a>    plt.margins(x<span class="op">=</span><span class="dv">0</span>, y<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-46"><a href="#cb7-46" tabindex="-1"></a>    plt.axis(<span class="st">"off"</span>)</span>
<span id="cb7-47"><a href="#cb7-47" tabindex="-1"></a>    images <span class="op">=</span> images.astype(<span class="bu">int</span>)</span>
<span id="cb7-48"><a href="#cb7-48" tabindex="-1"></a>    <span class="cf">for</span> row <span class="kw">in</span> <span class="bu">range</span>(grid_size):</span>
<span id="cb7-49"><a href="#cb7-49" tabindex="-1"></a>        <span class="cf">for</span> col <span class="kw">in</span> <span class="bu">range</span>(grid_size):</span>
<span id="cb7-50"><a href="#cb7-50" tabindex="-1"></a>            index <span class="op">=</span> row <span class="op">*</span> grid_size <span class="op">+</span> col</span>
<span id="cb7-51"><a href="#cb7-51" tabindex="-1"></a>            plt.subplot(grid_size, grid_size, index <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb7-52"><a href="#cb7-52" tabindex="-1"></a>            plt.imshow(images[index].astype(<span class="st">"uint8"</span>))</span>
<span id="cb7-53"><a href="#cb7-53" tabindex="-1"></a>            plt.axis(<span class="st">"off"</span>)</span>
<span id="cb7-54"><a href="#cb7-54" tabindex="-1"></a>            plt.margins(x<span class="op">=</span><span class="dv">0</span>, y<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-55"><a href="#cb7-55" tabindex="-1"></a>    plt.savefig(</span>
<span id="cb7-56"><a href="#cb7-56" tabindex="-1"></a>        fname<span class="op">=</span>path,</span>
<span id="cb7-57"><a href="#cb7-57" tabindex="-1"></a>        pad_inches<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb7-58"><a href="#cb7-58" tabindex="-1"></a>        bbox_inches<span class="op">=</span><span class="st">"tight"</span>,</span>
<span id="cb7-59"><a href="#cb7-59" tabindex="-1"></a>        transparent<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb7-60"><a href="#cb7-60" tabindex="-1"></a>        dpi<span class="op">=</span><span class="dv">60</span>,</span>
<span id="cb7-61"><a href="#cb7-61" tabindex="-1"></a>    )</span>
<span id="cb7-62"><a href="#cb7-62" tabindex="-1"></a></span>
<span id="cb7-63"><a href="#cb7-63" tabindex="-1"></a></span>
<span id="cb7-64"><a href="#cb7-64" tabindex="-1"></a>images <span class="op">=</span> np.concatenate(images)</span>
<span id="cb7-65"><a href="#cb7-65" tabindex="-1"></a>plot_grid(images, <span class="st">"4-way-interpolation.jpg"</span>, interpolation_steps)</span></code></pre></div>
<p>We can also interpolate while allowing diffusion noise to vary by
dropping the <code>diffusion_noise</code> parameter:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>images <span class="op">=</span> []</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a><span class="cf">for</span> batch <span class="kw">in</span> <span class="bu">range</span>(batches):</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>    images.append(model.generate_image(batched_encodings[batch], batch_size<span class="op">=</span>batch_size))</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>images <span class="op">=</span> np.concatenate(images)</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>plot_grid(images, <span class="st">"4-way-interpolation-varying-noise.jpg"</span>, interpolation_steps)</span></code></pre></div>
<p>Next up – let’s go for some walks!</p>
</div>
<div class="section level2">
<h2 id="a-walk-around-a-text-prompt">A walk around a text prompt<a class="anchor" aria-label="anchor" href="#a-walk-around-a-text-prompt"></a>
</h2>
<p>Our next experiment will be to go for a walk around the latent
manifold starting from a point produced by a particular prompt.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>walk_steps <span class="op">=</span> <span class="dv">150</span></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>batches <span class="op">=</span> walk_steps <span class="op">//</span> batch_size</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>step_size <span class="op">=</span> <span class="fl">0.005</span></span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>encoding <span class="op">=</span> ops.squeeze(</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>    model.encode_text(<span class="st">"The Eiffel Tower in the style of starry night"</span>)</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>)</span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a><span class="co"># Note that (77, 768) is the shape of the text encoding.</span></span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a>delta <span class="op">=</span> ops.ones_like(encoding) <span class="op">*</span> step_size</span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a>walked_encodings <span class="op">=</span> []</span>
<span id="cb9-13"><a href="#cb9-13" tabindex="-1"></a><span class="cf">for</span> step_index <span class="kw">in</span> <span class="bu">range</span>(walk_steps):</span>
<span id="cb9-14"><a href="#cb9-14" tabindex="-1"></a>    walked_encodings.append(encoding)</span>
<span id="cb9-15"><a href="#cb9-15" tabindex="-1"></a>    encoding <span class="op">+=</span> delta</span>
<span id="cb9-16"><a href="#cb9-16" tabindex="-1"></a>walked_encodings <span class="op">=</span> ops.stack(walked_encodings)</span>
<span id="cb9-17"><a href="#cb9-17" tabindex="-1"></a>batched_encodings <span class="op">=</span> ops.split(walked_encodings, batches)</span>
<span id="cb9-18"><a href="#cb9-18" tabindex="-1"></a></span>
<span id="cb9-19"><a href="#cb9-19" tabindex="-1"></a>images <span class="op">=</span> []</span>
<span id="cb9-20"><a href="#cb9-20" tabindex="-1"></a><span class="cf">for</span> batch <span class="kw">in</span> <span class="bu">range</span>(batches):</span>
<span id="cb9-21"><a href="#cb9-21" tabindex="-1"></a>    images <span class="op">+=</span> [</span>
<span id="cb9-22"><a href="#cb9-22" tabindex="-1"></a>        Image.fromarray(img)</span>
<span id="cb9-23"><a href="#cb9-23" tabindex="-1"></a>        <span class="cf">for</span> img <span class="kw">in</span> model.generate_image(</span>
<span id="cb9-24"><a href="#cb9-24" tabindex="-1"></a>            batched_encodings[batch],</span>
<span id="cb9-25"><a href="#cb9-25" tabindex="-1"></a>            batch_size<span class="op">=</span>batch_size,</span>
<span id="cb9-26"><a href="#cb9-26" tabindex="-1"></a>            num_steps<span class="op">=</span><span class="dv">25</span>,</span>
<span id="cb9-27"><a href="#cb9-27" tabindex="-1"></a>            diffusion_noise<span class="op">=</span>noise,</span>
<span id="cb9-28"><a href="#cb9-28" tabindex="-1"></a>        )</span>
<span id="cb9-29"><a href="#cb9-29" tabindex="-1"></a>    ]</span>
<span id="cb9-30"><a href="#cb9-30" tabindex="-1"></a></span>
<span id="cb9-31"><a href="#cb9-31" tabindex="-1"></a>export_as_gif(<span class="st">"eiffel-tower-starry-night.gif"</span>, images, rubber_band<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="float">
<img src="https://i.imgur.com/9MMYtal.gif" alt="Eiffel tower walk gif"><div class="figcaption">Eiffel tower walk gif</div>
</div>
<p>Perhaps unsurprisingly, walking too far from the encoder’s latent
manifold produces images that look incoherent. Try it for yourself by
setting your own prompt, and adjusting <code>step_size</code> to
increase or decrease the magnitude of the walk. Note that when the
magnitude of the walk gets large, the walk often leads into areas which
produce extremely noisy images.</p>
</div>
<div class="section level2">
<h2 id="a-circular-walk-through-the-diffusion-noise-space-for-a-single-prompt">A circular walk through the diffusion noise space for a single
prompt<a class="anchor" aria-label="anchor" href="#a-circular-walk-through-the-diffusion-noise-space-for-a-single-prompt"></a>
</h2>
<p>Our final experiment is to stick to one prompt and explore the
variety of images that the diffusion model can produce from that prompt.
We do this by controlling the noise that is used to seed the diffusion
process.</p>
<p>We create two noise components, <code>x</code> and <code>y</code>,
and do a walk from 0 to 2π, summing the cosine of our <code>x</code>
component and the sin of our <code>y</code> component to produce noise.
Using this approach, the end of our walk arrives at the same noise
inputs where we began our walk, so we get a “loopable” result!</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"An oil paintings of cows in a field next to a windmill in Holland"</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>encoding <span class="op">=</span> ops.squeeze(model.encode_text(prompt))</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>walk_steps <span class="op">=</span> <span class="dv">150</span></span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>batches <span class="op">=</span> walk_steps <span class="op">//</span> batch_size</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>walk_noise_x <span class="op">=</span> keras.random.normal(noise.shape, dtype<span class="op">=</span><span class="st">"float64"</span>)</span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>walk_noise_y <span class="op">=</span> keras.random.normal(noise.shape, dtype<span class="op">=</span><span class="st">"float64"</span>)</span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a>walk_scale_x <span class="op">=</span> ops.cos(ops.linspace(<span class="dv">0</span>, <span class="dv">2</span>, walk_steps) <span class="op">*</span> math.pi)</span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a>walk_scale_y <span class="op">=</span> ops.sin(ops.linspace(<span class="dv">0</span>, <span class="dv">2</span>, walk_steps) <span class="op">*</span> math.pi)</span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a>noise_x <span class="op">=</span> ops.tensordot(walk_scale_x, walk_noise_x, axes<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a>noise_y <span class="op">=</span> ops.tensordot(walk_scale_y, walk_noise_y, axes<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a>noise <span class="op">=</span> ops.add(noise_x, noise_y)</span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a>batched_noise <span class="op">=</span> ops.split(noise, batches)</span>
<span id="cb10-16"><a href="#cb10-16" tabindex="-1"></a></span>
<span id="cb10-17"><a href="#cb10-17" tabindex="-1"></a>images <span class="op">=</span> []</span>
<span id="cb10-18"><a href="#cb10-18" tabindex="-1"></a><span class="cf">for</span> batch <span class="kw">in</span> <span class="bu">range</span>(batches):</span>
<span id="cb10-19"><a href="#cb10-19" tabindex="-1"></a>    images <span class="op">+=</span> [</span>
<span id="cb10-20"><a href="#cb10-20" tabindex="-1"></a>        Image.fromarray(img)</span>
<span id="cb10-21"><a href="#cb10-21" tabindex="-1"></a>        <span class="cf">for</span> img <span class="kw">in</span> model.generate_image(</span>
<span id="cb10-22"><a href="#cb10-22" tabindex="-1"></a>            encoding,</span>
<span id="cb10-23"><a href="#cb10-23" tabindex="-1"></a>            batch_size<span class="op">=</span>batch_size,</span>
<span id="cb10-24"><a href="#cb10-24" tabindex="-1"></a>            num_steps<span class="op">=</span><span class="dv">25</span>,</span>
<span id="cb10-25"><a href="#cb10-25" tabindex="-1"></a>            diffusion_noise<span class="op">=</span>batched_noise[batch],</span>
<span id="cb10-26"><a href="#cb10-26" tabindex="-1"></a>        )</span>
<span id="cb10-27"><a href="#cb10-27" tabindex="-1"></a>    ]</span>
<span id="cb10-28"><a href="#cb10-28" tabindex="-1"></a></span>
<span id="cb10-29"><a href="#cb10-29" tabindex="-1"></a>export_as_gif(<span class="st">"cows.gif"</span>, images)</span></code></pre></div>
<div class="float">
<img src="../../../../../../../img/examples/generative/random_walks_with_stable_diffusion/happycows.gif" alt="Happy Cows"><div class="figcaption">Happy Cows</div>
</div>
<p>Experiment with your own prompts and with different values of
<code>unconditional_guidance_scale</code>!</p>
</div>
<div class="section level2">
<h2 id="conclusion">Conclusion<a class="anchor" aria-label="anchor" href="#conclusion"></a>
</h2>
<p>Stable Diffusion offers a lot more than just single text-to-image
generation. Exploring the latent manifold of the text encoder and the
noise space of the diffusion model are two fun ways to experience the
power of this model, and KerasCV makes it easy!</p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
