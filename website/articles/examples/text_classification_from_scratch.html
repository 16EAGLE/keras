<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Text classification from scratch â€¢ keras</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../../bootstrap-toc.css">
<script src="../../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../../pkgdown.css" rel="stylesheet">
<script src="../../pkgdown.js"></script><link href="../../extra.css" rel="stylesheet">
<script src="../../extra.js"></script><meta property="og:title" content="Text classification from scratch">
<meta property="og:description" content="Text sentiment classification starting from raw text files.">
<meta property="og:image" content="https://keras.posit.co/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../../index.html">keras</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">2.13.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../../index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Tutorials
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/getting_started.html">Getting Started</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    </li>
    <li>
      <a href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    </li>
    <li>
      <a href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Guides
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li class="dropdown-header">Guides (New for TF 2.6)</li>
    <li>
      <a href="../../articles/new-guides/python_subclasses.html">Python Subclasses</a>
    </li>
    <li>
      <a href="../../articles/new-guides/making_new_layers_and_models_via_subclassing.html">Making New Layers and Models via Subclassing</a>
    </li>
    <li>
      <a href="../../articles/new-guides/customizing_what_happens_in_fit.html">Customizing What Happens in Fit</a>
    </li>
    <li>
      <a href="../../articles/new-guides/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    </li>
    <li>
      <a href="../../articles/new-guides/preprocessing_layers.html">Working with Preprocessing Layers</a>
    </li>
    <li>
      <a href="../../argicles/new-guides/working_with_rnns.html">Working with RNNs</a>
    </li>
    <li class="divider">
    </li>
<li class="dropdown-header">Using Keras</li>
    <li>
      <a href="../../articles/guide_keras.html">Guide to Keras Basics</a>
    </li>
    <li>
      <a href="../../articles/sequential_model.html">Sequential Model in Depth</a>
    </li>
    <li>
      <a href="../../articles/functional_api.html">Functional API in Depth</a>
    </li>
    <li>
      <a href="../../articles/about_keras_models.html">About Keras Models</a>
    </li>
    <li>
      <a href="../../articles/about_keras_layers.html">About Keras Layers</a>
    </li>
    <li>
      <a href="../../articles/training_visualization.html">Training Visualization</a>
    </li>
    <li>
      <a href="../../articles/applications.html">Pre-Trained Models</a>
    </li>
    <li>
      <a href="../../articles/faq.html">Frequently Asked Questions</a>
    </li>
    <li>
      <a href="../../articles/why_use_keras.html">Why Use Keras?</a>
    </li>
    <li class="divider">
    </li>
<li class="dropdown-header">Advanced</li>
    <li>
      <a href="../../articles/eager_guide.html">Eager Execution</a>
    </li>
    <li>
      <a href="../../articles/training_callbacks.html">Training Callbacks</a>
    </li>
    <li>
      <a href="../../articles/backend.html">Keras Backend</a>
    </li>
    <li>
      <a href="../../articles/custom_layers.html">Custom Layers</a>
    </li>
    <li>
      <a href="../../articles/custom_models.html">Custom Models</a>
    </li>
    <li>
      <a href="../../articles/saving_serializing.html">Saving and serializing</a>
    </li>
  </ul>
</li>
<li>
  <a href="../../articles/learn.html">Learn</a>
</li>
<li>
  <a href="../../articles/tools.html">Tools</a>
</li>
<li>
  <a href="../../articles/examples/index.html">Examples</a>
</li>
<li>
  <a href="../../reference/index.html">Reference</a>
</li>
<li>
  <a href="../../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/rstudio/keras/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Text classification from scratch</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/text_classification_from_scratch.Rmd" class="external-link"><code>vignettes/examples/text_classification_from_scratch.Rmd</code></a></small>
      <div class="hidden name"><code>text_classification_from_scratch.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>This example shows how to do text classification starting from raw
text (as a set of text files on disk). We demonstrate the workflow on
the IMDB sentiment classification dataset (unprocessed version). We use
the <code>TextVectorization</code> layer for word splitting &amp;
indexing.</p>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> TextVectorization</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="load-the-data-imdb-movie-review-sentiment-classification">Load the data: IMDB movie review sentiment classification<a class="anchor" aria-label="anchor" href="#load-the-data-imdb-movie-review-sentiment-classification"></a>
</h2>
<p>Letâ€™s download the data and inspect its structure.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>fpath <span class="op">=</span> keras.utils.get_file(</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>    origin<span class="op">=</span><span class="st">"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>)</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>dirpath <span class="op">=</span> Path(fpath).parent.absolute()</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>os.system(<span class="ss">f"tar -xf </span><span class="sc">{</span>fpath<span class="sc">}</span><span class="ss"> -C </span><span class="sc">{</span>dirpath<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
<p>The <code>aclImdb</code> folder contains a <code>train</code> and
<code>test</code> subfolder:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>os.system(<span class="ss">f"ls </span><span class="sc">{</span>dirpath<span class="sc">}</span><span class="ss">/aclImdb"</span>)</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>os.system(<span class="ss">f"ls </span><span class="sc">{</span>dirpath<span class="sc">}</span><span class="ss">/aclImdb/train"</span>)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>os.system(<span class="ss">f"ls </span><span class="sc">{</span>dirpath<span class="sc">}</span><span class="ss">/aclImdb/test"</span>)</span></code></pre></div>
<p>The <code>aclImdb/train/pos</code> and <code>aclImdb/train/neg</code>
folders contain text files, each of which represents one review (either
positive or negative):</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>os.system(<span class="ss">f"cat </span><span class="sc">{</span>dirpath<span class="sc">}</span><span class="ss">/aclImdb/train/pos/6248_7.txt"</span>)</span></code></pre></div>
<p>We are only interested in the <code>pos</code> and <code>neg</code>
subfolders, so letâ€™s delete the rest:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>os.system(<span class="ss">f"rm -r </span><span class="sc">{</span>dirpath<span class="sc">}</span><span class="ss">/aclImdb/train/unsup"</span>)</span></code></pre></div>
<p>You can use the utility
<code>keras.utils.text_dataset_from_directory</code> to generate a
labeled <code>tf.data.Dataset</code> object from a set of text files on
disk filed into class-specific folders.</p>
<p>Letâ€™s use it to generate the training, validation, and test datasets.
The validation and training datasets are generated from two subsets of
the <code>train</code> directory, with 20% of samples going to the
validation dataset and 80% going to the training dataset.</p>
<p>Having a validation dataset in addition to the test dataset is useful
for tuning hyperparameters, such as the model architecture, for which
the test dataset should not be used.</p>
<p>Before putting the model out into the real world however, it should
be retrained using all available training data (without creating a
validation dataset), so its performance is maximized.</p>
<p>When using the <code>validation_split</code> &amp;
<code>subset</code> arguments, make sure to either specify a random
seed, or to pass <code>shuffle=False</code>, so that the validation
&amp; training splits you get have no overlap.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>raw_train_ds, raw_val_ds <span class="op">=</span> keras.utils.text_dataset_from_directory(</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>    <span class="ss">f"</span><span class="sc">{</span>dirpath<span class="sc">}</span><span class="ss">/aclImdb/train"</span>,</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>    batch_size<span class="op">=</span>batch_size,</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>    validation_split<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>    subset<span class="op">=</span><span class="st">"both"</span>,</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>    seed<span class="op">=</span><span class="dv">1337</span>,</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>)</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>raw_test_ds <span class="op">=</span> keras.utils.text_dataset_from_directory(</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>    <span class="ss">f"</span><span class="sc">{</span>dirpath<span class="sc">}</span><span class="ss">/aclImdb/test"</span>, batch_size<span class="op">=</span>batch_size</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>)</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of batches in raw_train_ds: </span><span class="sc">{</span>raw_train_ds<span class="sc">.</span>cardinality()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of batches in raw_val_ds: </span><span class="sc">{</span>raw_val_ds<span class="sc">.</span>cardinality()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of batches in raw_test_ds: </span><span class="sc">{</span>raw_test_ds<span class="sc">.</span>cardinality()<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
<p>Letâ€™s preview a few samples:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="co"># It's important to take a look at your raw data to ensure your normalization</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="co"># and tokenization will work as expected. We can do that by taking a few</span></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a><span class="co"># examples from the training set and looking at them.</span></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a><span class="co"># This is one of the places where eager execution shines:</span></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a><span class="co"># we can just evaluate these tensors using .numpy()</span></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a><span class="co"># instead of needing to evaluate them in a Session/Graph context.</span></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a><span class="cf">for</span> text_batch, label_batch <span class="kw">in</span> raw_train_ds.take(<span class="dv">1</span>):</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>        <span class="bu">print</span>(text_batch.numpy()[i])</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>        <span class="bu">print</span>(label_batch.numpy()[i])</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="prepare-the-data">Prepare the data<a class="anchor" aria-label="anchor" href="#prepare-the-data"></a>
</h2>
<p>In particular, we remove <code>&lt;br /&gt;</code> tags.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="co"># Having looked at our data above, we see that the raw text contains HTML break</span></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a><span class="co"># tags of the form '&lt;br /&gt;'. These tags will not be removed by the default</span></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a><span class="co"># standardizer (which doesn't strip HTML). Because of this, we will need to</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a><span class="co"># create a custom standardization function.</span></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a><span class="kw">def</span> custom_standardization(input_data):</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>    lowercase <span class="op">=</span> tf.strings.lower(input_data)</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>    stripped_html <span class="op">=</span> tf.strings.regex_replace(lowercase, <span class="st">"&lt;br /&gt;"</span>, <span class="st">" "</span>)</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>    <span class="cf">return</span> tf.strings.regex_replace(</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>        stripped_html, <span class="ss">f"[</span><span class="sc">{</span>re<span class="sc">.</span>escape(string.punctuation)<span class="sc">}</span><span class="ss">]"</span>, <span class="st">""</span></span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>    )</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a><span class="co"># Model constants.</span></span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>max_features <span class="op">=</span> <span class="dv">20000</span></span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a>embedding_dim <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a>sequence_length <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" tabindex="-1"></a><span class="co"># Now that we have our custom standardization, we can instantiate our text</span></span>
<span id="cb8-19"><a href="#cb8-19" tabindex="-1"></a><span class="co"># vectorization layer. We are using this layer to normalize, split, and map</span></span>
<span id="cb8-20"><a href="#cb8-20" tabindex="-1"></a><span class="co"># strings to integers, so we set our 'output_mode' to 'int'.</span></span>
<span id="cb8-21"><a href="#cb8-21" tabindex="-1"></a><span class="co"># Note that we're using the default split function,</span></span>
<span id="cb8-22"><a href="#cb8-22" tabindex="-1"></a><span class="co"># and the custom standardization defined above.</span></span>
<span id="cb8-23"><a href="#cb8-23" tabindex="-1"></a><span class="co"># We also set an explicit maximum sequence length, since the CNNs later in our</span></span>
<span id="cb8-24"><a href="#cb8-24" tabindex="-1"></a><span class="co"># model won't support ragged sequences.</span></span>
<span id="cb8-25"><a href="#cb8-25" tabindex="-1"></a>vectorize_layer <span class="op">=</span> TextVectorization(</span>
<span id="cb8-26"><a href="#cb8-26" tabindex="-1"></a>    standardize<span class="op">=</span>custom_standardization,</span>
<span id="cb8-27"><a href="#cb8-27" tabindex="-1"></a>    max_tokens<span class="op">=</span>max_features,</span>
<span id="cb8-28"><a href="#cb8-28" tabindex="-1"></a>    output_mode<span class="op">=</span><span class="st">"int"</span>,</span>
<span id="cb8-29"><a href="#cb8-29" tabindex="-1"></a>    output_sequence_length<span class="op">=</span>sequence_length,</span>
<span id="cb8-30"><a href="#cb8-30" tabindex="-1"></a>)</span>
<span id="cb8-31"><a href="#cb8-31" tabindex="-1"></a></span>
<span id="cb8-32"><a href="#cb8-32" tabindex="-1"></a><span class="co"># Now that the vocab layer has been created, call `adapt` on a text-only</span></span>
<span id="cb8-33"><a href="#cb8-33" tabindex="-1"></a><span class="co"># dataset to create the vocabulary. You don't have to batch, but for very large</span></span>
<span id="cb8-34"><a href="#cb8-34" tabindex="-1"></a><span class="co"># datasets this means you're not keeping spare copies of the dataset in memory.</span></span>
<span id="cb8-35"><a href="#cb8-35" tabindex="-1"></a></span>
<span id="cb8-36"><a href="#cb8-36" tabindex="-1"></a><span class="co"># Let's make a text-only dataset (no labels):</span></span>
<span id="cb8-37"><a href="#cb8-37" tabindex="-1"></a>text_ds <span class="op">=</span> raw_train_ds.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: x)</span>
<span id="cb8-38"><a href="#cb8-38" tabindex="-1"></a><span class="co"># Let's call `adapt`:</span></span>
<span id="cb8-39"><a href="#cb8-39" tabindex="-1"></a>vectorize_layer.adapt(text_ds)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="two-options-to-vectorize-the-data">Two options to vectorize the data<a class="anchor" aria-label="anchor" href="#two-options-to-vectorize-the-data"></a>
</h2>
<p>There are 2 ways we can use our text vectorization layer:</p>
<p><strong>Option 1: Make it part of the model</strong>, so as to obtain
a model that processes raw strings, like this:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>text_input <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">1</span>,), dtype<span class="op">=</span>tf.string, name<span class="op">=</span><span class="st">'text'</span>)</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>x <span class="op">=</span> vectorize_layer(text_input)</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>x <span class="op">=</span> layers.Embedding(max_features <span class="op">+</span> <span class="dv">1</span>, embedding_dim)(x)</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>...</span></code></pre></div>
<p><strong>Option 2: Apply it to the text dataset</strong> to obtain a
dataset of word indices, then feed it into a model that expects integer
sequences as inputs.</p>
<p>An important difference between the two is that option 2 enables you
to do <strong>asynchronous CPU processing and buffering</strong> of your
data when training on GPU. So if youâ€™re training the model on GPU, you
probably want to go with this option to get the best performance. This
is what we will do below.</p>
<p>If we were to export our model to production, weâ€™d ship a model that
accepts raw strings as input, like in the code snippet for option 1
above. This can be done after training. We do this in the last
section.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="kw">def</span> vectorize_text(text, label):</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>    text <span class="op">=</span> tf.expand_dims(text, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>    <span class="cf">return</span> vectorize_layer(text), label</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a><span class="co"># Vectorize the data.</span></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>train_ds <span class="op">=</span> raw_train_ds.<span class="bu">map</span>(vectorize_text)</span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>val_ds <span class="op">=</span> raw_val_ds.<span class="bu">map</span>(vectorize_text)</span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a>test_ds <span class="op">=</span> raw_test_ds.<span class="bu">map</span>(vectorize_text)</span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a><span class="co"># Do async prefetching / buffering of the data for best performance on GPU.</span></span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a>train_ds <span class="op">=</span> train_ds.cache().prefetch(buffer_size<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a>val_ds <span class="op">=</span> val_ds.cache().prefetch(buffer_size<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a>test_ds <span class="op">=</span> test_ds.cache().prefetch(buffer_size<span class="op">=</span><span class="dv">10</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="build-a-model">Build a model<a class="anchor" aria-label="anchor" href="#build-a-model"></a>
</h2>
<p>We choose a simple 1D convnet starting with an <code>Embedding</code>
layer.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="co"># A integer input for vocab indices.</span></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(sequence_length,), dtype<span class="op">=</span><span class="st">"int64"</span>)</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a><span class="co"># Next, we add a layer to map those vocab indices into a space of dimensionality</span></span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a><span class="co"># 'embedding_dim'.</span></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Embedding(max_features, embedding_dim)(inputs)</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>x <span class="op">=</span> layers.Dropout(<span class="fl">0.5</span>)(x)</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a><span class="co"># Conv1D + global max pooling</span></span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a>x <span class="op">=</span> layers.Conv1D(<span class="dv">128</span>, <span class="dv">7</span>, padding<span class="op">=</span><span class="st">"valid"</span>, activation<span class="op">=</span><span class="st">"relu"</span>, strides<span class="op">=</span><span class="dv">3</span>)(x)</span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a>x <span class="op">=</span> layers.Conv1D(<span class="dv">128</span>, <span class="dv">7</span>, padding<span class="op">=</span><span class="st">"valid"</span>, activation<span class="op">=</span><span class="st">"relu"</span>, strides<span class="op">=</span><span class="dv">3</span>)(x)</span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a>x <span class="op">=</span> layers.GlobalMaxPooling1D()(x)</span>
<span id="cb11-12"><a href="#cb11-12" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" tabindex="-1"></a><span class="co"># We add a vanilla hidden layer:</span></span>
<span id="cb11-14"><a href="#cb11-14" tabindex="-1"></a>x <span class="op">=</span> layers.Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb11-15"><a href="#cb11-15" tabindex="-1"></a>x <span class="op">=</span> layers.Dropout(<span class="fl">0.5</span>)(x)</span>
<span id="cb11-16"><a href="#cb11-16" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" tabindex="-1"></a><span class="co"># We project onto a single unit output layer, and squash it with a sigmoid:</span></span>
<span id="cb11-18"><a href="#cb11-18" tabindex="-1"></a>predictions <span class="op">=</span> layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span>, name<span class="op">=</span><span class="st">"predictions"</span>)(x)</span>
<span id="cb11-19"><a href="#cb11-19" tabindex="-1"></a>model <span class="op">=</span> keras.Model(inputs, predictions)</span>
<span id="cb11-20"><a href="#cb11-20" tabindex="-1"></a>model.summary()</span>
<span id="cb11-21"><a href="#cb11-21" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb11-22"><a href="#cb11-22" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">"binary_crossentropy"</span>, optimizer<span class="op">=</span><span class="st">"adam"</span>, metrics<span class="op">=</span>[<span class="st">"accuracy"</span>]</span>
<span id="cb11-23"><a href="#cb11-23" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="train-the-model">Train the model<a class="anchor" aria-label="anchor" href="#train-the-model"></a>
</h2>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="co"># Fit the model using the train and test datasets.</span></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>model.fit(train_ds, validation_data<span class="op">=</span>val_ds, epochs<span class="op">=</span>epochs)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="evaluate-the-model-on-the-test-set">Evaluate the model on the test set<a class="anchor" aria-label="anchor" href="#evaluate-the-model-on-the-test-set"></a>
</h2>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>model.evaluate(test_ds)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="make-an-end-to-end-model">Make an end-to-end model<a class="anchor" aria-label="anchor" href="#make-an-end-to-end-model"></a>
</h2>
<p>If you want to obtain a model capable of processing raw strings, you
can simply create a new model (using the weights we just trained):</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="co"># A string input</span></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">1</span>,), dtype<span class="op">=</span><span class="st">"string"</span>)</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a><span class="co"># Turn strings into vocab indices</span></span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>indices <span class="op">=</span> vectorize_layer(inputs)</span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a><span class="co"># Turn vocab indices into predictions</span></span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a>outputs <span class="op">=</span> model(indices)</span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a><span class="co"># Our end to end model</span></span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a>end_to_end_model <span class="op">=</span> keras.Model(inputs, outputs)</span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a>end_to_end_model.<span class="bu">compile</span>(</span>
<span id="cb14-11"><a href="#cb14-11" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">"binary_crossentropy"</span>, optimizer<span class="op">=</span><span class="st">"adam"</span>, metrics<span class="op">=</span>[<span class="st">"accuracy"</span>]</span>
<span id="cb14-12"><a href="#cb14-12" tabindex="-1"></a>)</span>
<span id="cb14-13"><a href="#cb14-13" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" tabindex="-1"></a><span class="co"># Test it with `raw_test_ds`, which yields raw strings</span></span>
<span id="cb14-15"><a href="#cb14-15" tabindex="-1"></a>end_to_end_model.evaluate(raw_test_ds)</span></code></pre></div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, FranÃ§ois Chollet, RStudio, Google.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
