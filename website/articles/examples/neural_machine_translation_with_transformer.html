<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Implementing a sequence-to-sequence Transformer and training it on a machine translation task.">
<title>English-to-Spanish translation with a sequence-to-sequence Transformer • keras</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><link href="../../extra.css" rel="stylesheet">
<script src="../../extra.js"></script><meta property="og:title" content="English-to-Spanish translation with a sequence-to-sequence Transformer">
<meta property="og:description" content="Implementing a sequence-to-sequence Transformer and training it on a machine translation task.">
<meta property="og:image" content="https://keras.posit.co/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-tutorials">Tutorials</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-tutorials">
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <a class="dropdown-item" href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <h6 class="dropdown-header" data-toc-skip>Guides (New for TF 2.6)</h6>
    <a class="dropdown-item" href="../../articles/new-guides/python_subclasses.html">Python Subclasses</a>
    <a class="dropdown-item" href="../../articles/new-guides/making_new_layers_and_models_via_subclassing.html">Making New Layers and Models via Subclassing</a>
    <a class="dropdown-item" href="../../articles/new-guides/customizing_what_happens_in_fit.html">Customizing What Happens in Fit</a>
    <a class="dropdown-item" href="../../articles/new-guides/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <a class="dropdown-item" href="../../articles/new-guides/preprocessing_layers.html">Working with Preprocessing Layers</a>
    <a class="dropdown-item" href="../../argicles/new-guides/working_with_rnns.html">Working with RNNs</a>
    <div class="dropdown-divider"></div>
    <h6 class="dropdown-header" data-toc-skip>Using Keras</h6>
    <a class="dropdown-item" href="../../articles/guide_keras.html">Guide to Keras Basics</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model in Depth</a>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API in Depth</a>
    <a class="dropdown-item" href="../../articles/about_keras_models.html">About Keras Models</a>
    <a class="dropdown-item" href="../../articles/about_keras_layers.html">About Keras Layers</a>
    <a class="dropdown-item" href="../../articles/training_visualization.html">Training Visualization</a>
    <a class="dropdown-item" href="../../articles/applications.html">Pre-Trained Models</a>
    <a class="dropdown-item" href="../../articles/faq.html">Frequently Asked Questions</a>
    <a class="dropdown-item" href="../../articles/why_use_keras.html">Why Use Keras?</a>
    <div class="dropdown-divider"></div>
    <h6 class="dropdown-header" data-toc-skip>Advanced</h6>
    <a class="dropdown-item" href="../../articles/eager_guide.html">Eager Execution</a>
    <a class="dropdown-item" href="../../articles/training_callbacks.html">Training Callbacks</a>
    <a class="dropdown-item" href="../../articles/backend.html">Keras Backend</a>
    <a class="dropdown-item" href="../../articles/custom_layers.html">Custom Layers</a>
    <a class="dropdown-item" href="../../articles/custom_models.html">Custom Models</a>
    <a class="dropdown-item" href="../../articles/saving_serializing.html">Saving and serializing</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../articles/learn.html">Learn</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../articles/tools.html">Tools</a>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../../logo.png" class="logo" alt=""><h1>English-to-Spanish translation with a sequence-to-sequence Transformer</h1>
                        <h4 data-toc-skip class="author"><a href="https://twitter.com/fchollet" class="external-link">fchollet</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/neural_machine_translation_with_transformer.Rmd" class="external-link"><code>vignettes/examples/neural_machine_translation_with_transformer.Rmd</code></a></small>
      <div class="d-none name"><code>neural_machine_translation_with_transformer.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>In this example, we’ll build a sequence-to-sequence Transformer
model, which we’ll train on an English-to-Spanish machine translation
task.</p>
<p>You’ll learn how to:</p>
<ul>
<li>Vectorize text using the Keras <code>TextVectorization</code>
layer.</li>
<li>Implement a <code>TransformerEncoder</code> layer, a
<code>TransformerDecoder</code> layer, and a
<code>PositionalEmbedding</code> layer.</li>
<li>Prepare data for training a sequence-to-sequence model.</li>
<li>Use the trained model to generate translations of never-seen-before
input sentences (sequence-to-sequence inference).</li>
</ul>
<p>The code featured here is adapted from the book <a href="https://www.manning.com/books/deep-learning-with-python-second-edition" class="external-link">Deep
Learning with Python, Second Edition</a> (chapter 11: Deep learning for
text). The present example is fairly barebones, so for detailed
explanations of how each building block works, as well as the theory
behind Transformers, I recommend reading the book.</p>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co"># We set the backend to TensorFlow. The code works with</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="co"># both `tensorflow` and `torch`. It does not work with JAX</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="co"># due to the behavior of `jax.numpy.tile` in a jit scope</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="co"># (used in `TransformerDecoder.get_causal_attention_mask()`:</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="co"># `tile` in JAX does not support a dynamic `reps` argument.</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="co"># You can make the code work in JAX by wrapping the</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="co"># inside of the `get_causal_attention_mask` method in</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="co"># a decorator to prevent jit compilation:</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="co"># `with jax.ensure_compile_time_eval():`.</span></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>os[<span class="st">"KERAS_BACKEND"</span>] <span class="op">=</span> <span class="st">"tensorflow"</span></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a><span class="im">import</span> pathlib</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a><span class="im">import</span> tensorflow.data <span class="im">as</span> tf_data</span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a><span class="im">import</span> tensorflow.strings <span class="im">as</span> tf_strings</span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb1-24"><a href="#cb1-24" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> ops</span>
<span id="cb1-25"><a href="#cb1-25" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> TextVectorization</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="downloading-the-data">Downloading the data<a class="anchor" aria-label="anchor" href="#downloading-the-data"></a>
</h2>
<p>We’ll be working with an English-to-Spanish translation dataset
provided by <a href="https://www.manythings.org/anki/" class="external-link">Anki</a>. Let’s
download it:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>text_file <span class="op">=</span> keras.utils.get_file(</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>    fname<span class="op">=</span><span class="st">"spa-eng.zip"</span>,</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>    origin<span class="op">=</span><span class="st">"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip"</span>,</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>    extract<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>)</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>text_file <span class="op">=</span> pathlib.Path(text_file).parent <span class="op">/</span> <span class="st">"spa-eng"</span> <span class="op">/</span> <span class="st">"spa.txt"</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="parsing-the-data">Parsing the data<a class="anchor" aria-label="anchor" href="#parsing-the-data"></a>
</h2>
<p>Each line contains an English sentence and its corresponding Spanish
sentence. The English sentence is the <em>source sequence</em> and
Spanish one is the <em>target sequence</em>. We prepend the token
<code>"[start]"</code> and we append the token <code>"[end]"</code> to
the Spanish sentence.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(text_file) <span class="im">as</span> f:</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>    lines <span class="op">=</span> f.read().split(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>text_pairs <span class="op">=</span> []</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="cf">for</span> line <span class="kw">in</span> lines:</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>    eng, spa <span class="op">=</span> line.split(<span class="st">"</span><span class="ch">\t</span><span class="st">"</span>)</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>    spa <span class="op">=</span> <span class="st">"[start] "</span> <span class="op">+</span> spa <span class="op">+</span> <span class="st">" [end]"</span></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>    text_pairs.append((eng, spa))</span></code></pre></div>
<p>Here’s what our sentence pairs look like:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>    <span class="bu">print</span>(random.choice(text_pairs))</span></code></pre></div>
<p>Now, let’s split the sentence pairs into a training set, a validation
set, and a test set.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>random.shuffle(text_pairs)</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>num_val_samples <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.15</span> <span class="op">*</span> <span class="bu">len</span>(text_pairs))</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>num_train_samples <span class="op">=</span> <span class="bu">len</span>(text_pairs) <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> num_val_samples</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>train_pairs <span class="op">=</span> text_pairs[:num_train_samples]</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>val_pairs <span class="op">=</span> text_pairs[num_train_samples : num_train_samples <span class="op">+</span> num_val_samples]</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>test_pairs <span class="op">=</span> text_pairs[num_train_samples <span class="op">+</span> num_val_samples :]</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(text_pairs)<span class="sc">}</span><span class="ss"> total pairs"</span>)</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(train_pairs)<span class="sc">}</span><span class="ss"> training pairs"</span>)</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(val_pairs)<span class="sc">}</span><span class="ss"> validation pairs"</span>)</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(test_pairs)<span class="sc">}</span><span class="ss"> test pairs"</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="vectorizing-the-text-data">Vectorizing the text data<a class="anchor" aria-label="anchor" href="#vectorizing-the-text-data"></a>
</h2>
<p>We’ll use two instances of the <code>TextVectorization</code> layer
to vectorize the text data (one for English and one for Spanish), that
is to say, to turn the original strings into integer sequences where
each integer represents the index of a word in a vocabulary.</p>
<p>The English layer will use the default string standardization (strip
punctuation characters) and splitting scheme (split on whitespace),
while the Spanish layer will use a custom standardization, where we add
the character <code>"¿"</code> to the set of punctuation characters to
be stripped.</p>
<p>Note: in a production-grade machine translation model, I would not
recommend stripping the punctuation characters in either language.
Instead, I would recommend turning each punctuation character into its
own token, which you could achieve by providing a custom
<code>split</code> function to the <code>TextVectorization</code>
layer.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>strip_chars <span class="op">=</span> string.punctuation <span class="op">+</span> <span class="st">"¿"</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>strip_chars <span class="op">=</span> strip_chars.replace(<span class="st">"["</span>, <span class="st">""</span>)</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>strip_chars <span class="op">=</span> strip_chars.replace(<span class="st">"]"</span>, <span class="st">""</span>)</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="dv">15000</span></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>sequence_length <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a><span class="kw">def</span> custom_standardization(input_string):</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>    lowercase <span class="op">=</span> tf_strings.lower(input_string)</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>    <span class="cf">return</span> tf_strings.regex_replace(lowercase, <span class="st">"[</span><span class="sc">%s</span><span class="st">]"</span> <span class="op">%</span> re.escape(strip_chars), <span class="st">""</span>)</span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a>eng_vectorization <span class="op">=</span> TextVectorization(</span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a>    max_tokens<span class="op">=</span>vocab_size,</span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a>    output_mode<span class="op">=</span><span class="st">"int"</span>,</span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a>    output_sequence_length<span class="op">=</span>sequence_length,</span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a>)</span>
<span id="cb6-20"><a href="#cb6-20" tabindex="-1"></a>spa_vectorization <span class="op">=</span> TextVectorization(</span>
<span id="cb6-21"><a href="#cb6-21" tabindex="-1"></a>    max_tokens<span class="op">=</span>vocab_size,</span>
<span id="cb6-22"><a href="#cb6-22" tabindex="-1"></a>    output_mode<span class="op">=</span><span class="st">"int"</span>,</span>
<span id="cb6-23"><a href="#cb6-23" tabindex="-1"></a>    output_sequence_length<span class="op">=</span>sequence_length <span class="op">+</span> <span class="dv">1</span>,</span>
<span id="cb6-24"><a href="#cb6-24" tabindex="-1"></a>    standardize<span class="op">=</span>custom_standardization,</span>
<span id="cb6-25"><a href="#cb6-25" tabindex="-1"></a>)</span>
<span id="cb6-26"><a href="#cb6-26" tabindex="-1"></a>train_eng_texts <span class="op">=</span> [pair[<span class="dv">0</span>] <span class="cf">for</span> pair <span class="kw">in</span> train_pairs]</span>
<span id="cb6-27"><a href="#cb6-27" tabindex="-1"></a>train_spa_texts <span class="op">=</span> [pair[<span class="dv">1</span>] <span class="cf">for</span> pair <span class="kw">in</span> train_pairs]</span>
<span id="cb6-28"><a href="#cb6-28" tabindex="-1"></a>eng_vectorization.adapt(train_eng_texts)</span>
<span id="cb6-29"><a href="#cb6-29" tabindex="-1"></a>spa_vectorization.adapt(train_spa_texts)</span></code></pre></div>
<p>Next, we’ll format our datasets.</p>
<p>At each training step, the model will seek to predict target words
N+1 (and beyond) using the source sentence and the target words 0 to
N.</p>
<p>As such, the training dataset will yield a tuple
<code>(inputs, targets)</code>, where:</p>
<ul>
<li>
<code>inputs</code> is a dictionary with the keys
<code>encoder_inputs</code> and <code>decoder_inputs</code>.
<code>encoder_inputs</code> is the vectorized source sentence and
<code>encoder_inputs</code> is the target sentence “so far”, that is to
say, the words 0 to N used to predict word N+1 (and beyond) in the
target sentence.</li>
<li>
<code>target</code> is the target sentence offset by one step: it
provides the next words in the target sentence – what the model will try
to predict.</li>
</ul>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="kw">def</span> format_dataset(eng, spa):</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>    eng <span class="op">=</span> eng_vectorization(eng)</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>    spa <span class="op">=</span> spa_vectorization(spa)</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>    <span class="cf">return</span> (</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>        {</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>            <span class="st">"encoder_inputs"</span>: eng,</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>            <span class="st">"decoder_inputs"</span>: spa[:, :<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>        },</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>        spa[:, <span class="dv">1</span>:],</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>    )</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a><span class="kw">def</span> make_dataset(pairs):</span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a>    eng_texts, spa_texts <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>pairs)</span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a>    eng_texts <span class="op">=</span> <span class="bu">list</span>(eng_texts)</span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a>    spa_texts <span class="op">=</span> <span class="bu">list</span>(spa_texts)</span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a>    dataset <span class="op">=</span> tf_data.Dataset.from_tensor_slices((eng_texts, spa_texts))</span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a>    dataset <span class="op">=</span> dataset.batch(batch_size)</span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a>    dataset <span class="op">=</span> dataset.<span class="bu">map</span>(format_dataset)</span>
<span id="cb7-20"><a href="#cb7-20" tabindex="-1"></a>    <span class="cf">return</span> dataset.shuffle(<span class="dv">2048</span>).prefetch(<span class="dv">16</span>).cache()</span>
<span id="cb7-21"><a href="#cb7-21" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" tabindex="-1"></a></span>
<span id="cb7-23"><a href="#cb7-23" tabindex="-1"></a>train_ds <span class="op">=</span> make_dataset(train_pairs)</span>
<span id="cb7-24"><a href="#cb7-24" tabindex="-1"></a>val_ds <span class="op">=</span> make_dataset(val_pairs)</span></code></pre></div>
<p>Let’s take a quick look at the sequence shapes (we have batches of 64
pairs, and all sequences are 20 steps long):</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="cf">for</span> inputs, targets <span class="kw">in</span> train_ds.take(<span class="dv">1</span>):</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'inputs["encoder_inputs"].shape: </span><span class="sc">{</span>inputs[<span class="st">"encoder_inputs"</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'inputs["decoder_inputs"].shape: </span><span class="sc">{</span>inputs[<span class="st">"decoder_inputs"</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"targets.shape: </span><span class="sc">{</span>targets<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="building-the-model">Building the model<a class="anchor" aria-label="anchor" href="#building-the-model"></a>
</h2>
<p>Our sequence-to-sequence Transformer consists of a
<code>TransformerEncoder</code> and a <code>TransformerDecoder</code>
chained together. To make the model aware of word order, we also use a
<code>PositionalEmbedding</code> layer.</p>
<p>The source sequence will be pass to the
<code>TransformerEncoder</code>, which will produce a new representation
of it. This new representation will then be passed to the
<code>TransformerDecoder</code>, together with the target sequence so
far (target words 0 to N). The <code>TransformerDecoder</code> will then
seek to predict the next words in the target sequence (N+1 and
beyond).</p>
<p>A key detail that makes this possible is causal masking (see method
<code>get_causal_attention_mask()</code> on the
<code>TransformerDecoder</code>). The <code>TransformerDecoder</code>
sees the entire sequences at once, and thus we must make sure that it
only uses information from target tokens 0 to N when predicting token
N+1 (otherwise, it could use information from the future, which would
result in a model that cannot be used at inference time).</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="im">import</span> keras.ops <span class="im">as</span> ops</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a><span class="kw">class</span> TransformerEncoder(layers.Layer):</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim, dense_dim, num_heads, <span class="op">**</span>kwargs):</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>        <span class="va">self</span>.embed_dim <span class="op">=</span> embed_dim</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>        <span class="va">self</span>.dense_dim <span class="op">=</span> dense_dim</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> layers.MultiHeadAttention(</span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a>            num_heads<span class="op">=</span>num_heads, key_dim<span class="op">=</span>embed_dim</span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a>        )</span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a>        <span class="va">self</span>.dense_proj <span class="op">=</span> keras.Sequential(</span>
<span id="cb9-13"><a href="#cb9-13" tabindex="-1"></a>            [</span>
<span id="cb9-14"><a href="#cb9-14" tabindex="-1"></a>                layers.Dense(dense_dim, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb9-15"><a href="#cb9-15" tabindex="-1"></a>                layers.Dense(embed_dim),</span>
<span id="cb9-16"><a href="#cb9-16" tabindex="-1"></a>            ]</span>
<span id="cb9-17"><a href="#cb9-17" tabindex="-1"></a>        )</span>
<span id="cb9-18"><a href="#cb9-18" tabindex="-1"></a>        <span class="va">self</span>.layernorm_1 <span class="op">=</span> layers.LayerNormalization()</span>
<span id="cb9-19"><a href="#cb9-19" tabindex="-1"></a>        <span class="va">self</span>.layernorm_2 <span class="op">=</span> layers.LayerNormalization()</span>
<span id="cb9-20"><a href="#cb9-20" tabindex="-1"></a>        <span class="va">self</span>.supports_masking <span class="op">=</span> <span class="va">True</span></span>
<span id="cb9-21"><a href="#cb9-21" tabindex="-1"></a></span>
<span id="cb9-22"><a href="#cb9-22" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb9-23"><a href="#cb9-23" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb9-24"><a href="#cb9-24" tabindex="-1"></a>            padding_mask <span class="op">=</span> ops.cast(mask[:, <span class="va">None</span>, :], dtype<span class="op">=</span><span class="st">"int32"</span>)</span>
<span id="cb9-25"><a href="#cb9-25" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb9-26"><a href="#cb9-26" tabindex="-1"></a>            padding_mask <span class="op">=</span> <span class="va">None</span></span>
<span id="cb9-27"><a href="#cb9-27" tabindex="-1"></a></span>
<span id="cb9-28"><a href="#cb9-28" tabindex="-1"></a>        attention_output <span class="op">=</span> <span class="va">self</span>.attention(</span>
<span id="cb9-29"><a href="#cb9-29" tabindex="-1"></a>            query<span class="op">=</span>inputs, value<span class="op">=</span>inputs, key<span class="op">=</span>inputs, attention_mask<span class="op">=</span>padding_mask</span>
<span id="cb9-30"><a href="#cb9-30" tabindex="-1"></a>        )</span>
<span id="cb9-31"><a href="#cb9-31" tabindex="-1"></a>        proj_input <span class="op">=</span> <span class="va">self</span>.layernorm_1(inputs <span class="op">+</span> attention_output)</span>
<span id="cb9-32"><a href="#cb9-32" tabindex="-1"></a>        proj_output <span class="op">=</span> <span class="va">self</span>.dense_proj(proj_input)</span>
<span id="cb9-33"><a href="#cb9-33" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.layernorm_2(proj_input <span class="op">+</span> proj_output)</span>
<span id="cb9-34"><a href="#cb9-34" tabindex="-1"></a></span>
<span id="cb9-35"><a href="#cb9-35" tabindex="-1"></a>    <span class="kw">def</span> get_config(<span class="va">self</span>):</span>
<span id="cb9-36"><a href="#cb9-36" tabindex="-1"></a>        config <span class="op">=</span> <span class="bu">super</span>().get_config()</span>
<span id="cb9-37"><a href="#cb9-37" tabindex="-1"></a>        config.update(</span>
<span id="cb9-38"><a href="#cb9-38" tabindex="-1"></a>            {</span>
<span id="cb9-39"><a href="#cb9-39" tabindex="-1"></a>                <span class="st">"embed_dim"</span>: <span class="va">self</span>.embed_dim,</span>
<span id="cb9-40"><a href="#cb9-40" tabindex="-1"></a>                <span class="st">"dense_dim"</span>: <span class="va">self</span>.dense_dim,</span>
<span id="cb9-41"><a href="#cb9-41" tabindex="-1"></a>                <span class="st">"num_heads"</span>: <span class="va">self</span>.num_heads,</span>
<span id="cb9-42"><a href="#cb9-42" tabindex="-1"></a>            }</span>
<span id="cb9-43"><a href="#cb9-43" tabindex="-1"></a>        )</span>
<span id="cb9-44"><a href="#cb9-44" tabindex="-1"></a>        <span class="cf">return</span> config</span>
<span id="cb9-45"><a href="#cb9-45" tabindex="-1"></a></span>
<span id="cb9-46"><a href="#cb9-46" tabindex="-1"></a></span>
<span id="cb9-47"><a href="#cb9-47" tabindex="-1"></a><span class="kw">class</span> PositionalEmbedding(layers.Layer):</span>
<span id="cb9-48"><a href="#cb9-48" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, sequence_length, vocab_size, embed_dim, <span class="op">**</span>kwargs):</span>
<span id="cb9-49"><a href="#cb9-49" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb9-50"><a href="#cb9-50" tabindex="-1"></a>        <span class="va">self</span>.token_embeddings <span class="op">=</span> layers.Embedding(</span>
<span id="cb9-51"><a href="#cb9-51" tabindex="-1"></a>            input_dim<span class="op">=</span>vocab_size, output_dim<span class="op">=</span>embed_dim</span>
<span id="cb9-52"><a href="#cb9-52" tabindex="-1"></a>        )</span>
<span id="cb9-53"><a href="#cb9-53" tabindex="-1"></a>        <span class="va">self</span>.position_embeddings <span class="op">=</span> layers.Embedding(</span>
<span id="cb9-54"><a href="#cb9-54" tabindex="-1"></a>            input_dim<span class="op">=</span>sequence_length, output_dim<span class="op">=</span>embed_dim</span>
<span id="cb9-55"><a href="#cb9-55" tabindex="-1"></a>        )</span>
<span id="cb9-56"><a href="#cb9-56" tabindex="-1"></a>        <span class="va">self</span>.sequence_length <span class="op">=</span> sequence_length</span>
<span id="cb9-57"><a href="#cb9-57" tabindex="-1"></a>        <span class="va">self</span>.vocab_size <span class="op">=</span> vocab_size</span>
<span id="cb9-58"><a href="#cb9-58" tabindex="-1"></a>        <span class="va">self</span>.embed_dim <span class="op">=</span> embed_dim</span>
<span id="cb9-59"><a href="#cb9-59" tabindex="-1"></a></span>
<span id="cb9-60"><a href="#cb9-60" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb9-61"><a href="#cb9-61" tabindex="-1"></a>        length <span class="op">=</span> ops.shape(inputs)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb9-62"><a href="#cb9-62" tabindex="-1"></a>        positions <span class="op">=</span> ops.arange(<span class="dv">0</span>, length, <span class="dv">1</span>)</span>
<span id="cb9-63"><a href="#cb9-63" tabindex="-1"></a>        embedded_tokens <span class="op">=</span> <span class="va">self</span>.token_embeddings(inputs)</span>
<span id="cb9-64"><a href="#cb9-64" tabindex="-1"></a>        embedded_positions <span class="op">=</span> <span class="va">self</span>.position_embeddings(positions)</span>
<span id="cb9-65"><a href="#cb9-65" tabindex="-1"></a>        <span class="cf">return</span> embedded_tokens <span class="op">+</span> embedded_positions</span>
<span id="cb9-66"><a href="#cb9-66" tabindex="-1"></a></span>
<span id="cb9-67"><a href="#cb9-67" tabindex="-1"></a>    <span class="kw">def</span> compute_mask(<span class="va">self</span>, inputs, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb9-68"><a href="#cb9-68" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb9-69"><a href="#cb9-69" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb9-70"><a href="#cb9-70" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb9-71"><a href="#cb9-71" tabindex="-1"></a>            <span class="cf">return</span> ops.not_equal(inputs, <span class="dv">0</span>)</span>
<span id="cb9-72"><a href="#cb9-72" tabindex="-1"></a></span>
<span id="cb9-73"><a href="#cb9-73" tabindex="-1"></a>    <span class="kw">def</span> get_config(<span class="va">self</span>):</span>
<span id="cb9-74"><a href="#cb9-74" tabindex="-1"></a>        config <span class="op">=</span> <span class="bu">super</span>().get_config()</span>
<span id="cb9-75"><a href="#cb9-75" tabindex="-1"></a>        config.update(</span>
<span id="cb9-76"><a href="#cb9-76" tabindex="-1"></a>            {</span>
<span id="cb9-77"><a href="#cb9-77" tabindex="-1"></a>                <span class="st">"sequence_length"</span>: <span class="va">self</span>.sequence_length,</span>
<span id="cb9-78"><a href="#cb9-78" tabindex="-1"></a>                <span class="st">"vocab_size"</span>: <span class="va">self</span>.vocab_size,</span>
<span id="cb9-79"><a href="#cb9-79" tabindex="-1"></a>                <span class="st">"embed_dim"</span>: <span class="va">self</span>.embed_dim,</span>
<span id="cb9-80"><a href="#cb9-80" tabindex="-1"></a>            }</span>
<span id="cb9-81"><a href="#cb9-81" tabindex="-1"></a>        )</span>
<span id="cb9-82"><a href="#cb9-82" tabindex="-1"></a>        <span class="cf">return</span> config</span>
<span id="cb9-83"><a href="#cb9-83" tabindex="-1"></a></span>
<span id="cb9-84"><a href="#cb9-84" tabindex="-1"></a></span>
<span id="cb9-85"><a href="#cb9-85" tabindex="-1"></a><span class="kw">class</span> TransformerDecoder(layers.Layer):</span>
<span id="cb9-86"><a href="#cb9-86" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim, latent_dim, num_heads, <span class="op">**</span>kwargs):</span>
<span id="cb9-87"><a href="#cb9-87" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb9-88"><a href="#cb9-88" tabindex="-1"></a>        <span class="va">self</span>.embed_dim <span class="op">=</span> embed_dim</span>
<span id="cb9-89"><a href="#cb9-89" tabindex="-1"></a>        <span class="va">self</span>.latent_dim <span class="op">=</span> latent_dim</span>
<span id="cb9-90"><a href="#cb9-90" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb9-91"><a href="#cb9-91" tabindex="-1"></a>        <span class="va">self</span>.attention_1 <span class="op">=</span> layers.MultiHeadAttention(</span>
<span id="cb9-92"><a href="#cb9-92" tabindex="-1"></a>            num_heads<span class="op">=</span>num_heads, key_dim<span class="op">=</span>embed_dim</span>
<span id="cb9-93"><a href="#cb9-93" tabindex="-1"></a>        )</span>
<span id="cb9-94"><a href="#cb9-94" tabindex="-1"></a>        <span class="va">self</span>.attention_2 <span class="op">=</span> layers.MultiHeadAttention(</span>
<span id="cb9-95"><a href="#cb9-95" tabindex="-1"></a>            num_heads<span class="op">=</span>num_heads, key_dim<span class="op">=</span>embed_dim</span>
<span id="cb9-96"><a href="#cb9-96" tabindex="-1"></a>        )</span>
<span id="cb9-97"><a href="#cb9-97" tabindex="-1"></a>        <span class="va">self</span>.dense_proj <span class="op">=</span> keras.Sequential(</span>
<span id="cb9-98"><a href="#cb9-98" tabindex="-1"></a>            [</span>
<span id="cb9-99"><a href="#cb9-99" tabindex="-1"></a>                layers.Dense(latent_dim, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb9-100"><a href="#cb9-100" tabindex="-1"></a>                layers.Dense(embed_dim),</span>
<span id="cb9-101"><a href="#cb9-101" tabindex="-1"></a>            ]</span>
<span id="cb9-102"><a href="#cb9-102" tabindex="-1"></a>        )</span>
<span id="cb9-103"><a href="#cb9-103" tabindex="-1"></a>        <span class="va">self</span>.layernorm_1 <span class="op">=</span> layers.LayerNormalization()</span>
<span id="cb9-104"><a href="#cb9-104" tabindex="-1"></a>        <span class="va">self</span>.layernorm_2 <span class="op">=</span> layers.LayerNormalization()</span>
<span id="cb9-105"><a href="#cb9-105" tabindex="-1"></a>        <span class="va">self</span>.layernorm_3 <span class="op">=</span> layers.LayerNormalization()</span>
<span id="cb9-106"><a href="#cb9-106" tabindex="-1"></a>        <span class="va">self</span>.supports_masking <span class="op">=</span> <span class="va">True</span></span>
<span id="cb9-107"><a href="#cb9-107" tabindex="-1"></a></span>
<span id="cb9-108"><a href="#cb9-108" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs, encoder_outputs, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb9-109"><a href="#cb9-109" tabindex="-1"></a>        causal_mask <span class="op">=</span> <span class="va">self</span>.get_causal_attention_mask(inputs)</span>
<span id="cb9-110"><a href="#cb9-110" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb9-111"><a href="#cb9-111" tabindex="-1"></a>            padding_mask <span class="op">=</span> ops.cast(mask[:, <span class="va">None</span>, :], dtype<span class="op">=</span><span class="st">"int32"</span>)</span>
<span id="cb9-112"><a href="#cb9-112" tabindex="-1"></a>            padding_mask <span class="op">=</span> ops.minimum(padding_mask, causal_mask)</span>
<span id="cb9-113"><a href="#cb9-113" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb9-114"><a href="#cb9-114" tabindex="-1"></a>            padding_mask <span class="op">=</span> <span class="va">None</span></span>
<span id="cb9-115"><a href="#cb9-115" tabindex="-1"></a></span>
<span id="cb9-116"><a href="#cb9-116" tabindex="-1"></a>        attention_output_1 <span class="op">=</span> <span class="va">self</span>.attention_1(</span>
<span id="cb9-117"><a href="#cb9-117" tabindex="-1"></a>            query<span class="op">=</span>inputs, value<span class="op">=</span>inputs, key<span class="op">=</span>inputs, attention_mask<span class="op">=</span>causal_mask</span>
<span id="cb9-118"><a href="#cb9-118" tabindex="-1"></a>        )</span>
<span id="cb9-119"><a href="#cb9-119" tabindex="-1"></a>        out_1 <span class="op">=</span> <span class="va">self</span>.layernorm_1(inputs <span class="op">+</span> attention_output_1)</span>
<span id="cb9-120"><a href="#cb9-120" tabindex="-1"></a></span>
<span id="cb9-121"><a href="#cb9-121" tabindex="-1"></a>        attention_output_2 <span class="op">=</span> <span class="va">self</span>.attention_2(</span>
<span id="cb9-122"><a href="#cb9-122" tabindex="-1"></a>            query<span class="op">=</span>out_1,</span>
<span id="cb9-123"><a href="#cb9-123" tabindex="-1"></a>            value<span class="op">=</span>encoder_outputs,</span>
<span id="cb9-124"><a href="#cb9-124" tabindex="-1"></a>            key<span class="op">=</span>encoder_outputs,</span>
<span id="cb9-125"><a href="#cb9-125" tabindex="-1"></a>            attention_mask<span class="op">=</span>padding_mask,</span>
<span id="cb9-126"><a href="#cb9-126" tabindex="-1"></a>        )</span>
<span id="cb9-127"><a href="#cb9-127" tabindex="-1"></a>        out_2 <span class="op">=</span> <span class="va">self</span>.layernorm_2(out_1 <span class="op">+</span> attention_output_2)</span>
<span id="cb9-128"><a href="#cb9-128" tabindex="-1"></a></span>
<span id="cb9-129"><a href="#cb9-129" tabindex="-1"></a>        proj_output <span class="op">=</span> <span class="va">self</span>.dense_proj(out_2)</span>
<span id="cb9-130"><a href="#cb9-130" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.layernorm_3(out_2 <span class="op">+</span> proj_output)</span>
<span id="cb9-131"><a href="#cb9-131" tabindex="-1"></a></span>
<span id="cb9-132"><a href="#cb9-132" tabindex="-1"></a>    <span class="kw">def</span> get_causal_attention_mask(<span class="va">self</span>, inputs):</span>
<span id="cb9-133"><a href="#cb9-133" tabindex="-1"></a>        input_shape <span class="op">=</span> ops.shape(inputs)</span>
<span id="cb9-134"><a href="#cb9-134" tabindex="-1"></a>        batch_size, sequence_length <span class="op">=</span> input_shape[<span class="dv">0</span>], input_shape[<span class="dv">1</span>]</span>
<span id="cb9-135"><a href="#cb9-135" tabindex="-1"></a>        i <span class="op">=</span> ops.arange(sequence_length)[:, <span class="va">None</span>]</span>
<span id="cb9-136"><a href="#cb9-136" tabindex="-1"></a>        j <span class="op">=</span> ops.arange(sequence_length)</span>
<span id="cb9-137"><a href="#cb9-137" tabindex="-1"></a>        mask <span class="op">=</span> ops.cast(i <span class="op">&gt;=</span> j, dtype<span class="op">=</span><span class="st">"int32"</span>)</span>
<span id="cb9-138"><a href="#cb9-138" tabindex="-1"></a>        mask <span class="op">=</span> ops.reshape(mask, (<span class="dv">1</span>, input_shape[<span class="dv">1</span>], input_shape[<span class="dv">1</span>]))</span>
<span id="cb9-139"><a href="#cb9-139" tabindex="-1"></a>        mult <span class="op">=</span> ops.concatenate(</span>
<span id="cb9-140"><a href="#cb9-140" tabindex="-1"></a>            [ops.expand_dims(batch_size, <span class="op">-</span><span class="dv">1</span>), ops.convert_to_tensor([<span class="dv">1</span>, <span class="dv">1</span>])],</span>
<span id="cb9-141"><a href="#cb9-141" tabindex="-1"></a>            axis<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb9-142"><a href="#cb9-142" tabindex="-1"></a>        )</span>
<span id="cb9-143"><a href="#cb9-143" tabindex="-1"></a>        <span class="cf">return</span> ops.tile(mask, mult)</span>
<span id="cb9-144"><a href="#cb9-144" tabindex="-1"></a></span>
<span id="cb9-145"><a href="#cb9-145" tabindex="-1"></a>    <span class="kw">def</span> get_config(<span class="va">self</span>):</span>
<span id="cb9-146"><a href="#cb9-146" tabindex="-1"></a>        config <span class="op">=</span> <span class="bu">super</span>().get_config()</span>
<span id="cb9-147"><a href="#cb9-147" tabindex="-1"></a>        config.update(</span>
<span id="cb9-148"><a href="#cb9-148" tabindex="-1"></a>            {</span>
<span id="cb9-149"><a href="#cb9-149" tabindex="-1"></a>                <span class="st">"embed_dim"</span>: <span class="va">self</span>.embed_dim,</span>
<span id="cb9-150"><a href="#cb9-150" tabindex="-1"></a>                <span class="st">"latent_dim"</span>: <span class="va">self</span>.latent_dim,</span>
<span id="cb9-151"><a href="#cb9-151" tabindex="-1"></a>                <span class="st">"num_heads"</span>: <span class="va">self</span>.num_heads,</span>
<span id="cb9-152"><a href="#cb9-152" tabindex="-1"></a>            }</span>
<span id="cb9-153"><a href="#cb9-153" tabindex="-1"></a>        )</span>
<span id="cb9-154"><a href="#cb9-154" tabindex="-1"></a>        <span class="cf">return</span> config</span></code></pre></div>
<p>Next, we assemble the end-to-end model.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>embed_dim <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>latent_dim <span class="op">=</span> <span class="dv">2048</span></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>num_heads <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>encoder_inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="va">None</span>,), dtype<span class="op">=</span><span class="st">"int64"</span>, name<span class="op">=</span><span class="st">"encoder_inputs"</span>)</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>x <span class="op">=</span> PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>encoder_outputs <span class="op">=</span> TransformerEncoder(embed_dim, latent_dim, num_heads)(x)</span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>encoder <span class="op">=</span> keras.Model(encoder_inputs, encoder_outputs)</span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a>decoder_inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="va">None</span>,), dtype<span class="op">=</span><span class="st">"int64"</span>, name<span class="op">=</span><span class="st">"decoder_inputs"</span>)</span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a>encoded_seq_inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="va">None</span>, embed_dim), name<span class="op">=</span><span class="st">"decoder_state_inputs"</span>)</span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a>x <span class="op">=</span> PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)</span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a>x <span class="op">=</span> TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)</span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a>x <span class="op">=</span> layers.Dropout(<span class="fl">0.5</span>)(x)</span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a>decoder_outputs <span class="op">=</span> layers.Dense(vocab_size, activation<span class="op">=</span><span class="st">"softmax"</span>)(x)</span>
<span id="cb10-16"><a href="#cb10-16" tabindex="-1"></a>decoder <span class="op">=</span> keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)</span>
<span id="cb10-17"><a href="#cb10-17" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" tabindex="-1"></a>decoder_outputs <span class="op">=</span> decoder([decoder_inputs, encoder_outputs])</span>
<span id="cb10-19"><a href="#cb10-19" tabindex="-1"></a>transformer <span class="op">=</span> keras.Model(</span>
<span id="cb10-20"><a href="#cb10-20" tabindex="-1"></a>    [encoder_inputs, decoder_inputs], decoder_outputs, name<span class="op">=</span><span class="st">"transformer"</span></span>
<span id="cb10-21"><a href="#cb10-21" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="training-our-model">Training our model<a class="anchor" aria-label="anchor" href="#training-our-model"></a>
</h2>
<p>We’ll use accuracy as a quick way to monitor training progress on the
validation data. Note that machine translation typically uses BLEU
scores as well as other metrics, rather than accuracy.</p>
<p>Here we only train for 1 epoch, but to get the model to actually
converge you should train for at least 30 epochs.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">1</span>  <span class="co"># This should be at least 30 for convergence</span></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>transformer.summary()</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>transformer.<span class="bu">compile</span>(</span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>    <span class="st">"rmsprop"</span>, loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>, metrics<span class="op">=</span>[<span class="st">"accuracy"</span>]</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>)</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>transformer.fit(train_ds, epochs<span class="op">=</span>epochs, validation_data<span class="op">=</span>val_ds)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="decoding-test-sentences">Decoding test sentences<a class="anchor" aria-label="anchor" href="#decoding-test-sentences"></a>
</h2>
<p>Finally, let’s demonstrate how to translate brand new English
sentences. We simply feed into the model the vectorized English sentence
as well as the target token <code>"[start]"</code>, then we repeatedly
generated the next token, until we hit the token
<code>"[end]"</code>.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>spa_vocab <span class="op">=</span> spa_vectorization.get_vocabulary()</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>spa_index_lookup <span class="op">=</span> <span class="bu">dict</span>(<span class="bu">zip</span>(<span class="bu">range</span>(<span class="bu">len</span>(spa_vocab)), spa_vocab))</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>max_decoded_sentence_length <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a><span class="kw">def</span> decode_sequence(input_sentence):</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a>    tokenized_input_sentence <span class="op">=</span> eng_vectorization([input_sentence])</span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a>    decoded_sentence <span class="op">=</span> <span class="st">"[start]"</span></span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_decoded_sentence_length):</span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a>        tokenized_target_sentence <span class="op">=</span> spa_vectorization([decoded_sentence])[:, :<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb12-11"><a href="#cb12-11" tabindex="-1"></a>        predictions <span class="op">=</span> transformer([tokenized_input_sentence, tokenized_target_sentence])</span>
<span id="cb12-12"><a href="#cb12-12" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" tabindex="-1"></a>        <span class="co"># ops.argmax(predictions[0, i, :]) is not a concrete value for jax here</span></span>
<span id="cb12-14"><a href="#cb12-14" tabindex="-1"></a>        sampled_token_index <span class="op">=</span> ops.convert_to_numpy(ops.argmax(predictions[<span class="dv">0</span>, i, :])).item(<span class="dv">0</span>)</span>
<span id="cb12-15"><a href="#cb12-15" tabindex="-1"></a>        sampled_token <span class="op">=</span> spa_index_lookup[sampled_token_index]</span>
<span id="cb12-16"><a href="#cb12-16" tabindex="-1"></a>        decoded_sentence <span class="op">+=</span> <span class="st">" "</span> <span class="op">+</span> sampled_token</span>
<span id="cb12-17"><a href="#cb12-17" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" tabindex="-1"></a>        <span class="cf">if</span> sampled_token <span class="op">==</span> <span class="st">"[end]"</span>:</span>
<span id="cb12-19"><a href="#cb12-19" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb12-20"><a href="#cb12-20" tabindex="-1"></a>    <span class="cf">return</span> decoded_sentence</span>
<span id="cb12-21"><a href="#cb12-21" tabindex="-1"></a></span>
<span id="cb12-22"><a href="#cb12-22" tabindex="-1"></a></span>
<span id="cb12-23"><a href="#cb12-23" tabindex="-1"></a>test_eng_texts <span class="op">=</span> [pair[<span class="dv">0</span>] <span class="cf">for</span> pair <span class="kw">in</span> test_pairs]</span>
<span id="cb12-24"><a href="#cb12-24" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">30</span>):</span>
<span id="cb12-25"><a href="#cb12-25" tabindex="-1"></a>    input_sentence <span class="op">=</span> random.choice(test_eng_texts)</span>
<span id="cb12-26"><a href="#cb12-26" tabindex="-1"></a>    translated <span class="op">=</span> decode_sequence(input_sentence)</span></code></pre></div>
<p>After 30 epochs, we get results such as:</p>
<blockquote>
<p>She handed him the money. [start] ella le pasó el dinero [end]</p>
</blockquote>
<blockquote>
<p>Tom has never heard Mary sing. [start] tom nunca ha oído cantar a
mary [end]</p>
</blockquote>
<blockquote>
<p>Perhaps she will come tomorrow. [start] tal vez ella vendrá mañana
[end]</p>
</blockquote>
<blockquote>
<p>I love to write. [start] me encanta escribir [end]</p>
</blockquote>
<blockquote>
<p>His French is improving little by little. [start] su francés va a
[UNK] sólo un poco [end]</p>
</blockquote>
<blockquote>
<p>My hotel told me to call you. [start] mi hotel me dijo que te [UNK]
[end]</p>
</blockquote>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, RStudio, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
