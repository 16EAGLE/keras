<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Use KerasNLP to train a sequence-to-sequence Transformer model on the machine translation task.">
<title>English-to-Spanish translation with KerasNLP • keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../../deps/JetBrains_Mono-0.4.7/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><meta property="og:title" content="English-to-Spanish translation with KerasNLP">
<meta property="og:description" content="Use KerasNLP to train a sequence-to-sequence Transformer model on the machine translation task.">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-getting-started">Getting Started</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-getting-started">
    <a class="dropdown-item" href="../../articles/intro_to_keras_for_engineers.html">Introduction to Keras for engineers</a>
    <h6 class="dropdown-header" data-toc-skip>Tutorials</h6>
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
  </div>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>English-to-Spanish translation with KerasNLP</h1>
                        <h4 data-toc-skip class="author"><a href="https://github.com/abheesht17/" class="external-link">Abheesht Sharma</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/neural_machine_translation_with_keras_nlp.Rmd" class="external-link"><code>vignettes/examples/neural_machine_translation_with_keras_nlp.Rmd</code></a></small>
      <div class="d-none name"><code>neural_machine_translation_with_keras_nlp.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>KerasNLP provides building blocks for NLP (model layers, tokenizers,
metrics, etc.) and makes it convenient to construct NLP pipelines.</p>
<p>In this example, we’ll use KerasNLP layers to build an
encoder-decoder Transformer model, and train it on the
English-to-Spanish machine translation task.</p>
<p>This example is based on the <a href="https://keras.io/examples/nlp/neural_machine_translation_with_transformer/" class="external-link">English-to-Spanish
NMT example</a> by <a href="https://twitter.com/fchollet" class="external-link">fchollet</a>.
The original example is more low-level and implements layers from
scratch, whereas this example uses KerasNLP to show some more advanced
approaches, such as subword tokenization and using metrics to compute
the quality of generated translations.</p>
<p>You’ll learn how to:</p>
<ul>
<li>Tokenize text using
<code>keras_nlp.tokenizers.WordPieceTokenizer</code>.</li>
<li>Implement a sequence-to-sequence Transformer model using KerasNLP’s
<code>keras_nlp.layers.TransformerEncoder</code>,
<code>keras_nlp.layers.TransformerDecoder</code> and
<code>keras_nlp.layers.TokenAndPositionEmbedding</code> layers, and
train it.</li>
<li>Use <code>keras_nlp.samplers</code> to generate translations of
unseen input sentences using the top-p decoding strategy!</li>
</ul>
<p>Don’t worry if you aren’t familiar with KerasNLP. This tutorial will
start with the basics. Let’s dive right in!</p>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<p>Before we start implementing the pipeline, let’s import all the
libraries we need.</p>
<p>!pip install -q rouge-score !pip install -q git+<a href="https://github.com/keras-team/keras-nlp.git" class="external-link uri">https://github.com/keras-team/keras-nlp.git</a> –upgrade</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> keras_nlp</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> pathlib</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> ops</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="im">import</span> tensorflow.data <span class="im">as</span> tf_data</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="im">from</span> tensorflow_text.tools.wordpiece_vocab <span class="im">import</span> (</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>    bert_vocab_from_dataset <span class="im">as</span> bert_vocab,</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>)</span></code></pre></div>
<p>Let’s also define our parameters/hyperparameters.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>EPOCHS <span class="op">=</span> <span class="dv">1</span>  <span class="co"># This should be at least 10 for convergence</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>MAX_SEQUENCE_LENGTH <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>ENG_VOCAB_SIZE <span class="op">=</span> <span class="dv">15000</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>SPA_VOCAB_SIZE <span class="op">=</span> <span class="dv">15000</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>EMBED_DIM <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>INTERMEDIATE_DIM <span class="op">=</span> <span class="dv">2048</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>NUM_HEADS <span class="op">=</span> <span class="dv">8</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="downloading-the-data">Downloading the data<a class="anchor" aria-label="anchor" href="#downloading-the-data"></a>
</h2>
<p>We’ll be working with an English-to-Spanish translation dataset
provided by <a href="https://www.manythings.org/anki/" class="external-link">Anki</a>. Let’s
download it:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>text_file <span class="op">=</span> keras.utils.get_file(</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>    fname<span class="op">=</span><span class="st">"spa-eng.zip"</span>,</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>    origin<span class="op">=</span><span class="st">"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip"</span>,</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>    extract<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>)</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>text_file <span class="op">=</span> pathlib.Path(text_file).parent <span class="op">/</span> <span class="st">"spa-eng"</span> <span class="op">/</span> <span class="st">"spa.txt"</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="parsing-the-data">Parsing the data<a class="anchor" aria-label="anchor" href="#parsing-the-data"></a>
</h2>
<p>Each line contains an English sentence and its corresponding Spanish
sentence. The English sentence is the <em>source sequence</em> and
Spanish one is the <em>target sequence</em>. Before adding the text to a
list, we convert it to lowercase.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(text_file) <span class="im">as</span> f:</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>    lines <span class="op">=</span> f.read().split(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>text_pairs <span class="op">=</span> []</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a><span class="cf">for</span> line <span class="kw">in</span> lines:</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>    eng, spa <span class="op">=</span> line.split(<span class="st">"</span><span class="ch">\t</span><span class="st">"</span>)</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>    eng <span class="op">=</span> eng.lower()</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>    spa <span class="op">=</span> spa.lower()</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>    text_pairs.append((eng, spa))</span></code></pre></div>
<p>Here’s what our sentence pairs look like:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>    <span class="bu">print</span>(random.choice(text_pairs))</span></code></pre></div>
<p>Now, let’s split the sentence pairs into a training set, a validation
set, and a test set.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>random.shuffle(text_pairs)</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>num_val_samples <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.15</span> <span class="op">*</span> <span class="bu">len</span>(text_pairs))</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>num_train_samples <span class="op">=</span> <span class="bu">len</span>(text_pairs) <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> num_val_samples</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>train_pairs <span class="op">=</span> text_pairs[:num_train_samples]</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>val_pairs <span class="op">=</span> text_pairs[num_train_samples : num_train_samples <span class="op">+</span> num_val_samples]</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>test_pairs <span class="op">=</span> text_pairs[num_train_samples <span class="op">+</span> num_val_samples :]</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(text_pairs)<span class="sc">}</span><span class="ss"> total pairs"</span>)</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(train_pairs)<span class="sc">}</span><span class="ss"> training pairs"</span>)</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(val_pairs)<span class="sc">}</span><span class="ss"> validation pairs"</span>)</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(test_pairs)<span class="sc">}</span><span class="ss"> test pairs"</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="tokenizing-the-data">Tokenizing the data<a class="anchor" aria-label="anchor" href="#tokenizing-the-data"></a>
</h2>
<p>We’ll define two tokenizers - one for the source language (English),
and the other for the target language (Spanish). We’ll be using
<code>keras_nlp.tokenizers.WordPieceTokenizer</code> to tokenize the
text. <code>keras_nlp.tokenizers.WordPieceTokenizer</code> takes a
WordPiece vocabulary and has functions for tokenizing the text, and
detokenizing sequences of tokens.</p>
<p>Before we define the two tokenizers, we first need to train them on
the dataset we have. The WordPiece tokenization algorithm is a subword
tokenization algorithm; training it on a corpus gives us a vocabulary of
subwords. A subword tokenizer is a compromise between word tokenizers
(word tokenizers need very large vocabularies for good coverage of input
words), and character tokenizers (characters don’t really encode meaning
like words do). Luckily, KerasNLP makes it very simple to train
WordPiece on a corpus with the
<code>keras_nlp.tokenizers.compute_word_piece_vocabulary</code>
utility.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="kw">def</span> train_word_piece(text_samples, vocab_size, reserved_tokens):</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>    word_piece_ds <span class="op">=</span> tf_data.Dataset.from_tensor_slices(text_samples)</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>    vocab <span class="op">=</span> keras_nlp.tokenizers.compute_word_piece_vocabulary(</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>        word_piece_ds.batch(<span class="dv">1000</span>).prefetch(<span class="dv">2</span>),</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>        vocabulary_size<span class="op">=</span>vocab_size,</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>        reserved_tokens<span class="op">=</span>reserved_tokens,</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>    )</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>    <span class="cf">return</span> vocab</span></code></pre></div>
<p>Every vocabulary has a few special, reserved tokens. We have four
such tokens:</p>
<ul>
<li>
<code>"[PAD]"</code> - Padding token. Padding tokens are appended to
the input sequence length when the input sequence length is shorter than
the maximum sequence length.</li>
<li>
<code>"[UNK]"</code> - Unknown token.</li>
<li>
<code>"[START]"</code> - Token that marks the start of the input
sequence.</li>
<li>
<code>"[END]"</code> - Token that marks the end of the input
sequence.</li>
</ul>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>reserved_tokens <span class="op">=</span> [<span class="st">"[PAD]"</span>, <span class="st">"[UNK]"</span>, <span class="st">"[START]"</span>, <span class="st">"[END]"</span>]</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>eng_samples <span class="op">=</span> [text_pair[<span class="dv">0</span>] <span class="cf">for</span> text_pair <span class="kw">in</span> train_pairs]</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>eng_vocab <span class="op">=</span> train_word_piece(eng_samples, ENG_VOCAB_SIZE, reserved_tokens)</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>spa_samples <span class="op">=</span> [text_pair[<span class="dv">1</span>] <span class="cf">for</span> text_pair <span class="kw">in</span> train_pairs]</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>spa_vocab <span class="op">=</span> train_word_piece(spa_samples, SPA_VOCAB_SIZE, reserved_tokens)</span></code></pre></div>
<p>Let’s see some tokens!</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"English Tokens: "</span>, eng_vocab[<span class="dv">100</span>:<span class="dv">110</span>])</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Spanish Tokens: "</span>, spa_vocab[<span class="dv">100</span>:<span class="dv">110</span>])</span></code></pre></div>
<p>Now, let’s define the tokenizers. We will configure the tokenizers
with the the vocabularies trained above.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>eng_tokenizer <span class="op">=</span> keras_nlp.tokenizers.WordPieceTokenizer(</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>    vocabulary<span class="op">=</span>eng_vocab, lowercase<span class="op">=</span><span class="va">False</span></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>)</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>spa_tokenizer <span class="op">=</span> keras_nlp.tokenizers.WordPieceTokenizer(</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>    vocabulary<span class="op">=</span>spa_vocab, lowercase<span class="op">=</span><span class="va">False</span></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>)</span></code></pre></div>
<p>Let’s try and tokenize a sample from our dataset! To verify whether
the text has been tokenized correctly, we can also detokenize the list
of tokens back to the original text.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>eng_input_ex <span class="op">=</span> text_pairs[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>eng_tokens_ex <span class="op">=</span> eng_tokenizer.tokenize(eng_input_ex)</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"English sentence: "</span>, eng_input_ex)</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Tokens: "</span>, eng_tokens_ex)</span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>    <span class="st">"Recovered text after detokenizing: "</span>,</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>    eng_tokenizer.detokenize(eng_tokens_ex),</span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a>)</span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a></span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" tabindex="-1"></a>spa_input_ex <span class="op">=</span> text_pairs[<span class="dv">0</span>][<span class="dv">1</span>]</span>
<span id="cb11-13"><a href="#cb11-13" tabindex="-1"></a>spa_tokens_ex <span class="op">=</span> spa_tokenizer.tokenize(spa_input_ex)</span>
<span id="cb11-14"><a href="#cb11-14" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Spanish sentence: "</span>, spa_input_ex)</span>
<span id="cb11-15"><a href="#cb11-15" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Tokens: "</span>, spa_tokens_ex)</span>
<span id="cb11-16"><a href="#cb11-16" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb11-17"><a href="#cb11-17" tabindex="-1"></a>    <span class="st">"Recovered text after detokenizing: "</span>,</span>
<span id="cb11-18"><a href="#cb11-18" tabindex="-1"></a>    spa_tokenizer.detokenize(spa_tokens_ex),</span>
<span id="cb11-19"><a href="#cb11-19" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="format-datasets">Format datasets<a class="anchor" aria-label="anchor" href="#format-datasets"></a>
</h2>
<p>Next, we’ll format our datasets.</p>
<p>At each training step, the model will seek to predict target words
N+1 (and beyond) using the source sentence and the target words 0 to
N.</p>
<p>As such, the training dataset will yield a tuple
<code>(inputs, targets)</code>, where:</p>
<ul>
<li>
<code>inputs</code> is a dictionary with the keys
<code>encoder_inputs</code> and <code>decoder_inputs</code>.
<code>encoder_inputs</code> is the tokenized source sentence and
<code>decoder_inputs</code> is the target sentence “so far”, that is to
say, the words 0 to N used to predict word N+1 (and beyond) in the
target sentence.</li>
<li>
<code>target</code> is the target sentence offset by one step: it
provides the next words in the target sentence – what the model will try
to predict.</li>
</ul>
<p>We will add special tokens, <code>"[START]"</code> and
<code>"[END]"</code>, to the input Spanish sentence after tokenizing the
text. We will also pad the input to a fixed length. This can be easily
done using <code>keras_nlp.layers.StartEndPacker</code>.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="kw">def</span> preprocess_batch(eng, spa):</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>    batch_size <span class="op">=</span> ops.shape(spa)[<span class="dv">0</span>]</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>    eng <span class="op">=</span> eng_tokenizer(eng)</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>    spa <span class="op">=</span> spa_tokenizer(spa)</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a>    <span class="co"># Pad `eng` to `MAX_SEQUENCE_LENGTH`.</span></span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a>    eng_start_end_packer <span class="op">=</span> keras_nlp.layers.StartEndPacker(</span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a>        sequence_length<span class="op">=</span>MAX_SEQUENCE_LENGTH,</span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a>        pad_value<span class="op">=</span>eng_tokenizer.token_to_id(<span class="st">"[PAD]"</span>),</span>
<span id="cb12-11"><a href="#cb12-11" tabindex="-1"></a>    )</span>
<span id="cb12-12"><a href="#cb12-12" tabindex="-1"></a>    eng <span class="op">=</span> eng_start_end_packer(eng)</span>
<span id="cb12-13"><a href="#cb12-13" tabindex="-1"></a></span>
<span id="cb12-14"><a href="#cb12-14" tabindex="-1"></a>    <span class="co"># Add special tokens (`"[START]"` and `"[</span><span class="re">END</span><span class="co">]"`) to `spa` and pad it as well.</span></span>
<span id="cb12-15"><a href="#cb12-15" tabindex="-1"></a>    spa_start_end_packer <span class="op">=</span> keras_nlp.layers.StartEndPacker(</span>
<span id="cb12-16"><a href="#cb12-16" tabindex="-1"></a>        sequence_length<span class="op">=</span>MAX_SEQUENCE_LENGTH <span class="op">+</span> <span class="dv">1</span>,</span>
<span id="cb12-17"><a href="#cb12-17" tabindex="-1"></a>        start_value<span class="op">=</span>spa_tokenizer.token_to_id(<span class="st">"[START]"</span>),</span>
<span id="cb12-18"><a href="#cb12-18" tabindex="-1"></a>        end_value<span class="op">=</span>spa_tokenizer.token_to_id(<span class="st">"[END]"</span>),</span>
<span id="cb12-19"><a href="#cb12-19" tabindex="-1"></a>        pad_value<span class="op">=</span>spa_tokenizer.token_to_id(<span class="st">"[PAD]"</span>),</span>
<span id="cb12-20"><a href="#cb12-20" tabindex="-1"></a>    )</span>
<span id="cb12-21"><a href="#cb12-21" tabindex="-1"></a>    spa <span class="op">=</span> spa_start_end_packer(spa)</span>
<span id="cb12-22"><a href="#cb12-22" tabindex="-1"></a></span>
<span id="cb12-23"><a href="#cb12-23" tabindex="-1"></a>    <span class="cf">return</span> (</span>
<span id="cb12-24"><a href="#cb12-24" tabindex="-1"></a>        {</span>
<span id="cb12-25"><a href="#cb12-25" tabindex="-1"></a>            <span class="st">"encoder_inputs"</span>: eng,</span>
<span id="cb12-26"><a href="#cb12-26" tabindex="-1"></a>            <span class="st">"decoder_inputs"</span>: spa[:, :<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb12-27"><a href="#cb12-27" tabindex="-1"></a>        },</span>
<span id="cb12-28"><a href="#cb12-28" tabindex="-1"></a>        spa[:, <span class="dv">1</span>:],</span>
<span id="cb12-29"><a href="#cb12-29" tabindex="-1"></a>    )</span>
<span id="cb12-30"><a href="#cb12-30" tabindex="-1"></a></span>
<span id="cb12-31"><a href="#cb12-31" tabindex="-1"></a></span>
<span id="cb12-32"><a href="#cb12-32" tabindex="-1"></a><span class="kw">def</span> make_dataset(pairs):</span>
<span id="cb12-33"><a href="#cb12-33" tabindex="-1"></a>    eng_texts, spa_texts <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>pairs)</span>
<span id="cb12-34"><a href="#cb12-34" tabindex="-1"></a>    eng_texts <span class="op">=</span> <span class="bu">list</span>(eng_texts)</span>
<span id="cb12-35"><a href="#cb12-35" tabindex="-1"></a>    spa_texts <span class="op">=</span> <span class="bu">list</span>(spa_texts)</span>
<span id="cb12-36"><a href="#cb12-36" tabindex="-1"></a>    dataset <span class="op">=</span> tf_data.Dataset.from_tensor_slices((eng_texts, spa_texts))</span>
<span id="cb12-37"><a href="#cb12-37" tabindex="-1"></a>    dataset <span class="op">=</span> dataset.batch(BATCH_SIZE)</span>
<span id="cb12-38"><a href="#cb12-38" tabindex="-1"></a>    dataset <span class="op">=</span> dataset.<span class="bu">map</span>(preprocess_batch, num_parallel_calls<span class="op">=</span>tf_data.AUTOTUNE)</span>
<span id="cb12-39"><a href="#cb12-39" tabindex="-1"></a>    <span class="cf">return</span> dataset.shuffle(<span class="dv">2048</span>).prefetch(<span class="dv">16</span>).cache()</span>
<span id="cb12-40"><a href="#cb12-40" tabindex="-1"></a></span>
<span id="cb12-41"><a href="#cb12-41" tabindex="-1"></a></span>
<span id="cb12-42"><a href="#cb12-42" tabindex="-1"></a>train_ds <span class="op">=</span> make_dataset(train_pairs)</span>
<span id="cb12-43"><a href="#cb12-43" tabindex="-1"></a>val_ds <span class="op">=</span> make_dataset(val_pairs)</span></code></pre></div>
<p>Let’s take a quick look at the sequence shapes (we have batches of 64
pairs, and all sequences are 40 steps long):</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="cf">for</span> inputs, targets <span class="kw">in</span> train_ds.take(<span class="dv">1</span>):</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'inputs["encoder_inputs"].shape: </span><span class="sc">{</span>inputs[<span class="st">"encoder_inputs"</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'inputs["decoder_inputs"].shape: </span><span class="sc">{</span>inputs[<span class="st">"decoder_inputs"</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"targets.shape: </span><span class="sc">{</span>targets<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="building-the-model">Building the model<a class="anchor" aria-label="anchor" href="#building-the-model"></a>
</h2>
<p>Now, let’s move on to the exciting part - defining our model! We
first need an embedding layer, i.e., a vector for every token in our
input sequence. This embedding layer can be initialised randomly. We
also need a positional embedding layer which encodes the word order in
the sequence. The convention is to add these two embeddings. KerasNLP
has a <code>keras_nlp.layers.TokenAndPositionEmbedding</code> layer
which does all of the above steps for us.</p>
<p>Our sequence-to-sequence Transformer consists of a
<code>keras_nlp.layers.TransformerEncoder</code> layer and a
<code>keras_nlp.layers.TransformerDecoder</code> layer chained
together.</p>
<p>The source sequence will be passed to
<code>keras_nlp.layers.TransformerEncoder</code>, which will produce a
new representation of it. This new representation will then be passed to
the <code>keras_nlp.layers.TransformerDecoder</code>, together with the
target sequence so far (target words 0 to N). The
<code>keras_nlp.layers.TransformerDecoder</code> will then seek to
predict the next words in the target sequence (N+1 and beyond).</p>
<p>A key detail that makes this possible is causal masking. The
<code>keras_nlp.layers.TransformerDecoder</code> sees the entire
sequence at once, and thus we must make sure that it only uses
information from target tokens 0 to N when predicting token N+1
(otherwise, it could use information from the future, which would result
in a model that cannot be used at inference time). Causal masking is
enabled by default in
<code>keras_nlp.layers.TransformerDecoder</code>.</p>
<p>We also need to mask the padding tokens (<code>"[PAD]"</code>). For
this, we can set the <code>mask_zero</code> argument of the
<code>keras_nlp.layers.TokenAndPositionEmbedding</code> layer to True.
This will then be propagated to all subsequent layers.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="co"># Encoder</span></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>encoder_inputs <span class="op">=</span> keras.Input(</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>    shape<span class="op">=</span>(<span class="va">None</span>,), dtype<span class="op">=</span><span class="st">"int64"</span>, name<span class="op">=</span><span class="st">"encoder_inputs"</span></span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>)</span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a>x <span class="op">=</span> keras_nlp.layers.TokenAndPositionEmbedding(</span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a>    vocabulary_size<span class="op">=</span>ENG_VOCAB_SIZE,</span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a>    sequence_length<span class="op">=</span>MAX_SEQUENCE_LENGTH,</span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a>    embedding_dim<span class="op">=</span>EMBED_DIM,</span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a>)(encoder_inputs)</span>
<span id="cb14-11"><a href="#cb14-11" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" tabindex="-1"></a>encoder_outputs <span class="op">=</span> keras_nlp.layers.TransformerEncoder(</span>
<span id="cb14-13"><a href="#cb14-13" tabindex="-1"></a>    intermediate_dim<span class="op">=</span>INTERMEDIATE_DIM, num_heads<span class="op">=</span>NUM_HEADS</span>
<span id="cb14-14"><a href="#cb14-14" tabindex="-1"></a>)(inputs<span class="op">=</span>x)</span>
<span id="cb14-15"><a href="#cb14-15" tabindex="-1"></a>encoder <span class="op">=</span> keras.Model(encoder_inputs, encoder_outputs)</span>
<span id="cb14-16"><a href="#cb14-16" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" tabindex="-1"></a><span class="co"># Decoder</span></span>
<span id="cb14-19"><a href="#cb14-19" tabindex="-1"></a>decoder_inputs <span class="op">=</span> keras.Input(</span>
<span id="cb14-20"><a href="#cb14-20" tabindex="-1"></a>    shape<span class="op">=</span>(<span class="va">None</span>,), dtype<span class="op">=</span><span class="st">"int64"</span>, name<span class="op">=</span><span class="st">"decoder_inputs"</span></span>
<span id="cb14-21"><a href="#cb14-21" tabindex="-1"></a>)</span>
<span id="cb14-22"><a href="#cb14-22" tabindex="-1"></a>encoded_seq_inputs <span class="op">=</span> keras.Input(</span>
<span id="cb14-23"><a href="#cb14-23" tabindex="-1"></a>    shape<span class="op">=</span>(<span class="va">None</span>, EMBED_DIM), name<span class="op">=</span><span class="st">"decoder_state_inputs"</span></span>
<span id="cb14-24"><a href="#cb14-24" tabindex="-1"></a>)</span>
<span id="cb14-25"><a href="#cb14-25" tabindex="-1"></a></span>
<span id="cb14-26"><a href="#cb14-26" tabindex="-1"></a>x <span class="op">=</span> keras_nlp.layers.TokenAndPositionEmbedding(</span>
<span id="cb14-27"><a href="#cb14-27" tabindex="-1"></a>    vocabulary_size<span class="op">=</span>SPA_VOCAB_SIZE,</span>
<span id="cb14-28"><a href="#cb14-28" tabindex="-1"></a>    sequence_length<span class="op">=</span>MAX_SEQUENCE_LENGTH,</span>
<span id="cb14-29"><a href="#cb14-29" tabindex="-1"></a>    embedding_dim<span class="op">=</span>EMBED_DIM,</span>
<span id="cb14-30"><a href="#cb14-30" tabindex="-1"></a>)(decoder_inputs)</span>
<span id="cb14-31"><a href="#cb14-31" tabindex="-1"></a></span>
<span id="cb14-32"><a href="#cb14-32" tabindex="-1"></a>x <span class="op">=</span> keras_nlp.layers.TransformerDecoder(</span>
<span id="cb14-33"><a href="#cb14-33" tabindex="-1"></a>    intermediate_dim<span class="op">=</span>INTERMEDIATE_DIM, num_heads<span class="op">=</span>NUM_HEADS</span>
<span id="cb14-34"><a href="#cb14-34" tabindex="-1"></a>)(decoder_sequence<span class="op">=</span>x, encoder_sequence<span class="op">=</span>encoded_seq_inputs)</span>
<span id="cb14-35"><a href="#cb14-35" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dropout(<span class="fl">0.5</span>)(x)</span>
<span id="cb14-36"><a href="#cb14-36" tabindex="-1"></a>decoder_outputs <span class="op">=</span> keras.layers.Dense(SPA_VOCAB_SIZE, activation<span class="op">=</span><span class="st">"softmax"</span>)(x)</span>
<span id="cb14-37"><a href="#cb14-37" tabindex="-1"></a>decoder <span class="op">=</span> keras.Model(</span>
<span id="cb14-38"><a href="#cb14-38" tabindex="-1"></a>    [</span>
<span id="cb14-39"><a href="#cb14-39" tabindex="-1"></a>        decoder_inputs,</span>
<span id="cb14-40"><a href="#cb14-40" tabindex="-1"></a>        encoded_seq_inputs,</span>
<span id="cb14-41"><a href="#cb14-41" tabindex="-1"></a>    ],</span>
<span id="cb14-42"><a href="#cb14-42" tabindex="-1"></a>    decoder_outputs,</span>
<span id="cb14-43"><a href="#cb14-43" tabindex="-1"></a>)</span>
<span id="cb14-44"><a href="#cb14-44" tabindex="-1"></a>decoder_outputs <span class="op">=</span> decoder([decoder_inputs, encoder_outputs])</span>
<span id="cb14-45"><a href="#cb14-45" tabindex="-1"></a></span>
<span id="cb14-46"><a href="#cb14-46" tabindex="-1"></a>transformer <span class="op">=</span> keras.Model(</span>
<span id="cb14-47"><a href="#cb14-47" tabindex="-1"></a>    [encoder_inputs, decoder_inputs],</span>
<span id="cb14-48"><a href="#cb14-48" tabindex="-1"></a>    decoder_outputs,</span>
<span id="cb14-49"><a href="#cb14-49" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"transformer"</span>,</span>
<span id="cb14-50"><a href="#cb14-50" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="training-our-model">Training our model<a class="anchor" aria-label="anchor" href="#training-our-model"></a>
</h2>
<p>We’ll use accuracy as a quick way to monitor training progress on the
validation data. Note that machine translation typically uses BLEU
scores as well as other metrics, rather than accuracy. However, in order
to use metrics like ROUGE, BLEU, etc. we will have decode the
probabilities and generate the text. Text generation is computationally
expensive, and performing this during training is not recommended.</p>
<p>Here we only train for 1 epoch, but to get the model to actually
converge you should train for at least 10 epochs.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>transformer.summary()</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>transformer.<span class="bu">compile</span>(</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>    <span class="st">"rmsprop"</span>, loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>, metrics<span class="op">=</span>[<span class="st">"accuracy"</span>]</span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>)</span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>transformer.fit(train_ds, epochs<span class="op">=</span>EPOCHS, validation_data<span class="op">=</span>val_ds)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="decoding-test-sentences-qualitative-analysis">Decoding test sentences (qualitative analysis)<a class="anchor" aria-label="anchor" href="#decoding-test-sentences-qualitative-analysis"></a>
</h2>
<p>Finally, let’s demonstrate how to translate brand new English
sentences. We simply feed into the model the tokenized English sentence
as well as the target token <code>"[START]"</code>. The model outputs
probabilities of the next token. We then we repeatedly generated the
next token conditioned on the tokens generated so far, until we hit the
token <code>"[END]"</code>.</p>
<p>For decoding, we will use the <code>keras_nlp.samplers</code> module
from KerasNLP. Greedy Decoding is a text decoding method which outputs
the most likely next token at each time step, i.e., the token with the
highest probability.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="kw">def</span> decode_sequences(input_sentences):</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>    batch_size <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>    <span class="co"># Tokenize the encoder input.</span></span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>    encoder_input_tokens <span class="op">=</span> ops.convert_to_tensor(eng_tokenizer(input_sentences))</span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(encoder_input_tokens[<span class="dv">0</span>]) <span class="op">&lt;</span> MAX_SEQUENCE_LENGTH:</span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a>        pads <span class="op">=</span> ops.full(</span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a>            (<span class="dv">1</span>, MAX_SEQUENCE_LENGTH <span class="op">-</span> <span class="bu">len</span>(encoder_input_tokens[<span class="dv">0</span>])), <span class="dv">0</span></span>
<span id="cb16-9"><a href="#cb16-9" tabindex="-1"></a>        )</span>
<span id="cb16-10"><a href="#cb16-10" tabindex="-1"></a>        encoder_input_tokens <span class="op">=</span> ops.concatenate([encoder_input_tokens, pads], <span class="dv">1</span>)</span>
<span id="cb16-11"><a href="#cb16-11" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" tabindex="-1"></a>    <span class="co"># Define a function that outputs the next token's probability given the</span></span>
<span id="cb16-13"><a href="#cb16-13" tabindex="-1"></a>    <span class="co"># input sequence.</span></span>
<span id="cb16-14"><a href="#cb16-14" tabindex="-1"></a>    <span class="kw">def</span> <span class="bu">next</span>(prompt, cache, index):</span>
<span id="cb16-15"><a href="#cb16-15" tabindex="-1"></a>        logits <span class="op">=</span> transformer([encoder_input_tokens, prompt])[:, index <span class="op">-</span> <span class="dv">1</span>, :]</span>
<span id="cb16-16"><a href="#cb16-16" tabindex="-1"></a>        <span class="co"># Ignore hidden states for now; only needed for contrastive search.</span></span>
<span id="cb16-17"><a href="#cb16-17" tabindex="-1"></a>        hidden_states <span class="op">=</span> <span class="va">None</span></span>
<span id="cb16-18"><a href="#cb16-18" tabindex="-1"></a>        <span class="cf">return</span> logits, hidden_states, cache</span>
<span id="cb16-19"><a href="#cb16-19" tabindex="-1"></a></span>
<span id="cb16-20"><a href="#cb16-20" tabindex="-1"></a>    <span class="co"># Build a prompt of length 40 with a start token and padding tokens.</span></span>
<span id="cb16-21"><a href="#cb16-21" tabindex="-1"></a>    length <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb16-22"><a href="#cb16-22" tabindex="-1"></a>    start <span class="op">=</span> ops.full((batch_size, <span class="dv">1</span>), spa_tokenizer.token_to_id(<span class="st">"[START]"</span>))</span>
<span id="cb16-23"><a href="#cb16-23" tabindex="-1"></a>    pad <span class="op">=</span> ops.full((batch_size, length <span class="op">-</span> <span class="dv">1</span>), spa_tokenizer.token_to_id(<span class="st">"[PAD]"</span>))</span>
<span id="cb16-24"><a href="#cb16-24" tabindex="-1"></a>    prompt <span class="op">=</span> ops.concatenate((start, pad), axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb16-25"><a href="#cb16-25" tabindex="-1"></a></span>
<span id="cb16-26"><a href="#cb16-26" tabindex="-1"></a>    generated_tokens <span class="op">=</span> keras_nlp.samplers.GreedySampler()(</span>
<span id="cb16-27"><a href="#cb16-27" tabindex="-1"></a>        <span class="bu">next</span>,</span>
<span id="cb16-28"><a href="#cb16-28" tabindex="-1"></a>        prompt,</span>
<span id="cb16-29"><a href="#cb16-29" tabindex="-1"></a>        end_token_id<span class="op">=</span>spa_tokenizer.token_to_id(<span class="st">"[END]"</span>),</span>
<span id="cb16-30"><a href="#cb16-30" tabindex="-1"></a>        index<span class="op">=</span><span class="dv">1</span>,  <span class="co"># Start sampling after start token.</span></span>
<span id="cb16-31"><a href="#cb16-31" tabindex="-1"></a>    )</span>
<span id="cb16-32"><a href="#cb16-32" tabindex="-1"></a>    generated_sentences <span class="op">=</span> spa_tokenizer.detokenize(generated_tokens)</span>
<span id="cb16-33"><a href="#cb16-33" tabindex="-1"></a>    <span class="cf">return</span> generated_sentences</span>
<span id="cb16-34"><a href="#cb16-34" tabindex="-1"></a></span>
<span id="cb16-35"><a href="#cb16-35" tabindex="-1"></a></span>
<span id="cb16-36"><a href="#cb16-36" tabindex="-1"></a>test_eng_texts <span class="op">=</span> [pair[<span class="dv">0</span>] <span class="cf">for</span> pair <span class="kw">in</span> test_pairs]</span>
<span id="cb16-37"><a href="#cb16-37" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb16-38"><a href="#cb16-38" tabindex="-1"></a>    input_sentence <span class="op">=</span> random.choice(test_eng_texts)</span>
<span id="cb16-39"><a href="#cb16-39" tabindex="-1"></a>    translated <span class="op">=</span> decode_sequences([input_sentence])</span>
<span id="cb16-40"><a href="#cb16-40" tabindex="-1"></a>    translated <span class="op">=</span> translated.numpy()[<span class="dv">0</span>].decode(<span class="st">"utf-8"</span>)</span>
<span id="cb16-41"><a href="#cb16-41" tabindex="-1"></a>    translated <span class="op">=</span> (</span>
<span id="cb16-42"><a href="#cb16-42" tabindex="-1"></a>        translated.replace(<span class="st">"[PAD]"</span>, <span class="st">""</span>)</span>
<span id="cb16-43"><a href="#cb16-43" tabindex="-1"></a>        .replace(<span class="st">"[START]"</span>, <span class="st">""</span>)</span>
<span id="cb16-44"><a href="#cb16-44" tabindex="-1"></a>        .replace(<span class="st">"[END]"</span>, <span class="st">""</span>)</span>
<span id="cb16-45"><a href="#cb16-45" tabindex="-1"></a>        .strip()</span>
<span id="cb16-46"><a href="#cb16-46" tabindex="-1"></a>    )</span>
<span id="cb16-47"><a href="#cb16-47" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"** Example </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> **"</span>)</span>
<span id="cb16-48"><a href="#cb16-48" tabindex="-1"></a>    <span class="bu">print</span>(input_sentence)</span>
<span id="cb16-49"><a href="#cb16-49" tabindex="-1"></a>    <span class="bu">print</span>(translated)</span>
<span id="cb16-50"><a href="#cb16-50" tabindex="-1"></a>    <span class="bu">print</span>()</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="evaluating-our-model-quantitative-analysis">Evaluating our model (quantitative analysis)<a class="anchor" aria-label="anchor" href="#evaluating-our-model-quantitative-analysis"></a>
</h2>
<p>There are many metrics which are used for text generation tasks.
Here, to evaluate translations generated by our model, let’s compute the
ROUGE-1 and ROUGE-2 scores. Essentially, ROUGE-N is a score based on the
number of common n-grams between the reference text and the generated
text. ROUGE-1 and ROUGE-2 use the number of common unigrams and bigrams,
respectively.</p>
<p>We will calculate the score over 30 test samples (since decoding is
an expensive process).</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>rouge_1 <span class="op">=</span> keras_nlp.metrics.RougeN(order<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>rouge_2 <span class="op">=</span> keras_nlp.metrics.RougeN(order<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a><span class="cf">for</span> test_pair <span class="kw">in</span> test_pairs[:<span class="dv">30</span>]:</span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a>    input_sentence <span class="op">=</span> test_pair[<span class="dv">0</span>]</span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a>    reference_sentence <span class="op">=</span> test_pair[<span class="dv">1</span>]</span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a>    translated_sentence <span class="op">=</span> decode_sequences([input_sentence])</span>
<span id="cb17-9"><a href="#cb17-9" tabindex="-1"></a>    translated_sentence <span class="op">=</span> translated_sentence.numpy()[<span class="dv">0</span>].decode(<span class="st">"utf-8"</span>)</span>
<span id="cb17-10"><a href="#cb17-10" tabindex="-1"></a>    translated_sentence <span class="op">=</span> (</span>
<span id="cb17-11"><a href="#cb17-11" tabindex="-1"></a>        translated_sentence.replace(<span class="st">"[PAD]"</span>, <span class="st">""</span>)</span>
<span id="cb17-12"><a href="#cb17-12" tabindex="-1"></a>        .replace(<span class="st">"[START]"</span>, <span class="st">""</span>)</span>
<span id="cb17-13"><a href="#cb17-13" tabindex="-1"></a>        .replace(<span class="st">"[END]"</span>, <span class="st">""</span>)</span>
<span id="cb17-14"><a href="#cb17-14" tabindex="-1"></a>        .strip()</span>
<span id="cb17-15"><a href="#cb17-15" tabindex="-1"></a>    )</span>
<span id="cb17-16"><a href="#cb17-16" tabindex="-1"></a></span>
<span id="cb17-17"><a href="#cb17-17" tabindex="-1"></a>    rouge_1(reference_sentence, translated_sentence)</span>
<span id="cb17-18"><a href="#cb17-18" tabindex="-1"></a>    rouge_2(reference_sentence, translated_sentence)</span>
<span id="cb17-19"><a href="#cb17-19" tabindex="-1"></a></span>
<span id="cb17-20"><a href="#cb17-20" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"ROUGE-1 Score: "</span>, rouge_1.result())</span>
<span id="cb17-21"><a href="#cb17-21" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"ROUGE-2 Score: "</span>, rouge_2.result())</span></code></pre></div>
<p>After 10 epochs, the scores are as follows:</p>
<table class="table">
<thead><tr class="header">
<th align="center"></th>
<th align="center"><strong>ROUGE-1</strong></th>
<th align="center"><strong>ROUGE-2</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="center"><strong>Precision</strong></td>
<td align="center">0.568</td>
<td align="center">0.374</td>
</tr>
<tr class="even">
<td align="center"><strong>Recall</strong></td>
<td align="center">0.615</td>
<td align="center">0.394</td>
</tr>
<tr class="odd">
<td align="center"><strong>F1 Score</strong></td>
<td align="center">0.579</td>
<td align="center">0.381</td>
</tr>
</tbody>
</table>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
