<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="How to serve TensorFlow models with TensorFlow Serving.">
<title>Serving TensorFlow models with TFServing • keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><meta property="og:title" content="Serving TensorFlow models with TFServing">
<meta property="og:description" content="How to serve TensorFlow models with TensorFlow Serving.">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-getting-started">Getting Started</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-getting-started">
    <a class="dropdown-item" href="../../articles/intro_to_keras_for_engineers.html">Introduction to Keras for engineers</a>
    <h6 class="dropdown-header" data-toc-skip>Tutorials</h6>
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
  </div>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Serving TensorFlow models with TFServing</h1>
                        <h4 data-toc-skip class="author"><a href="https://www.linkedin.com/in/dimitre-oliveira-7a1a0113a/" class="external-link">Dimitre
Oliveira</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/tf_serving.Rmd" class="external-link"><code>vignettes/examples/tf_serving.Rmd</code></a></small>
      <div class="d-none name"><code>tf_serving.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>Once you build a machine learning model, the next step is to serve
it. You may want to do that by exposing your model as an endpoint
service. There are many frameworks that you can use to do that, but the
TensorFlow ecosystem has its own solution called <a href="https://www.tensorflow.org/tfx/guide/serving" class="external-link">TensorFlow
Serving</a>.</p>
<p>From the TensorFlow Serving <a href="https://github.com/tensorflow/serving" class="external-link">GitHub page</a>:</p>
<blockquote>
<p>TensorFlow Serving is a flexible, high-performance serving system for
machine learning models, designed for production environments. It deals
with the inference aspect of machine learning, taking models after
training and managing their lifetimes, providing clients with versioned
access via a high-performance, reference-counted lookup table.
TensorFlow Serving provides out-of-the-box integration with TensorFlow
models, but can be easily extended to serve other types of models and
data.”</p>
</blockquote>
<p>To note a few features:</p>
<ul>
<li>It can serve multiple models, or multiple versions of the same model
simultaneously</li>
<li>It exposes both gRPC as well as HTTP inference endpoints</li>
<li>It allows deployment of new model versions without changing any
client code</li>
<li>It supports canarying new versions and A/B testing experimental
models</li>
<li>It adds minimal latency to inference time due to efficient,
low-overhead implementation</li>
<li>It features a scheduler that groups individual inference requests
into batches for joint execution on GPU, with configurable latency
controls</li>
<li>It supports many servables: Tensorflow models, embeddings,
vocabularies, feature transformations and even non-Tensorflow-based
machine learning models</li>
</ul>
<p>This guide creates a simple <a href="https://arxiv.org/abs/1704.04861" class="external-link">MobileNet</a> model using the <a href="https://keras.io/api/applications/" class="external-link">Keras applications API</a>,
and then serves it with <a href="https://www.tensorflow.org/tfx/guide/serving" class="external-link">TensorFlow
Serving</a>. The focus is on TensorFlow Serving, rather than the
modeling and training in TensorFlow.</p>
<blockquote>
<p>Note: you can find a Colab notebook with the full working code at <a href="https://colab.research.google.com/drive/1nwuIJa4so1XzYU0ngq8tX_-SGTO295Mu?usp=sharing" class="external-link">this
link</a>.</p>
</blockquote>
</div>
<div class="section level2">
<h2 id="dependencies">Dependencies<a class="anchor" aria-label="anchor" href="#dependencies"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">import</span> shutil</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="model">Model<a class="anchor" aria-label="anchor" href="#model"></a>
</h2>
<p>Here we load a pre-trained <a href="https://arxiv.org/abs/1704.04861" class="external-link">MobileNet</a> from the <a href="https://keras.io/api/applications/" class="external-link">Keras applications</a>, this
is the model that we are going to serve.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>model <span class="op">=</span> keras.applications.MobileNet()</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="preprocessing">Preprocessing<a class="anchor" aria-label="anchor" href="#preprocessing"></a>
</h2>
<p>Most models don’t work out of the box on raw data, they usually
require some kind of preprocessing step to adjust the data to the model
requirements, in the case of this MobileNet we can see from its <a href="https://keras.io/api/applications/mobilenet/" class="external-link">API page</a> that it
requires three basic steps for its input images:</p>
<ul>
<li>Pixel values normalized to the <code>[0, 1]</code> range</li>
<li>Pixel values scaled to the <code>[-1, 1]</code> range</li>
<li>Images with the shape of <code>(224, 224, 3)</code> meaning
<code>(height, width, channels)</code>
</li>
</ul>
<p>We can do all of that with the following function:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="kw">def</span> preprocess(image, mean<span class="op">=</span><span class="fl">0.5</span>, std<span class="op">=</span><span class="fl">0.5</span>, shape<span class="op">=</span>(<span class="dv">224</span>, <span class="dv">224</span>)):</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>    <span class="co">"""Scale, normalize and resizes images."""</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>    image <span class="op">=</span> image <span class="op">/</span> <span class="fl">255.0</span>  <span class="co"># Scale</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>    image <span class="op">=</span> (image <span class="op">-</span> mean) <span class="op">/</span> std  <span class="co"># Normalize</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>    image <span class="op">=</span> tf.image.resize(image, shape)  <span class="co"># Resize</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>    <span class="cf">return</span> image</span></code></pre></div>
<p><strong>A note regarding preprocessing and postprocessing using the
“keras.applications” API</strong></p>
<p>All models that are available at the <a href="https://keras.io/api/applications/" class="external-link">Keras applications</a> API
also provide <code>preprocess_input</code> and
<code>decode_predictions</code> functions, those functions are
respectively responsible for the preprocessing and postprocessing of
each model, and already contains all the logic necessary for those
steps. That is the recommended way to process inputs and outputs when
using Keras applications models. For this guide, we are not using them
to present the advantages of custom signatures in a clearer way.</p>
</div>
<div class="section level2">
<h2 id="postprocessing">Postprocessing<a class="anchor" aria-label="anchor" href="#postprocessing"></a>
</h2>
<p>In the same context most models output values that need extra
processing to meet the user requirements, for instance, the user does
not want to know the logits values for each class given an image, what
the user wants is to know from which class it belongs. For our model,
this translates to the following transformations on top of the model
outputs:</p>
<ul>
<li>Get the index of the class with the highest prediction</li>
<li>Get the name of the class from that index</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="co"># Download human-readable labels for ImageNet.</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>imagenet_labels_url <span class="op">=</span> <span class="st">"https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt"</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>response <span class="op">=</span> requests.get(imagenet_labels_url)</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a><span class="co"># Skiping backgroung class</span></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>labels <span class="op">=</span> [x <span class="cf">for</span> x <span class="kw">in</span> response.text.split(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>) <span class="cf">if</span> x <span class="op">!=</span> <span class="st">""</span>][<span class="dv">1</span>:]</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a><span class="co"># Convert the labels to the TensorFlow data format</span></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>tf_labels <span class="op">=</span> tf.constant(labels, dtype<span class="op">=</span>tf.string)</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a><span class="kw">def</span> postprocess(prediction, labels<span class="op">=</span>tf_labels):</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>    <span class="co">"""Convert from probs to labels."""</span></span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>    indices <span class="op">=</span> tf.argmax(prediction, axis<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># Index with highest prediction</span></span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a>    label <span class="op">=</span> tf.gather(params<span class="op">=</span>labels, indices<span class="op">=</span>indices)  <span class="co"># Class name</span></span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>    <span class="cf">return</span> label</span></code></pre></div>
<p>Now let’s download a banana picture and see how everything comes
together.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>response <span class="op">=</span> requests.get(<span class="st">"https://i.imgur.com/j9xCCzn.jpeg"</span>, stream<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"banana.jpeg"</span>, <span class="st">"wb"</span>) <span class="im">as</span> f:</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>    shutil.copyfileobj(response.raw, f)</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>sample_img <span class="op">=</span> plt.imread(<span class="st">"./banana.jpeg"</span>)</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Original image shape: </span><span class="sc">{</span>sample_img<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Original image pixel range: (</span><span class="sc">{</span>sample_img<span class="sc">.</span><span class="bu">min</span>()<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>sample_img<span class="sc">.</span><span class="bu">max</span>()<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>plt.imshow(sample_img)</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>plt.show()</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>preprocess_img <span class="op">=</span> preprocess(sample_img)</span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Preprocessed image shape: </span><span class="sc">{</span>preprocess_img<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a>    <span class="ss">f"Preprocessed image pixel range: (</span><span class="sc">{</span>preprocess_img<span class="sc">.</span>numpy()<span class="sc">.</span><span class="bu">min</span>()<span class="sc">}</span><span class="ss">,"</span>,</span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>    <span class="ss">f"</span><span class="sc">{</span>preprocess_img<span class="sc">.</span>numpy()<span class="sc">.</span><span class="bu">max</span>()<span class="sc">}</span><span class="ss">)"</span>,</span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>)</span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a>batched_img <span class="op">=</span> tf.expand_dims(preprocess_img, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a>batched_img <span class="op">=</span> tf.cast(batched_img, tf.float32)</span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Batched image shape: </span><span class="sc">{</span>batched_img<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" tabindex="-1"></a>model_outputs <span class="op">=</span> model(batched_img)</span>
<span id="cb5-24"><a href="#cb5-24" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model output shape: </span><span class="sc">{</span>model_outputs<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-25"><a href="#cb5-25" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Predicted class: </span><span class="sc">{</span>postprocess(model_outputs)<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="save-the-model">Save the model<a class="anchor" aria-label="anchor" href="#save-the-model"></a>
</h2>
<p>To load our trained model into TensorFlow Serving, we first need to
save it in <a href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model" class="external-link">SavedModel</a>
format. This will create a protobuf file in a well-defined directory
hierarchy, and will include a version number. <a href="https://www.tensorflow.org/tfx/guide/serving" class="external-link">TensorFlow
Serving</a> allows us to select which version of a model, or “servable”
we want to use when we make inference requests. Each version will be
exported to a different sub-directory under the given path.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>model_dir <span class="op">=</span> <span class="st">"./model"</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>model_version <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>model_export_path <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>model_dir<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>model_version<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>tf.saved_model.save(</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>    model,</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>    export_dir<span class="op">=</span>model_export_path,</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>)</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"SavedModel files: </span><span class="sc">{</span>os<span class="sc">.</span>listdir(model_export_path)<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="examine-your-saved-model">Examine your saved model<a class="anchor" aria-label="anchor" href="#examine-your-saved-model"></a>
</h2>
<p>We’ll use the command line utility <code>saved_model_cli</code> to
look at the <a href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/MetaGraphDef" class="external-link">MetaGraphDefs</a>
(the models) and <a href="https://www.tensorflow.org/tfx/serving/signature_defs" class="external-link">SignatureDefs</a>
(the methods you can call) in our SavedModel. See <a href="https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/saved_model.md#cli-to-inspect-and-execute-savedmodel" class="external-link">this
discussion of the SavedModel CLI</a> in the TensorFlow Guide.</p>
<p>saved_model_cli show –dir {model_export_path} –tag_set serve
–signature_def serving_default</p>
<p>That tells us a lot about our model! For instance, we can see that
its inputs have a 4D shape <code>(-1, 224, 224, 3)</code> which means
<code>(batch_size, height, width, channels)</code>, also note that this
model requires a specific image shape <code>(224, 224, 3)</code> this
means that we may need to reshape our images before sending them to the
model. We can also see that the model’s outputs have a
<code>(-1, 1000)</code> shape which are the logits for the 1000 classes
of the <a href="https://www.image-net.org" class="external-link">ImageNet</a> dataset.</p>
<p>This information doesn’t tell us everything, like the fact that the
pixel values needs to be in the <code>[-1, 1]</code> range, but it’s a
great start.</p>
</div>
<div class="section level2">
<h2 id="serve-your-model-with-tensorflow-serving">Serve your model with TensorFlow Serving<a class="anchor" aria-label="anchor" href="#serve-your-model-with-tensorflow-serving"></a>
</h2>
<div class="section level3">
<h3 id="install-tfserving">Install TFServing<a class="anchor" aria-label="anchor" href="#install-tfserving"></a>
</h3>
<p>We’re preparing to install TensorFlow Serving using <a href="https://wiki.debian.org/Aptitude" class="external-link">Aptitude</a> since this Colab
runs in a Debian environment. We’ll add the
<code>tensorflow-model-server</code> package to the list of packages
that Aptitude knows about. Note that we’re running as root.</p>
<blockquote>
<p>Note: This example is running TensorFlow Serving natively, but <a href="https://www.tensorflow.org/tfx/serving/docker" class="external-link">you can also run it
in a Docker container</a>, which is one of the easiest ways to get
started using TensorFlow Serving.</p>
</blockquote>
<pre class="shell"><code>wget 'http://storage.googleapis.com/tensorflow-serving-apt/pool/tensorflow-model-server-universal-2.8.0/t/tensorflow-model-server-universal/tensorflow-model-server-universal_2.8.0_all.deb'
dpkg -i tensorflow-model-server-universal_2.8.0_all.deb</code></pre>
</div>
<div class="section level3">
<h3 id="start-running-tensorflow-serving">Start running TensorFlow Serving<a class="anchor" aria-label="anchor" href="#start-running-tensorflow-serving"></a>
</h3>
<p>This is where we start running TensorFlow Serving and load our model.
After it loads, we can start making inference requests using REST. There
are some important parameters:</p>
<ul>
<li>
<code>port</code>: The port that you’ll use for gRPC requests.</li>
<li>
<code>rest_api_port</code>: The port that you’ll use for REST
requests.</li>
<li>
<code>model_name</code>: You’ll use this in the URL of REST
requests. It can be anything.</li>
<li>
<code>model_base_path</code>: This is the path to the directory
where you’ve saved your model.</li>
</ul>
<p>Check the <a href="https://github.com/tensorflow/serving/blob/master/tensorflow_serving/model_servers/main.cc" class="external-link">TFServing
API reference</a> to get all the parameters available.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="co"># Environment variable with the path to the model</span></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>os.environ[<span class="st">"MODEL_DIR"</span>] <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>model_dir<span class="sc">}</span><span class="ss">"</span></span></code></pre></div>
<pre class="shell"><code>%%bash --bg
nohup tensorflow_model_server \
  --port=8500 \
  --rest_api_port=8501 \
  --model_name=model \
  --model_base_path=$MODEL_DIR &gt;server.log 2&gt;&amp;1</code></pre>
<pre class="shell"><code># We can check the logs to the server to help troubleshooting
!cat server.log</code></pre>
<p>outputs:</p>
<pre><code>[warn] getaddrinfo: address family for nodename not supported
[evhttp_server.cc : 245] NET_LOG: Entering the event loop ...</code></pre>
<pre class="shell"><code># Now we can check if tensorflow is in the active services
!sudo lsof -i -P -n | grep LISTEN</code></pre>
<p>outputs:</p>
<pre><code>node         7 root   21u  IPv6  19100      0t0  TCP *:8080 (LISTEN)
kernel_ma   34 root    7u  IPv4  18874      0t0  TCP 172.28.0.12:6000 (LISTEN)
colab-fil   63 root    5u  IPv4  17975      0t0  TCP *:3453 (LISTEN)
colab-fil   63 root    6u  IPv6  17976      0t0  TCP *:3453 (LISTEN)
jupyter-n   81 root    6u  IPv4  18092      0t0  TCP 172.28.0.12:9000 (LISTEN)
python3    101 root   23u  IPv4  18252      0t0  TCP 127.0.0.1:44915 (LISTEN)
python3    132 root    3u  IPv4  20548      0t0  TCP 127.0.0.1:15264 (LISTEN)
python3    132 root    4u  IPv4  20549      0t0  TCP 127.0.0.1:37977 (LISTEN)
python3    132 root    9u  IPv4  20662      0t0  TCP 127.0.0.1:40689 (LISTEN)
tensorflo 1101 root    5u  IPv4  35543      0t0  TCP *:8500 (LISTEN)
tensorflo 1101 root   12u  IPv4  35548      0t0  TCP *:8501 (LISTEN)</code></pre>
</div>
</div>
<div class="section level2">
<h2 id="make-a-request-to-your-model-in-tensorflow-serving">Make a request to your model in TensorFlow Serving<a class="anchor" aria-label="anchor" href="#make-a-request-to-your-model-in-tensorflow-serving"></a>
</h2>
<p>Now let’s create the JSON object for an inference request, and see
how well our model classifies it:</p>
<div class="section level3">
<h3 id="rest-api">REST API<a class="anchor" aria-label="anchor" href="#rest-api"></a>
</h3>
<div class="section level4">
<h4 id="newest-version-of-the-servable">Newest version of the servable<a class="anchor" aria-label="anchor" href="#newest-version-of-the-servable"></a>
</h4>
<p>We’ll send a predict request as a POST to our server’s REST endpoint,
and pass it as an example. We’ll ask our server to give us the latest
version of our servable by not specifying a particular version.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>data <span class="op">=</span> json.dumps(</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>    {</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>        <span class="st">"signature_name"</span>: <span class="st">"serving_default"</span>,</span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>        <span class="st">"instances"</span>: batched_img.numpy().tolist(),</span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>    }</span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a>)</span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"http://localhost:8501/v1/models/model:predict"</span></span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a><span class="kw">def</span> predict_rest(json_data, url):</span>
<span id="cb14-11"><a href="#cb14-11" tabindex="-1"></a>    json_response <span class="op">=</span> requests.post(url, data<span class="op">=</span>json_data)</span>
<span id="cb14-12"><a href="#cb14-12" tabindex="-1"></a>    response <span class="op">=</span> json.loads(json_response.text)</span>
<span id="cb14-13"><a href="#cb14-13" tabindex="-1"></a>    rest_outputs <span class="op">=</span> np.array(response[<span class="st">"predictions"</span>])</span>
<span id="cb14-14"><a href="#cb14-14" tabindex="-1"></a>    <span class="cf">return</span> rest_outputs</span></code></pre></div>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>rest_outputs <span class="op">=</span> predict_rest(data, url)</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"REST output shape: </span><span class="sc">{</span>rest_outputs<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Predicted class: </span><span class="sc">{</span>postprocess(rest_outputs)<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
<p>outputs:</p>
<pre><code>REST output shape: (1, 1000)
Predicted class: [b'banana']</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="grpc-api">gRPC API<a class="anchor" aria-label="anchor" href="#grpc-api"></a>
</h3>
<p><a href="https://grpc.io/" class="external-link">gRPC</a> is based on the Remote Procedure
Call (RPC) model and is a technology for implementing RPC APIs that uses
HTTP 2.0 as its underlying transport protocol. gRPC is usually preferred
for low-latency, highly scalable, and distributed systems. If you wanna
know more about the REST vs gRPC tradeoffs, checkout <a href="https://cloud.google.com/blog/products/api-management/understanding-grpc-openapi-and-rest-and-when-to-use-them" class="external-link">this
article</a>.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="im">import</span> grpc</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a><span class="co"># Create a channel that will be connected to the gRPC port of the container</span></span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>channel <span class="op">=</span> grpc.insecure_channel(<span class="st">"localhost:8500"</span>)</span></code></pre></div>
<pre class="shell"><code>pip install -q tensorflow_serving_api</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a><span class="im">from</span> tensorflow_serving.apis <span class="im">import</span> predict_pb2, prediction_service_pb2_grpc</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a><span class="co"># Create a stub made for prediction</span></span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a><span class="co"># This stub will be used to send the gRPCrequest to the TF Server</span></span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a>stub <span class="op">=</span> prediction_service_pb2_grpc.PredictionServiceStub(channel)</span></code></pre></div>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="co"># Get the serving_input key</span></span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a>loaded_model <span class="op">=</span> tf.saved_model.load(model_export_path)</span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a>input_name <span class="op">=</span> <span class="bu">list</span>(</span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a>    loaded_model.signatures[<span class="st">"serving_default"</span>]</span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a>    .structured_input_signature[<span class="dv">1</span>]</span>
<span id="cb20-6"><a href="#cb20-6" tabindex="-1"></a>    .keys()</span>
<span id="cb20-7"><a href="#cb20-7" tabindex="-1"></a>)[<span class="dv">0</span>]</span></code></pre></div>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a><span class="kw">def</span> predict_grpc(data, input_name, stub):</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a>    <span class="co"># Create a gRPC request made for prediction</span></span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a>    request <span class="op">=</span> predict_pb2.PredictRequest()</span>
<span id="cb21-4"><a href="#cb21-4" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" tabindex="-1"></a>    <span class="co"># Set the name of the model, for this use case it is "model"</span></span>
<span id="cb21-6"><a href="#cb21-6" tabindex="-1"></a>    request.model_spec.name <span class="op">=</span> <span class="st">"model"</span></span>
<span id="cb21-7"><a href="#cb21-7" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" tabindex="-1"></a>    <span class="co"># Set which signature is used to format the gRPC query</span></span>
<span id="cb21-9"><a href="#cb21-9" tabindex="-1"></a>    <span class="co"># here the default one "serving_default"</span></span>
<span id="cb21-10"><a href="#cb21-10" tabindex="-1"></a>    request.model_spec.signature_name <span class="op">=</span> <span class="st">"serving_default"</span></span>
<span id="cb21-11"><a href="#cb21-11" tabindex="-1"></a></span>
<span id="cb21-12"><a href="#cb21-12" tabindex="-1"></a>    <span class="co"># Set the input as the data</span></span>
<span id="cb21-13"><a href="#cb21-13" tabindex="-1"></a>    <span class="co"># tf.make_tensor_proto turns a TensorFlow tensor into a Protobuf tensor</span></span>
<span id="cb21-14"><a href="#cb21-14" tabindex="-1"></a>    request.inputs[input_name].CopyFrom(tf.make_tensor_proto(data.numpy().tolist()))</span>
<span id="cb21-15"><a href="#cb21-15" tabindex="-1"></a></span>
<span id="cb21-16"><a href="#cb21-16" tabindex="-1"></a>    <span class="co"># Send the gRPC request to the TF Server</span></span>
<span id="cb21-17"><a href="#cb21-17" tabindex="-1"></a>    result <span class="op">=</span> stub.Predict(request)</span>
<span id="cb21-18"><a href="#cb21-18" tabindex="-1"></a>    <span class="cf">return</span> result</span>
<span id="cb21-19"><a href="#cb21-19" tabindex="-1"></a></span>
<span id="cb21-20"><a href="#cb21-20" tabindex="-1"></a>grpc_outputs <span class="op">=</span> predict_grpc(batched_img, input_name, stub)</span>
<span id="cb21-21"><a href="#cb21-21" tabindex="-1"></a>grpc_outputs <span class="op">=</span> np.array([grpc_outputs.outputs[<span class="st">'predictions'</span>].float_val])</span>
<span id="cb21-22"><a href="#cb21-22" tabindex="-1"></a></span>
<span id="cb21-23"><a href="#cb21-23" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"gRPC output shape: </span><span class="sc">{</span>grpc_outputs<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-24"><a href="#cb21-24" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Predicted class: </span><span class="sc">{</span>postprocess(grpc_outputs)<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
<p>outputs:</p>
<pre><code>gRPC output shape: (1, 1000)
Predicted class: [b'banana']</code></pre>
</div>
</div>
<div class="section level2">
<h2 id="custom-signature">Custom signature<a class="anchor" aria-label="anchor" href="#custom-signature"></a>
</h2>
<p>Note that for this model we always need to preprocess and postprocess
all samples to get the desired output, this can get quite tricky if are
maintaining and serving several models developed by a large team, and
each one of them might require different processing logic.</p>
<p>TensorFlow allows us to customize the model graph to embed all of
that processing logic, which makes model serving much easier, there are
different ways to achieve this, but since we are going to server the
models using TFServing we can customize the model graph straight into
the serving signature.</p>
<p>We can just use the following code to export the same model that
already contains the preprocessing and postprocessing logic as the
default signature, this allows this model to make predictions on raw
data.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a><span class="kw">def</span> export_model(model, labels):</span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a>    <span class="at">@tf.function</span>(</span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a>        input_signature<span class="op">=</span>[tf.TensorSpec([<span class="va">None</span>, <span class="va">None</span>, <span class="va">None</span>, <span class="dv">3</span>], tf.float32)]</span>
<span id="cb23-4"><a href="#cb23-4" tabindex="-1"></a>    )</span>
<span id="cb23-5"><a href="#cb23-5" tabindex="-1"></a>    <span class="kw">def</span> serving_fn(image):</span>
<span id="cb23-6"><a href="#cb23-6" tabindex="-1"></a>        processed_img <span class="op">=</span> preprocess(image)</span>
<span id="cb23-7"><a href="#cb23-7" tabindex="-1"></a>        probs <span class="op">=</span> model(processed_img)</span>
<span id="cb23-8"><a href="#cb23-8" tabindex="-1"></a>        label <span class="op">=</span> postprocess(probs)</span>
<span id="cb23-9"><a href="#cb23-9" tabindex="-1"></a>        <span class="cf">return</span> {<span class="st">"label"</span>: label}</span>
<span id="cb23-10"><a href="#cb23-10" tabindex="-1"></a></span>
<span id="cb23-11"><a href="#cb23-11" tabindex="-1"></a>    <span class="cf">return</span> serving_fn</span>
<span id="cb23-12"><a href="#cb23-12" tabindex="-1"></a></span>
<span id="cb23-13"><a href="#cb23-13" tabindex="-1"></a></span>
<span id="cb23-14"><a href="#cb23-14" tabindex="-1"></a>model_sig_version <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb23-15"><a href="#cb23-15" tabindex="-1"></a>model_sig_export_path <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>model_dir<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>model_sig_version<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb23-16"><a href="#cb23-16" tabindex="-1"></a></span>
<span id="cb23-17"><a href="#cb23-17" tabindex="-1"></a>tf.saved_model.save(</span>
<span id="cb23-18"><a href="#cb23-18" tabindex="-1"></a>    model,</span>
<span id="cb23-19"><a href="#cb23-19" tabindex="-1"></a>    export_dir<span class="op">=</span>model_sig_export_path,</span>
<span id="cb23-20"><a href="#cb23-20" tabindex="-1"></a>    signatures<span class="op">=</span>{<span class="st">"serving_default"</span>: export_model(model, labels)},</span>
<span id="cb23-21"><a href="#cb23-21" tabindex="-1"></a>)</span></code></pre></div>
<p>saved_model_cli show –dir {model_sig_export_path} –tag_set serve
–signature_def serving_default</p>
<p>Note that this model has a different signature, its input is still 4D
but now with a <code>(-1, -1, -1, 3)</code> shape, which means that it
supports images with any height and width size. Its output also has a
different shape, it no longer outputs the 1000-long logits.</p>
<p>We can test the model’s prediction using a specific signature using
this API below:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a>batched_raw_img <span class="op">=</span> tf.expand_dims(sample_img, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a>batched_raw_img <span class="op">=</span> tf.cast(batched_raw_img, tf.float32)</span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" tabindex="-1"></a>loaded_model <span class="op">=</span> tf.saved_model.load(model_sig_export_path)</span>
<span id="cb24-5"><a href="#cb24-5" tabindex="-1"></a>loaded_model.signatures[<span class="st">"serving_default"</span>](<span class="op">**</span>{<span class="st">"image"</span>: batched_raw_img})</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="prediction-using-a-particular-version-of-the-servable">Prediction using a particular version of the servable<a class="anchor" aria-label="anchor" href="#prediction-using-a-particular-version-of-the-servable"></a>
</h2>
<p>Now let’s specify a particular version of our servable. Note that
when we saved the model with a custom signature we used a different
folder, the first model was saved in folder <code>/1</code> (version 1),
and the one with a custom signature in folder <code>/2</code> (version
2). By default, TFServing will serve all models that share the same base
parent folder.</p>
<div class="section level3">
<h3 id="rest-api-1">REST API<a class="anchor" aria-label="anchor" href="#rest-api-1"></a>
</h3>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a>data <span class="op">=</span> json.dumps(</span>
<span id="cb25-2"><a href="#cb25-2" tabindex="-1"></a>    {</span>
<span id="cb25-3"><a href="#cb25-3" tabindex="-1"></a>        <span class="st">"signature_name"</span>: <span class="st">"serving_default"</span>,</span>
<span id="cb25-4"><a href="#cb25-4" tabindex="-1"></a>        <span class="st">"instances"</span>: batched_raw_img.numpy().tolist(),</span>
<span id="cb25-5"><a href="#cb25-5" tabindex="-1"></a>    }</span>
<span id="cb25-6"><a href="#cb25-6" tabindex="-1"></a>)</span>
<span id="cb25-7"><a href="#cb25-7" tabindex="-1"></a>url_sig <span class="op">=</span> <span class="st">"http://localhost:8501/v1/models/model/versions/2:predict"</span></span></code></pre></div>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"REST output shape: </span><span class="sc">{</span>rest_outputs<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Predicted class: </span><span class="sc">{</span>rest_outputs<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
<p>outputs:</p>
<pre><code>REST output shape: (1,)
Predicted class: ['banana']</code></pre>
</div>
<div class="section level3">
<h3 id="grpc-api-1">gRPC API<a class="anchor" aria-label="anchor" href="#grpc-api-1"></a>
</h3>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a>channel <span class="op">=</span> grpc.insecure_channel(<span class="st">"localhost:8500"</span>)</span>
<span id="cb28-2"><a href="#cb28-2" tabindex="-1"></a>stub <span class="op">=</span> prediction_service_pb2_grpc.PredictionServiceStub(channel)</span></code></pre></div>
<div class="sourceCode" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a>input_name <span class="op">=</span> <span class="bu">list</span>(</span>
<span id="cb29-2"><a href="#cb29-2" tabindex="-1"></a>    loaded_model.signatures[<span class="st">"serving_default"</span>]</span>
<span id="cb29-3"><a href="#cb29-3" tabindex="-1"></a>    .structured_input_signature[<span class="dv">1</span>]</span>
<span id="cb29-4"><a href="#cb29-4" tabindex="-1"></a>    .keys()</span>
<span id="cb29-5"><a href="#cb29-5" tabindex="-1"></a>)[<span class="dv">0</span>]</span></code></pre></div>
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a>grpc_outputs <span class="op">=</span> predict_grpc(batched_raw_img, input_name, stub)</span>
<span id="cb30-2"><a href="#cb30-2" tabindex="-1"></a>grpc_outputs <span class="op">=</span> np.array([grpc_outputs.outputs[<span class="st">'label'</span>].string_val])</span>
<span id="cb30-3"><a href="#cb30-3" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"gRPC output shape: </span><span class="sc">{</span>grpc_outputs<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb30-5"><a href="#cb30-5" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Predicted class: </span><span class="sc">{</span>grpc_outputs<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
<p>outputs:</p>
<pre><code>gRPC output shape: (1, 1)
Predicted class: [[b'banana']]</code></pre>
</div>
</div>
<div class="section level2">
<h2 id="additional-resources">Additional resources<a class="anchor" aria-label="anchor" href="#additional-resources"></a>
</h2>
<ul>
<li><a href="https://colab.research.google.com/drive/1nwuIJa4so1XzYU0ngq8tX_-SGTO295Mu?usp=sharing" class="external-link">Colab
notebook with the full working code</a></li>
<li><a href="https://www.tensorflow.org/tfx/tutorials/serving/rest_simple#make_a_request_to_your_model_in_tensorflow_serving" class="external-link">Train
and serve a TensorFlow model with TensorFlow Serving - TensorFlow
blog</a></li>
<li><a href="https://www.youtube.com/playlist?list=PLQY2H8rRoyvwHdpVQVohY7-qcYf2s1UYK" class="external-link">TensorFlow
Serving playlist - TensorFlow YouTube channel</a></li>
</ul>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
