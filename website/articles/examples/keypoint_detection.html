<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Keypoint Detection with Transfer Learning â€¢ keras</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../../bootstrap-toc.css">
<script src="../../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../../pkgdown.css" rel="stylesheet">
<script src="../../pkgdown.js"></script><link href="../../extra.css" rel="stylesheet">
<script src="../../extra.js"></script><meta property="og:title" content="Keypoint Detection with Transfer Learning">
<meta property="og:description" content="Training a keypoint detector with data augmentation and transfer learning.">
<meta property="og:image" content="https://keras.posit.co/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../../index.html">keras</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">2.13.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../../index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Tutorials
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/getting_started.html">Getting Started</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    </li>
    <li>
      <a href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    </li>
    <li>
      <a href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Guides
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li class="dropdown-header">Guides (New for TF 2.6)</li>
    <li>
      <a href="../../articles/new-guides/python_subclasses.html">Python Subclasses</a>
    </li>
    <li>
      <a href="../../articles/new-guides/making_new_layers_and_models_via_subclassing.html">Making New Layers and Models via Subclassing</a>
    </li>
    <li>
      <a href="../../articles/new-guides/customizing_what_happens_in_fit.html">Customizing What Happens in Fit</a>
    </li>
    <li>
      <a href="../../articles/new-guides/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    </li>
    <li>
      <a href="../../articles/new-guides/preprocessing_layers.html">Working with Preprocessing Layers</a>
    </li>
    <li>
      <a href="../../argicles/new-guides/working_with_rnns.html">Working with RNNs</a>
    </li>
    <li class="divider">
    </li>
<li class="dropdown-header">Using Keras</li>
    <li>
      <a href="../../articles/guide_keras.html">Guide to Keras Basics</a>
    </li>
    <li>
      <a href="../../articles/sequential_model.html">Sequential Model in Depth</a>
    </li>
    <li>
      <a href="../../articles/functional_api.html">Functional API in Depth</a>
    </li>
    <li>
      <a href="../../articles/about_keras_models.html">About Keras Models</a>
    </li>
    <li>
      <a href="../../articles/about_keras_layers.html">About Keras Layers</a>
    </li>
    <li>
      <a href="../../articles/training_visualization.html">Training Visualization</a>
    </li>
    <li>
      <a href="../../articles/applications.html">Pre-Trained Models</a>
    </li>
    <li>
      <a href="../../articles/faq.html">Frequently Asked Questions</a>
    </li>
    <li>
      <a href="../../articles/why_use_keras.html">Why Use Keras?</a>
    </li>
    <li class="divider">
    </li>
<li class="dropdown-header">Advanced</li>
    <li>
      <a href="../../articles/eager_guide.html">Eager Execution</a>
    </li>
    <li>
      <a href="../../articles/training_callbacks.html">Training Callbacks</a>
    </li>
    <li>
      <a href="../../articles/backend.html">Keras Backend</a>
    </li>
    <li>
      <a href="../../articles/custom_layers.html">Custom Layers</a>
    </li>
    <li>
      <a href="../../articles/custom_models.html">Custom Models</a>
    </li>
    <li>
      <a href="../../articles/saving_serializing.html">Saving and serializing</a>
    </li>
  </ul>
</li>
<li>
  <a href="../../articles/learn.html">Learn</a>
</li>
<li>
  <a href="../../articles/tools.html">Tools</a>
</li>
<li>
  <a href="../../articles/examples/index.html">Examples</a>
</li>
<li>
  <a href="../../reference/index.html">Reference</a>
</li>
<li>
  <a href="../../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/rstudio/keras/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Keypoint Detection with Transfer Learning</h1>
                        <h4 data-toc-skip class="author"><a href="https://twitter.com/RisingSayak" class="external-link">Sayak Paul</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/keypoint_detection.Rmd" class="external-link"><code>vignettes/examples/keypoint_detection.Rmd</code></a></small>
      <div class="hidden name"><code>keypoint_detection.Rmd</code></div>

    </div>

    
    
<p>Keypoint detection consists of locating key object parts. For
example, the key parts of our faces include nose tips, eyebrows, eye
corners, and so on. These parts help to represent the underlying object
in a feature-rich manner. Keypoint detection has applications that
include pose estimation, face detection, etc.</p>
<p>In this example, we will build a keypoint detector using the <a href="https://github.com/benjiebob/StanfordExtra" class="external-link">StanfordExtra
dataset</a>, using transfer learning. This example requires TensorFlow
2.4 or higher, as well as <a href="https://imgaug.readthedocs.io/" class="external-link"><code>imgaug</code></a> library,
which can be installed using the following command:</p>
<p>pip install -q -U imgaug</p>
<div class="section level2">
<h2 id="data-collection">Data collection<a class="anchor" aria-label="anchor" href="#data-collection"></a>
</h2>
<p>The StanfordExtra dataset contains 12,000 images of dogs together
with keypoints and segmentation maps. It is developed from the <a href="http://vision.stanford.edu/aditya86/ImageNetDogs/" class="external-link">Stanford dogs
dataset</a>. It can be downloaded with the command below:</p>
<p>wget -q <a href="http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar" class="external-link uri">http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar</a></p>
<p>Annotations are provided as a single JSON file in the StanfordExtra
dataset and one needs to fill <a href="https://forms.gle/sRtbicgxsWvRtRmUA" class="external-link">this form</a> to get access
to it. The authors explicitly instruct users not to share the JSON file,
and this example respects this wish: you should obtain the JSON file
yourself.</p>
<p>The JSON file is expected to be locally available as
<code>stanfordextra_v12.zip</code>.</p>
<p>After the files are downloaded, we can extract the archives.</p>
<p>tar xf images.tar unzip -qq ~/stanfordextra_v12.zip</p>
</div>
<div class="section level2">
<h2 id="imports">Imports<a class="anchor" aria-label="anchor" href="#imports"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">from</span> imgaug.augmentables.kps <span class="im">import</span> KeypointsOnImage</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">from</span> imgaug.augmentables.kps <span class="im">import</span> Keypoint</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">import</span> imgaug.augmenters <span class="im">as</span> iaa</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a><span class="im">import</span> os</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="define-hyperparameters">Define hyperparameters<a class="anchor" aria-label="anchor" href="#define-hyperparameters"></a>
</h2>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>IMG_SIZE <span class="op">=</span> <span class="dv">224</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>EPOCHS <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>NUM_KEYPOINTS <span class="op">=</span> <span class="dv">24</span> <span class="op">*</span> <span class="dv">2</span>  <span class="co"># 24 pairs each having x and y coordinates</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="load-data">Load data<a class="anchor" aria-label="anchor" href="#load-data"></a>
</h2>
<p>The authors also provide a metadata file that specifies additional
information about the keypoints, like color information, animal pose
name, etc. We will load this file in a <code>pandas</code> dataframe to
extract information for visualization purposes.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>IMG_DIR <span class="op">=</span> <span class="st">"Images"</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>JSON <span class="op">=</span> <span class="st">"StanfordExtra_V12/StanfordExtra_v12.json"</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>KEYPOINT_DEF <span class="op">=</span> <span class="st">"https://github.com/benjiebob/StanfordExtra/raw/master/keypoint_definitions.csv"</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="co"># Load the ground-truth annotations.</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(JSON) <span class="im">as</span> infile:</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>    json_data <span class="op">=</span> json.load(infile)</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a><span class="co"># Set up a dictionary, mapping all the ground-truth information</span></span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a><span class="co"># with respect to the path of the image.</span></span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>json_dict <span class="op">=</span> {i[<span class="st">"img_path"</span>]: i <span class="cf">for</span> i <span class="kw">in</span> json_data}</span></code></pre></div>
<p>A single entry of <code>json_dict</code> looks like the
following:</p>
<pre><code>'n02085782-Japanese_spaniel/n02085782_2886.jpg':
{'img_bbox': [205, 20, 116, 201],
 'img_height': 272,
 'img_path': 'n02085782-Japanese_spaniel/n02085782_2886.jpg',
 'img_width': 350,
 'is_multiple_dogs': False,
 'joints': [[108.66666666666667, 252.0, 1],
            [147.66666666666666, 229.0, 1],
            [163.5, 208.5, 1],
            [0, 0, 0],
            [0, 0, 0],
            [0, 0, 0],
            [54.0, 244.0, 1],
            [77.33333333333333, 225.33333333333334, 1],
            [79.0, 196.5, 1],
            [0, 0, 0],
            [0, 0, 0],
            [0, 0, 0],
            [0, 0, 0],
            [0, 0, 0],
            [150.66666666666666, 86.66666666666667, 1],
            [88.66666666666667, 73.0, 1],
            [116.0, 106.33333333333333, 1],
            [109.0, 123.33333333333333, 1],
            [0, 0, 0],
            [0, 0, 0],
            [0, 0, 0],
            [0, 0, 0],
            [0, 0, 0],
            [0, 0, 0]],
 'seg': ...}</code></pre>
<p>In this example, the keys we are interested in are:</p>
<ul>
<li><code>img_path</code></li>
<li><code>joints</code></li>
</ul>
<p>There are a total of 24 entries present inside <code>joints</code>.
Each entry has 3 values:</p>
<ul>
<li>x-coordinate</li>
<li>y-coordinate</li>
<li>visibility flag of the keypoints (1 indicates visibility and 0
indicates non-visibility)</li>
</ul>
<p>As we can see <code>joints</code> contain multiple
<code>[0, 0, 0]</code> entries which denote that those keypoints were
not labeled. In this example, we will consider both non-visible as well
as unlabeled keypoints in order to allow mini-batch learning.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># Load the metdata definition file and preview it.</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>keypoint_def <span class="op">=</span> pd.read_csv(KEYPOINT_DEF)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>keypoint_def.head()</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a><span class="co"># Extract the colours and labels.</span></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>colours <span class="op">=</span> keypoint_def[<span class="st">"Hex colour"</span>].values.tolist()</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>colours <span class="op">=</span> [<span class="st">"#"</span> <span class="op">+</span> colour <span class="cf">for</span> colour <span class="kw">in</span> colours]</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>labels <span class="op">=</span> keypoint_def[<span class="st">"Name"</span>].values.tolist()</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a><span class="co"># Utility for reading an image and for getting its annotations.</span></span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a><span class="kw">def</span> get_dog(name):</span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>    data <span class="op">=</span> json_dict[name]</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>    img_data <span class="op">=</span> plt.imread(os.path.join(IMG_DIR, data[<span class="st">"img_path"</span>]))</span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a>    <span class="co"># If the image is RGBA convert it to RGB.</span></span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>    <span class="cf">if</span> img_data.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> <span class="dv">4</span>:</span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>        img_data <span class="op">=</span> img_data.astype(np.uint8)</span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a>        img_data <span class="op">=</span> Image.fromarray(img_data)</span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a>        img_data <span class="op">=</span> np.array(img_data.convert(<span class="st">"RGB"</span>))</span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a>    data[<span class="st">"img_data"</span>] <span class="op">=</span> img_data</span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a>    <span class="cf">return</span> data</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="visualize-data">Visualize data<a class="anchor" aria-label="anchor" href="#visualize-data"></a>
</h2>
<p>Now, we write a utility function to visualize the images and their
keypoints.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="co"># Parts of this code come from here:</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a><span class="co"># https://github.com/benjiebob/StanfordExtra/blob/master/demo.ipynb</span></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a><span class="kw">def</span> visualize_keypoints(images, keypoints):</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="bu">len</span>(images), ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">12</span>))</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>    [ax.axis(<span class="st">"off"</span>) <span class="cf">for</span> ax <span class="kw">in</span> np.ravel(axes)]</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>    <span class="cf">for</span> (ax_orig, ax_all), image, current_keypoint <span class="kw">in</span> <span class="bu">zip</span>(</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>        axes, images, keypoints</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>    ):</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>        ax_orig.imshow(image)</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>        ax_all.imshow(image)</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a>        <span class="co"># If the keypoints were formed by `imgaug` then the coordinates need</span></span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a>        <span class="co"># to be iterated differently.</span></span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(current_keypoint, KeypointsOnImage):</span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a>            <span class="cf">for</span> idx, kp <span class="kw">in</span> <span class="bu">enumerate</span>(current_keypoint.keypoints):</span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a>                ax_all.scatter(</span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a>                    [kp.x],</span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a>                    [kp.y],</span>
<span id="cb6-20"><a href="#cb6-20" tabindex="-1"></a>                    c<span class="op">=</span>colours[idx],</span>
<span id="cb6-21"><a href="#cb6-21" tabindex="-1"></a>                    marker<span class="op">=</span><span class="st">"x"</span>,</span>
<span id="cb6-22"><a href="#cb6-22" tabindex="-1"></a>                    s<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb6-23"><a href="#cb6-23" tabindex="-1"></a>                    linewidths<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb6-24"><a href="#cb6-24" tabindex="-1"></a>                )</span>
<span id="cb6-25"><a href="#cb6-25" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb6-26"><a href="#cb6-26" tabindex="-1"></a>            current_keypoint <span class="op">=</span> np.array(current_keypoint)</span>
<span id="cb6-27"><a href="#cb6-27" tabindex="-1"></a>            <span class="co"># Since the last entry is the visibility flag, we discard it.</span></span>
<span id="cb6-28"><a href="#cb6-28" tabindex="-1"></a>            current_keypoint <span class="op">=</span> current_keypoint[:, :<span class="dv">2</span>]</span>
<span id="cb6-29"><a href="#cb6-29" tabindex="-1"></a>            <span class="cf">for</span> idx, (x, y) <span class="kw">in</span> <span class="bu">enumerate</span>(current_keypoint):</span>
<span id="cb6-30"><a href="#cb6-30" tabindex="-1"></a>                ax_all.scatter(</span>
<span id="cb6-31"><a href="#cb6-31" tabindex="-1"></a>                    [x], [y], c<span class="op">=</span>colours[idx], marker<span class="op">=</span><span class="st">"x"</span>, s<span class="op">=</span><span class="dv">50</span>, linewidths<span class="op">=</span><span class="dv">5</span></span>
<span id="cb6-32"><a href="#cb6-32" tabindex="-1"></a>                )</span>
<span id="cb6-33"><a href="#cb6-33" tabindex="-1"></a></span>
<span id="cb6-34"><a href="#cb6-34" tabindex="-1"></a>    plt.tight_layout(pad<span class="op">=</span><span class="fl">2.0</span>)</span>
<span id="cb6-35"><a href="#cb6-35" tabindex="-1"></a>    plt.show()</span>
<span id="cb6-36"><a href="#cb6-36" tabindex="-1"></a></span>
<span id="cb6-37"><a href="#cb6-37" tabindex="-1"></a></span>
<span id="cb6-38"><a href="#cb6-38" tabindex="-1"></a><span class="co"># Select four samples randomly for visualization.</span></span>
<span id="cb6-39"><a href="#cb6-39" tabindex="-1"></a>samples <span class="op">=</span> <span class="bu">list</span>(json_dict.keys())</span>
<span id="cb6-40"><a href="#cb6-40" tabindex="-1"></a>num_samples <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb6-41"><a href="#cb6-41" tabindex="-1"></a>selected_samples <span class="op">=</span> np.random.choice(samples, num_samples, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-42"><a href="#cb6-42" tabindex="-1"></a></span>
<span id="cb6-43"><a href="#cb6-43" tabindex="-1"></a>images, keypoints <span class="op">=</span> [], []</span>
<span id="cb6-44"><a href="#cb6-44" tabindex="-1"></a></span>
<span id="cb6-45"><a href="#cb6-45" tabindex="-1"></a><span class="cf">for</span> sample <span class="kw">in</span> selected_samples:</span>
<span id="cb6-46"><a href="#cb6-46" tabindex="-1"></a>    data <span class="op">=</span> get_dog(sample)</span>
<span id="cb6-47"><a href="#cb6-47" tabindex="-1"></a>    image <span class="op">=</span> data[<span class="st">"img_data"</span>]</span>
<span id="cb6-48"><a href="#cb6-48" tabindex="-1"></a>    keypoint <span class="op">=</span> data[<span class="st">"joints"</span>]</span>
<span id="cb6-49"><a href="#cb6-49" tabindex="-1"></a></span>
<span id="cb6-50"><a href="#cb6-50" tabindex="-1"></a>    images.append(image)</span>
<span id="cb6-51"><a href="#cb6-51" tabindex="-1"></a>    keypoints.append(keypoint)</span>
<span id="cb6-52"><a href="#cb6-52" tabindex="-1"></a></span>
<span id="cb6-53"><a href="#cb6-53" tabindex="-1"></a>visualize_keypoints(images, keypoints)</span></code></pre></div>
<p>The plots show that we have images of non-uniform sizes, which is
expected in most real-world scenarios. However, if we resize these
images to have a uniform shape (for instance (224 x 224)) their
ground-truth annotations will also be affected. The same applies if we
apply any geometric transformation (horizontal flip, for e.g.) to an
image. Fortunately, <code>imgaug</code> provides utilities that can
handle this issue. In the next section, we will write a data generator
inheriting the <a href="https://keras.io/api/utils/python_utils/#sequence-class" class="external-link"><code>keras.utils.Sequence</code></a>
class that applies data augmentation on batches of data using
<code>imgaug</code>.</p>
</div>
<div class="section level2">
<h2 id="prepare-data-generator">Prepare data generator<a class="anchor" aria-label="anchor" href="#prepare-data-generator"></a>
</h2>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="kw">class</span> KeyPointsDataset(keras.utils.PyDataset):</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>        <span class="va">self</span>, image_keys, aug, batch_size<span class="op">=</span>BATCH_SIZE, train<span class="op">=</span><span class="va">True</span>, <span class="op">**</span>kwargs</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>    ):</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>        <span class="va">self</span>.image_keys <span class="op">=</span> image_keys</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>        <span class="va">self</span>.aug <span class="op">=</span> aug</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>        <span class="va">self</span>.batch_size <span class="op">=</span> batch_size</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>        <span class="va">self</span>.train <span class="op">=</span> train</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>        <span class="va">self</span>.on_epoch_end()</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.image_keys) <span class="op">//</span> <span class="va">self</span>.batch_size</span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a>    <span class="kw">def</span> on_epoch_end(<span class="va">self</span>):</span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a>        <span class="va">self</span>.indexes <span class="op">=</span> np.arange(<span class="bu">len</span>(<span class="va">self</span>.image_keys))</span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.train:</span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a>            np.random.shuffle(<span class="va">self</span>.indexes)</span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, index):</span>
<span id="cb7-21"><a href="#cb7-21" tabindex="-1"></a>        indexes <span class="op">=</span> <span class="va">self</span>.indexes[</span>
<span id="cb7-22"><a href="#cb7-22" tabindex="-1"></a>            index <span class="op">*</span> <span class="va">self</span>.batch_size : (index <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> <span class="va">self</span>.batch_size</span>
<span id="cb7-23"><a href="#cb7-23" tabindex="-1"></a>        ]</span>
<span id="cb7-24"><a href="#cb7-24" tabindex="-1"></a>        image_keys_temp <span class="op">=</span> [<span class="va">self</span>.image_keys[k] <span class="cf">for</span> k <span class="kw">in</span> indexes]</span>
<span id="cb7-25"><a href="#cb7-25" tabindex="-1"></a>        (images, keypoints) <span class="op">=</span> <span class="va">self</span>.__data_generation(image_keys_temp)</span>
<span id="cb7-26"><a href="#cb7-26" tabindex="-1"></a></span>
<span id="cb7-27"><a href="#cb7-27" tabindex="-1"></a>        <span class="cf">return</span> (images, keypoints)</span>
<span id="cb7-28"><a href="#cb7-28" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" tabindex="-1"></a>    <span class="kw">def</span> __data_generation(<span class="va">self</span>, image_keys_temp):</span>
<span id="cb7-30"><a href="#cb7-30" tabindex="-1"></a>        batch_images <span class="op">=</span> np.empty(</span>
<span id="cb7-31"><a href="#cb7-31" tabindex="-1"></a>            (<span class="va">self</span>.batch_size, IMG_SIZE, IMG_SIZE, <span class="dv">3</span>), dtype<span class="op">=</span><span class="st">"int"</span></span>
<span id="cb7-32"><a href="#cb7-32" tabindex="-1"></a>        )</span>
<span id="cb7-33"><a href="#cb7-33" tabindex="-1"></a>        batch_keypoints <span class="op">=</span> np.empty(</span>
<span id="cb7-34"><a href="#cb7-34" tabindex="-1"></a>            (<span class="va">self</span>.batch_size, <span class="dv">1</span>, <span class="dv">1</span>, NUM_KEYPOINTS), dtype<span class="op">=</span><span class="st">"float32"</span></span>
<span id="cb7-35"><a href="#cb7-35" tabindex="-1"></a>        )</span>
<span id="cb7-36"><a href="#cb7-36" tabindex="-1"></a></span>
<span id="cb7-37"><a href="#cb7-37" tabindex="-1"></a>        <span class="cf">for</span> i, key <span class="kw">in</span> <span class="bu">enumerate</span>(image_keys_temp):</span>
<span id="cb7-38"><a href="#cb7-38" tabindex="-1"></a>            data <span class="op">=</span> get_dog(key)</span>
<span id="cb7-39"><a href="#cb7-39" tabindex="-1"></a>            current_keypoint <span class="op">=</span> np.array(data[<span class="st">"joints"</span>])[:, :<span class="dv">2</span>]</span>
<span id="cb7-40"><a href="#cb7-40" tabindex="-1"></a>            kps <span class="op">=</span> []</span>
<span id="cb7-41"><a href="#cb7-41" tabindex="-1"></a></span>
<span id="cb7-42"><a href="#cb7-42" tabindex="-1"></a>            <span class="co"># To apply our data augmentation pipeline, we first need to</span></span>
<span id="cb7-43"><a href="#cb7-43" tabindex="-1"></a>            <span class="co"># form Keypoint objects with the original coordinates.</span></span>
<span id="cb7-44"><a href="#cb7-44" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(current_keypoint)):</span>
<span id="cb7-45"><a href="#cb7-45" tabindex="-1"></a>                kps.append(</span>
<span id="cb7-46"><a href="#cb7-46" tabindex="-1"></a>                    Keypoint(x<span class="op">=</span>current_keypoint[j][<span class="dv">0</span>], y<span class="op">=</span>current_keypoint[j][<span class="dv">1</span>])</span>
<span id="cb7-47"><a href="#cb7-47" tabindex="-1"></a>                )</span>
<span id="cb7-48"><a href="#cb7-48" tabindex="-1"></a></span>
<span id="cb7-49"><a href="#cb7-49" tabindex="-1"></a>            <span class="co"># We then project the original image and its keypoint coordinates.</span></span>
<span id="cb7-50"><a href="#cb7-50" tabindex="-1"></a>            current_image <span class="op">=</span> data[<span class="st">"img_data"</span>]</span>
<span id="cb7-51"><a href="#cb7-51" tabindex="-1"></a>            kps_obj <span class="op">=</span> KeypointsOnImage(kps, shape<span class="op">=</span>current_image.shape)</span>
<span id="cb7-52"><a href="#cb7-52" tabindex="-1"></a></span>
<span id="cb7-53"><a href="#cb7-53" tabindex="-1"></a>            <span class="co"># Apply the augmentation pipeline.</span></span>
<span id="cb7-54"><a href="#cb7-54" tabindex="-1"></a>            (new_image, new_kps_obj) <span class="op">=</span> <span class="va">self</span>.aug(</span>
<span id="cb7-55"><a href="#cb7-55" tabindex="-1"></a>                image<span class="op">=</span>current_image, keypoints<span class="op">=</span>kps_obj</span>
<span id="cb7-56"><a href="#cb7-56" tabindex="-1"></a>            )</span>
<span id="cb7-57"><a href="#cb7-57" tabindex="-1"></a>            batch_images[i,] <span class="op">=</span> new_image</span>
<span id="cb7-58"><a href="#cb7-58" tabindex="-1"></a></span>
<span id="cb7-59"><a href="#cb7-59" tabindex="-1"></a>            <span class="co"># Parse the coordinates from the new keypoint object.</span></span>
<span id="cb7-60"><a href="#cb7-60" tabindex="-1"></a>            kp_temp <span class="op">=</span> []</span>
<span id="cb7-61"><a href="#cb7-61" tabindex="-1"></a>            <span class="cf">for</span> keypoint <span class="kw">in</span> new_kps_obj:</span>
<span id="cb7-62"><a href="#cb7-62" tabindex="-1"></a>                kp_temp.append(np.nan_to_num(keypoint.x))</span>
<span id="cb7-63"><a href="#cb7-63" tabindex="-1"></a>                kp_temp.append(np.nan_to_num(keypoint.y))</span>
<span id="cb7-64"><a href="#cb7-64" tabindex="-1"></a></span>
<span id="cb7-65"><a href="#cb7-65" tabindex="-1"></a>            <span class="co"># More on why this reshaping later.</span></span>
<span id="cb7-66"><a href="#cb7-66" tabindex="-1"></a>            batch_keypoints[i,] <span class="op">=</span> np.array(kp_temp).reshape(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">24</span> <span class="op">*</span> <span class="dv">2</span>)</span>
<span id="cb7-67"><a href="#cb7-67" tabindex="-1"></a></span>
<span id="cb7-68"><a href="#cb7-68" tabindex="-1"></a>        <span class="co"># Scale the coordinates to [0, 1] range.</span></span>
<span id="cb7-69"><a href="#cb7-69" tabindex="-1"></a>        batch_keypoints <span class="op">=</span> batch_keypoints <span class="op">/</span> IMG_SIZE</span>
<span id="cb7-70"><a href="#cb7-70" tabindex="-1"></a></span>
<span id="cb7-71"><a href="#cb7-71" tabindex="-1"></a>        <span class="cf">return</span> (batch_images, batch_keypoints)</span></code></pre></div>
<p>To know more about how to operate with keypoints in
<code>imgaug</code> check out <a href="https://imgaug.readthedocs.io/en/latest/source/examples_keypoints.html" class="external-link">this
document</a>.</p>
</div>
<div class="section level2">
<h2 id="define-augmentation-transforms">Define augmentation transforms<a class="anchor" aria-label="anchor" href="#define-augmentation-transforms"></a>
</h2>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>train_aug <span class="op">=</span> iaa.Sequential(</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>    [</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>        iaa.Resize(IMG_SIZE, interpolation<span class="op">=</span><span class="st">"linear"</span>),</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>        iaa.Fliplr(<span class="fl">0.3</span>),</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>        <span class="co"># `Sometimes()` applies a function randomly to the inputs with</span></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>        <span class="co"># a given probability (0.3, in this case).</span></span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>        iaa.Sometimes(<span class="fl">0.3</span>, iaa.Affine(rotate<span class="op">=</span><span class="dv">10</span>, scale<span class="op">=</span>(<span class="fl">0.5</span>, <span class="fl">0.7</span>))),</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>    ]</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>)</span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>test_aug <span class="op">=</span> iaa.Sequential([iaa.Resize(IMG_SIZE, interpolation<span class="op">=</span><span class="st">"linear"</span>)])</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="create-training-and-validation-splits">Create training and validation splits<a class="anchor" aria-label="anchor" href="#create-training-and-validation-splits"></a>
</h2>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>np.random.shuffle(samples)</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>train_keys, validation_keys <span class="op">=</span> (</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>    samples[<span class="bu">int</span>(<span class="bu">len</span>(samples) <span class="op">*</span> <span class="fl">0.15</span>) :],</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>    samples[: <span class="bu">int</span>(<span class="bu">len</span>(samples) <span class="op">*</span> <span class="fl">0.15</span>)],</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="data-generator-investigation">Data generator investigation<a class="anchor" aria-label="anchor" href="#data-generator-investigation"></a>
</h2>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>train_dataset <span class="op">=</span> KeyPointsDataset(</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>    train_keys, train_aug, workers<span class="op">=</span><span class="dv">2</span>, use_multiprocessing<span class="op">=</span><span class="va">True</span></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>)</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>validation_dataset <span class="op">=</span> KeyPointsDataset(</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>    validation_keys, test_aug, train<span class="op">=</span><span class="va">False</span>, workers<span class="op">=</span><span class="dv">2</span>, use_multiprocessing<span class="op">=</span><span class="va">True</span></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>)</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total batches in training set: </span><span class="sc">{</span><span class="bu">len</span>(train_dataset)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total batches in validation set: </span><span class="sc">{</span><span class="bu">len</span>(validation_dataset)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a>sample_images, sample_keypoints <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_dataset))</span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a><span class="cf">assert</span> sample_keypoints.<span class="bu">max</span>() <span class="op">==</span> <span class="fl">1.0</span></span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a><span class="cf">assert</span> sample_keypoints.<span class="bu">min</span>() <span class="op">==</span> <span class="fl">0.0</span></span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a>sample_keypoints <span class="op">=</span> sample_keypoints[:<span class="dv">4</span>].reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">24</span>, <span class="dv">2</span>) <span class="op">*</span> IMG_SIZE</span>
<span id="cb10-16"><a href="#cb10-16" tabindex="-1"></a>visualize_keypoints(sample_images[:<span class="dv">4</span>], sample_keypoints)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="model-building">Model building<a class="anchor" aria-label="anchor" href="#model-building"></a>
</h2>
<p>The <a href="http://vision.stanford.edu/aditya86/ImageNetDogs/" class="external-link">Stanford dogs
dataset</a> (on which the StanfordExtra dataset is based) was built
using the <a href="http://image-net.org/" class="external-link">ImageNet-1k dataset</a>. So,
it is likely that the models pretrained on the ImageNet-1k dataset would
be useful for this task. We will use a MobileNetV2 pre-trained on this
dataset as a backbone to extract meaningful features from the images and
then pass those to a custom regression head for predicting
coordinates.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="kw">def</span> get_model():</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>    <span class="co"># Load the pre-trained weights of MobileNetV2 and freeze the weights</span></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>    backbone <span class="op">=</span> keras.applications.MobileNetV2(</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>        weights<span class="op">=</span><span class="st">"imagenet"</span>,</span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>        include_top<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>        input_shape<span class="op">=</span>(IMG_SIZE, IMG_SIZE, <span class="dv">3</span>),</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>    )</span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a>    backbone.trainable <span class="op">=</span> <span class="va">False</span></span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a></span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a>    inputs <span class="op">=</span> layers.Input((IMG_SIZE, IMG_SIZE, <span class="dv">3</span>))</span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a>    x <span class="op">=</span> keras.applications.mobilenet_v2.preprocess_input(inputs)</span>
<span id="cb11-12"><a href="#cb11-12" tabindex="-1"></a>    x <span class="op">=</span> backbone(x)</span>
<span id="cb11-13"><a href="#cb11-13" tabindex="-1"></a>    x <span class="op">=</span> layers.Dropout(<span class="fl">0.3</span>)(x)</span>
<span id="cb11-14"><a href="#cb11-14" tabindex="-1"></a>    x <span class="op">=</span> layers.SeparableConv2D(</span>
<span id="cb11-15"><a href="#cb11-15" tabindex="-1"></a>        NUM_KEYPOINTS, kernel_size<span class="op">=</span><span class="dv">5</span>, strides<span class="op">=</span><span class="dv">1</span>, activation<span class="op">=</span><span class="st">"relu"</span></span>
<span id="cb11-16"><a href="#cb11-16" tabindex="-1"></a>    )(x)</span>
<span id="cb11-17"><a href="#cb11-17" tabindex="-1"></a>    outputs <span class="op">=</span> layers.SeparableConv2D(</span>
<span id="cb11-18"><a href="#cb11-18" tabindex="-1"></a>        NUM_KEYPOINTS, kernel_size<span class="op">=</span><span class="dv">3</span>, strides<span class="op">=</span><span class="dv">1</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span></span>
<span id="cb11-19"><a href="#cb11-19" tabindex="-1"></a>    )(x)</span>
<span id="cb11-20"><a href="#cb11-20" tabindex="-1"></a></span>
<span id="cb11-21"><a href="#cb11-21" tabindex="-1"></a>    <span class="cf">return</span> keras.Model(inputs, outputs, name<span class="op">=</span><span class="st">"keypoint_detector"</span>)</span></code></pre></div>
<p>Our custom network is fully-convolutional which makes it more
parameter-friendly than the same version of the network having
fully-connected dense layers.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>get_model().summary()</span></code></pre></div>
<p>Notice the output shape of the network:
<code>(None, 1, 1, 48)</code>. This is why we have reshaped the
coordinates as:
<code>batch_keypoints[i, :] = np.array(kp_temp).reshape(1, 1, 24 * 2)</code>.</p>
</div>
<div class="section level2">
<h2 id="model-compilation-and-training">Model compilation and training<a class="anchor" aria-label="anchor" href="#model-compilation-and-training"></a>
</h2>
<p>For this example, we will train the network only for five epochs.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>model <span class="op">=</span> get_model()</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">"mse"</span>, optimizer<span class="op">=</span>keras.optimizers.Adam(<span class="fl">1e-4</span>))</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>model.fit(train_dataset, validation_data<span class="op">=</span>validation_dataset, epochs<span class="op">=</span>EPOCHS)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="make-predictions-and-visualize-them">Make predictions and visualize them<a class="anchor" aria-label="anchor" href="#make-predictions-and-visualize-them"></a>
</h2>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>sample_val_images, sample_val_keypoints <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(validation_dataset))</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>sample_val_images <span class="op">=</span> sample_val_images[:<span class="dv">4</span>]</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>sample_val_keypoints <span class="op">=</span> sample_val_keypoints[:<span class="dv">4</span>].reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">24</span>, <span class="dv">2</span>) <span class="op">*</span> IMG_SIZE</span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>predictions <span class="op">=</span> model.predict(sample_val_images).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">24</span>, <span class="dv">2</span>) <span class="op">*</span> IMG_SIZE</span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a><span class="co"># Ground-truth</span></span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a>visualize_keypoints(sample_val_images, sample_val_keypoints)</span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a><span class="co"># Predictions</span></span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a>visualize_keypoints(sample_val_images, predictions)</span></code></pre></div>
<p>Predictions will likely improve with more training.</p>
</div>
<div class="section level2">
<h2 id="going-further">Going further<a class="anchor" aria-label="anchor" href="#going-further"></a>
</h2>
<ul>
<li>Try using other augmentation transforms from <code>imgaug</code> to
investigate how that changes the results.</li>
<li>Here, we transferred the features from the pre-trained network
linearly that is we did not <a href="https://keras.io/guides/transfer_learning/" class="external-link">fine-tune</a> it. You
are encouraged to fine-tune it on this task and see if that improves the
performance. You can also try different architectures and see how they
affect the final performance.</li>
</ul>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, FranÃ§ois Chollet, RStudio, Google.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
