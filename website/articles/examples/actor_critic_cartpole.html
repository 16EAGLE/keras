<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Implement Actor Critic Method in CartPole environment.">
<title>Actor Critic Method â€¢ keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../../deps/Fira_Mono-0.4.8/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><meta property="og:title" content="Actor Critic Method">
<meta property="og:description" content="Implement Actor Critic Method in CartPole environment.">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-getting-started">Getting Started</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-getting-started">
    <a class="dropdown-item" href="../../articles/intro_to_keras_for_engineers.html">Introduction to Keras for engineers</a>
    <h6 class="dropdown-header" data-toc-skip>Tutorials</h6>
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
  </div>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Actor Critic Method</h1>
                        <h4 data-toc-skip class="author"><a href="https://twitter.com/NandanApoorv" class="external-link">Apoorv Nandan</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/actor_critic_cartpole.Rmd" class="external-link"><code>vignettes/examples/actor_critic_cartpole.Rmd</code></a></small>
      <div class="d-none name"><code>actor_critic_cartpole.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>This script shows an implementation of Actor Critic method on
CartPole-V0 environment.</p>
<div class="section level3">
<h3 id="actor-critic-method">Actor Critic Method<a class="anchor" aria-label="anchor" href="#actor-critic-method"></a>
</h3>
<p>As an agent takes actions and moves through an environment, it learns
to map the observed state of the environment to two possible
outputs:</p>
<ol style="list-style-type: decimal">
<li>Recommended action: A probability value for each action in the
action space. The part of the agent responsible for this output is
called the <strong>actor</strong>.</li>
<li>Estimated rewards in the future: Sum of all rewards it expects to
receive in the future. The part of the agent responsible for this output
is the <strong>critic</strong>.</li>
</ol>
<p>Agent and Critic learn to perform their tasks, such that the
recommended actions from the actor maximize the rewards.</p>
</div>
<div class="section level3">
<h3 id="cartpole-v1">CartPole-V1<a class="anchor" aria-label="anchor" href="#cartpole-v1"></a>
</h3>
<p>A pole is attached to a cart placed on a frictionless track. The
agent has to apply force to move the cart. It is rewarded for every time
step the pole remains upright. The agent, therefore, must learn to keep
the pole from falling over.</p>
</div>
<div class="section level3">
<h3 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h3>
<ul>
<li><a href="http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf" class="external-link">CartPole</a></li>
<li><a href="https://hal.inria.fr/hal-00840470/document" class="external-link">Actor Critic
Method</a></li>
</ul>
</div>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>os.environ[<span class="st">"KERAS_BACKEND"</span>] <span class="op">=</span> <span class="st">"tensorflow"</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="im">import</span> gym</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a><span class="co"># Configuration parameters for the whole setup</span></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">42</span></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">0.99</span>  <span class="co"># Discount factor for past rewards</span></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a>max_steps_per_episode <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">"CartPole-v1"</span>, new_step_api<span class="op">=</span><span class="va">True</span>)  <span class="co"># Create the environment</span></span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a>env.reset(seed<span class="op">=</span>seed)</span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a>eps <span class="op">=</span> np.finfo(</span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a>    np.float32</span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a>).eps.item()  <span class="co"># Smallest number such that 1.0 + eps != 1.0</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="implement-actor-critic-network">Implement Actor Critic network<a class="anchor" aria-label="anchor" href="#implement-actor-critic-network"></a>
</h2>
<p>This network learns two functions:</p>
<ol style="list-style-type: decimal">
<li>Actor: This takes as input the state of our environment and returns
a probability value for each action in its action space.</li>
<li>Critic: This takes as input the state of our environment and returns
an estimate of total rewards in the future.</li>
</ol>
<p>In our implementation, they share the initial layer.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>num_inputs <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>num_actions <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>num_hidden <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>inputs <span class="op">=</span> layers.Input(shape<span class="op">=</span>(num_inputs,))</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>common <span class="op">=</span> layers.Dense(num_hidden, activation<span class="op">=</span><span class="st">"relu"</span>)(inputs)</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>action <span class="op">=</span> layers.Dense(num_actions, activation<span class="op">=</span><span class="st">"softmax"</span>)(common)</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>critic <span class="op">=</span> layers.Dense(<span class="dv">1</span>)(common)</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>[action, critic])</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="train">Train<a class="anchor" aria-label="anchor" href="#train"></a>
</h2>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.Adam(learning_rate<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>huber_loss <span class="op">=</span> keras.losses.Huber()</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>action_probs_history <span class="op">=</span> []</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>critic_value_history <span class="op">=</span> []</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>rewards_history <span class="op">=</span> []</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>running_reward <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>episode_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a><span class="cf">while</span> <span class="va">True</span>:  <span class="co"># Run until solved</span></span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>    state <span class="op">=</span> env.reset()</span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>    episode_reward <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>    <span class="cf">with</span> tf.GradientTape() <span class="im">as</span> tape:</span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a>        <span class="cf">for</span> timestep <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, max_steps_per_episode):</span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a>            <span class="co"># env.render(); Adding this line would show the attempts</span></span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a>            <span class="co"># of the agent in a pop up window.</span></span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a>            state <span class="op">=</span> tf.convert_to_tensor(state)</span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a>            state <span class="op">=</span> tf.expand_dims(state, <span class="dv">0</span>)</span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" tabindex="-1"></a>            <span class="co"># Predict action probabilities and estimated future rewards</span></span>
<span id="cb3-21"><a href="#cb3-21" tabindex="-1"></a>            <span class="co"># from environment state</span></span>
<span id="cb3-22"><a href="#cb3-22" tabindex="-1"></a>            action_probs, critic_value <span class="op">=</span> model(state)</span>
<span id="cb3-23"><a href="#cb3-23" tabindex="-1"></a>            critic_value_history.append(critic_value[<span class="dv">0</span>, <span class="dv">0</span>])</span>
<span id="cb3-24"><a href="#cb3-24" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" tabindex="-1"></a>            <span class="co"># Sample action from action probability distribution</span></span>
<span id="cb3-26"><a href="#cb3-26" tabindex="-1"></a>            action <span class="op">=</span> np.random.choice(num_actions, p<span class="op">=</span>np.squeeze(action_probs))</span>
<span id="cb3-27"><a href="#cb3-27" tabindex="-1"></a>            action_probs_history.append(tf.math.log(action_probs[<span class="dv">0</span>, action]))</span>
<span id="cb3-28"><a href="#cb3-28" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" tabindex="-1"></a>            <span class="co"># Apply the sampled action in our environment</span></span>
<span id="cb3-30"><a href="#cb3-30" tabindex="-1"></a>            state, reward, done, _, _ <span class="op">=</span> env.step(action)</span>
<span id="cb3-31"><a href="#cb3-31" tabindex="-1"></a>            rewards_history.append(reward)</span>
<span id="cb3-32"><a href="#cb3-32" tabindex="-1"></a>            episode_reward <span class="op">+=</span> reward</span>
<span id="cb3-33"><a href="#cb3-33" tabindex="-1"></a></span>
<span id="cb3-34"><a href="#cb3-34" tabindex="-1"></a>            <span class="cf">if</span> done:</span>
<span id="cb3-35"><a href="#cb3-35" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb3-36"><a href="#cb3-36" tabindex="-1"></a></span>
<span id="cb3-37"><a href="#cb3-37" tabindex="-1"></a>        <span class="co"># Update running reward to check condition for solving</span></span>
<span id="cb3-38"><a href="#cb3-38" tabindex="-1"></a>        running_reward <span class="op">=</span> <span class="fl">0.05</span> <span class="op">*</span> episode_reward <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> <span class="fl">0.05</span>) <span class="op">*</span> running_reward</span>
<span id="cb3-39"><a href="#cb3-39" tabindex="-1"></a></span>
<span id="cb3-40"><a href="#cb3-40" tabindex="-1"></a>        <span class="co"># Calculate expected value from rewards</span></span>
<span id="cb3-41"><a href="#cb3-41" tabindex="-1"></a>        <span class="co"># - At each timestep what was the total reward received after that timestep</span></span>
<span id="cb3-42"><a href="#cb3-42" tabindex="-1"></a>        <span class="co"># - Rewards in the past are discounted by multiplying them with gamma</span></span>
<span id="cb3-43"><a href="#cb3-43" tabindex="-1"></a>        <span class="co"># - These are the labels for our critic</span></span>
<span id="cb3-44"><a href="#cb3-44" tabindex="-1"></a>        returns <span class="op">=</span> []</span>
<span id="cb3-45"><a href="#cb3-45" tabindex="-1"></a>        discounted_sum <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-46"><a href="#cb3-46" tabindex="-1"></a>        <span class="cf">for</span> r <span class="kw">in</span> rewards_history[::<span class="op">-</span><span class="dv">1</span>]:</span>
<span id="cb3-47"><a href="#cb3-47" tabindex="-1"></a>            discounted_sum <span class="op">=</span> r <span class="op">+</span> gamma <span class="op">*</span> discounted_sum</span>
<span id="cb3-48"><a href="#cb3-48" tabindex="-1"></a>            returns.insert(<span class="dv">0</span>, discounted_sum)</span>
<span id="cb3-49"><a href="#cb3-49" tabindex="-1"></a></span>
<span id="cb3-50"><a href="#cb3-50" tabindex="-1"></a>        <span class="co"># Normalize</span></span>
<span id="cb3-51"><a href="#cb3-51" tabindex="-1"></a>        returns <span class="op">=</span> np.array(returns)</span>
<span id="cb3-52"><a href="#cb3-52" tabindex="-1"></a>        returns <span class="op">=</span> (returns <span class="op">-</span> np.mean(returns)) <span class="op">/</span> (np.std(returns) <span class="op">+</span> eps)</span>
<span id="cb3-53"><a href="#cb3-53" tabindex="-1"></a>        returns <span class="op">=</span> returns.tolist()</span>
<span id="cb3-54"><a href="#cb3-54" tabindex="-1"></a></span>
<span id="cb3-55"><a href="#cb3-55" tabindex="-1"></a>        <span class="co"># Calculating loss values to update our network</span></span>
<span id="cb3-56"><a href="#cb3-56" tabindex="-1"></a>        history <span class="op">=</span> <span class="bu">zip</span>(action_probs_history, critic_value_history, returns)</span>
<span id="cb3-57"><a href="#cb3-57" tabindex="-1"></a>        actor_losses <span class="op">=</span> []</span>
<span id="cb3-58"><a href="#cb3-58" tabindex="-1"></a>        critic_losses <span class="op">=</span> []</span>
<span id="cb3-59"><a href="#cb3-59" tabindex="-1"></a>        <span class="cf">for</span> log_prob, value, ret <span class="kw">in</span> history:</span>
<span id="cb3-60"><a href="#cb3-60" tabindex="-1"></a>            <span class="co"># At this point in history, the critic estimated that we would get a</span></span>
<span id="cb3-61"><a href="#cb3-61" tabindex="-1"></a>            <span class="co"># total reward = `value` in the future. We took an action with log probability</span></span>
<span id="cb3-62"><a href="#cb3-62" tabindex="-1"></a>            <span class="co"># of `log_prob` and ended up recieving a total reward = `ret`.</span></span>
<span id="cb3-63"><a href="#cb3-63" tabindex="-1"></a>            <span class="co"># The actor must be updated so that it predicts an action that leads to</span></span>
<span id="cb3-64"><a href="#cb3-64" tabindex="-1"></a>            <span class="co"># high rewards (compared to critic's estimate) with high probability.</span></span>
<span id="cb3-65"><a href="#cb3-65" tabindex="-1"></a>            diff <span class="op">=</span> ret <span class="op">-</span> value</span>
<span id="cb3-66"><a href="#cb3-66" tabindex="-1"></a>            actor_losses.append(<span class="op">-</span>log_prob <span class="op">*</span> diff)  <span class="co"># actor loss</span></span>
<span id="cb3-67"><a href="#cb3-67" tabindex="-1"></a></span>
<span id="cb3-68"><a href="#cb3-68" tabindex="-1"></a>            <span class="co"># The critic must be updated so that it predicts a better estimate of</span></span>
<span id="cb3-69"><a href="#cb3-69" tabindex="-1"></a>            <span class="co"># the future rewards.</span></span>
<span id="cb3-70"><a href="#cb3-70" tabindex="-1"></a>            critic_losses.append(</span>
<span id="cb3-71"><a href="#cb3-71" tabindex="-1"></a>                huber_loss(tf.expand_dims(value, <span class="dv">0</span>), tf.expand_dims(ret, <span class="dv">0</span>))</span>
<span id="cb3-72"><a href="#cb3-72" tabindex="-1"></a>            )</span>
<span id="cb3-73"><a href="#cb3-73" tabindex="-1"></a></span>
<span id="cb3-74"><a href="#cb3-74" tabindex="-1"></a>        <span class="co"># Backpropagation</span></span>
<span id="cb3-75"><a href="#cb3-75" tabindex="-1"></a>        loss_value <span class="op">=</span> <span class="bu">sum</span>(actor_losses) <span class="op">+</span> <span class="bu">sum</span>(critic_losses)</span>
<span id="cb3-76"><a href="#cb3-76" tabindex="-1"></a>        grads <span class="op">=</span> tape.gradient(loss_value, model.trainable_variables)</span>
<span id="cb3-77"><a href="#cb3-77" tabindex="-1"></a>        optimizer.apply_gradients(<span class="bu">zip</span>(grads, model.trainable_variables))</span>
<span id="cb3-78"><a href="#cb3-78" tabindex="-1"></a></span>
<span id="cb3-79"><a href="#cb3-79" tabindex="-1"></a>        <span class="co"># Clear the loss and reward history</span></span>
<span id="cb3-80"><a href="#cb3-80" tabindex="-1"></a>        action_probs_history.clear()</span>
<span id="cb3-81"><a href="#cb3-81" tabindex="-1"></a>        critic_value_history.clear()</span>
<span id="cb3-82"><a href="#cb3-82" tabindex="-1"></a>        rewards_history.clear()</span>
<span id="cb3-83"><a href="#cb3-83" tabindex="-1"></a></span>
<span id="cb3-84"><a href="#cb3-84" tabindex="-1"></a>    <span class="co"># Log details</span></span>
<span id="cb3-85"><a href="#cb3-85" tabindex="-1"></a>    episode_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb3-86"><a href="#cb3-86" tabindex="-1"></a>    <span class="cf">if</span> episode_count <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb3-87"><a href="#cb3-87" tabindex="-1"></a>        template <span class="op">=</span> <span class="st">"running reward: </span><span class="sc">{:.2f}</span><span class="st"> at episode </span><span class="sc">{}</span><span class="st">"</span></span>
<span id="cb3-88"><a href="#cb3-88" tabindex="-1"></a>        <span class="bu">print</span>(template.<span class="bu">format</span>(running_reward, episode_count))</span>
<span id="cb3-89"><a href="#cb3-89" tabindex="-1"></a></span>
<span id="cb3-90"><a href="#cb3-90" tabindex="-1"></a>    <span class="cf">if</span> running_reward <span class="op">&gt;</span> <span class="dv">195</span>:  <span class="co"># Condition to consider the task solved</span></span>
<span id="cb3-91"><a href="#cb3-91" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Solved at episode </span><span class="sc">{}</span><span class="st">!"</span>.<span class="bu">format</span>(episode_count))</span>
<span id="cb3-92"><a href="#cb3-92" tabindex="-1"></a>        <span class="cf">break</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="visualizations">Visualizations<a class="anchor" aria-label="anchor" href="#visualizations"></a>
</h2>
<p>In early stages of training: <img src="https://i.imgur.com/5gCs5kH.gif" alt="Imgur"></p>
<p>In later stages of training: <img src="https://i.imgur.com/5ziiZUD.gif" alt="Imgur"></p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, FranÃ§ois Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
