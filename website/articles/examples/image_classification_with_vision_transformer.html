<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Implementing the Vision Transformer (ViT) model for image classification.">
<title>Image classification with Vision Transformer • keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><meta property="og:title" content="Image classification with Vision Transformer">
<meta property="og:description" content="Implementing the Vision Transformer (ViT) model for image classification.">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-getting-started">Getting Started</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-getting-started">
    <a class="dropdown-item" href="../../articles/intro_to_keras_for_engineers.html">Introduction to Keras for engineers</a>
    <h6 class="dropdown-header" data-toc-skip>Tutorials</h6>
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
  </div>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Image classification with Vision Transformer</h1>
                        <h4 data-toc-skip class="author"><a href="https://www.linkedin.com/in/khalid-salama-24403144/" class="external-link">Khalid
Salama</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/image_classification_with_vision_transformer.Rmd" class="external-link"><code>vignettes/examples/image_classification_with_vision_transformer.Rmd</code></a></small>
      <div class="d-none name"><code>image_classification_with_vision_transformer.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>This example implements the <a href="https://arxiv.org/abs/2010.11929" class="external-link">Vision Transformer (ViT)</a>
model by Alexey Dosovitskiy et al. for image classification, and
demonstrates it on the CIFAR-100 dataset. The ViT model applies the
Transformer architecture with self-attention to sequences of image
patches, without using convolution layers.</p>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>os.environ[<span class="st">"KERAS_BACKEND"</span>] <span class="op">=</span> <span class="st">"jax"</span>  <span class="co"># @param ["tensorflow", "jax", "torch"]</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> ops</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="prepare-the-data">Prepare the data<a class="anchor" aria-label="anchor" href="#prepare-the-data"></a>
</h2>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>num_classes <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>input_shape <span class="op">=</span> (<span class="dv">32</span>, <span class="dv">32</span>, <span class="dv">3</span>)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>(x_train, y_train), (x_test, y_test) <span class="op">=</span> keras.datasets.cifar100.load_data()</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x_train shape: </span><span class="sc">{</span>x_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> - y_train shape: </span><span class="sc">{</span>y_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x_test shape: </span><span class="sc">{</span>x_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> - y_test shape: </span><span class="sc">{</span>y_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="configure-the-hyperparameters">Configure the hyperparameters<a class="anchor" aria-label="anchor" href="#configure-the-hyperparameters"></a>
</h2>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>weight_decay <span class="op">=</span> <span class="fl">0.0001</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>image_size <span class="op">=</span> <span class="dv">72</span>  <span class="co"># We'll resize input images to this size</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>patch_size <span class="op">=</span> <span class="dv">6</span>  <span class="co"># Size of the patches to be extract from the input images</span></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>num_patches <span class="op">=</span> (image_size <span class="op">//</span> patch_size) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>projection_dim <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>num_heads <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>transformer_units <span class="op">=</span> [</span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>    projection_dim <span class="op">*</span> <span class="dv">2</span>,</span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>    projection_dim,</span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a>]  <span class="co"># Size of the transformer layers</span></span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a>transformer_layers <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a>mlp_head_units <span class="op">=</span> [</span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a>    <span class="dv">2048</span>,</span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a>    <span class="dv">1024</span>,</span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a>]  <span class="co"># Size of the dense layers of the final classifier</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="use-data-augmentation">Use data augmentation<a class="anchor" aria-label="anchor" href="#use-data-augmentation"></a>
</h2>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>data_augmentation <span class="op">=</span> keras.Sequential(</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>    [</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>        layers.Normalization(),</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>        layers.Resizing(image_size, image_size),</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>        layers.RandomFlip(<span class="st">"horizontal"</span>),</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>        layers.RandomRotation(factor<span class="op">=</span><span class="fl">0.02</span>),</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>        layers.RandomZoom(height_factor<span class="op">=</span><span class="fl">0.2</span>, width_factor<span class="op">=</span><span class="fl">0.2</span>),</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>    ],</span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"data_augmentation"</span>,</span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>)</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a><span class="co"># Compute the mean and the variance of the training data for normalization.</span></span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>data_augmentation.layers[<span class="dv">0</span>].adapt(x_train)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="implement-multilayer-perceptron-mlp">Implement multilayer perceptron (MLP)<a class="anchor" aria-label="anchor" href="#implement-multilayer-perceptron-mlp"></a>
</h2>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="kw">def</span> mlp(x, hidden_units, dropout_rate):</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>    <span class="cf">for</span> units <span class="kw">in</span> hidden_units:</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>        x <span class="op">=</span> layers.Dense(units, activation<span class="op">=</span>keras.activations.gelu)(x)</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>        x <span class="op">=</span> layers.Dropout(dropout_rate)(x)</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>    <span class="cf">return</span> x</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="implement-patch-creation-as-a-layer">Implement patch creation as a layer<a class="anchor" aria-label="anchor" href="#implement-patch-creation-as-a-layer"></a>
</h2>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="kw">class</span> Patches(layers.Layer):</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, patch_size):</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>        <span class="va">self</span>.patch_size <span class="op">=</span> patch_size</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, images):</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>        input_shape <span class="op">=</span> ops.backend.shape(images)</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>        batch_size <span class="op">=</span> input_shape[<span class="dv">0</span>]</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>        height <span class="op">=</span> input_shape[<span class="dv">1</span>]</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>        width <span class="op">=</span> input_shape[<span class="dv">2</span>]</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>        channels <span class="op">=</span> input_shape[<span class="dv">3</span>]</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>        num_patches_h <span class="op">=</span> height <span class="op">//</span> <span class="va">self</span>.patch_size</span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a>        num_patches_w <span class="op">=</span> width <span class="op">//</span> <span class="va">self</span>.patch_size</span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a>        patches <span class="op">=</span> keras.ops.image.extract_patches(images, size<span class="op">=</span><span class="va">self</span>.patch_size)</span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a>        patches <span class="op">=</span> ops.reshape(</span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a>            patches,</span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a>            (</span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a>                batch_size,</span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a>                num_patches_h <span class="op">*</span> num_patches_w,</span>
<span id="cb6-20"><a href="#cb6-20" tabindex="-1"></a>                <span class="va">self</span>.patch_size <span class="op">*</span> <span class="va">self</span>.patch_size <span class="op">*</span> channels,</span>
<span id="cb6-21"><a href="#cb6-21" tabindex="-1"></a>            ),</span>
<span id="cb6-22"><a href="#cb6-22" tabindex="-1"></a>        )</span>
<span id="cb6-23"><a href="#cb6-23" tabindex="-1"></a>        <span class="cf">return</span> patches</span>
<span id="cb6-24"><a href="#cb6-24" tabindex="-1"></a></span>
<span id="cb6-25"><a href="#cb6-25" tabindex="-1"></a>    <span class="kw">def</span> get_config(<span class="va">self</span>):</span>
<span id="cb6-26"><a href="#cb6-26" tabindex="-1"></a>        config <span class="op">=</span> <span class="bu">super</span>().get_config()</span>
<span id="cb6-27"><a href="#cb6-27" tabindex="-1"></a>        config.update({<span class="st">"patch_size"</span>: <span class="va">self</span>.patch_size})</span>
<span id="cb6-28"><a href="#cb6-28" tabindex="-1"></a>        <span class="cf">return</span> config</span></code></pre></div>
<p>Let’s display patches for a sample image</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">4</span>))</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>image <span class="op">=</span> x_train[np.random.choice(<span class="bu">range</span>(x_train.shape[<span class="dv">0</span>]))]</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>plt.imshow(image.astype(<span class="st">"uint8"</span>))</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>plt.axis(<span class="st">"off"</span>)</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>resized_image <span class="op">=</span> ops.image.resize(</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>    ops.convert_to_tensor([image]), size<span class="op">=</span>(image_size, image_size)</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>)</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>patches <span class="op">=</span> Patches(patch_size)(resized_image)</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image size: </span><span class="sc">{</span>image_size<span class="sc">}</span><span class="ss"> X </span><span class="sc">{</span>image_size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Patch size: </span><span class="sc">{</span>patch_size<span class="sc">}</span><span class="ss"> X </span><span class="sc">{</span>patch_size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Patches per image: </span><span class="sc">{</span>patches<span class="sc">.</span>shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Elements per patch: </span><span class="sc">{</span>patches<span class="sc">.</span>shape[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">int</span>(np.sqrt(patches.shape[<span class="dv">1</span>]))</span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">4</span>))</span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a><span class="cf">for</span> i, patch <span class="kw">in</span> <span class="bu">enumerate</span>(patches[<span class="dv">0</span>]):</span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a>    ax <span class="op">=</span> plt.subplot(n, n, i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a>    patch_img <span class="op">=</span> ops.reshape(patch, (patch_size, patch_size, <span class="dv">3</span>))</span>
<span id="cb7-20"><a href="#cb7-20" tabindex="-1"></a>    plt.imshow(ops.convert_to_numpy(patch_img).astype(<span class="st">"uint8"</span>))</span>
<span id="cb7-21"><a href="#cb7-21" tabindex="-1"></a>    plt.axis(<span class="st">"off"</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="implement-the-patch-encoding-layer">Implement the patch encoding layer<a class="anchor" aria-label="anchor" href="#implement-the-patch-encoding-layer"></a>
</h2>
<p>The <code>PatchEncoder</code> layer will linearly transform a patch
by projecting it into a vector of size <code>projection_dim</code>. In
addition, it adds a learnable position embedding to the projected
vector.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="kw">class</span> PatchEncoder(layers.Layer):</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_patches, projection_dim):</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>        <span class="va">self</span>.num_patches <span class="op">=</span> num_patches</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>        <span class="va">self</span>.projection <span class="op">=</span> layers.Dense(units<span class="op">=</span>projection_dim)</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>        <span class="va">self</span>.position_embedding <span class="op">=</span> layers.Embedding(</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>            input_dim<span class="op">=</span>num_patches, output_dim<span class="op">=</span>projection_dim</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>        )</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, patch):</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>        positions <span class="op">=</span> ops.expand_dims(</span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>            ops.arange(start<span class="op">=</span><span class="dv">0</span>, stop<span class="op">=</span><span class="va">self</span>.num_patches, step<span class="op">=</span><span class="dv">1</span>), axis<span class="op">=</span><span class="dv">0</span></span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>        )</span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>        projected_patches <span class="op">=</span> <span class="va">self</span>.projection(patch)</span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a>        encoded <span class="op">=</span> projected_patches <span class="op">+</span> <span class="va">self</span>.position_embedding(positions)</span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a>        <span class="cf">return</span> encoded</span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" tabindex="-1"></a>    <span class="kw">def</span> get_config(<span class="va">self</span>):</span>
<span id="cb8-19"><a href="#cb8-19" tabindex="-1"></a>        config <span class="op">=</span> <span class="bu">super</span>().get_config()</span>
<span id="cb8-20"><a href="#cb8-20" tabindex="-1"></a>        config.update({<span class="st">"num_patches"</span>: <span class="va">self</span>.num_patches})</span>
<span id="cb8-21"><a href="#cb8-21" tabindex="-1"></a>        <span class="cf">return</span> config</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="build-the-vit-model">Build the ViT model<a class="anchor" aria-label="anchor" href="#build-the-vit-model"></a>
</h2>
<p>The ViT model consists of multiple Transformer blocks, which use the
<code>layers.MultiHeadAttention</code> layer as a self-attention
mechanism applied to the sequence of patches. The Transformer blocks
produce a <code>[batch_size, num_patches, projection_dim]</code> tensor,
which is processed via an classifier head with softmax to produce the
final class probabilities output.</p>
<p>Unlike the technique described in the <a href="https://arxiv.org/abs/2010.11929" class="external-link">paper</a>, which prepends a
learnable embedding to the sequence of encoded patches to serve as the
image representation, all the outputs of the final Transformer block are
reshaped with <code>layers.Flatten()</code> and used as the image
representation input to the classifier head. Note that the
<code>layers.GlobalAveragePooling1D</code> layer could also be used
instead to aggregate the outputs of the Transformer block, especially
when the number of patches and the projection dimensions are large.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="kw">def</span> create_vit_classifier():</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>    inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>input_shape)</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>    <span class="co"># Augment data.</span></span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>    augmented <span class="op">=</span> data_augmentation(inputs)</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>    <span class="co"># Create patches.</span></span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>    patches <span class="op">=</span> Patches(patch_size)(augmented)</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>    <span class="co"># Encode patches.</span></span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>    encoded_patches <span class="op">=</span> PatchEncoder(num_patches, projection_dim)(patches)</span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a>    <span class="co"># Create multiple layers of the Transformer block.</span></span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(transformer_layers):</span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a>        <span class="co"># Layer normalization 1.</span></span>
<span id="cb9-13"><a href="#cb9-13" tabindex="-1"></a>        x1 <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span><span class="fl">1e-6</span>)(encoded_patches)</span>
<span id="cb9-14"><a href="#cb9-14" tabindex="-1"></a>        <span class="co"># Create a multi-head attention layer.</span></span>
<span id="cb9-15"><a href="#cb9-15" tabindex="-1"></a>        attention_output <span class="op">=</span> layers.MultiHeadAttention(</span>
<span id="cb9-16"><a href="#cb9-16" tabindex="-1"></a>            num_heads<span class="op">=</span>num_heads, key_dim<span class="op">=</span>projection_dim, dropout<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb9-17"><a href="#cb9-17" tabindex="-1"></a>        )(x1, x1)</span>
<span id="cb9-18"><a href="#cb9-18" tabindex="-1"></a>        <span class="co"># Skip connection 1.</span></span>
<span id="cb9-19"><a href="#cb9-19" tabindex="-1"></a>        x2 <span class="op">=</span> layers.Add()([attention_output, encoded_patches])</span>
<span id="cb9-20"><a href="#cb9-20" tabindex="-1"></a>        <span class="co"># Layer normalization 2.</span></span>
<span id="cb9-21"><a href="#cb9-21" tabindex="-1"></a>        x3 <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span><span class="fl">1e-6</span>)(x2)</span>
<span id="cb9-22"><a href="#cb9-22" tabindex="-1"></a>        <span class="co"># MLP.</span></span>
<span id="cb9-23"><a href="#cb9-23" tabindex="-1"></a>        x3 <span class="op">=</span> mlp(x3, hidden_units<span class="op">=</span>transformer_units, dropout_rate<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb9-24"><a href="#cb9-24" tabindex="-1"></a>        <span class="co"># Skip connection 2.</span></span>
<span id="cb9-25"><a href="#cb9-25" tabindex="-1"></a>        encoded_patches <span class="op">=</span> layers.Add()([x3, x2])</span>
<span id="cb9-26"><a href="#cb9-26" tabindex="-1"></a></span>
<span id="cb9-27"><a href="#cb9-27" tabindex="-1"></a>    <span class="co"># Create a [batch_size, projection_dim] tensor.</span></span>
<span id="cb9-28"><a href="#cb9-28" tabindex="-1"></a>    representation <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span><span class="fl">1e-6</span>)(encoded_patches)</span>
<span id="cb9-29"><a href="#cb9-29" tabindex="-1"></a>    representation <span class="op">=</span> layers.Flatten()(representation)</span>
<span id="cb9-30"><a href="#cb9-30" tabindex="-1"></a>    representation <span class="op">=</span> layers.Dropout(<span class="fl">0.5</span>)(representation)</span>
<span id="cb9-31"><a href="#cb9-31" tabindex="-1"></a>    <span class="co"># Add MLP.</span></span>
<span id="cb9-32"><a href="#cb9-32" tabindex="-1"></a>    features <span class="op">=</span> mlp(</span>
<span id="cb9-33"><a href="#cb9-33" tabindex="-1"></a>        representation, hidden_units<span class="op">=</span>mlp_head_units, dropout_rate<span class="op">=</span><span class="fl">0.5</span></span>
<span id="cb9-34"><a href="#cb9-34" tabindex="-1"></a>    )</span>
<span id="cb9-35"><a href="#cb9-35" tabindex="-1"></a>    <span class="co"># Classify outputs.</span></span>
<span id="cb9-36"><a href="#cb9-36" tabindex="-1"></a>    logits <span class="op">=</span> layers.Dense(num_classes)(features)</span>
<span id="cb9-37"><a href="#cb9-37" tabindex="-1"></a>    <span class="co"># Create the Keras model.</span></span>
<span id="cb9-38"><a href="#cb9-38" tabindex="-1"></a>    model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>logits)</span>
<span id="cb9-39"><a href="#cb9-39" tabindex="-1"></a>    <span class="cf">return</span> model</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="compile-train-and-evaluate-the-mode">Compile, train, and evaluate the mode<a class="anchor" aria-label="anchor" href="#compile-train-and-evaluate-the-mode"></a>
</h2>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="kw">def</span> run_experiment(model):</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>    optimizer <span class="op">=</span> keras.optimizers.AdamW(</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>        learning_rate<span class="op">=</span>learning_rate, weight_decay<span class="op">=</span>weight_decay</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>    )</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>    model.<span class="bu">compile</span>(</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>        loss<span class="op">=</span>keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a>        metrics<span class="op">=</span>[</span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a>            keras.metrics.SparseCategoricalAccuracy(name<span class="op">=</span><span class="st">"accuracy"</span>),</span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a>            keras.metrics.SparseTopKCategoricalAccuracy(</span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a>                <span class="dv">5</span>, name<span class="op">=</span><span class="st">"top-5-accuracy"</span></span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a>            ),</span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a>        ],</span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a>    )</span>
<span id="cb10-16"><a href="#cb10-16" tabindex="-1"></a></span>
<span id="cb10-17"><a href="#cb10-17" tabindex="-1"></a>    checkpoint_filepath <span class="op">=</span> <span class="st">"/tmp/checkpoint"</span></span>
<span id="cb10-18"><a href="#cb10-18" tabindex="-1"></a>    checkpoint_callback <span class="op">=</span> keras.callbacks.ModelCheckpoint(</span>
<span id="cb10-19"><a href="#cb10-19" tabindex="-1"></a>        checkpoint_filepath,</span>
<span id="cb10-20"><a href="#cb10-20" tabindex="-1"></a>        monitor<span class="op">=</span><span class="st">"val_accuracy"</span>,</span>
<span id="cb10-21"><a href="#cb10-21" tabindex="-1"></a>        save_best_only<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb10-22"><a href="#cb10-22" tabindex="-1"></a>        save_weights_only<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb10-23"><a href="#cb10-23" tabindex="-1"></a>    )</span>
<span id="cb10-24"><a href="#cb10-24" tabindex="-1"></a></span>
<span id="cb10-25"><a href="#cb10-25" tabindex="-1"></a>    history <span class="op">=</span> model.fit(</span>
<span id="cb10-26"><a href="#cb10-26" tabindex="-1"></a>        x<span class="op">=</span>x_train,</span>
<span id="cb10-27"><a href="#cb10-27" tabindex="-1"></a>        y<span class="op">=</span>y_train,</span>
<span id="cb10-28"><a href="#cb10-28" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb10-29"><a href="#cb10-29" tabindex="-1"></a>        epochs<span class="op">=</span>num_epochs,</span>
<span id="cb10-30"><a href="#cb10-30" tabindex="-1"></a>        validation_split<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb10-31"><a href="#cb10-31" tabindex="-1"></a>        callbacks<span class="op">=</span>[checkpoint_callback],</span>
<span id="cb10-32"><a href="#cb10-32" tabindex="-1"></a>    )</span>
<span id="cb10-33"><a href="#cb10-33" tabindex="-1"></a></span>
<span id="cb10-34"><a href="#cb10-34" tabindex="-1"></a>    model.load_weights(checkpoint_filepath)</span>
<span id="cb10-35"><a href="#cb10-35" tabindex="-1"></a>    _, accuracy, top_5_accuracy <span class="op">=</span> model.evaluate(x_test, y_test)</span>
<span id="cb10-36"><a href="#cb10-36" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Test accuracy: </span><span class="sc">{</span><span class="bu">round</span>(accuracy <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">%"</span>)</span>
<span id="cb10-37"><a href="#cb10-37" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Test top 5 accuracy: </span><span class="sc">{</span><span class="bu">round</span>(top_5_accuracy <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">%"</span>)</span>
<span id="cb10-38"><a href="#cb10-38" tabindex="-1"></a></span>
<span id="cb10-39"><a href="#cb10-39" tabindex="-1"></a>    <span class="cf">return</span> history</span>
<span id="cb10-40"><a href="#cb10-40" tabindex="-1"></a></span>
<span id="cb10-41"><a href="#cb10-41" tabindex="-1"></a></span>
<span id="cb10-42"><a href="#cb10-42" tabindex="-1"></a>vit_classifier <span class="op">=</span> create_vit_classifier()</span>
<span id="cb10-43"><a href="#cb10-43" tabindex="-1"></a>history <span class="op">=</span> run_experiment(vit_classifier)</span>
<span id="cb10-44"><a href="#cb10-44" tabindex="-1"></a></span>
<span id="cb10-45"><a href="#cb10-45" tabindex="-1"></a></span>
<span id="cb10-46"><a href="#cb10-46" tabindex="-1"></a><span class="kw">def</span> plot_history(item):</span>
<span id="cb10-47"><a href="#cb10-47" tabindex="-1"></a>    plt.plot(history.history[item], label<span class="op">=</span>item)</span>
<span id="cb10-48"><a href="#cb10-48" tabindex="-1"></a>    plt.plot(history.history[<span class="st">"val_"</span> <span class="op">+</span> item], label<span class="op">=</span><span class="st">"val_"</span> <span class="op">+</span> item)</span>
<span id="cb10-49"><a href="#cb10-49" tabindex="-1"></a>    plt.xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb10-50"><a href="#cb10-50" tabindex="-1"></a>    plt.ylabel(item)</span>
<span id="cb10-51"><a href="#cb10-51" tabindex="-1"></a>    plt.title(<span class="st">"Train and Validation </span><span class="sc">{}</span><span class="st"> Over Epochs"</span>.<span class="bu">format</span>(item), fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb10-52"><a href="#cb10-52" tabindex="-1"></a>    plt.legend()</span>
<span id="cb10-53"><a href="#cb10-53" tabindex="-1"></a>    plt.grid()</span>
<span id="cb10-54"><a href="#cb10-54" tabindex="-1"></a>    plt.show()</span>
<span id="cb10-55"><a href="#cb10-55" tabindex="-1"></a></span>
<span id="cb10-56"><a href="#cb10-56" tabindex="-1"></a></span>
<span id="cb10-57"><a href="#cb10-57" tabindex="-1"></a>plot_history(<span class="st">"loss"</span>)</span>
<span id="cb10-58"><a href="#cb10-58" tabindex="-1"></a>plot_history(<span class="st">"top-5-accuracy"</span>)</span></code></pre></div>
<p>After 100 epochs, the ViT model achieves around 55% accuracy and 82%
top-5 accuracy on the test data. These are not competitive results on
the CIFAR-100 dataset, as a ResNet50V2 trained from scratch on the same
data can achieve 67% accuracy.</p>
<p>Note that the state of the art results reported in the <a href="https://arxiv.org/abs/2010.11929" class="external-link">paper</a> are achieved by
pre-training the ViT model using the JFT-300M dataset, then fine-tuning
it on the target dataset. To improve the model quality without
pre-training, you can try to train the model for more epochs, use a
larger number of Transformer layers, resize the input images, change the
patch size, or increase the projection dimensions. Besides, as mentioned
in the paper, the quality of the model is affected not only by
architecture choices, but also by parameters such as the learning rate
schedule, optimizer, weight decay, etc. In practice, it’s recommended to
fine-tune a ViT model that was pre-trained using a large,
high-resolution dataset.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.9000.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
