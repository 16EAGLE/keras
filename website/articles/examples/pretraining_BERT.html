<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Pretraining BERT using Hugging Face Transformers on NSP and MLM.">
<title>Pretraining BERT with Hugging Face Transformers • keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../../deps/Fira_Mono-0.4.8/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><meta property="og:title" content="Pretraining BERT with Hugging Face Transformers">
<meta property="og:description" content="Pretraining BERT using Hugging Face Transformers on NSP and MLM.">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-getting-started">Getting Started</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-getting-started">
    <a class="dropdown-item" href="../../articles/intro_to_keras_for_engineers.html">Introduction to Keras for engineers</a>
    <h6 class="dropdown-header" data-toc-skip>Tutorials</h6>
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
  </div>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Pretraining BERT with Hugging Face Transformers</h1>
                        <h4 data-toc-skip class="author">Sreyan
Ghosh</h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/pretraining_BERT.Rmd" class="external-link"><code>vignettes/examples/pretraining_BERT.Rmd</code></a></small>
      <div class="d-none name"><code>pretraining_BERT.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<div class="section level3">
<h3 id="bert-bidirectional-encoder-representations-from-transformers">BERT (Bidirectional Encoder Representations from Transformers)<a class="anchor" aria-label="anchor" href="#bert-bidirectional-encoder-representations-from-transformers"></a>
</h3>
<p>In the field of computer vision, researchers have repeatedly shown
the value of transfer learning — pretraining a neural network model on a
known task/dataset, for instance ImageNet classification, and then
performing fine-tuning — using the trained neural network as the basis
of a new specific-purpose model. In recent years, researchers have shown
that a similar technique can be useful in many natural language
tasks.</p>
<p>BERT makes use of Transformer, an attention mechanism that learns
contextual relations between words (or subwords) in a text. In its
vanilla form, Transformer includes two separate mechanisms — an encoder
that reads the text input and a decoder that produces a prediction for
the task. Since BERT’s goal is to generate a language model, only the
encoder mechanism is necessary. The detailed workings of Transformer are
described in a paper by Google.</p>
<p>As opposed to directional models, which read the text input
sequentially (left-to-right or right-to-left), the Transformer encoder
reads the entire sequence of words at once. Therefore it is considered
bidirectional, though it would be more accurate to say that it’s
non-directional. This characteristic allows the model to learn the
context of a word based on all of its surroundings (left and right of
the word).</p>
<p>When training language models, a challenge is defining a prediction
goal. Many models predict the next word in a sequence
(e.g. <code>"The child came home from _"</code>), a directional approach
which inherently limits context learning. To overcome this challenge,
BERT uses two training strategies:</p>
</div>
<div class="section level3">
<h3 id="masked-language-modeling-mlm">Masked Language Modeling (MLM)<a class="anchor" aria-label="anchor" href="#masked-language-modeling-mlm"></a>
</h3>
<p>Before feeding word sequences into BERT, 15% of the words in each
sequence are replaced with a <code>[MASK]</code> token. The model then
attempts to predict the original value of the masked words, based on the
context provided by the other, non-masked, words in the sequence.</p>
</div>
<div class="section level3">
<h3 id="next-sentence-prediction-nsp">Next Sentence Prediction (NSP)<a class="anchor" aria-label="anchor" href="#next-sentence-prediction-nsp"></a>
</h3>
<p>In the BERT training process, the model receives pairs of sentences
as input and learns to predict if the second sentence in the pair is the
subsequent sentence in the original document. During training, 50% of
the inputs are a pair in which the second sentence is the subsequent
sentence in the original document, while in the other 50% a random
sentence from the corpus is chosen as the second sentence. The
assumption is that the random sentence will represent a disconnect from
the first sentence.</p>
<p>Though Google provides a pretrained BERT checkpoint for English, you
may often need to either pretrain the model from scratch for a different
language, or do a continued-pretraining to fit the model to a new
domain. In this notebook, we pretrain BERT from scratch optimizing both
MLM and NSP objectves using 🤗 Transformers on the <code>WikiText</code>
English dataset loaded from 🤗 Datasets.</p>
</div>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="section level3">
<h3 id="installing-the-requirements">Installing the requirements<a class="anchor" aria-label="anchor" href="#installing-the-requirements"></a>
</h3>
<p>pip install git+<a href="https://github.com/huggingface/transformers.git" class="external-link uri">https://github.com/huggingface/transformers.git</a> pip
install datasets pip install huggingface-hub pip install nltk</p>
</div>
<div class="section level3">
<h3 id="importing-the-necessary-libraries">Importing the necessary libraries<a class="anchor" aria-label="anchor" href="#importing-the-necessary-libraries"></a>
</h3>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>nltk.download(<span class="st">"punkt"</span>)</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="co"># Set random seed</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>keras.utils.set_random_seed(<span class="dv">42</span>)</span></code></pre></div>
</div>
<div class="section level3">
<h3 id="define-certain-variables">Define certain variables<a class="anchor" aria-label="anchor" href="#define-certain-variables"></a>
</h3>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>TOKENIZER_BATCH_SIZE <span class="op">=</span> <span class="dv">256</span>  <span class="co"># Batch-size to train the tokenizer on</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>TOKENIZER_VOCABULARY <span class="op">=</span> <span class="dv">25000</span>  <span class="co"># Total number of unique subwords the tokenizer can have</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>BLOCK_SIZE <span class="op">=</span> <span class="dv">128</span>  <span class="co"># Maximum number of tokens in an input sample</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>NSP_PROB <span class="op">=</span> <span class="fl">0.50</span>  <span class="co"># Probability that the next sentence is the actual next sentence in NSP</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>SHORT_SEQ_PROB <span class="op">=</span> <span class="fl">0.1</span>  <span class="co"># Probability of generating shorter sequences to minimize the mismatch between pretraining and fine-tuning.</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>MAX_LENGTH <span class="op">=</span> <span class="dv">512</span>  <span class="co"># Maximum number of tokens in an input sample after padding</span></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>MLM_PROB <span class="op">=</span> <span class="fl">0.2</span>  <span class="co"># Probability with which tokens are masked in MLM</span></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>TRAIN_BATCH_SIZE <span class="op">=</span> <span class="dv">2</span>  <span class="co"># Batch-size for pretraining the model on</span></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>MAX_EPOCHS <span class="op">=</span> <span class="dv">1</span>  <span class="co"># Maximum number of epochs to train the model for</span></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>LEARNING_RATE <span class="op">=</span> <span class="fl">1e-4</span>  <span class="co"># Learning rate for training the model</span></span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a>MODEL_CHECKPOINT <span class="op">=</span> <span class="st">"bert-base-cased"</span>  <span class="co"># Name of pretrained model from 🤗 Model Hub</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="load-the-wikitext-dataset">Load the WikiText dataset<a class="anchor" aria-label="anchor" href="#load-the-wikitext-dataset"></a>
</h2>
<p>We now download the <code>WikiText</code> language modeling dataset.
It is a collection of over 100 million tokens extracted from the set of
verified “Good” and “Featured” articles on Wikipedia.</p>
<p>We load the dataset from <a href="https://github.com/huggingface/datasets" class="external-link">🤗 Datasets</a>. For the
purpose of demonstration in this notebook, we work with only the
<code>train</code> split of the dataset. This can be easily done with
the <code>load_dataset</code> function.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(<span class="st">"wikitext"</span>, <span class="st">"wikitext-2-raw-v1"</span>)</span></code></pre></div>
<p>The dataset just has one column which is the raw text, and this is
all we need for pretraining BERT!</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="bu">print</span>(dataset)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="training-a-new-tokenizer">Training a new Tokenizer<a class="anchor" aria-label="anchor" href="#training-a-new-tokenizer"></a>
</h2>
<p>First we train our own tokenizer from scratch on our corpus, so that
can we can use it to train our language model from scratch.</p>
<p>But why would you need to train a tokenizer? That’s because
Transformer models very often use subword tokenization algorithms, and
they need to be trained to identify the parts of words that are often
present in the corpus you are using.</p>
<p>The 🤗 Transformers <code>Tokenizer</code> (as the name indicates)
will tokenize the inputs (including converting the tokens to their
corresponding IDs in the pretrained vocabulary) and put it in a format
the model expects, as well as generate the other inputs that model
requires.</p>
<p>First we make a list of all the raw documents from the
<code>WikiText</code> corpus:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>all_texts <span class="op">=</span> [</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>    doc <span class="cf">for</span> doc <span class="kw">in</span> dataset[<span class="st">"train"</span>][<span class="st">"text"</span>] <span class="cf">if</span> <span class="bu">len</span>(doc) <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> <span class="kw">not</span> doc.startswith(<span class="st">" ="</span>)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>]</span></code></pre></div>
<p>Next we make a <code>batch_iterator</code> function that will aid us
to train our tokenizer.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="kw">def</span> batch_iterator():</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(all_texts), TOKENIZER_BATCH_SIZE):</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>        <span class="cf">yield</span> all_texts[i : i <span class="op">+</span> TOKENIZER_BATCH_SIZE]</span></code></pre></div>
<p>In this notebook, we train a tokenizer with the exact same algorithms
and parameters as an existing one. For instance, we train a new version
of the <code>BERT-CASED</code> tokenzier on <code>Wikitext-2</code>
using the same tokenization algorithm.</p>
<p>First we need to load the tokenizer we want to use as a model:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)</span></code></pre></div>
<p>Now we train our tokenizer using the entire <code>train</code> split
of the <code>Wikitext-2</code> dataset.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>tokenizer <span class="op">=</span> tokenizer.train_new_from_iterator(</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>    batch_iterator(), vocab_size<span class="op">=</span>TOKENIZER_VOCABULARY</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>)</span></code></pre></div>
<p>So now we our done training our new tokenizer! Next we move on to the
data pre-processing steps.</p>
</div>
<div class="section level2">
<h2 id="data-pre-processing">Data Pre-processing<a class="anchor" aria-label="anchor" href="#data-pre-processing"></a>
</h2>
<p>For the sake of demonstrating the workflow, in this notebook we only
take small subsets of the entire WikiText <code>train</code> and
<code>test</code> splits.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>dataset[<span class="st">"train"</span>] <span class="op">=</span> dataset[<span class="st">"train"</span>].select([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>)])</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>dataset[<span class="st">"validation"</span>] <span class="op">=</span> dataset[<span class="st">"validation"</span>].select([i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>)])</span></code></pre></div>
<p>Before we can feed those texts to our model, we need to pre-process
them and get them ready for the task. As mentioned earlier, the BERT
pretraining task includes two tasks in total, the <code>NSP</code> task
and the <code>MLM</code> task. 🤗 Transformers have an easy to implement
<code>collator</code> called the
<code>DataCollatorForLanguageModeling</code>. However, we need to get
the data ready for <code>NSP</code> manually.</p>
<p>Next we write a simple function called the
<code>prepare_train_features</code> that helps us in the pre-processing
and is compatible with 🤗 Datasets. To summarize, our pre-processing
function should:</p>
<ul>
<li>Get the dataset ready for the NSP task by creating pairs of
sentences (A,B), where B either actually follows A, or B is randomly
sampled from somewhere else in the corpus. It should also generate a
corresponding label for each pair, which is 1 if B actually follows A
and 0 if not.</li>
<li>Tokenize the text dataset into it’s corresponding token ids that
will be used for embedding look-up in BERT</li>
<li>Create additional inputs for the model like
<code>token_type_ids</code>, <code>attention_mask</code>, etc.</li>
</ul>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="co"># We define the maximum number of tokens after tokenization that each training sample</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a><span class="co"># will have</span></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>max_num_tokens <span class="op">=</span> BLOCK_SIZE <span class="op">-</span> tokenizer.num_special_tokens_to_add(pair<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a><span class="kw">def</span> prepare_train_features(examples):</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>    <span class="co">"""Function to prepare features for NSP task</span></span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a><span class="co">    Arguments:</span></span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a><span class="co">      examples: A dictionary with 1 key ("text")</span></span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a><span class="co">        text: List of raw documents (str)</span></span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a><span class="co">      examples:  A dictionary with 4 keys</span></span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a><span class="co">        input_ids: List of tokenized, concatnated, and batched</span></span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a><span class="co">          sentences from the individual raw documents (int)</span></span>
<span id="cb10-16"><a href="#cb10-16" tabindex="-1"></a><span class="co">        token_type_ids: List of integers (0 or 1) corresponding</span></span>
<span id="cb10-17"><a href="#cb10-17" tabindex="-1"></a><span class="co">          to: 0 for senetence no. 1 and padding, 1 for sentence</span></span>
<span id="cb10-18"><a href="#cb10-18" tabindex="-1"></a><span class="co">          no. 2</span></span>
<span id="cb10-19"><a href="#cb10-19" tabindex="-1"></a><span class="co">        attention_mask: List of integers (0 or 1) corresponding</span></span>
<span id="cb10-20"><a href="#cb10-20" tabindex="-1"></a><span class="co">          to: 1 for non-padded tokens, 0 for padded</span></span>
<span id="cb10-21"><a href="#cb10-21" tabindex="-1"></a><span class="co">        next_sentence_label: List of integers (0 or 1) corresponding</span></span>
<span id="cb10-22"><a href="#cb10-22" tabindex="-1"></a><span class="co">          to: 1 if the second sentence actually follows the first,</span></span>
<span id="cb10-23"><a href="#cb10-23" tabindex="-1"></a><span class="co">          0 if the senetence is sampled from somewhere else in the corpus</span></span>
<span id="cb10-24"><a href="#cb10-24" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb10-25"><a href="#cb10-25" tabindex="-1"></a></span>
<span id="cb10-26"><a href="#cb10-26" tabindex="-1"></a>    <span class="co"># Remove un-wanted samples from the training set</span></span>
<span id="cb10-27"><a href="#cb10-27" tabindex="-1"></a>    examples[<span class="st">"document"</span>] <span class="op">=</span> [</span>
<span id="cb10-28"><a href="#cb10-28" tabindex="-1"></a>        d.strip() <span class="cf">for</span> d <span class="kw">in</span> examples[<span class="st">"text"</span>] <span class="cf">if</span> <span class="bu">len</span>(d) <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> <span class="kw">not</span> d.startswith(<span class="st">" ="</span>)</span>
<span id="cb10-29"><a href="#cb10-29" tabindex="-1"></a>    ]</span>
<span id="cb10-30"><a href="#cb10-30" tabindex="-1"></a>    <span class="co"># Split the documents from the dataset into it's individual sentences</span></span>
<span id="cb10-31"><a href="#cb10-31" tabindex="-1"></a>    examples[<span class="st">"sentences"</span>] <span class="op">=</span> [</span>
<span id="cb10-32"><a href="#cb10-32" tabindex="-1"></a>        nltk.tokenize.sent_tokenize(document) <span class="cf">for</span> document <span class="kw">in</span> examples[<span class="st">"document"</span>]</span>
<span id="cb10-33"><a href="#cb10-33" tabindex="-1"></a>    ]</span>
<span id="cb10-34"><a href="#cb10-34" tabindex="-1"></a>    <span class="co"># Convert the tokens into ids using the trained tokenizer</span></span>
<span id="cb10-35"><a href="#cb10-35" tabindex="-1"></a>    examples[<span class="st">"tokenized_sentences"</span>] <span class="op">=</span> [</span>
<span id="cb10-36"><a href="#cb10-36" tabindex="-1"></a>        [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent)) <span class="cf">for</span> sent <span class="kw">in</span> doc]</span>
<span id="cb10-37"><a href="#cb10-37" tabindex="-1"></a>        <span class="cf">for</span> doc <span class="kw">in</span> examples[<span class="st">"sentences"</span>]</span>
<span id="cb10-38"><a href="#cb10-38" tabindex="-1"></a>    ]</span>
<span id="cb10-39"><a href="#cb10-39" tabindex="-1"></a></span>
<span id="cb10-40"><a href="#cb10-40" tabindex="-1"></a>    <span class="co"># Define the outputs</span></span>
<span id="cb10-41"><a href="#cb10-41" tabindex="-1"></a>    examples[<span class="st">"input_ids"</span>] <span class="op">=</span> []</span>
<span id="cb10-42"><a href="#cb10-42" tabindex="-1"></a>    examples[<span class="st">"token_type_ids"</span>] <span class="op">=</span> []</span>
<span id="cb10-43"><a href="#cb10-43" tabindex="-1"></a>    examples[<span class="st">"attention_mask"</span>] <span class="op">=</span> []</span>
<span id="cb10-44"><a href="#cb10-44" tabindex="-1"></a>    examples[<span class="st">"next_sentence_label"</span>] <span class="op">=</span> []</span>
<span id="cb10-45"><a href="#cb10-45" tabindex="-1"></a></span>
<span id="cb10-46"><a href="#cb10-46" tabindex="-1"></a>    <span class="cf">for</span> doc_index, document <span class="kw">in</span> <span class="bu">enumerate</span>(examples[<span class="st">"tokenized_sentences"</span>]):</span>
<span id="cb10-47"><a href="#cb10-47" tabindex="-1"></a>        current_chunk <span class="op">=</span> []  <span class="co"># a buffer stored current working segments</span></span>
<span id="cb10-48"><a href="#cb10-48" tabindex="-1"></a>        current_length <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-49"><a href="#cb10-49" tabindex="-1"></a>        i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-50"><a href="#cb10-50" tabindex="-1"></a></span>
<span id="cb10-51"><a href="#cb10-51" tabindex="-1"></a>        <span class="co"># We *usually* want to fill up the entire sequence since we are padding</span></span>
<span id="cb10-52"><a href="#cb10-52" tabindex="-1"></a>        <span class="co"># to `block_size` anyways, so short sequences are generally wasted</span></span>
<span id="cb10-53"><a href="#cb10-53" tabindex="-1"></a>        <span class="co"># computation. However, we *sometimes*</span></span>
<span id="cb10-54"><a href="#cb10-54" tabindex="-1"></a>        <span class="co"># (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter</span></span>
<span id="cb10-55"><a href="#cb10-55" tabindex="-1"></a>        <span class="co"># sequences to minimize the mismatch between pretraining and fine-tuning.</span></span>
<span id="cb10-56"><a href="#cb10-56" tabindex="-1"></a>        <span class="co"># The `target_seq_length` is just a rough target however, whereas</span></span>
<span id="cb10-57"><a href="#cb10-57" tabindex="-1"></a>        <span class="co"># `block_size` is a hard limit.</span></span>
<span id="cb10-58"><a href="#cb10-58" tabindex="-1"></a>        target_seq_length <span class="op">=</span> max_num_tokens</span>
<span id="cb10-59"><a href="#cb10-59" tabindex="-1"></a></span>
<span id="cb10-60"><a href="#cb10-60" tabindex="-1"></a>        <span class="cf">if</span> random.random() <span class="op">&lt;</span> SHORT_SEQ_PROB:</span>
<span id="cb10-61"><a href="#cb10-61" tabindex="-1"></a>            target_seq_length <span class="op">=</span> random.randint(<span class="dv">2</span>, max_num_tokens)</span>
<span id="cb10-62"><a href="#cb10-62" tabindex="-1"></a></span>
<span id="cb10-63"><a href="#cb10-63" tabindex="-1"></a>        <span class="cf">while</span> i <span class="op">&lt;</span> <span class="bu">len</span>(document):</span>
<span id="cb10-64"><a href="#cb10-64" tabindex="-1"></a>            segment <span class="op">=</span> document[i]</span>
<span id="cb10-65"><a href="#cb10-65" tabindex="-1"></a>            current_chunk.append(segment)</span>
<span id="cb10-66"><a href="#cb10-66" tabindex="-1"></a>            current_length <span class="op">+=</span> <span class="bu">len</span>(segment)</span>
<span id="cb10-67"><a href="#cb10-67" tabindex="-1"></a>            <span class="cf">if</span> i <span class="op">==</span> <span class="bu">len</span>(document) <span class="op">-</span> <span class="dv">1</span> <span class="kw">or</span> current_length <span class="op">&gt;=</span> target_seq_length:</span>
<span id="cb10-68"><a href="#cb10-68" tabindex="-1"></a>                <span class="cf">if</span> current_chunk:</span>
<span id="cb10-69"><a href="#cb10-69" tabindex="-1"></a>                    <span class="co"># `a_end` is how many segments from `current_chunk` go into the `A`</span></span>
<span id="cb10-70"><a href="#cb10-70" tabindex="-1"></a>                    <span class="co"># (first) sentence.</span></span>
<span id="cb10-71"><a href="#cb10-71" tabindex="-1"></a>                    a_end <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb10-72"><a href="#cb10-72" tabindex="-1"></a>                    <span class="cf">if</span> <span class="bu">len</span>(current_chunk) <span class="op">&gt;=</span> <span class="dv">2</span>:</span>
<span id="cb10-73"><a href="#cb10-73" tabindex="-1"></a>                        a_end <span class="op">=</span> random.randint(<span class="dv">1</span>, <span class="bu">len</span>(current_chunk) <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb10-74"><a href="#cb10-74" tabindex="-1"></a></span>
<span id="cb10-75"><a href="#cb10-75" tabindex="-1"></a>                    tokens_a <span class="op">=</span> []</span>
<span id="cb10-76"><a href="#cb10-76" tabindex="-1"></a>                    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(a_end):</span>
<span id="cb10-77"><a href="#cb10-77" tabindex="-1"></a>                        tokens_a.extend(current_chunk[j])</span>
<span id="cb10-78"><a href="#cb10-78" tabindex="-1"></a></span>
<span id="cb10-79"><a href="#cb10-79" tabindex="-1"></a>                    tokens_b <span class="op">=</span> []</span>
<span id="cb10-80"><a href="#cb10-80" tabindex="-1"></a></span>
<span id="cb10-81"><a href="#cb10-81" tabindex="-1"></a>                    <span class="cf">if</span> <span class="bu">len</span>(current_chunk) <span class="op">==</span> <span class="dv">1</span> <span class="kw">or</span> random.random() <span class="op">&lt;</span> NSP_PROB:</span>
<span id="cb10-82"><a href="#cb10-82" tabindex="-1"></a>                        is_random_next <span class="op">=</span> <span class="va">True</span></span>
<span id="cb10-83"><a href="#cb10-83" tabindex="-1"></a>                        target_b_length <span class="op">=</span> target_seq_length <span class="op">-</span> <span class="bu">len</span>(tokens_a)</span>
<span id="cb10-84"><a href="#cb10-84" tabindex="-1"></a></span>
<span id="cb10-85"><a href="#cb10-85" tabindex="-1"></a>                        <span class="co"># This should rarely go for more than one iteration for large</span></span>
<span id="cb10-86"><a href="#cb10-86" tabindex="-1"></a>                        <span class="co"># corpora. However, just to be careful, we try to make sure that</span></span>
<span id="cb10-87"><a href="#cb10-87" tabindex="-1"></a>                        <span class="co"># the random document is not the same as the document</span></span>
<span id="cb10-88"><a href="#cb10-88" tabindex="-1"></a>                        <span class="co"># we're processing.</span></span>
<span id="cb10-89"><a href="#cb10-89" tabindex="-1"></a>                        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb10-90"><a href="#cb10-90" tabindex="-1"></a>                            random_document_index <span class="op">=</span> random.randint(</span>
<span id="cb10-91"><a href="#cb10-91" tabindex="-1"></a>                                <span class="dv">0</span>, <span class="bu">len</span>(examples[<span class="st">"tokenized_sentences"</span>]) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb10-92"><a href="#cb10-92" tabindex="-1"></a>                            )</span>
<span id="cb10-93"><a href="#cb10-93" tabindex="-1"></a>                            <span class="cf">if</span> random_document_index <span class="op">!=</span> doc_index:</span>
<span id="cb10-94"><a href="#cb10-94" tabindex="-1"></a>                                <span class="cf">break</span></span>
<span id="cb10-95"><a href="#cb10-95" tabindex="-1"></a></span>
<span id="cb10-96"><a href="#cb10-96" tabindex="-1"></a>                        random_document <span class="op">=</span> examples[<span class="st">"tokenized_sentences"</span>][</span>
<span id="cb10-97"><a href="#cb10-97" tabindex="-1"></a>                            random_document_index</span>
<span id="cb10-98"><a href="#cb10-98" tabindex="-1"></a>                        ]</span>
<span id="cb10-99"><a href="#cb10-99" tabindex="-1"></a>                        random_start <span class="op">=</span> random.randint(<span class="dv">0</span>, <span class="bu">len</span>(random_document) <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb10-100"><a href="#cb10-100" tabindex="-1"></a>                        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(random_start, <span class="bu">len</span>(random_document)):</span>
<span id="cb10-101"><a href="#cb10-101" tabindex="-1"></a>                            tokens_b.extend(random_document[j])</span>
<span id="cb10-102"><a href="#cb10-102" tabindex="-1"></a>                            <span class="cf">if</span> <span class="bu">len</span>(tokens_b) <span class="op">&gt;=</span> target_b_length:</span>
<span id="cb10-103"><a href="#cb10-103" tabindex="-1"></a>                                <span class="cf">break</span></span>
<span id="cb10-104"><a href="#cb10-104" tabindex="-1"></a>                        <span class="co"># We didn't actually use these segments so we "put them back" so</span></span>
<span id="cb10-105"><a href="#cb10-105" tabindex="-1"></a>                        <span class="co"># they don't go to waste.</span></span>
<span id="cb10-106"><a href="#cb10-106" tabindex="-1"></a>                        num_unused_segments <span class="op">=</span> <span class="bu">len</span>(current_chunk) <span class="op">-</span> a_end</span>
<span id="cb10-107"><a href="#cb10-107" tabindex="-1"></a>                        i <span class="op">-=</span> num_unused_segments</span>
<span id="cb10-108"><a href="#cb10-108" tabindex="-1"></a>                    <span class="cf">else</span>:</span>
<span id="cb10-109"><a href="#cb10-109" tabindex="-1"></a>                        is_random_next <span class="op">=</span> <span class="va">False</span></span>
<span id="cb10-110"><a href="#cb10-110" tabindex="-1"></a>                        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(a_end, <span class="bu">len</span>(current_chunk)):</span>
<span id="cb10-111"><a href="#cb10-111" tabindex="-1"></a>                            tokens_b.extend(current_chunk[j])</span>
<span id="cb10-112"><a href="#cb10-112" tabindex="-1"></a></span>
<span id="cb10-113"><a href="#cb10-113" tabindex="-1"></a>                    input_ids <span class="op">=</span> tokenizer.build_inputs_with_special_tokens(</span>
<span id="cb10-114"><a href="#cb10-114" tabindex="-1"></a>                        tokens_a, tokens_b</span>
<span id="cb10-115"><a href="#cb10-115" tabindex="-1"></a>                    )</span>
<span id="cb10-116"><a href="#cb10-116" tabindex="-1"></a>                    <span class="co"># add token type ids, 0 for sentence a, 1 for sentence b</span></span>
<span id="cb10-117"><a href="#cb10-117" tabindex="-1"></a>                    token_type_ids <span class="op">=</span> tokenizer.create_token_type_ids_from_sequences(</span>
<span id="cb10-118"><a href="#cb10-118" tabindex="-1"></a>                        tokens_a, tokens_b</span>
<span id="cb10-119"><a href="#cb10-119" tabindex="-1"></a>                    )</span>
<span id="cb10-120"><a href="#cb10-120" tabindex="-1"></a></span>
<span id="cb10-121"><a href="#cb10-121" tabindex="-1"></a>                    padded <span class="op">=</span> tokenizer.pad(</span>
<span id="cb10-122"><a href="#cb10-122" tabindex="-1"></a>                        {<span class="st">"input_ids"</span>: input_ids, <span class="st">"token_type_ids"</span>: token_type_ids},</span>
<span id="cb10-123"><a href="#cb10-123" tabindex="-1"></a>                        padding<span class="op">=</span><span class="st">"max_length"</span>,</span>
<span id="cb10-124"><a href="#cb10-124" tabindex="-1"></a>                        max_length<span class="op">=</span>MAX_LENGTH,</span>
<span id="cb10-125"><a href="#cb10-125" tabindex="-1"></a>                    )</span>
<span id="cb10-126"><a href="#cb10-126" tabindex="-1"></a></span>
<span id="cb10-127"><a href="#cb10-127" tabindex="-1"></a>                    examples[<span class="st">"input_ids"</span>].append(padded[<span class="st">"input_ids"</span>])</span>
<span id="cb10-128"><a href="#cb10-128" tabindex="-1"></a>                    examples[<span class="st">"token_type_ids"</span>].append(padded[<span class="st">"token_type_ids"</span>])</span>
<span id="cb10-129"><a href="#cb10-129" tabindex="-1"></a>                    examples[<span class="st">"attention_mask"</span>].append(padded[<span class="st">"attention_mask"</span>])</span>
<span id="cb10-130"><a href="#cb10-130" tabindex="-1"></a>                    examples[<span class="st">"next_sentence_label"</span>].append(<span class="dv">1</span> <span class="cf">if</span> is_random_next <span class="cf">else</span> <span class="dv">0</span>)</span>
<span id="cb10-131"><a href="#cb10-131" tabindex="-1"></a>                    current_chunk <span class="op">=</span> []</span>
<span id="cb10-132"><a href="#cb10-132" tabindex="-1"></a>                    current_length <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-133"><a href="#cb10-133" tabindex="-1"></a>            i <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb10-134"><a href="#cb10-134" tabindex="-1"></a></span>
<span id="cb10-135"><a href="#cb10-135" tabindex="-1"></a>    <span class="co"># We delete all the un-necessary columns from our dataset</span></span>
<span id="cb10-136"><a href="#cb10-136" tabindex="-1"></a>    <span class="kw">del</span> examples[<span class="st">"document"</span>]</span>
<span id="cb10-137"><a href="#cb10-137" tabindex="-1"></a>    <span class="kw">del</span> examples[<span class="st">"sentences"</span>]</span>
<span id="cb10-138"><a href="#cb10-138" tabindex="-1"></a>    <span class="kw">del</span> examples[<span class="st">"text"</span>]</span>
<span id="cb10-139"><a href="#cb10-139" tabindex="-1"></a>    <span class="kw">del</span> examples[<span class="st">"tokenized_sentences"</span>]</span>
<span id="cb10-140"><a href="#cb10-140" tabindex="-1"></a></span>
<span id="cb10-141"><a href="#cb10-141" tabindex="-1"></a>    <span class="cf">return</span> examples</span>
<span id="cb10-142"><a href="#cb10-142" tabindex="-1"></a></span>
<span id="cb10-143"><a href="#cb10-143" tabindex="-1"></a></span>
<span id="cb10-144"><a href="#cb10-144" tabindex="-1"></a>tokenized_dataset <span class="op">=</span> dataset.<span class="bu">map</span>(</span>
<span id="cb10-145"><a href="#cb10-145" tabindex="-1"></a>    prepare_train_features,</span>
<span id="cb10-146"><a href="#cb10-146" tabindex="-1"></a>    batched<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb10-147"><a href="#cb10-147" tabindex="-1"></a>    remove_columns<span class="op">=</span>[<span class="st">"text"</span>],</span>
<span id="cb10-148"><a href="#cb10-148" tabindex="-1"></a>    num_proc<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb10-149"><a href="#cb10-149" tabindex="-1"></a>)</span></code></pre></div>
<p>For MLM we are going to use the same preprocessing as before for our
dataset with one additional step: we randomly mask some tokens (by
replacing them by [MASK]) and the labels will be adjusted to only
include the masked tokens (we don’t have to predict the non-masked
tokens). If you use a tokenizer you trained yourself, make sure the
[MASK] token is among the special tokens you passed during training!</p>
<p>To get the data ready for MLM, we simply use the
<code>collator</code> called the
<code>DataCollatorForLanguageModeling</code> provided by the 🤗
Transformers library on our dataset that is already ready for the NSP
task. The <code>collator</code> expects certain parameters. We use the
default ones from the original BERT paper in this notebook. The
<code>return_tensors='tf'</code> ensures that we get
<code>tf.Tensor</code> objects back.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> DataCollatorForLanguageModeling</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>collater <span class="op">=</span> DataCollatorForLanguageModeling(</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>    tokenizer<span class="op">=</span>tokenizer, mlm<span class="op">=</span><span class="va">True</span>, mlm_probability<span class="op">=</span>MLM_PROB, return_tensors<span class="op">=</span><span class="st">"tf"</span></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>)</span></code></pre></div>
<p>Next we define our training set with which we train our model. Again,
🤗 Datasets provides us with the <code>to_tf_dataset</code> method which
will help us integrate our dataset with the <code>collator</code>
defined above. The method expects certain parameters:</p>
<ul>
<li>
<strong>columns</strong>: the columns which will serve as our
independant variables</li>
<li>
<strong>label_cols</strong>: the columns which will serve as our
labels or dependant variables</li>
<li>
<strong>batch_size</strong>: our batch size for training</li>
<li>
<strong>shuffle</strong>: whether we want to shuffle our training
dataset</li>
<li>
<strong>collate_fn</strong>: our collator function</li>
</ul>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>train <span class="op">=</span> tokenized_dataset[<span class="st">"train"</span>].to_tf_dataset(</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>    columns<span class="op">=</span>[<span class="st">"input_ids"</span>, <span class="st">"token_type_ids"</span>, <span class="st">"attention_mask"</span>],</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>    label_cols<span class="op">=</span>[<span class="st">"labels"</span>, <span class="st">"next_sentence_label"</span>],</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>    batch_size<span class="op">=</span>TRAIN_BATCH_SIZE,</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>    collate_fn<span class="op">=</span>collater,</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a>)</span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a>validation <span class="op">=</span> tokenized_dataset[<span class="st">"validation"</span>].to_tf_dataset(</span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a>    columns<span class="op">=</span>[<span class="st">"input_ids"</span>, <span class="st">"token_type_ids"</span>, <span class="st">"attention_mask"</span>],</span>
<span id="cb12-11"><a href="#cb12-11" tabindex="-1"></a>    label_cols<span class="op">=</span>[<span class="st">"labels"</span>, <span class="st">"next_sentence_label"</span>],</span>
<span id="cb12-12"><a href="#cb12-12" tabindex="-1"></a>    batch_size<span class="op">=</span>TRAIN_BATCH_SIZE,</span>
<span id="cb12-13"><a href="#cb12-13" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-14"><a href="#cb12-14" tabindex="-1"></a>    collate_fn<span class="op">=</span>collater,</span>
<span id="cb12-15"><a href="#cb12-15" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="defining-the-model">Defining the model<a class="anchor" aria-label="anchor" href="#defining-the-model"></a>
</h2>
<p>To define our model, first we need to define a config which will help
us define certain parameters of our model architecture. This includes
parameters like number of transformer layers, number of attention heads,
hidden dimension, etc. For this notebook, we try to define the exact
config defined in the original BERT paper.</p>
<p>We can easily achieve this using the <code>BertConfig</code> class
from the 🤗 Transformers library. The <code>from_pretrained()</code>
method expects the name of a model. Here we define the simplest model
with which we also trained our model, i.e.,
<code>bert-base-cased</code>.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertConfig</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>config <span class="op">=</span> BertConfig.from_pretrained(MODEL_CHECKPOINT)</span></code></pre></div>
<p>For defining our model we use the <code>TFBertForPreTraining</code>
class from the 🤗 Transformers library. This class internally handles
everything starting from defining our model, to unpacking our inputs and
calculating the loss. So we need not do anything ourselves except
defining the model with the correct <code>config</code> we want!</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> TFBertForPreTraining</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>model <span class="op">=</span> TFBertForPreTraining(config)</span></code></pre></div>
<p>Now we define our optimizer and compile the model. The loss
calculation is handled internally and so we need not worry about
that!</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="im">from</span> keras.optimizers <span class="im">import</span> Adam</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span>Adam(learning_rate<span class="op">=</span>LEARNING_RATE))</span></code></pre></div>
<p>Finally all steps are done and now we can start training our
model!</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>model.fit(train, validation_data<span class="op">=</span>validation, epochs<span class="op">=</span>MAX_EPOCHS)</span></code></pre></div>
<p>Our model has now been trained! We suggest to please train the model
on the complete dataset for atleast 50 epochs for decent performance.
The pretrained model now acts as a language model and is meant to be
fine-tuned on a downstream task. Thus it can now be fine-tuned on any
downstream task like Question Answering, Text Classification etc.!</p>
<p>Now you can push this model to 🤗 Model Hub and also share it with
with all your friends, family, favorite pets: they can all load it with
the identifier <code>"your-username/the-name-you-picked"</code> so for
instance:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>model.push_to_hub(<span class="st">"pretrained-bert"</span>, organization<span class="op">=</span><span class="st">"keras-io"</span>)</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>tokenizer.push_to_hub(<span class="st">"pretrained-bert"</span>, organization<span class="op">=</span><span class="st">"keras-io"</span>)</span></code></pre></div>
<p>And after you push your model this is how you can load it in the
future!</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> TFBertForPreTraining</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>model <span class="op">=</span> TFBertForPreTraining.from_pretrained(<span class="st">"your-username/my-awesome-model"</span>)</span></code></pre></div>
<p>or, since it’s a pretrained model and you would generally use it for
fine-tuning on a downstream task, you can also load it for some other
task like:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> TFBertForSequenceClassification</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a>model <span class="op">=</span> TFBertForSequenceClassification.from_pretrained(<span class="st">"your-username/my-awesome-model"</span>)</span></code></pre></div>
<p>In this case, the pretraining head will be dropped and the model will
just be initialized with the transformer layers. A new task-specific
head will be added with random weights.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
