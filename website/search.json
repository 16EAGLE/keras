[{"path":"https://keras.posit.co/articles/custom_train_step_in_jax.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Customizing what happens in `fit()` with JAX","text":"’re supervised learning, can use fit() everything works smoothly. need take control every little detail, can write training loop entirely scratch. need custom training algorithm, still want benefit convenient features fit(), callbacks, built-distribution support, step fusing? core principle Keras progressive disclosure complexity. always able get lower-level workflows gradual way. shouldn’t fall cliff high-level functionality doesn’t exactly match use case. able gain control small details retaining commensurate amount high-level convenience. need customize fit() , override training step function Model class. function called fit() every batch data. able call fit() usual – running learning algorithm. Note pattern prevent building models Functional API. can whether ’re building Sequential models, Functional API models, subclassed models. Let’s see works.","code":""},{"path":"https://keras.posit.co/articles/custom_train_step_in_jax.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Customizing what happens in `fit()` with JAX","text":"","code":"import os  # This guide can only be run with the JAX backend. os.environ[\"KERAS_BACKEND\"] = \"jax\"  import jax import keras import numpy as np"},{"path":"https://keras.posit.co/articles/custom_train_step_in_jax.html","id":"a-first-simple-example","dir":"Articles","previous_headings":"","what":"A first simple example","title":"Customizing what happens in `fit()` with JAX","text":"Let’s start simple example: create new class subclasses keras.Model. implement fully-stateless compute_loss_and_updates() method compute loss well updated values non-trainable variables model. Internally, calls stateless_call() built-compute_loss(). implement fully-stateless train_step() method compute current metric values (including loss) well updated values trainable variables, optimizer variables, metric variables. Note can also take account sample_weight argument : Unpacking data x, y, sample_weight = data Passing sample_weight compute_loss() Passing sample_weight alongside y y_pred metrics stateless_update_state() Let’s try :","code":"class CustomModel(keras.Model):     def compute_loss_and_updates(         self,         trainable_variables,         non_trainable_variables,         x,         y,         training=False,     ):         y_pred, non_trainable_variables = self.stateless_call(             trainable_variables,             non_trainable_variables,             x,             training=training,         )         loss = self.compute_loss(x, y, y_pred)         return loss, (y_pred, non_trainable_variables)      def train_step(self, state, data):         (             trainable_variables,             non_trainable_variables,             optimizer_variables,             metrics_variables,         ) = state         x, y = data          # Get the gradient function.         grad_fn = jax.value_and_grad(             self.compute_loss_and_updates, has_aux=True         )          # Compute the gradients.         (loss, (y_pred, non_trainable_variables)), grads = grad_fn(             trainable_variables,             non_trainable_variables,             x,             y,             training=True,         )          # Update trainable variables and optimizer variables.         (             trainable_variables,             optimizer_variables,         ) = self.optimizer.stateless_apply(             optimizer_variables, grads, trainable_variables         )          # Update metrics.         new_metrics_vars = []         for metric in self.metrics:             this_metric_vars = metrics_variables[                 len(new_metrics_vars) : len(new_metrics_vars)                 + len(metric.variables)             ]             if metric.name == \"loss\":                 this_metric_vars = metric.stateless_update_state(                     this_metric_vars, loss                 )             else:                 this_metric_vars = metric.stateless_update_state(                     this_metric_vars, y, y_pred                 )             logs = metric.stateless_result(this_metric_vars)             new_metrics_vars += this_metric_vars          # Return metric logs and updated state variables.         state = (             trainable_variables,             non_trainable_variables,             optimizer_variables,             new_metrics_vars,         )         return logs, state # Construct and compile an instance of CustomModel inputs = keras.Input(shape=(32,)) outputs = keras.layers.Dense(1)(inputs) model = CustomModel(inputs, outputs) model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])  # Just use `fit` as usual x = np.random.random((1000, 32)) y = np.random.random((1000, 1)) model.fit(x, y, epochs=3)"},{"path":"https://keras.posit.co/articles/custom_train_step_in_jax.html","id":"going-lower-level","dir":"Articles","previous_headings":"","what":"Going lower-level","title":"Customizing what happens in `fit()` with JAX","text":"Naturally, just skip passing loss function compile(), instead everything manually train_step. Likewise metrics. ’s lower-level example, uses compile() configure optimizer:","code":"class CustomModel(keras.Model):     def __init__(self, *args, **kwargs):         super().__init__(*args, **kwargs)         self.loss_tracker = keras.metrics.Mean(name=\"loss\")         self.mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")         self.loss_fn = keras.losses.MeanSquaredError()      def compute_loss_and_updates(         self,         trainable_variables,         non_trainable_variables,         x,         y,         training=False,     ):         y_pred, non_trainable_variables = self.stateless_call(             trainable_variables,             non_trainable_variables,             x,             training=training,         )         loss = self.loss_fn(y, y_pred)         return loss, (y_pred, non_trainable_variables)      def train_step(self, state, data):         (             trainable_variables,             non_trainable_variables,             optimizer_variables,             metrics_variables,         ) = state         x, y = data          # Get the gradient function.         grad_fn = jax.value_and_grad(             self.compute_loss_and_updates, has_aux=True         )          # Compute the gradients.         (loss, (y_pred, non_trainable_variables)), grads = grad_fn(             trainable_variables,             non_trainable_variables,             x,             y,             training=True,         )          # Update trainable variables and optimizer variables.         (             trainable_variables,             optimizer_variables,         ) = self.optimizer.stateless_apply(             optimizer_variables, grads, trainable_variables         )          # Update metrics.         loss_tracker_vars = metrics_variables[             : len(self.loss_tracker.variables)         ]         mae_metric_vars = metrics_variables[len(self.loss_tracker.variables) :]          loss_tracker_vars = self.loss_tracker.stateless_update_state(             loss_tracker_vars, loss         )         mae_metric_vars = self.mae_metric.stateless_update_state(             mae_metric_vars, y, y_pred         )          logs = {}         logs[self.loss_tracker.name] = self.loss_tracker.stateless_result(             loss_tracker_vars         )         logs[self.mae_metric.name] = self.mae_metric.stateless_result(             mae_metric_vars         )          new_metrics_vars = loss_tracker_vars + mae_metric_vars          # Return metric logs and updated state variables.         state = (             trainable_variables,             non_trainable_variables,             optimizer_variables,             new_metrics_vars,         )         return logs, state      @property     def metrics(self):         # We list our `Metric` objects here so that `reset_states()` can be         # called automatically at the start of each epoch         # or at the start of `evaluate()`.         return [self.loss_tracker, self.mae_metric]   # Construct an instance of CustomModel inputs = keras.Input(shape=(32,)) outputs = keras.layers.Dense(1)(inputs) model = CustomModel(inputs, outputs)  # We don't passs a loss or metrics here. model.compile(optimizer=\"adam\")  # Just use `fit` as usual -- you can use callbacks, etc. x = np.random.random((1000, 32)) y = np.random.random((1000, 1)) model.fit(x, y, epochs=5)"},{"path":"https://keras.posit.co/articles/custom_train_step_in_jax.html","id":"providing-your-own-evaluation-step","dir":"Articles","previous_headings":"","what":"Providing your own evaluation step","title":"Customizing what happens in `fit()` with JAX","text":"want calls model.evaluate()? override test_step exactly way. ’s looks like: ’s !","code":"class CustomModel(keras.Model):     def test_step(self, state, data):         # Unpack the data.         x, y = data         (             trainable_variables,             non_trainable_variables,             metrics_variables,         ) = state          # Compute predictions and loss.         y_pred, non_trainable_variables = self.stateless_call(             trainable_variables,             non_trainable_variables,             x,             training=False,         )         loss = self.compute_loss(x, y, y_pred)          # Update metrics.         new_metrics_vars = []         for metric in self.metrics:             this_metric_vars = metrics_variables[                 len(new_metrics_vars) : len(new_metrics_vars)                 + len(metric.variables)             ]             if metric.name == \"loss\":                 this_metric_vars = metric.stateless_update_state(                     this_metric_vars, loss                 )             else:                 this_metric_vars = metric.stateless_update_state(                     this_metric_vars, y, y_pred                 )             logs = metric.stateless_result(this_metric_vars)             new_metrics_vars += this_metric_vars          # Return metric logs and updated state variables.         state = (             trainable_variables,             non_trainable_variables,             new_metrics_vars,         )         return logs, state   # Construct an instance of CustomModel inputs = keras.Input(shape=(32,)) outputs = keras.layers.Dense(1)(inputs) model = CustomModel(inputs, outputs) model.compile(loss=\"mse\", metrics=[\"mae\"])  # Evaluate with our custom test_step x = np.random.random((1000, 32)) y = np.random.random((1000, 1)) model.evaluate(x, y)"},{"path":"https://keras.posit.co/articles/custom_train_step_in_tensorflow.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Customizing what happens in `fit()` with TensorFlow","text":"’re supervised learning, can use fit() everything works smoothly. need take control every little detail, can write training loop entirely scratch. need custom training algorithm, still want benefit convenient features fit(), callbacks, built-distribution support, step fusing? core principle Keras progressive disclosure complexity. always able get lower-level workflows gradual way. shouldn’t fall cliff high-level functionality doesn’t exactly match use case. able gain control small details retaining commensurate amount high-level convenience. need customize fit() , override training step function Model class. function called fit() every batch data. able call fit() usual – running learning algorithm. Note pattern prevent building models Functional API. can whether ’re building Sequential models, Functional API models, subclassed models. Let’s see works.","code":""},{"path":"https://keras.posit.co/articles/custom_train_step_in_tensorflow.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Customizing what happens in `fit()` with TensorFlow","text":"","code":"library(tensorflow) ## ## Attaching package: 'tensorflow' ## The following objects are masked from 'package:keras': ## ##     set_random_seed, shape library(keras3) library(reticulate)"},{"path":"https://keras.posit.co/articles/custom_train_step_in_tensorflow.html","id":"a-first-simple-example","dir":"Articles","previous_headings":"","what":"A first simple example","title":"Customizing what happens in `fit()` with TensorFlow","text":"Let’s start simple example: create new class subclasses Model. just override method train_step(self, data). return dictionary mapping metric names (including loss) current value. input argument data gets passed fit training data: pass arrays, calling fit(x, y, ...), data list (x, y) pass tf_dataset, calling fit(dataset, ...), data gets yielded dataset batch. body train_step() method, implement regular training update, similar already familiar . Importantly, compute loss via self.compute_loss(), wraps loss(es) function(s) passed compile(). Similarly, call metric$update_state(y, y_pred) metrics self$metrics, update state metrics passed compile(), query results self$metrics end retrieve current value. Let’s try :","code":"CustomModel <- new_model_class(   \"CustomModel\",   train_step = function(data) {     c(x, y, sample_weight) %<-% unpack_x_y_sample_weight(data)      with(tf$GradientTape() %as% tape, {       y_pred <- self(x, training = TRUE)       loss <- self$compute_loss(y = y, y_pred=y_pred, sample_weight = sample_weight)     })      # Compute gradients     trainable_vars <- self$trainable_variables     gradients <- tape$gradient(loss, trainable_vars)      # Update weights     self$optimizer$apply(gradients, trainable_vars)      # Update metrics (includes the metric that tracks the loss)     for (metric in self$metrics) {       if (metric$name == \"loss\")         metric$update_state(loss)       else         metric$update_state(y, y_pred)     }      # Return a dict mapping metric names to current value     metrics <- lapply(self$metrics, function(m) m$result())     metrics <- setNames(metrics, sapply(self$metrics, function(m) m$name))     metrics   } ) # Construct and compile an instance of CustomModel inputs <- layer_input(shape = 32) outputs <- layer_dense(inputs, 1) model <- CustomModel(inputs, outputs) model %>% compile(optimizer=\"adam\", loss=\"mse\", metrics=\"mae\")  # Just use `fit` as usual x <- random_normal(c(1000, 32)) y <- random_normal(c(1000, 1)) model %>% fit(x, y, epochs=3) ## Epoch 1/3 ## 32/32 - 0s - 8ms/step - loss: 2.9118 - mae: 1.3597 ## Epoch 2/3 ## 32/32 - 0s - 1ms/step - loss: 2.6026 - mae: 1.2856 ## Epoch 3/3 ## 32/32 - 0s - 763us/step - loss: 2.3378 - mae: 1.2193"},{"path":"https://keras.posit.co/articles/custom_train_step_in_tensorflow.html","id":"going-lower-level","dir":"Articles","previous_headings":"","what":"Going lower-level","title":"Customizing what happens in `fit()` with TensorFlow","text":"Naturally, just skip passing loss function compile(), instead everything manually train_step. Likewise metrics. ’s lower-level example, uses compile() configure optimizer: start creating Metric instances track loss MAE score (__init__()). implement custom train_step() updates state metrics (calling update_state() ), query (via result()) return current average value, displayed progress bar pass callback. Note need call reset_states() metrics epoch! Otherwise calling result() return average since start training, whereas usually work per-epoch averages. Thankfully, framework can us: just list metric want reset metrics property model. model call reset_states() object listed beginning fit() epoch beginning call evaluate().","code":"CustomModel <- new_model_class(   \"CustomModel\",   initialize = function(...) {     super$initialize(...)     self$loss_tracker <- metric_mean(name = \"loss\")     self$mae_metric <- metric_mean_absolute_error(name = \"mae\")     self$loss_fn <- loss_mean_squared_error()   },   train_step = function(data) {     c(x, y, sample_weight) %<-% unpack_x_y_sample_weight(data)      with(tf$GradientTape() %as% tape, {       y_pred <- self(x, training = TRUE)       loss <- self$loss_fn(y, y_pred, sample_weight = sample_weight)     })      # Compute gradients     trainable_vars <- self$trainable_variables     gradients <- tape$gradient(loss, trainable_vars)      # Update weights     self$optimizer$apply(gradients, trainable_vars)      # Compute our own metrics     self$loss_tracker$update_state(loss)     self$mae_metric$update_state(y, y_pred)      # Return a dict mapping metric names to current value     list(       loss = self$loss_tracker$result(),       mae = self$mae_metric$result()     )   },   metrics = mark_active(function() {     # We list our `Metric` objects here so that `reset_states()` can be     # called automatically at the start of each epoch     # or at the start of `evaluate()`.     list(self$loss_tracker, self$mae_metric)   }) )   # Construct and compile an instance of CustomModel inputs <- layer_input(shape = 32) outputs <- layer_dense(inputs, 1) model <- CustomModel(inputs, outputs)  # We don't passs a loss or metrics here. model %>% compile(optimizer=\"adam\")  # Just use `fit` as usual x <- random_normal(c(1000, 32)) y <- random_normal(c(1000, 1)) model %>% fit(x, y, epochs=3) ## Epoch 1/3 ## 32/32 - 0s - 5ms/step - loss: 2.6540 - mae: 1.2901 ## Epoch 2/3 ## 32/32 - 0s - 754us/step - loss: 2.4139 - mae: 1.2303 ## Epoch 3/3 ## 32/32 - 0s - 746us/step - loss: 2.2080 - mae: 1.1761"},{"path":"https://keras.posit.co/articles/custom_train_step_in_tensorflow.html","id":"supporting-sample_weight-class_weight","dir":"Articles","previous_headings":"","what":"Supporting sample_weight & class_weight","title":"Customizing what happens in `fit()` with TensorFlow","text":"may noticed first basic example didn’t make mention sample weighting. want support fit() arguments sample_weight class_weight, ’d simply following: Unpack sample_weight data argument Pass compute_loss & update_state (course, also just apply manually don’t rely compile() losses & metrics) ’s .","code":"CustomModel <- new_model_class(   \"CustomModel\",   train_step = function(data) {     c(x, y, sample_weight) %<-% unpack_x_y_sample_weight(data)      with(tf$GradientTape() %as% tape, {       y_pred <- self(x, training = TRUE)       loss <- self$compute_loss(y = y, y_pred = y_pred, sample_weight = sample_weight)     })      # Compute gradients     trainable_vars <- self$trainable_variables     gradients <- tape$gradient(loss, trainable_vars)      # Update weights     self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))      # Update metrics (includes the metric that tracks the loss)     for (metric in self$metrics) {       if (metric$name == \"loss\") {         metric$update_state(loss)       } else {         metric$update_state(y, y_pred, sample_weight = sample_weight)       }     }      # Return a dict mapping metric names to current value     metrics <- lapply(self$metrics, function(m) m$result())     metrics <- setNames(metrics, sapply(self$metrics, function(m) m$name))     metrics   } )   # Construct and compile an instance of CustomModel inputs <- layer_input(shape = 32) outputs <- layer_dense(inputs, units = 1) model <- CustomModel(inputs, outputs) model %>% compile(optimizer=\"adam\", loss=\"mse\", metrics=\"mae\")  # You can now use sample_weight argument x <- random_normal(c(1000, 32)) y <- random_normal(c(1000, 1)) sw <- random_normal(c(1000, 1)) model %>% fit(x, y, sample_weight=sw, epochs=3) ## Epoch 1/3 ## 32/32 - 0s - 6ms/step - loss: 0.1607 - mae: 1.3018 ## Epoch 2/3 ## 32/32 - 0s - 747us/step - loss: 0.1452 - mae: 1.2999 ## Epoch 3/3 ## 32/32 - 0s - 743us/step - loss: 0.1335 - mae: 1.2986"},{"path":"https://keras.posit.co/articles/custom_train_step_in_tensorflow.html","id":"providing-your-own-evaluation-step","dir":"Articles","previous_headings":"","what":"Providing your own evaluation step","title":"Customizing what happens in `fit()` with TensorFlow","text":"want calls model.evaluate()? override test_step exactly way. ’s looks like:","code":"CustomModel <- new_model_class(   \"CustomModel\",   test_step = function(data) {     # Unpack the data     c(x, y, sw) %<-% unpack_x_y_sample_weight(data)     # Compute predictions     y_pred = self(x, training = FALSE)     # Updates the metrics tracking the loss     self$compute_loss(y = y, y_pred = y_pred, sample_weight = sw)     # Update the metrics.     for (metric in self$metrics) {       if (metric$name != \"loss\") {         metric$update_state(y, y_pred, sample_weight = sw)       }     }     # Return a dict mapping metric names to current value.     # Note that it will include the loss (tracked in self.metrics).     metrics <- lapply(self$metrics, function(m) m$result())     metrics <- setNames(metrics, sapply(self$metrics, function(m) m$name))     metrics   } )  # Construct an instance of CustomModel inputs <- layer_input(shape = 32) outputs <- layer_dense(inputs, 1) model <- CustomModel(inputs, outputs) model %>% compile(loss = \"mse\", metrics = \"mae\")  # Evaluate with our custom test_step x <- random_normal(c(1000, 32)) y <- random_normal(c(1000, 1)) model %>% evaluate(x, y) ## 32/32 - 0s - 2ms/step - loss: 0.0000e+00 - mae: 1.3947 ##            loss compile_metrics ##        0.000000        1.394695"},{"path":"https://keras.posit.co/articles/custom_train_step_in_tensorflow.html","id":"wrapping-up-an-end-to-end-gan-example","dir":"Articles","previous_headings":"","what":"Wrapping up: an end-to-end GAN example","title":"Customizing what happens in `fit()` with TensorFlow","text":"Let’s walk end--end example leverages everything just learned. Let’s consider: generator network meant generate 28x28x1 images. discriminator network meant classify 28x28x1 images two classes (“fake” “real”). One optimizer . loss function train discriminator. ’s feature-complete GAN class, overriding compile() use signature, implementing entire GAN algorithm 17 lines train_step: Let’s test-drive : ideas behind deep learning simple, implementation painful?","code":"# Create the discriminator discriminator <- keras_model_sequential(name = \"discriminator\", input_shape = c(28, 28, 1)) %>%   layer_conv_2d(filters = 64, kernel_size = c(3, 3), strides = c(2, 2), padding = \"same\") %>%   layer_activation_leaky_relu(negative_slope = 0.2) %>%   layer_conv_2d(filters = 128, kernel_size = c(3, 3), strides = c(2, 2), padding = \"same\") %>%   layer_activation_leaky_relu(negative_slope = 0.2) %>%   layer_global_max_pooling_2d() %>%   layer_dense(units = 1)   # Create the generator latent_dim <- 128 generator <- keras_model_sequential(name = \"generator\", input_shape = latent_dim) %>%   layer_dense(7 * 7 * 128) %>%   layer_activation_leaky_relu(negative_slope = 0.2) %>%   layer_reshape(target_shape = c(7, 7, 128)) %>%   layer_conv_2d_transpose(filters = 128, kernel_size = c(4, 4), strides = c(2, 2), padding = \"same\") %>%   layer_activation_leaky_relu(negative_slope = 0.2) %>%   layer_conv_2d_transpose(filters = 128, kernel_size = c(4, 4), strides = c(2, 2), padding = \"same\") %>%   layer_activation_leaky_relu(negative_slope = 0.2) %>%   layer_conv_2d(filters = 1, kernel_size = c(7, 7), padding = \"same\", activation = \"sigmoid\") GAN <- new_model_class(   \"GAN\",   initialize = function(discriminator, generator, latent_dim, ...) {     super$initialize(...)     self$discriminator <- discriminator     self$generator <- generator     self$latent_dim <- as.integer(latent_dim)     self$d_loss_tracker <- metric_mean(name=\"d_loss\")     self$g_loss_tracker = metric_mean(name=\"g_loss\")   },   compile = function(d_optimizer, g_optimizer, loss_fn, ...) {     super$compile(...)     self$d_optimizer <- d_optimizer     self$g_optimizer <- g_optimizer     self$loss_fn <- loss_fn   },   metrics = mark_active(function() {     list(self$d_loss_tracker, self$g_loss_tracker)   }),   train_step = function(real_images) {     # Sample random points in the latent space     batch_size <- tf$shape(real_images)[1]     random_latent_vectors <-       tf$random$normal(shape = c(batch_size, self$latent_dim))      # Decode them to fake images     generated_images <- self$generator(random_latent_vectors)      # Combine them with real images     combined_images <-       tf$concat(list(generated_images, real_images),                 axis = 0L)      # Assemble labels discriminating real from fake images     labels <-       tf$concat(list(tf$ones(c(batch_size, 1L)),                      tf$zeros(c(batch_size, 1L))),                 axis = 0L)      # Add random noise to the labels - important trick!     labels %<>% `+`(tf$random$uniform(tf$shape(.), maxval = 0.05))      # Train the discriminator     with(tf$GradientTape() %as% tape, {       predictions <- self$discriminator(combined_images)       d_loss <- self$loss_fn(labels, predictions)     })     grads <- tape$gradient(d_loss, self$discriminator$trainable_weights)     self$d_optimizer$apply_gradients(       zip_lists(grads, self$discriminator$trainable_weights))      # Sample random points in the latent space     random_latent_vectors <-       tf$random$normal(shape = c(batch_size, self$latent_dim))      # Assemble labels that say \"all real images\"     misleading_labels <- tf$zeros(c(batch_size, 1L))      # Train the generator (note that we should *not* update the weights     # of the discriminator)!     with(tf$GradientTape() %as% tape, {       predictions <- self$discriminator(self$generator(random_latent_vectors))       g_loss <- self$loss_fn(misleading_labels, predictions)     })     grads <- tape$gradient(g_loss, self$generator$trainable_weights)     self$g_optimizer$apply_gradients(       zip_lists(grads, self$generator$trainable_weights))      list(d_loss = d_loss, g_loss = g_loss)   } ) batch_size <- 64 c(c(x_train, .), c(x_test, .)) %<-% dataset_mnist() all_digits <- k_concatenate(list(x_train, x_test)) all_digits <- k_reshape(all_digits, c(-1, 28, 28, 1)) dataset <- all_digits %>%   tfdatasets::tensor_slices_dataset() %>%   tfdatasets::dataset_map(function(x) tf$cast(x, \"float\")/255) %>%   tfdatasets::dataset_shuffle(buffer_size = 1024) %>%   tfdatasets::dataset_batch(batch_size = batch_size)  gan <- GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim) gan$compile(     d_optimizer = optimizer_adam(learning_rate=0.0003),     g_optimizer = optimizer_adam(learning_rate=0.0003),     loss_fn = loss_binary_crossentropy(from_logits = TRUE) ) ## <keras.src.losses.losses.BinaryCrossentropy object at 0x2d12cf310> # To limit the execution time, we only train on 100 batches. You can train on # the entire dataset. You will need about 20 epochs to get nice results. gan %>% fit(   tfdatasets::dataset_take(dataset, 100),   epochs = 1 ) ## 100/100 - 21s - 209ms/step - d_loss: 0.0000e+00 - g_loss: 0.0000e+00"},{"path":"https://keras.posit.co/articles/custom_train_step_in_torch.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Customizing what happens in `fit()` with PyTorch","text":"’re supervised learning, can use fit() everything works smoothly. need take control every little detail, can write training loop entirely scratch. need custom training algorithm, still want benefit convenient features fit(), callbacks, built-distribution support, step fusing? core principle Keras progressive disclosure complexity. always able get lower-level workflows gradual way. shouldn’t fall cliff high-level functionality doesn’t exactly match use case. able gain control small details retaining commensurate amount high-level convenience. need customize fit() , override training step function Model class. function called fit() every batch data. able call fit() usual – running learning algorithm. Note pattern prevent building models Functional API. can whether ’re building Sequential models, Functional API models, subclassed models. Let’s see works.","code":""},{"path":"https://keras.posit.co/articles/custom_train_step_in_torch.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Customizing what happens in `fit()` with PyTorch","text":"","code":"import os  # This guide can only be run with the torch backend. os.environ[\"KERAS_BACKEND\"] = \"torch\"  import torch import keras from keras import layers import numpy as np"},{"path":"https://keras.posit.co/articles/custom_train_step_in_torch.html","id":"a-first-simple-example","dir":"Articles","previous_headings":"","what":"A first simple example","title":"Customizing what happens in `fit()` with PyTorch","text":"Let’s start simple example: create new class subclasses keras.Model. just override method train_step(self, data). return dictionary mapping metric names (including loss) current value. input argument data gets passed fit training data: pass NumPy arrays, calling fit(x, y, ...), data tuple (x, y) pass torch.utils.data.DataLoader tf.data.Dataset, calling fit(dataset, ...), data gets yielded dataset batch. body train_step() method, implement regular training update, similar already familiar . Importantly, compute loss via self.compute_loss(), wraps loss(es) function(s) passed compile(). Similarly, call metric.update_state(y, y_pred) metrics self.metrics, update state metrics passed compile(), query results self.metrics end retrieve current value. Let’s try :","code":"class CustomModel(keras.Model):     def train_step(self, data):         # Unpack the data. Its structure depends on your model and         # on what you pass to `fit()`.         x, y = data          # Call torch.nn.Module.zero_grad() to clear the leftover gradients         # for the weights from the previous train step.         self.zero_grad()          # Compute loss         y_pred = self(x, training=True)  # Forward pass         loss = self.compute_loss(y=y, y_pred=y_pred)          # Call torch.Tensor.backward() on the loss to compute gradients         # for the weights.         loss.backward()          trainable_weights = [v for v in self.trainable_weights]         gradients = [v.value.grad for v in trainable_weights]          # Update weights         with torch.no_grad():             self.optimizer.apply(gradients, trainable_weights)          # Update metrics (includes the metric that tracks the loss)         for metric in self.metrics:             if metric.name == \"loss\":                 metric.update_state(loss)             else:                 metric.update_state(y, y_pred)          # Return a dict mapping metric names to current value         # Note that it will include the loss (tracked in self.metrics).         return {m.name: m.result() for m in self.metrics} # Construct and compile an instance of CustomModel inputs = keras.Input(shape=(32,)) outputs = keras.layers.Dense(1)(inputs) model = CustomModel(inputs, outputs) model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])  # Just use `fit` as usual x = np.random.random((1000, 32)) y = np.random.random((1000, 1)) model.fit(x, y, epochs=3)"},{"path":"https://keras.posit.co/articles/custom_train_step_in_torch.html","id":"going-lower-level","dir":"Articles","previous_headings":"","what":"Going lower-level","title":"Customizing what happens in `fit()` with PyTorch","text":"Naturally, just skip passing loss function compile(), instead everything manually train_step. Likewise metrics. ’s lower-level example, uses compile() configure optimizer: start creating Metric instances track loss MAE score (__init__()). implement custom train_step() updates state metrics (calling update_state() ), query (via result()) return current average value, displayed progress bar pass callback. Note need call reset_states() metrics epoch! Otherwise calling result() return average since start training, whereas usually work per-epoch averages. Thankfully, framework can us: just list metric want reset metrics property model. model call reset_states() object listed beginning fit() epoch beginning call evaluate().","code":"class CustomModel(keras.Model):     def __init__(self, *args, **kwargs):         super().__init__(*args, **kwargs)         self.loss_tracker = keras.metrics.Mean(name=\"loss\")         self.mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")         self.loss_fn = keras.losses.MeanSquaredError()      def train_step(self, data):         x, y = data          # Call torch.nn.Module.zero_grad() to clear the leftover gradients         # for the weights from the previous train step.         self.zero_grad()          # Compute loss         y_pred = self(x, training=True)  # Forward pass         loss = self.loss_fn(y, y_pred)          # Call torch.Tensor.backward() on the loss to compute gradients         # for the weights.         loss.backward()          trainable_weights = [v for v in self.trainable_weights]         gradients = [v.value.grad for v in trainable_weights]          # Update weights         with torch.no_grad():             self.optimizer.apply(gradients, trainable_weights)          # Compute our own metrics         self.loss_tracker.update_state(loss)         self.mae_metric.update_state(y, y_pred)         return {             \"loss\": self.loss_tracker.result(),             \"mae\": self.mae_metric.result(),         }      @property     def metrics(self):         # We list our `Metric` objects here so that `reset_states()` can be         # called automatically at the start of each epoch         # or at the start of `evaluate()`.         return [self.loss_tracker, self.mae_metric]   # Construct an instance of CustomModel inputs = keras.Input(shape=(32,)) outputs = keras.layers.Dense(1)(inputs) model = CustomModel(inputs, outputs)  # We don't passs a loss or metrics here. model.compile(optimizer=\"adam\")  # Just use `fit` as usual -- you can use callbacks, etc. x = np.random.random((1000, 32)) y = np.random.random((1000, 1)) model.fit(x, y, epochs=5)"},{"path":"https://keras.posit.co/articles/custom_train_step_in_torch.html","id":"supporting-sample_weight-class_weight","dir":"Articles","previous_headings":"","what":"Supporting sample_weight & class_weight","title":"Customizing what happens in `fit()` with PyTorch","text":"may noticed first basic example didn’t make mention sample weighting. want support fit() arguments sample_weight class_weight, ’d simply following: Unpack sample_weight data argument Pass compute_loss & update_state (course, also just apply manually don’t rely compile() losses & metrics) ’s .","code":"class CustomModel(keras.Model):     def train_step(self, data):         # Unpack the data. Its structure depends on your model and         # on what you pass to `fit()`.         if len(data) == 3:             x, y, sample_weight = data         else:             sample_weight = None             x, y = data          # Call torch.nn.Module.zero_grad() to clear the leftover gradients         # for the weights from the previous train step.         self.zero_grad()          # Compute loss         y_pred = self(x, training=True)  # Forward pass         loss = self.compute_loss(             y=y,             y_pred=y_pred,             sample_weight=sample_weight,         )          # Call torch.Tensor.backward() on the loss to compute gradients         # for the weights.         loss.backward()          trainable_weights = [v for v in self.trainable_weights]         gradients = [v.value.grad for v in trainable_weights]          # Update weights         with torch.no_grad():             self.optimizer.apply(gradients, trainable_weights)          # Update metrics (includes the metric that tracks the loss)         for metric in self.metrics:             if metric.name == \"loss\":                 metric.update_state(loss)             else:                 metric.update_state(y, y_pred, sample_weight=sample_weight)          # Return a dict mapping metric names to current value         # Note that it will include the loss (tracked in self.metrics).         return {m.name: m.result() for m in self.metrics}   # Construct and compile an instance of CustomModel inputs = keras.Input(shape=(32,)) outputs = keras.layers.Dense(1)(inputs) model = CustomModel(inputs, outputs) model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])  # You can now use sample_weight argument x = np.random.random((1000, 32)) y = np.random.random((1000, 1)) sw = np.random.random((1000, 1)) model.fit(x, y, sample_weight=sw, epochs=3)"},{"path":"https://keras.posit.co/articles/custom_train_step_in_torch.html","id":"providing-your-own-evaluation-step","dir":"Articles","previous_headings":"","what":"Providing your own evaluation step","title":"Customizing what happens in `fit()` with PyTorch","text":"want calls model.evaluate()? override test_step exactly way. ’s looks like:","code":"class CustomModel(keras.Model):     def test_step(self, data):         # Unpack the data         x, y = data         # Compute predictions         y_pred = self(x, training=False)         # Updates the metrics tracking the loss         loss = self.compute_loss(y=y, y_pred=y_pred)         # Update the metrics.         for metric in self.metrics:             if metric.name == \"loss\":                 metric.update_state(loss)             else:                 metric.update_state(y, y_pred)         # Return a dict mapping metric names to current value.         # Note that it will include the loss (tracked in self.metrics).         return {m.name: m.result() for m in self.metrics}   # Construct an instance of CustomModel inputs = keras.Input(shape=(32,)) outputs = keras.layers.Dense(1)(inputs) model = CustomModel(inputs, outputs) model.compile(loss=\"mse\", metrics=[\"mae\"])  # Evaluate with our custom test_step x = np.random.random((1000, 32)) y = np.random.random((1000, 1)) model.evaluate(x, y)"},{"path":"https://keras.posit.co/articles/custom_train_step_in_torch.html","id":"wrapping-up-an-end-to-end-gan-example","dir":"Articles","previous_headings":"","what":"Wrapping up: an end-to-end GAN example","title":"Customizing what happens in `fit()` with PyTorch","text":"Let’s walk end--end example leverages everything just learned. Let’s consider: generator network meant generate 28x28x1 images. discriminator network meant classify 28x28x1 images two classes (“fake” “real”). One optimizer . loss function train discriminator. ’s feature-complete GAN class, overriding compile() use signature, implementing entire GAN algorithm 17 lines train_step: Let’s test-drive : ideas behind deep learning simple, implementation painful?","code":"# Create the discriminator discriminator = keras.Sequential(     [         keras.Input(shape=(28, 28, 1)),         layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),         layers.LeakyReLU(negative_slope=0.2),         layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),         layers.LeakyReLU(negative_slope=0.2),         layers.GlobalMaxPooling2D(),         layers.Dense(1),     ],     name=\"discriminator\", )  # Create the generator latent_dim = 128 generator = keras.Sequential(     [         keras.Input(shape=(latent_dim,)),         # We want to generate 128 coefficients to reshape into a 7x7x128 map         layers.Dense(7 * 7 * 128),         layers.LeakyReLU(negative_slope=0.2),         layers.Reshape((7, 7, 128)),         layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),         layers.LeakyReLU(negative_slope=0.2),         layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),         layers.LeakyReLU(negative_slope=0.2),         layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),     ],     name=\"generator\", ) class GAN(keras.Model):     def __init__(self, discriminator, generator, latent_dim):         super().__init__()         self.discriminator = discriminator         self.generator = generator         self.latent_dim = latent_dim         self.d_loss_tracker = keras.metrics.Mean(name=\"d_loss\")         self.g_loss_tracker = keras.metrics.Mean(name=\"g_loss\")         self.seed_generator = keras.random.SeedGenerator(1337)         self.built = True      @property     def metrics(self):         return [self.d_loss_tracker, self.g_loss_tracker]      def compile(self, d_optimizer, g_optimizer, loss_fn):         super().compile()         self.d_optimizer = d_optimizer         self.g_optimizer = g_optimizer         self.loss_fn = loss_fn      def train_step(self, real_images):         if isinstance(real_images, tuple):             real_images = real_images[0]         # Sample random points in the latent space         batch_size = real_images.shape[0]         random_latent_vectors = keras.random.normal(             shape=(batch_size, self.latent_dim), seed=self.seed_generator         )          # Decode them to fake images         generated_images = self.generator(random_latent_vectors)          # Combine them with real images         real_images = torch.tensor(real_images)         combined_images = torch.concat([generated_images, real_images], axis=0)          # Assemble labels discriminating real from fake images         labels = torch.concat(             [torch.ones((batch_size, 1)), torch.zeros((batch_size, 1))], axis=0         )         # Add random noise to the labels - important trick!         labels += 0.05 * keras.random.uniform(             labels.shape, seed=self.seed_generator         )          # Train the discriminator         self.zero_grad()         predictions = self.discriminator(combined_images)         d_loss = self.loss_fn(labels, predictions)         d_loss.backward()         grads = [v.value.grad for v in self.discriminator.trainable_weights]         with torch.no_grad():             self.d_optimizer.apply(grads, self.discriminator.trainable_weights)          # Sample random points in the latent space         random_latent_vectors = keras.random.normal(             shape=(batch_size, self.latent_dim), seed=self.seed_generator         )          # Assemble labels that say \"all real images\"         misleading_labels = torch.zeros((batch_size, 1))          # Train the generator (note that we should *not* update the weights         # of the discriminator)!         self.zero_grad()         predictions = self.discriminator(self.generator(random_latent_vectors))         g_loss = self.loss_fn(misleading_labels, predictions)         grads = g_loss.backward()         grads = [v.value.grad for v in self.generator.trainable_weights]         with torch.no_grad():             self.g_optimizer.apply(grads, self.generator.trainable_weights)          # Update metrics and return their value.         self.d_loss_tracker.update_state(d_loss)         self.g_loss_tracker.update_state(g_loss)         return {             \"d_loss\": self.d_loss_tracker.result(),             \"g_loss\": self.g_loss_tracker.result(),         } # Prepare the dataset. We use both the training & test MNIST digits. batch_size = 64 (x_train, _), (x_test, _) = keras.datasets.mnist.load_data() all_digits = np.concatenate([x_train, x_test]) all_digits = all_digits.astype(\"float32\") / 255.0 all_digits = np.reshape(all_digits, (-1, 28, 28, 1))  # Create a TensorDataset dataset = torch.utils.data.TensorDataset(     torch.from_numpy(all_digits), torch.from_numpy(all_digits) ) # Create a DataLoader dataloader = torch.utils.data.DataLoader(     dataset, batch_size=batch_size, shuffle=True )  gan = GAN(     discriminator=discriminator, generator=generator, latent_dim=latent_dim ) gan.compile(     d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),     g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),     loss_fn=keras.losses.BinaryCrossentropy(from_logits=True), )  gan.fit(dataloader, epochs=1)"},{"path":"https://keras.posit.co/articles/customizing_saving_and_serialization.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Customizing Saving and Serialization","text":"guide covers advanced methods can customized Keras saving. users, methods outlined primary Serialize, save, export guide sufficient.","code":""},{"path":"https://keras.posit.co/articles/customizing_saving_and_serialization.html","id":"apis","dir":"Articles","previous_headings":"Introduction","what":"APIs","title":"Customizing Saving and Serialization","text":"cover following APIs: save_assets() load_assets() save_own_variables() load_own_variables() get_build_config() build_from_config() get_compile_config() compile_from_config() restoring model, get executed following order: build_from_config() compile_from_config() load_own_variables() load_assets()","code":""},{"path":"https://keras.posit.co/articles/customizing_saving_and_serialization.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Customizing Saving and Serialization","text":"","code":"import os import numpy as np import tensorflow as tf import keras"},{"path":"https://keras.posit.co/articles/customizing_saving_and_serialization.html","id":"state-saving-customization","dir":"Articles","previous_headings":"","what":"State saving customization","title":"Customizing Saving and Serialization","text":"methods determine state model’s layers saved calling model.save(). can override take full control state saving process.","code":""},{"path":"https://keras.posit.co/articles/customizing_saving_and_serialization.html","id":"save_own_variables-and-load_own_variables","dir":"Articles","previous_headings":"State saving customization","what":"save_own_variables() and load_own_variables()","title":"Customizing Saving and Serialization","text":"methods save load state variables layer model.save() keras.models.load_model() called, respectively. default, state variables saved loaded weights layer (trainable non-trainable). default implementation save_own_variables(): store used methods dictionary can populated layer variables. Let’s take look example customizing . Example:","code":"def save_own_variables(self, store):     all_vars = self._trainable_weights + self._non_trainable_weights     for i, v in enumerate(all_vars):         store[f\"{i}\"] = v.numpy() @keras.utils.register_keras_serializable(package=\"my_custom_package\") class LayerWithCustomVariables(keras.layers.Dense):     def __init__(self, units, **kwargs):         super().__init__(units, **kwargs)         self.stored_variables = tf.Variable(             np.random.random((10,)), name=\"special_arr\", dtype=tf.float32         )      def save_own_variables(self, store):         super().save_own_variables(store)         # Stores the value of the `tf.Variable` upon saving         store[\"variables\"] = self.stored_variables.numpy()      def load_own_variables(self, store):         # Assigns the value of the `tf.Variable` upon loading         self.stored_variables.assign(store[\"variables\"])         # Load the remaining weights         for i, v in enumerate(self.weights):             v.assign(store[f\"{i}\"])         # Note: You must specify how all variables (including layer weights)         # are loaded in `load_own_variables.`      def call(self, inputs):         return super().call(inputs) * self.stored_variables   model = keras.Sequential([LayerWithCustomVariables(1)])  ref_input = np.random.random((8, 10)) ref_output = np.random.random((8,)) model.compile(optimizer=\"adam\", loss=\"mean_squared_error\") model.fit(ref_input, ref_output)  model.save(\"custom_vars_model.keras\") restored_model = keras.models.load_model(\"custom_vars_model.keras\")  np.testing.assert_allclose(     model.layers[0].stored_variables.numpy(),     restored_model.layers[0].stored_variables.numpy(), )"},{"path":"https://keras.posit.co/articles/customizing_saving_and_serialization.html","id":"save_assets-and-load_assets","dir":"Articles","previous_headings":"State saving customization","what":"save_assets() and load_assets()","title":"Customizing Saving and Serialization","text":"methods can added model class definition store load additional information model needs. example, NLP domain layers TextVectorization layers IndexLookup layers may need store associated vocabulary (lookup table) text file upon saving. Let’s take basics workflow simple file assets.txt. Example:","code":"@keras.saving.register_keras_serializable(package=\"my_custom_package\") class LayerWithCustomAssets(keras.layers.Dense):     def __init__(self, vocab=None, *args, **kwargs):         super().__init__(*args, **kwargs)         self.vocab = vocab      def save_assets(self, inner_path):         # Writes the vocab (sentence) to text file at save time.         with open(os.path.join(inner_path, \"vocabulary.txt\"), \"w\") as f:             f.write(self.vocab)      def load_assets(self, inner_path):         # Reads the vocab (sentence) from text file at load time.         with open(os.path.join(inner_path, \"vocabulary.txt\"), \"r\") as f:             text = f.read()         self.vocab = text.replace(\"<unk>\", \"little\")   model = keras.Sequential(     [LayerWithCustomAssets(vocab=\"Mary had a <unk> lamb.\", units=5)] )  x = np.random.random((10, 10)) y = model(x)  model.save(\"custom_assets_model.keras\") restored_model = keras.models.load_model(\"custom_assets_model.keras\")  np.testing.assert_string_equal(     restored_model.layers[0].vocab, \"Mary had a little lamb.\" )"},{"path":[]},{"path":"https://keras.posit.co/articles/customizing_saving_and_serialization.html","id":"get_build_config-and-build_from_config","dir":"Articles","previous_headings":"build and compile saving customization","what":"get_build_config() and build_from_config()","title":"Customizing Saving and Serialization","text":"methods work together save layer’s built states restore upon loading. default, includes build config dictionary layer’s input shape, overriding methods can used include Variables Lookup Tables can useful restore built model. Example:","code":"@keras.saving.register_keras_serializable(package=\"my_custom_package\") class LayerWithCustomBuild(keras.layers.Layer):     def __init__(self, units=32, **kwargs):         super().__init__(**kwargs)         self.units = units      def call(self, inputs):         return tf.matmul(inputs, self.w) + self.b      def get_config(self):         return dict(units=self.units, **super().get_config())      def build(self, input_shape, layer_init):         # Note the customization in overriding `build()` adds an extra argument.         # Therefore, we will need to manually call build with `layer_init` argument         # before the first execution of `call()`.         super().build(input_shape)         self.w = self.add_weight(             shape=(input_shape[-1], self.units),             initializer=layer_init,             trainable=True,         )         self.b = self.add_weight(             shape=(self.units,),             initializer=layer_init,             trainable=True,         )         self.layer_init = layer_init      def get_build_config(self):         build_config = super().get_build_config()  # only gives `input_shape`         build_config.update(             {\"layer_init\": self.layer_init}  # Stores our initializer for `build()`         )         return build_config      def build_from_config(self, config):         # Calls `build()` with the parameters at loading time         self.build(config[\"input_shape\"], config[\"layer_init\"])   custom_layer = LayerWithCustomBuild(units=16) custom_layer.build(input_shape=(8,), layer_init=\"random_normal\")  model = keras.Sequential(     [         custom_layer,         keras.layers.Dense(1, activation=\"sigmoid\"),     ] )  x = np.random.random((16, 8)) y = model(x)  model.save(\"custom_build_model.keras\") restored_model = keras.models.load_model(\"custom_build_model.keras\")  np.testing.assert_equal(restored_model.layers[0].layer_init, \"random_normal\") np.testing.assert_equal(restored_model.built, True)"},{"path":"https://keras.posit.co/articles/customizing_saving_and_serialization.html","id":"get_compile_config-and-compile_from_config","dir":"Articles","previous_headings":"build and compile saving customization","what":"get_compile_config() and compile_from_config()","title":"Customizing Saving and Serialization","text":"methods work together save information model compiled (optimizers, losses, etc.) restore re-compile model information. Overriding methods can useful compiling restored model custom optimizers, custom losses, etc., need deserialized prior calling model.compile compile_from_config(). Let’s take look example . Example:","code":"@keras.saving.register_keras_serializable(package=\"my_custom_package\") def small_square_sum_loss(y_true, y_pred):     loss = tf.math.squared_difference(y_pred, y_true)     loss = loss / 10.0     loss = tf.reduce_sum(loss, axis=1)     return loss   @keras.saving.register_keras_serializable(package=\"my_custom_package\") def mean_pred(y_true, y_pred):     return tf.reduce_mean(y_pred)   @keras.saving.register_keras_serializable(package=\"my_custom_package\") class ModelWithCustomCompile(keras.Model):     def __init__(self):         super().__init__()         self.dense1 = keras.layers.Dense(8, activation=\"relu\")         self.dense2 = keras.layers.Dense(4, activation=\"softmax\")      def call(self, inputs):         x = self.dense1(inputs)         return self.dense2(x)      def compile(self, optimizer, loss_fn, metrics):         super().compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)         self.model_optimizer = optimizer         self.loss_fn = loss_fn         self.loss_metrics = metrics      def get_compile_config(self):         # These parameters will be serialized at saving time.         return {             \"model_optimizer\": self.model_optimizer,             \"loss_fn\": self.loss_fn,             \"metric\": self.loss_metrics,         }      def compile_from_config(self, config):         # Deserializes the compile parameters (important, since many are custom)         optimizer = keras.utils.deserialize_keras_object(config[\"model_optimizer\"])         loss_fn = keras.utils.deserialize_keras_object(config[\"loss_fn\"])         metrics = keras.utils.deserialize_keras_object(config[\"metric\"])          # Calls compile with the deserialized parameters         self.compile(optimizer=optimizer, loss_fn=loss_fn, metrics=metrics)   model = ModelWithCustomCompile() model.compile(     optimizer=\"SGD\", loss_fn=small_square_sum_loss, metrics=[\"accuracy\", mean_pred] )  x = np.random.random((4, 8)) y = np.random.random((4,))  model.fit(x, y)  model.save(\"custom_compile_model.keras\") restored_model = keras.models.load_model(\"custom_compile_model.keras\")  np.testing.assert_equal(model.model_optimizer, restored_model.model_optimizer) np.testing.assert_equal(model.loss_fn, restored_model.loss_fn) np.testing.assert_equal(model.loss_metrics, restored_model.loss_metrics)"},{"path":"https://keras.posit.co/articles/customizing_saving_and_serialization.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Customizing Saving and Serialization","text":"Using methods learned tutorial allows wide variety use cases, allowing saving loading complex models exotic assets state elements. recap: save_own_variables load_own_variables determine states saved loaded. save_assets load_assets can added store load additional information model needs. get_build_config build_from_config save restore model’s built states. get_compile_config compile_from_config save restore model’s compiled states.","code":""},{"path":"https://keras.posit.co/articles/customizing_what_happens_in_fit.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Customizing what happens in `fit()`","text":"’re supervised learning, can use fit() everything works smoothly. need write training loop scratch, can use GradientTape take control every little detail. need custom training algorithm, still want benefit convenient features fit(), callbacks, built-distribution support, step fusing? core principle Keras progressive disclosure complexity. always able get lower-level workflows gradual way. shouldn’t fall cliff high-level functionality doesn’t exactly match use case. able gain control small details retaining commensurate amount high-level convenience. need customize fit() , override training step function Model class. function called fit() every batch data. able call fit() usual – running learning algorithm. Note pattern prevent building models Functional API. can whether ’re building Sequential models, Functional API models, subclassed models. Let’s see works.","code":""},{"path":"https://keras.posit.co/articles/customizing_what_happens_in_fit.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Customizing what happens in `fit()`","text":"Requires TensorFlow 2.8 later.","code":"library(keras3) library(tensorflow) ## ## Attaching package: 'tensorflow' ## The following objects are masked from 'package:keras': ## ##     set_random_seed, shape library(reticulate)"},{"path":"https://keras.posit.co/articles/customizing_what_happens_in_fit.html","id":"a-first-simple-example","dir":"Articles","previous_headings":"","what":"A first simple example","title":"Customizing what happens in `fit()`","text":"Let’s start simple example: create new class subclasses Model. just override method train_step(self, data). return dictionary mapping metric names (including loss) current value. input argument data gets passed fit training data: pass arrays, calling fit(x, y, ...), data tuple (x, y) pass tf_dataset, calling fit(dataset, ...), data gets yielded dataset batch. body train_step method, implement regular training update, similar already familiar . Importantly, compute loss via self$compute_loss(), wraps loss(es) function(s) passed compile(). Similarly, call metric$update_state(y, y_pred) metrics self$metrics, update state metrics passed compile(), query results self.metrics end retrieve current value. Let’s try :","code":"custom_model <- new_model_class(   \"custom_model\",   train_step = function(data, ...) {     c(x, y) %<-% data      with(tf$GradientTape() %as% tape, {       y_pred <- self(x, training = TRUE)       loss <- self$compute_loss(y = y, y_pred = y_pred)     })      # Compute gradients     trainable_vars <- self$trainable_variables     gradients <- tape$gradient(loss, trainable_vars)      # Update weights     self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))      # Update metrics (includes the metric that tracks the loss)     for (metric in self$metrics) {       if (metric$name == \"loss\") {         metric$update_state(loss)       } else {         metric$update_state(y, y_pred)       }     }      # Return a dict mapping metric names to current value     metrics <- lapply(self$metrics, function(m) m$result())     metrics <- setNames(metrics, sapply(self$metrics, function(m) m$name))     metrics   } ) # Construct and compile an instance of CustomModel inputs <- layer_input(shape = 32) outputs <- layer_dense(inputs, 1)  model <- custom_model(inputs, outputs) model %>% compile(optimizer=\"adam\", loss=\"mse\", metrics=\"mae\")  # Just use `fit` as usual x <- random_normal(c(1000, 32)) y <- random_normal(c(1000, 1)) model %>% fit(x, y, epochs=3) ## Epoch 1/3 ## 32/32 - 0s - 8ms/step - loss: 2.9118 - mae: 1.3597 ## Epoch 2/3 ## 32/32 - 0s - 1ms/step - loss: 2.6026 - mae: 1.2856 ## Epoch 3/3 ## 32/32 - 0s - 699us/step - loss: 2.3378 - mae: 1.2193"},{"path":"https://keras.posit.co/articles/customizing_what_happens_in_fit.html","id":"going-lower-level","dir":"Articles","previous_headings":"","what":"Going lower-level","title":"Customizing what happens in `fit()`","text":"Naturally, just skip passing loss function compile(), instead everything manually train_step. Likewise metrics. ’s lower-level example, uses compile() configure optimizer: start creating Metric instances track loss MAE score (initialize()). implement custom train_step() updates state metrics (calling update_state() ), query (via result()) return current average value, displayed progress bar pass callback. Note need call reset_states() metrics epoch! Otherwise calling result() return average since start training, whereas usually work per-epoch averages. Thankfully, framework can us: just list metric want reset metrics property model. model call reset_states() object listed beginning fit() epoch beginning call evaluate().","code":"custom_model <- new_model_class(   \"custom_model\",   initialize = function(...) {     super$initialize(...)     self$loss_tracker <- metric_mean(name = \"loss\")     self$mae_metric <- metric_mean_absolute_error(name = \"mae\")   },   train_step = function(data) {     c(x, y) %<-% data      with(tf$GradientTape() %as% tape, {       y_pred <- self(x, training = TRUE)       loss <- loss_mean_squared_error(y, y_pred)     })      # Compute gradients     trainable_vars <- self$trainable_variables     gradients <- tape$gradient(loss, trainable_vars)      # Update weights     self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))      # Compute our own metrics     self$loss_tracker$update_state(loss)     self$mae_metric$update_state(y, y_pred)      list(       loss = self$loss_tracker$result(),       mae = self$mae_metric$result()     )   },    metrics = mark_active(function() {     # We list our `Metric` objects here so that `reset_states()` can be     # called automatically at the start of each epoch     # or at the start of `evaluate()`.     # If you don't implement this property, you have to call     # `reset_states()` yourself at the time of your choosing.     list(self$loss_tracker, self$mae_metric)   }) )  # Construct an instance of CustomModel inputs <- layer_input(shape = 32) outputs <- layer_dense(inputs, 1) model <- custom_model(inputs, outputs)  # We don't passs a loss or metrics here. model %>% compile(optimizer=\"adam\")  # Just use `fit` as usual -- you can use callbacks, etc. x <- random_normal(c(1000, 32)) y <- random_normal(c(1000, 1)) model %>% fit(x, y, epochs=5) ## Epoch 1/5 ## 32/32 - 0s - 5ms/step - loss: 2.6469 - mae: 1.2901 ## Epoch 2/5 ## 32/32 - 0s - 713us/step - loss: 2.4090 - mae: 1.2306 ## Epoch 3/5 ## 32/32 - 0s - 678us/step - loss: 2.2032 - mae: 1.1763 ## Epoch 4/5 ## 32/32 - 0s - 631us/step - loss: 2.0239 - mae: 1.1270 ## Epoch 5/5 ## 32/32 - 0s - 657us/step - loss: 1.8687 - mae: 1.0827"},{"path":"https://keras.posit.co/articles/customizing_what_happens_in_fit.html","id":"supporting-sample_weight-class_weight","dir":"Articles","previous_headings":"","what":"Supporting sample_weight & class_weight","title":"Customizing what happens in `fit()`","text":"may noticed first basic example didn’t make mention sample weighting. want support fit() arguments sample_weight class_weight, ’d simply following: Unpack sample_weight data argument Pass compute_loss & update_state (course, also just apply manually don’t rely compile() losses & metrics) ’s .","code":"custom_model <- new_model_class(   \"custom_model\",   train_step = function(data) {     c(x, y, sample_weight) %<-% unpack_x_y_sample_weight(data)      with(tf$GradientTape() %as% tape, {       y_pred <- self(x, training = TRUE)       loss <- self$compute_loss(y = y, y_pred = y_pred, sample_weight = sample_weight)     })      # Compute gradients     trainable_vars <- self$trainable_variables     gradients <- tape$gradient(loss, trainable_vars)      # Update weights     self$optimizer$apply_gradients(zip_lists(gradients, trainable_vars))      # Update metrics (includes the metric that tracks the loss)     for (metric in self$metrics) {       if (metric$name == \"loss\") {         metric$update_state(loss)       } else {         metric$update_state(y, y_pred, sample_weight = sample_weight)       }     }      # Return a dict mapping metric names to current value     metrics <- lapply(self$metrics, function(m) m$result())     metrics <- setNames(metrics, sapply(self$metrics, function(m) m$name))     metrics   } )   # Construct and compile an instance of CustomModel inputs <- layer_input(shape = 32) outputs <- layer_dense(inputs, units = 1) model <- custom_model(inputs, outputs) model %>% compile(optimizer=\"adam\", loss=\"mse\", metrics=\"mae\")  # You can now use sample_weight argument x <- random_normal(c(1000, 32)) y <- random_normal(c(1000, 1)) sw <- random_normal(c(1000, 1)) model %>% fit(x, y, sample_weight=sw, epochs=3) ## Epoch 1/3 ## 32/32 - 0s - 6ms/step - loss: 0.1607 - mae: 1.3018 ## Epoch 2/3 ## 32/32 - 0s - 737us/step - loss: 0.1452 - mae: 1.2999 ## Epoch 3/3 ## 32/32 - 0s - 701us/step - loss: 0.1335 - mae: 1.2986"},{"path":"https://keras.posit.co/articles/customizing_what_happens_in_fit.html","id":"providing-your-own-evaluation-step","dir":"Articles","previous_headings":"","what":"Providing your own evaluation step","title":"Customizing what happens in `fit()`","text":"want calls model.evaluate()? override test_step exactly way. ’s looks like:","code":"custom_model <- new_model_class(   \"custom_model\",   test_step = function(data) {     # Unpack the data     c(x, y, sw) %<-% unpack_x_y_sample_weight(data)     # Compute predictions     y_pred = self(x, training = FALSE)     # Updates the metrics tracking the loss     self$compute_loss(y = y, y_pred = y_pred, sample_weight = sw)     # Update the metrics.     for (metric in self$metrics) {       if (metric$name != \"loss\") {         metric$update_state(y, y_pred, sample_weight = sw)       }     }     # Return a dict mapping metric names to current value.     # Note that it will include the loss (tracked in self.metrics).     metrics <- lapply(self$metrics, function(m) m$result())     metrics <- setNames(metrics, sapply(self$metrics, function(m) m$name))     metrics   } )  # Construct an instance of CustomModel inputs <- layer_input(shape = 32) outputs <- layer_dense(inputs, 1) model <- custom_model(inputs, outputs) model %>% compile(loss = \"mse\", metrics = \"mae\")  # Evaluate with our custom test_step x <- random_normal(c(1000, 32)) y <- random_normal(c(1000, 1)) model %>% evaluate(x, y) ## 32/32 - 0s - 2ms/step - loss: 0.0000e+00 - mae: 1.3947 ##            loss compile_metrics ##        0.000000        1.394695"},{"path":"https://keras.posit.co/articles/customizing_what_happens_in_fit.html","id":"wrapping-up-an-end-to-end-gan-example","dir":"Articles","previous_headings":"","what":"Wrapping up: an end-to-end GAN example","title":"Customizing what happens in `fit()`","text":"Let’s walk end--end example leverages everything just learned. Let’s consider: generator network meant generate 28x28x1 images. discriminator network meant classify 28x28x1 images two classes (“fake” “real”). One optimizer . loss function train discriminator. ’s feature-complete GAN class, overriding compile() use signature, implementing entire GAN algorithm 17 lines train_step: Let’s test-drive : ideas behind deep learning simple, implementation painful?","code":"# Create the discriminator discriminator <- keras_model_sequential(name = \"discriminator\", input_shape = c(28, 28, 1)) %>%   layer_conv_2d(filters = 64, kernel_size = c(3, 3), strides = c(2, 2), padding = \"same\") %>%   layer_activation_leaky_relu(negative_slope = 0.2) %>%   layer_conv_2d(filters = 128, kernel_size = c(3, 3), strides = c(2, 2), padding = \"same\") %>%   layer_activation_leaky_relu(negative_slope = 0.2) %>%   layer_global_max_pooling_2d() %>%   layer_dense(units = 1)   # Create the generator latent_dim <- 128 generator <- keras_model_sequential(name = \"generator\", input_shape = latent_dim) %>%   layer_dense(7 * 7 * 128) %>%   layer_activation_leaky_relu(negative_slope = 0.2) %>%   layer_reshape(target_shape = c(7, 7, 128)) %>%   layer_conv_2d_transpose(filters = 128, kernel_size = c(4, 4), strides = c(2, 2), padding = \"same\") %>%   layer_activation_leaky_relu(negative_slope = 0.2) %>%   layer_conv_2d_transpose(filters = 128, kernel_size = c(4, 4), strides = c(2, 2), padding = \"same\") %>%   layer_activation_leaky_relu(negative_slope = 0.2) %>%   layer_conv_2d(filters = 1, kernel_size = c(7, 7), padding = \"same\", activation = \"sigmoid\") gan <- new_model_class(   \"GAN\",   initialize = function(discriminator, generator, latent_dim, ...) {     super$initialize(...)     self$discriminator <- discriminator     self$generator <- generator     self$latent_dim <- as.integer(latent_dim)     self$d_loss_tracker <- metric_mean(name=\"d_loss\")     self$g_loss_tracker = metric_mean(name=\"g_loss\")   },   compile = function(d_optimizer, g_optimizer, loss_fn, ...) {     super$compile(...)     self$d_optimizer <- d_optimizer     self$g_optimizer <- g_optimizer     self$loss_fn <- loss_fn   },   train_step = function(real_images) {     # Sample random points in the latent space     batch_size <- tf$shape(real_images)[1]     random_latent_vectors <-       tf$random$normal(shape = c(batch_size, self$latent_dim))      # Decode them to fake images     generated_images <- self$generator(random_latent_vectors)      # Combine them with real images     combined_images <-       tf$concat(list(generated_images, real_images),                 axis = 0L)      # Assemble labels discriminating real from fake images     labels <-       tf$concat(list(tf$ones(c(batch_size, 1L)),                      tf$zeros(c(batch_size, 1L))),                 axis = 0L)      # Add random noise to the labels - important trick!     labels %<>% `+`(tf$random$uniform(tf$shape(.), maxval = 0.05))      # Train the discriminator     with(tf$GradientTape() %as% tape, {       predictions <- self$discriminator(combined_images)       d_loss <- self$loss_fn(labels, predictions)     })     grads <- tape$gradient(d_loss, self$discriminator$trainable_weights)     self$d_optimizer$apply_gradients(       zip_lists(grads, self$discriminator$trainable_weights))      # Sample random points in the latent space     random_latent_vectors <-       tf$random$normal(shape = c(batch_size, self$latent_dim))      # Assemble labels that say \"all real images\"     misleading_labels <- tf$zeros(c(batch_size, 1L))      # Train the generator (note that we should *not* update the weights     # of the discriminator)!     with(tf$GradientTape() %as% tape, {       predictions <- self$discriminator(self$generator(random_latent_vectors))       g_loss <- self$loss_fn(misleading_labels, predictions)     })     grads <- tape$gradient(g_loss, self$generator$trainable_weights)     self$g_optimizer$apply_gradients(       zip_lists(grads, self$generator$trainable_weights))      list(d_loss = d_loss, g_loss = g_loss)   } ) # Prepare the dataset. We use both the training & test MNIST digits. batch_size <- 64 c(c(x_train, .), c(x_test, .)) %<-% dataset_mnist() all_digits <- k_concatenate(list(x_train, x_test)) all_digits <- k_reshape(all_digits, c(-1, 28, 28, 1)) dataset <- all_digits %>%   tfdatasets::tensor_slices_dataset() %>%   tfdatasets::dataset_map(function(x) tf$cast(x, \"float\")/255) %>%   tfdatasets::dataset_shuffle(buffer_size = 1024) %>%   tfdatasets::dataset_batch(batch_size = batch_size)  gan <- gan(discriminator=discriminator, generator=generator, latent_dim=latent_dim) gan$compile(     d_optimizer = optimizer_adam(learning_rate=0.0003),     g_optimizer = optimizer_adam(learning_rate=0.0003),     loss_fn = loss_binary_crossentropy(from_logits = TRUE) ) ## <keras.src.losses.losses.BinaryCrossentropy object at 0x2c95bdd20> # To limit the execution time, we only train on 100 batches. You can train on # the entire dataset. You will need about 20 epochs to get nice results. gan %>% fit(   tfdatasets::dataset_take(dataset, 100),   epochs = 1 ) ## 100/100 - 22s - 222ms/step - d_loss: 0.0000e+00 - g_loss: 0.0000e+00 - loss: 0.0000e+00"},{"path":"https://keras.posit.co/articles/distributed_training.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Multi-GPU and distributed training","text":"generally two ways distribute computation across multiple devices: Data parallelism, single model gets replicated multiple devices multiple machines. processes different batches data, merge results. exist many variants setup, differ different model replicas merge results, whether stay sync every batch whether loosely coupled, etc. Model parallelism, different parts single model run different devices, processing single batch data together. works best models naturally-parallel architecture, models feature multiple branches. guide focuses data parallelism, particular synchronous data parallelism, different replicas model stay sync batch process. Synchronicity keeps model convergence behavior identical see single-device training. Specifically, guide teaches use tf.distribute API train Keras models multiple GPUs, minimal changes code, following two setups: multiple GPUs (typically 2 8) installed single machine (single host, multi-device training). common setup researchers small-scale industry workflows. cluster many machines, hosting one multiple GPUs (multi-worker distributed training). good setup large-scale industry workflows, e.g. training high-resolution image classification models tens millions images using 20-100 GPUs.","code":""},{"path":"https://keras.posit.co/articles/distributed_training.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Multi-GPU and distributed training","text":"","code":"import tensorflow as tf import keras"},{"path":"https://keras.posit.co/articles/distributed_training.html","id":"single-host-multi-device-synchronous-training","dir":"Articles","previous_headings":"","what":"Single-host, multi-device synchronous training","title":"Multi-GPU and distributed training","text":"setup, one machine several GPUs (typically 2 8). device run copy model (called replica). simplicity, follows, ’ll assume ’re dealing 8 GPUs, loss generality. works step training: current batch data (called global batch) split 8 different sub-batches (called local batches). instance, global batch 512 samples, 8 local batches 64 samples. 8 replicas independently processes local batch: run forward pass, backward pass, outputting gradient weights respect loss model local batch. weight updates originating local gradients efficiently merged across 8 replicas. done end every step, replicas always stay sync. practice, process synchronously updating weights model replicas handled level individual weight variable. done mirrored variable object. use single-host, multi-device synchronous training Keras model, use tf.distribute.MirroredStrategy API. ’s works: Instantiate MirroredStrategy, optionally configuring specific devices want use (default strategy use GPUs available). Use strategy object open scope, within scope, create Keras objects need contain variables. Typically, means creating & compiling model inside distribution scope. Train model via fit() usual. Importantly, recommend use tf.data.Dataset objects load data multi-device distributed workflow. Schematically, looks like : ’s simple end--end runnable example:","code":"# Create a MirroredStrategy. strategy = tf.distribute.MirroredStrategy() print('Number of devices: {}'.format(strategy.num_replicas_in_sync))  # Open a strategy scope. with strategy.scope():   # Everything that creates variables should be under the strategy scope.   # In general this is only model construction & `compile()`.   model = Model(...)   model.compile(...)  # Train the model on all available devices. model.fit(train_dataset, validation_data=val_dataset, ...)  # Test the model on all available devices. model.evaluate(test_dataset) def get_compiled_model():     # Make a simple 2-layer densely-connected neural network.     inputs = keras.Input(shape=(784,))     x = keras.layers.Dense(256, activation=\"relu\")(inputs)     x = keras.layers.Dense(256, activation=\"relu\")(x)     outputs = keras.layers.Dense(10)(x)     model = keras.Model(inputs, outputs)     model.compile(         optimizer=keras.optimizers.Adam(),         loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),         metrics=[keras.metrics.SparseCategoricalAccuracy()],     )     return model   def get_dataset():     batch_size = 32     num_val_samples = 10000      # Return the MNIST dataset in the form of a `tf.data.Dataset`.     (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()      # Preprocess the data (these are Numpy arrays)     x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255     x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255     y_train = y_train.astype(\"float32\")     y_test = y_test.astype(\"float32\")      # Reserve num_val_samples samples for validation     x_val = x_train[-num_val_samples:]     y_val = y_train[-num_val_samples:]     x_train = x_train[:-num_val_samples]     y_train = y_train[:-num_val_samples]     return (         tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size),         tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size),         tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size),     )   # Create a MirroredStrategy. strategy = tf.distribute.MirroredStrategy() print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))  # Open a strategy scope. with strategy.scope():     # Everything that creates variables should be under the strategy scope.     # In general this is only model construction & `compile()`.     model = get_compiled_model()  # Train the model on all available devices. train_dataset, val_dataset, test_dataset = get_dataset() model.fit(train_dataset, epochs=2, validation_data=val_dataset)  # Test the model on all available devices. model.evaluate(test_dataset)"},{"path":"https://keras.posit.co/articles/distributed_training.html","id":"using-callbacks-to-ensure-fault-tolerance","dir":"Articles","previous_headings":"","what":"Using callbacks to ensure fault tolerance","title":"Multi-GPU and distributed training","text":"using distributed training, always make sure strategy recover failure (fault tolerance). simplest way handle pass ModelCheckpoint callback fit(), save model regular intervals (e.g. every 100 batches every epoch). can restart training saved model. ’s simple example:","code":"import os from tensorflow import keras  # Prepare a directory to store all the checkpoints. checkpoint_dir = \"./ckpt\" if not os.path.exists(checkpoint_dir):     os.makedirs(checkpoint_dir)   def make_or_restore_model():     # Either restore the latest model, or create a fresh one     # if there is no checkpoint available.     checkpoints = [checkpoint_dir + \"/\" + name for name in os.listdir(checkpoint_dir)]     if checkpoints:         latest_checkpoint = max(checkpoints, key=os.path.getctime)         print(\"Restoring from\", latest_checkpoint)         return keras.models.load_model(latest_checkpoint)     print(\"Creating a new model\")     return get_compiled_model()   def run_training(epochs=1):     # Create a MirroredStrategy.     strategy = tf.distribute.MirroredStrategy()      # Open a strategy scope and create/restore the model     with strategy.scope():         model = make_or_restore_model()      callbacks = [         # This callback saves a SavedModel every epoch         # We include the current epoch in the folder name.         keras.callbacks.ModelCheckpoint(             filepath=checkpoint_dir + \"/ckpt-{epoch}\", save_freq=\"epoch\"         )     ]     model.fit(         train_dataset,         epochs=epochs,         callbacks=callbacks,         validation_data=val_dataset,         verbose=2,     )   # Running the first time creates the model run_training(epochs=1)  # Calling the same function again will resume from where we left off run_training(epochs=1)"},{"path":"https://keras.posit.co/articles/distributed_training.html","id":"tf-data-performance-tips","dir":"Articles","previous_headings":"","what":"tf.data performance tips","title":"Multi-GPU and distributed training","text":"distributed training, efficiency load data can often become critical. tips make sure tf.data pipelines run fast possible. Note dataset batching creating dataset, make sure batched global batch size. instance, 8 GPUs capable running batch 64 samples, call use global batch size 512. Calling dataset.cache() call .cache() dataset, data cached running first iteration data. Every subsequent iteration use cached data. cache can memory (default) local file specify. can improve performance : data expected change iteration iteration reading data remote distributed filesystem reading data local disk, data fit memory workflow significantly IO-bound (e.g. reading & decoding image files). Calling dataset.prefetch(buffer_size) almost always call .prefetch(buffer_size) creating dataset. means data pipeline run asynchronously model, new samples preprocessed stored buffer current batch samples used train model. next batch prefetched GPU memory time current batch .","code":""},{"path":"https://keras.posit.co/articles/distributed_training.html","id":"multi-worker-distributed-synchronous-training","dir":"Articles","previous_headings":"","what":"Multi-worker distributed synchronous training","title":"Multi-GPU and distributed training","text":"works setup, multiple machines (called workers), one several GPUs . Much like happens single-host training, available GPU run one model replica, value variables replica kept sync batch. Importantly, current implementation assumes workers number GPUs (homogeneous cluster). use Set cluster (provide pointers ). Set appropriate TF_CONFIG environment variable worker. tells worker role communicate peers. worker, run model construction & compilation code within scope MultiWorkerMirroredStrategy object, similarly single-host training. Run evaluation code designated evaluator machine. Setting cluster First, set cluster (collective machines). machine individually setup able run model (typically, machine run Docker image) able access data source (e.g. GCS). Cluster management beyond scope guide. document help get started. can also take look Kubeflow. Setting TF_CONFIG environment variable code running worker almost code used single-host workflow (except different tf.distribute strategy object), one significant difference single-host workflow multi-worker workflow need set TF_CONFIG environment variable machine running cluster. TF_CONFIG environment variable JSON string specifies: cluster configuration, list addresses & ports machines make cluster worker’s “task”, role specific machine play within cluster. One example TF_CONFIG : multi-worker synchronous training setup, valid roles (task types) machines “worker” “evaluator”. example, 8 machines 4 GPUs , 7 workers one evaluator. workers train model, one processing sub-batches global batch. One workers (worker 0) serve “chief”, particular kind worker responsible saving logs checkpoints later reuse (typically Cloud storage location). evaluator runs continuous loop loads latest checkpoint saved chief worker, runs evaluation (asynchronously workers) writes evaluation logs (e.g. TensorBoard logs). Running code worker run training code worker (including chief) evaluation code evaluator. training code basically use single-host setup, except using MultiWorkerMirroredStrategy instead MirroredStrategy. worker run code (minus difference explained note ), including callbacks. Note: Callbacks save model checkpoints logs save different directory worker. standard practice workers save local disk (typically temporary), except worker 0, save TensorBoard logs checkpoints Cloud storage location later access & reuse. evaluator simply use MirroredStrategy (since runs single machine need communicate machines) call model.evaluate(). loading latest checkpoint saved chief worker Cloud storage location, save evaluation logs location chief logs.","code":"os.environ['TF_CONFIG'] = json.dumps({     'cluster': {         'worker': [\"localhost:12345\", \"localhost:23456\"]     },     'task': {'type': 'worker', 'index': 0} })"},{"path":"https://keras.posit.co/articles/distributed_training.html","id":"example-code-running-in-a-multi-worker-setup","dir":"Articles","previous_headings":"Multi-worker distributed synchronous training","what":"Example: code running in a multi-worker setup","title":"Multi-GPU and distributed training","text":"chief (worker 0): workers: evaluator:","code":"# Set TF_CONFIG os.environ['TF_CONFIG'] = json.dumps({     'cluster': {         'worker': [\"localhost:12345\", \"localhost:23456\"]     },     'task': {'type': 'worker', 'index': 0} })  # Open a strategy scope and create/restore the model. strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() with strategy.scope():   model = make_or_restore_model()  callbacks = [     # This callback saves a SavedModel every 100 batches     keras.callbacks.ModelCheckpoint(filepath='path/to/cloud/location/ckpt',                                     save_freq=100),     keras.callbacks.TensorBoard('path/to/cloud/location/tb/') ] model.fit(train_dataset,           callbacks=callbacks,           ...) # Set TF_CONFIG worker_index = 1  # For instance os.environ['TF_CONFIG'] = json.dumps({     'cluster': {         'worker': [\"localhost:12345\", \"localhost:23456\"]     },     'task': {'type': 'worker', 'index': worker_index} })  # Open a strategy scope and create/restore the model. # You can restore from the checkpoint saved by the chief. strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() with strategy.scope():   model = make_or_restore_model()  callbacks = [     keras.callbacks.ModelCheckpoint(filepath='local/path/ckpt', save_freq=100),     keras.callbacks.TensorBoard('local/path/tb/') ] model.fit(train_dataset,           callbacks=callbacks,           ...) strategy = tf.distribute.MirroredStrategy() with strategy.scope():   model = make_or_restore_model()  # Restore from the checkpoint saved by the chief.  results = model.evaluate(val_dataset) # Then, log the results on a shared location, write TensorBoard logs, etc"},{"path":"https://keras.posit.co/articles/distributed_training.html","id":"further-reading","dir":"Articles","previous_headings":"Multi-worker distributed synchronous training","what":"Further reading","title":"Multi-GPU and distributed training","text":"TensorFlow distributed training guide Tutorial multi-worker training Keras MirroredStrategy docs MultiWorkerMirroredStrategy docs Distributed training tf.keras Weights & Biases","code":""},{"path":"https://keras.posit.co/articles/distributed_training_with_jax.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Multi-GPU distributed training with JAX","text":"generally two ways distribute computation across multiple devices: Data parallelism, single model gets replicated multiple devices multiple machines. processes different batches data, merge results. exist many variants setup, differ different model replicas merge results, whether stay sync every batch whether loosely coupled, etc. Model parallelism, different parts single model run different devices, processing single batch data together. works best models naturally-parallel architecture, models feature multiple branches. guide focuses data parallelism, particular synchronous data parallelism, different replicas model stay sync batch process. Synchronicity keeps model convergence behavior identical see single-device training. Specifically, guide teaches use jax.sharding APIs train Keras models, minimal changes code, multiple GPUs TPUS (typically 2 16) installed single machine (single host, multi-device training). common setup researchers small-scale industry workflows.","code":""},{"path":"https://keras.posit.co/articles/distributed_training_with_jax.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Multi-GPU distributed training with JAX","text":"Let’s start defining function creates model train, function creates dataset train (MNIST case).","code":"import os  os.environ[\"KERAS_BACKEND\"] = \"jax\"  import jax import numpy as np import tensorflow as tf import keras  from jax.experimental import mesh_utils from jax.sharding import Mesh from jax.sharding import NamedSharding from jax.sharding import PartitionSpec as P   def get_model():     # Make a simple convnet with batch normalization and dropout.     inputs = keras.Input(shape=(28, 28, 1))     x = keras.layers.Rescaling(1.0 / 255.0)(inputs)     x = keras.layers.Conv2D(         filters=12, kernel_size=3, padding=\"same\", use_bias=False     )(x)     x = keras.layers.BatchNormalization(scale=False, center=True)(x)     x = keras.layers.ReLU()(x)     x = keras.layers.Conv2D(         filters=24,         kernel_size=6,         use_bias=False,         strides=2,     )(x)     x = keras.layers.BatchNormalization(scale=False, center=True)(x)     x = keras.layers.ReLU()(x)     x = keras.layers.Conv2D(         filters=32,         kernel_size=6,         padding=\"same\",         strides=2,         name=\"large_k\",     )(x)     x = keras.layers.BatchNormalization(scale=False, center=True)(x)     x = keras.layers.ReLU()(x)     x = keras.layers.GlobalAveragePooling2D()(x)     x = keras.layers.Dense(256, activation=\"relu\")(x)     x = keras.layers.Dropout(0.5)(x)     outputs = keras.layers.Dense(10)(x)     model = keras.Model(inputs, outputs)     return model   def get_datasets():     # Load the data and split it between train and test sets     (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()      # Scale images to the [0, 1] range     x_train = x_train.astype(\"float32\")     x_test = x_test.astype(\"float32\")     # Make sure images have shape (28, 28, 1)     x_train = np.expand_dims(x_train, -1)     x_test = np.expand_dims(x_test, -1)     print(\"x_train shape:\", x_train.shape)     print(x_train.shape[0], \"train samples\")     print(x_test.shape[0], \"test samples\")      # Create TF Datasets     train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))     eval_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))     return train_data, eval_data"},{"path":"https://keras.posit.co/articles/distributed_training_with_jax.html","id":"single-host-multi-device-synchronous-training","dir":"Articles","previous_headings":"","what":"Single-host, multi-device synchronous training","title":"Multi-GPU distributed training with JAX","text":"setup, one machine several GPUs TPUs (typically 2 16). device run copy model (called replica). simplicity, follows, ’ll assume ’re dealing 8 GPUs, loss generality. works step training: current batch data (called global batch) split 8 different sub-batches (called local batches). instance, global batch 512 samples, 8 local batches 64 samples. 8 replicas independently processes local batch: run forward pass, backward pass, outputting gradient weights respect loss model local batch. weight updates originating local gradients efficiently merged across 8 replicas. done end every step, replicas always stay sync. practice, process synchronously updating weights model replicas handled level individual weight variable. done using jax.sharding.NamedSharding configured replicate variables. use single-host, multi-device synchronous training Keras model, use jax.sharding features. ’s works: first create device mesh using mesh_utils.create_device_mesh. specify want replicate model optimizer variables across devices using spec axis. specify want shard data across devices using spec splits along batch dimension. use jax.device_put replicate model optimizer variables across devices. happens beginning. training loop, batch process, use jax.device_put split batch across devices invoking train step. ’s flow, step split utility function: ’s !","code":"# Config num_epochs = 2 batch_size = 64  train_data, eval_data = get_datasets() train_data = train_data.batch(batch_size, drop_remainder=True)  model = get_model() optimizer = keras.optimizers.Adam(1e-3) loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Initialize all state with .build() (one_batch, one_batch_labels) = next(iter(train_data)) model.build(one_batch) optimizer.build(model.trainable_variables)   # This is the loss function that will be differentiated. # Keras provides a pure functional forward pass: model.stateless_call def compute_loss(trainable_variables, non_trainable_variables, x, y):     y_pred, updated_non_trainable_variables = model.stateless_call(         trainable_variables, non_trainable_variables, x     )     loss_value = loss(y, y_pred)     return loss_value, updated_non_trainable_variables   # Function to compute gradients compute_gradients = jax.value_and_grad(compute_loss, has_aux=True)   # Training step, Keras provides a pure functional optimizer.stateless_apply @jax.jit def train_step(train_state, x, y):     (         trainable_variables,         non_trainable_variables,         optimizer_variables,     ) = train_state     (loss_value, non_trainable_variables), grads = compute_gradients(         trainable_variables, non_trainable_variables, x, y     )      trainable_variables, optimizer_variables = optimizer.stateless_apply(         optimizer_variables, grads, trainable_variables     )      return loss_value, (         trainable_variables,         non_trainable_variables,         optimizer_variables,     )   # Replicate the model and optimizer variable on all devices def get_replicated_train_state(devices):     # All variables will be replicated on all devices     var_mesh = Mesh(devices, axis_names=(\"_\"))     # In NamedSharding, axes not mentioned are replicated (all axes here)     var_replication = NamedSharding(var_mesh, P())      # Apply the distribution settings to the model variables     trainable_variables = jax.device_put(         model.trainable_variables, var_replication     )     non_trainable_variables = jax.device_put(         model.non_trainable_variables, var_replication     )     optimizer_variables = jax.device_put(optimizer.variables, var_replication)      # Combine all state in a tuple     return (trainable_variables, non_trainable_variables, optimizer_variables)   num_devices = len(jax.local_devices()) print(f\"Running on {num_devices} devices: {jax.local_devices()}\") devices = mesh_utils.create_device_mesh((num_devices,))  # Data will be split along the batch axis data_mesh = Mesh(devices, axis_names=(\"batch\",))  # naming axes of the mesh data_sharding = NamedSharding(     data_mesh,     P(         \"batch\",     ), )  # naming axes of the sharded partition  # Display data sharding x, y = next(iter(train_data)) sharded_x = jax.device_put(x.numpy(), data_sharding) print(\"Data sharding\") jax.debug.visualize_array_sharding(jax.numpy.reshape(sharded_x, [-1, 28 * 28]))  train_state = get_replicated_train_state(devices)  # Custom training loop for epoch in range(num_epochs):     data_iter = iter(train_data)     for data in data_iter:         x, y = data         sharded_x = jax.device_put(x.numpy(), data_sharding)         loss_value, train_state = train_step(train_state, sharded_x, y.numpy())     print(\"Epoch\", epoch, \"loss:\", loss_value)  # Post-processing model state update to write them back into the model trainable_variables, non_trainable_variables, optimizer_variables = train_state for variable, value in zip(model.trainable_variables, trainable_variables):     variable.assign(value) for variable, value in zip(     model.non_trainable_variables, non_trainable_variables ):     variable.assign(value)"},{"path":"https://keras.posit.co/articles/distributed_training_with_tensorflow.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Multi-GPU distributed training with TensorFlow","text":"generally two ways distribute computation across multiple devices: Data parallelism, single model gets replicated multiple devices multiple machines. processes different batches data, merge results. exist many variants setup, differ different model replicas merge results, whether stay sync every batch whether loosely coupled, etc. Model parallelism, different parts single model run different devices, processing single batch data together. works best models naturally-parallel architecture, models feature multiple branches. guide focuses data parallelism, particular synchronous data parallelism, different replicas model stay sync batch process. Synchronicity keeps model convergence behavior identical see single-device training. Specifically, guide teaches use tf.distribute API train Keras models multiple GPUs, minimal changes code, multiple GPUs (typically 2 16) installed single machine (single host, multi-device training). common setup researchers small-scale industry workflows.","code":""},{"path":"https://keras.posit.co/articles/distributed_training_with_tensorflow.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Multi-GPU distributed training with TensorFlow","text":"","code":"library(keras3) library(tensorflow) ## ## Attaching package: 'tensorflow' ## The following objects are masked from 'package:keras3': ## ##     set_random_seed, shape library(tfdatasets) ## ## Attaching package: 'tfdatasets' ## The following object is masked from 'package:keras3': ## ##     shape"},{"path":"https://keras.posit.co/articles/distributed_training_with_tensorflow.html","id":"single-host-multi-device-synchronous-training","dir":"Articles","previous_headings":"","what":"Single-host, multi-device synchronous training","title":"Multi-GPU distributed training with TensorFlow","text":"setup, one machine several GPUs (typically 2 16). device run copy model (called replica). simplicity, follows, ’ll assume ’re dealing 8 GPUs, loss generality. works step training: current batch data (called global batch) split 8 different sub-batches (called local batches). instance, global batch 512 samples, 8 local batches 64 samples. 8 replicas independently processes local batch: run forward pass, backward pass, outputting gradient weights respect loss model local batch. weight updates originating local gradients efficiently merged across 8 replicas. done end every step, replicas always stay sync. practice, process synchronously updating weights model replicas handled level individual weight variable. done mirrored variable object. use single-host, multi-device synchronous training Keras model, use tf$distribute$MirroredStrategy API. ’s works: Instantiate MirroredStrategy, optionally configuring specific devices want use (default strategy use GPUs available). Use strategy object open scope, within scope, create Keras objects need contain variables. Typically, means creating & compiling model inside distribution scope. cases, first call fit() may also create variables, ’s good idea put fit() call scope well. Train model via fit() usual. Importantly, recommend use tf.data.Dataset objects load data multi-device distributed workflow. Schematically, looks like : ’s simple end--end runnable example:","code":"# Create a MirroredStrategy. strategy <- tf$distribute$MirroredStrategy() cat(sprintf('Number of devices: %d', strategy$num_replicas_in_sync))  # Open a strategy scope. with(startegy$scope(), {   # Everything that creates variables should be under the strategy scope.   # In general this is only model construction & `compile()`.   model <- Model(...)   model %>% compile(...)    # Train the model on all available devices.   model %>% fit(train_dataset, validation_data=val_dataset, ...)    # Test the model on all available devices.   model %>% evaluate(test_dataset) }) get_compiled_model <- function() {   inputs <- layer_input(shape = 784)   outputs <- inputs %>%     layer_dense(units = 256, activation = \"relu\") %>%     layer_dense(units = 256, activation = \"relu\") %>%     layer_dense(units = 10)   model <- keras_model(inputs, outputs)   model %>% compile(     optimizer = optimizer_adam(),     loss = loss_sparse_categorical_crossentropy(from_logits=TRUE),     metrics=list(metric_sparse_categorical_accuracy())   )   model }  get_dataset <- function() {   batch_size <- 64   c(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_mnist()   x_train <- array_reshape(x_train, c(-1, 784))   x_test <- array_reshape(x_test, c(-1, 784))    # Reserve 10,000 samples for validation.   x_val <- x_train[1:10000,]   y_val <- y_train[1:10000]   x_train = x_train[-c(1:10000),]   y_train = y_train[-c(1:10000)]    # Prepare the training dataset.   train_dataset <- list(x_train, y_train) %>%     tensor_slices_dataset() %>%     dataset_batch(batch_size)    # Prepare the validation dataset.   val_dataset <- list(x_val, y_val) %>%     tensor_slices_dataset() %>%     dataset_batch(batch_size)    # Prepare the test dataset.   test_dataset <- list(x_test, y_test) %>%     tensor_slices_dataset() %>%     dataset_batch(batch_size)    list(train_dataset, val_dataset, test_dataset) }  # Create a MirroredStrategy. strategy <- tf$distribute$MirroredStrategy() cat(sprintf('Number of devices: %d', strategy$num_replicas_in_sync)) ## Number of devices: 1 # Open a strategy scope. with(strategy$scope(), {   # Everything that creates variables should be under the strategy scope.   # In general this is only model construction & `compile()`.   model <- get_compiled_model()    # Train the model on all available devices.   c(train_dataset, val_dataset, test_dataset) %<-% get_dataset()   model %>% fit(train_dataset, epochs=2, validation_data=val_dataset)    # Test the model on all available devices.   model %>% evaluate(test_dataset) }) ## Epoch 1/2 ## 782/782 - 3s - 3ms/step - loss: 2.0840 - sparse_categorical_accuracy: 0.8817 - val_loss: 0.6871 - val_sparse_categorical_accuracy: 0.9126 ## Epoch 2/2 ## 782/782 - 2s - 3ms/step - loss: 0.4243 - sparse_categorical_accuracy: 0.9337 - val_loss: 0.4643 - val_sparse_categorical_accuracy: 0.9277 ## 157/157 - 0s - 992us/step - loss: 0.4673 - sparse_categorical_accuracy: 0.9292 ##            loss compile_metrics ##        0.467276        0.929200"},{"path":"https://keras.posit.co/articles/distributed_training_with_tensorflow.html","id":"using-callbacks-to-ensure-fault-tolerance","dir":"Articles","previous_headings":"","what":"Using callbacks to ensure fault tolerance","title":"Multi-GPU distributed training with TensorFlow","text":"using distributed training, always make sure strategy recover failure (fault tolerance). simplest way handle pass ModelCheckpoint callback fit(), save model regular intervals (e.g. every 100 batches every epoch). can restart training saved model. ’s simple example:","code":"# Prepare a directory to store all the checkpoints. checkpoint_dir <- \"./ckpt\" if (!dir.exists(checkpoint_dir)) {   dir.create(checkpoint_dir) }  make_or_restore_model <- function() {   # Either restore the latest model, or create a fresh one   # if there is no checkpoint available.   checkpoints <- list.files(checkpoint_dir, full.names=TRUE)    if (length(checkpoints) > 0) {     latest_checkpoint <- tail(checkpoints, 1)     load_model(latest_checkpoint)   } else {     get_compiled_model()   } }   run_training <- function(epochs=1) {   # Create a MirroredStrategy.   strategy <- tf$distribute$MirroredStrategy()    # Open a strategy scope and create/restore the model   with(strategy$scope(), {     model <- make_or_restore_model()      callbacks <- list(       # This callback saves a SavedModel every epoch       # We include the current epoch in the folder name.       callback_model_checkpoint(         filepath = paste0(checkpoint_dir, \"/ckpt-{epoch}.keras\"),         save_freq=\"epoch\",       )     )      model %>% fit(       train_dataset,       epochs=epochs,       callbacks=callbacks,       validation_data=val_dataset,       verbose=2     )   }) }  # Running the first time creates the model run_training(epochs=1) ## Error in load_model(latest_checkpoint): could not find function \"load_model\" # Calling the same function again will resume from where we left off run_training(epochs=1) ## Error in load_model(latest_checkpoint): could not find function \"load_model\""},{"path":"https://keras.posit.co/articles/distributed_training_with_tensorflow.html","id":"tfdata-performance-tips","dir":"Articles","previous_headings":"","what":"tf$data performance tips","title":"Multi-GPU distributed training with TensorFlow","text":"distributed training, efficiency load data can often become critical. tips make sure tf$data pipelines run fast possible. Note dataset batching creating dataset, make sure batched global batch size. instance, 8 GPUs capable running batch 64 samples, call use global batch size 512. Calling dataset_cache() call dataset_cache() dataset, data cached running first iteration data. Every subsequent iteration use cached data. cache can memory (default) local file specify. can improve performance : data expected change iteration iteration reading data remote distributed filesystem reading data local disk, data fit memory workflow significantly IO-bound (e.g. reading & decoding image files). Calling dataset_prefetch(buffer_size) almost always call dataset_prefetch(buffer_size) creating dataset. means data pipeline run asynchronously model, new samples preprocessed stored buffer current batch samples used train model. next batch prefetched GPU memory time current batch . ’s !","code":""},{"path":"https://keras.posit.co/articles/distributed_training_with_torch.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Multi-GPU distributed training with PyTorch","text":"generally two ways distribute computation across multiple devices: Data parallelism, single model gets replicated multiple devices multiple machines. processes different batches data, merge results. exist many variants setup, differ different model replicas merge results, whether stay sync every batch whether loosely coupled, etc. Model parallelism, different parts single model run different devices, processing single batch data together. works best models naturally-parallel architecture, models feature multiple branches. guide focuses data parallelism, particular synchronous data parallelism, different replicas model stay sync batch process. Synchronicity keeps model convergence behavior identical see single-device training. Specifically, guide teaches use PyTorch’s DistributedDataParallel module wrapper train Keras, minimal changes code, multiple GPUs (typically 2 16) installed single machine (single host, multi-device training). common setup researchers small-scale industry workflows.","code":""},{"path":"https://keras.posit.co/articles/distributed_training_with_torch.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Multi-GPU distributed training with PyTorch","text":"Let’s start defining function creates model train, function creates dataset train (MNIST case). Next, let’s define simple PyTorch training loop targets GPU (note calls .cuda()).","code":"import os  os.environ[\"KERAS_BACKEND\"] = \"torch\"  import torch import numpy as np import keras   def get_model():     # Make a simple convnet with batch normalization and dropout.     inputs = keras.Input(shape=(28, 28, 1))     x = keras.layers.Rescaling(1.0 / 255.0)(inputs)     x = keras.layers.Conv2D(         filters=12, kernel_size=3, padding=\"same\", use_bias=False     )(x)     x = keras.layers.BatchNormalization(scale=False, center=True)(x)     x = keras.layers.ReLU()(x)     x = keras.layers.Conv2D(         filters=24,         kernel_size=6,         use_bias=False,         strides=2,     )(x)     x = keras.layers.BatchNormalization(scale=False, center=True)(x)     x = keras.layers.ReLU()(x)     x = keras.layers.Conv2D(         filters=32,         kernel_size=6,         padding=\"same\",         strides=2,         name=\"large_k\",     )(x)     x = keras.layers.BatchNormalization(scale=False, center=True)(x)     x = keras.layers.ReLU()(x)     x = keras.layers.GlobalAveragePooling2D()(x)     x = keras.layers.Dense(256, activation=\"relu\")(x)     x = keras.layers.Dropout(0.5)(x)     outputs = keras.layers.Dense(10)(x)     model = keras.Model(inputs, outputs)     return model   def get_dataset():     # Load the data and split it between train and test sets     (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()      # Scale images to the [0, 1] range     x_train = x_train.astype(\"float32\")     x_test = x_test.astype(\"float32\")     # Make sure images have shape (28, 28, 1)     x_train = np.expand_dims(x_train, -1)     x_test = np.expand_dims(x_test, -1)     print(\"x_train shape:\", x_train.shape)      # Create a TensorDataset     dataset = torch.utils.data.TensorDataset(         torch.from_numpy(x_train), torch.from_numpy(y_train)     )     return dataset def train_model(model, dataloader, num_epochs, optimizer, loss_fn):     for epoch in range(num_epochs):         running_loss = 0.0         running_loss_count = 0         for batch_idx, (inputs, targets) in enumerate(dataloader):             inputs = inputs.cuda(non_blocking=True)             targets = targets.cuda(non_blocking=True)              # Forward pass             outputs = model(inputs)             loss = loss_fn(outputs, targets)              # Backward and optimize             optimizer.zero_grad()             loss.backward()             optimizer.step()              running_loss += loss.item()             running_loss_count += 1          # Print loss statistics         print(             f\"Epoch {epoch + 1}/{num_epochs}, \"             f\"Loss: {running_loss / running_loss_count}\"         )"},{"path":"https://keras.posit.co/articles/distributed_training_with_torch.html","id":"single-host-multi-device-synchronous-training","dir":"Articles","previous_headings":"","what":"Single-host, multi-device synchronous training","title":"Multi-GPU distributed training with PyTorch","text":"setup, one machine several GPUs (typically 2 16). device run copy model (called replica). simplicity, follows, ’ll assume ’re dealing 8 GPUs, loss generality. works step training: current batch data (called global batch) split 8 different sub-batches (called local batches). instance, global batch 512 samples, 8 local batches 64 samples. 8 replicas independently processes local batch: run forward pass, backward pass, outputting gradient weights respect loss model local batch. weight updates originating local gradients efficiently merged across 8 replicas. done end every step, replicas always stay sync. practice, process synchronously updating weights model replicas handled level individual weight variable. done mirrored variable object. use single-host, multi-device synchronous training Keras model, use torch.nn.parallel.DistributedDataParallel module wrapper. ’s works: use torch.multiprocessing.start_processes start multiple Python processes, one per device. process run per_device_launch_fn function. uses torch.distributed.init_process_group torch.cuda.set_device configure device used process. uses torch.utils.data.distributed.DistributedSampler torch.utils.data.DataLoader turn data distributed data loader. also uses torch.nn.parallel.DistributedDataParallel turn model distributed PyTorch module. calls train_model function. train_model function run process, model using separate device process. ’s flow, step split utility function: Time start multiple processes: ’s !","code":"# Config num_gpu = torch.cuda.device_count() num_epochs = 2 batch_size = 64 print(f\"Running on {num_gpu} GPUs\")   def setup_device(current_gpu_index, num_gpus):     # Device setup     os.environ[\"MASTER_ADDR\"] = \"localhost\"     os.environ[\"MASTER_PORT\"] = \"56492\"     device = torch.device(\"cuda:{}\".format(current_gpu_index))     torch.distributed.init_process_group(         backend=\"nccl\",         init_method=\"env://\",         world_size=num_gpus,         rank=current_gpu_index,     )     torch.cuda.set_device(device)   def cleanup():     torch.distributed.destroy_process_group()   def prepare_dataloader(dataset, current_gpu_index, num_gpus, batch_size):     sampler = torch.utils.data.distributed.DistributedSampler(         dataset,         num_replicas=num_gpus,         rank=current_gpu_index,         shuffle=False,     )     dataloader = torch.utils.data.DataLoader(         dataset,         sampler=sampler,         batch_size=batch_size,         shuffle=False,     )     return dataloader   def per_device_launch_fn(current_gpu_index, num_gpu):     # Setup the process groups     setup_device(current_gpu_index, num_gpu)      dataset = get_dataset()     model = get_model()      # prepare the dataloader     dataloader = prepare_dataloader(         dataset, current_gpu_index, num_gpu, batch_size     )      # Instantiate the torch optimizer     optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)      # Instantiate the torch loss function     loss_fn = torch.nn.CrossEntropyLoss()      # Put model on device     model = model.to(current_gpu_index)     ddp_model = torch.nn.parallel.DistributedDataParallel(         model, device_ids=[current_gpu_index], output_device=current_gpu_index     )      train_model(ddp_model, dataloader, num_epochs, optimizer, loss_fn)      cleanup() if __name__ == \"__main__\":     # We use the \"fork\" method rather than \"spawn\" to support notebooks     torch.multiprocessing.start_processes(         per_device_launch_fn,         args=(num_gpu,),         nprocs=num_gpu,         join=True,         start_method=\"fork\",     )"},{"path":"https://keras.posit.co/articles/examples/actor_critic_cartpole.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Actor Critic Method","text":"script shows implementation Actor Critic method CartPole-V0 environment.","code":""},{"path":"https://keras.posit.co/articles/examples/actor_critic_cartpole.html","id":"actor-critic-method","dir":"Articles > Examples","previous_headings":"Introduction","what":"Actor Critic Method","title":"Actor Critic Method","text":"agent takes actions moves environment, learns map observed state environment two possible outputs: Recommended action: probability value action action space. part agent responsible output called actor. Estimated rewards future: Sum rewards expects receive future. part agent responsible output critic. Agent Critic learn perform tasks, recommended actions actor maximize rewards.","code":""},{"path":"https://keras.posit.co/articles/examples/actor_critic_cartpole.html","id":"cartpole-v1","dir":"Articles > Examples","previous_headings":"Introduction","what":"CartPole-V1","title":"Actor Critic Method","text":"pole attached cart placed frictionless track. agent apply force move cart. rewarded every time step pole remains upright. agent, therefore, must learn keep pole falling .","code":""},{"path":"https://keras.posit.co/articles/examples/actor_critic_cartpole.html","id":"references","dir":"Articles > Examples","previous_headings":"Introduction","what":"References","title":"Actor Critic Method","text":"CartPole Actor Critic Method","code":""},{"path":"https://keras.posit.co/articles/examples/actor_critic_cartpole.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Actor Critic Method","text":"","code":"import os  os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  import keras as keras from keras import layers  import gym import numpy as np import tensorflow as tf   # Configuration parameters for the whole setup seed = 42 gamma = 0.99  # Discount factor for past rewards max_steps_per_episode = 10000 env = gym.make(\"CartPole-v1\", new_step_api=True)  # Create the environment env.reset(seed=seed) eps = np.finfo(     np.float32 ).eps.item()  # Smallest number such that 1.0 + eps != 1.0"},{"path":"https://keras.posit.co/articles/examples/actor_critic_cartpole.html","id":"implement-actor-critic-network","dir":"Articles > Examples","previous_headings":"","what":"Implement Actor Critic network","title":"Actor Critic Method","text":"network learns two functions: Actor: takes input state environment returns probability value action action space. Critic: takes input state environment returns estimate total rewards future. implementation, share initial layer.","code":"num_inputs = 4 num_actions = 2 num_hidden = 128  inputs = layers.Input(shape=(num_inputs,)) common = layers.Dense(num_hidden, activation=\"relu\")(inputs) action = layers.Dense(num_actions, activation=\"softmax\")(common) critic = layers.Dense(1)(common)  model = keras.Model(inputs=inputs, outputs=[action, critic])"},{"path":"https://keras.posit.co/articles/examples/actor_critic_cartpole.html","id":"train","dir":"Articles > Examples","previous_headings":"","what":"Train","title":"Actor Critic Method","text":"","code":"optimizer = keras.optimizers.Adam(learning_rate=0.01) huber_loss = keras.losses.Huber() action_probs_history = [] critic_value_history = [] rewards_history = [] running_reward = 0 episode_count = 0  while True:  # Run until solved     state = env.reset()     episode_reward = 0     with tf.GradientTape() as tape:         for timestep in range(1, max_steps_per_episode):             # env.render(); Adding this line would show the attempts             # of the agent in a pop up window.              state = tf.convert_to_tensor(state)             state = tf.expand_dims(state, 0)              # Predict action probabilities and estimated future rewards             # from environment state             action_probs, critic_value = model(state)             critic_value_history.append(critic_value[0, 0])              # Sample action from action probability distribution             action = np.random.choice(num_actions, p=np.squeeze(action_probs))             action_probs_history.append(tf.math.log(action_probs[0, action]))              # Apply the sampled action in our environment             state, reward, done, _, _ = env.step(action)             rewards_history.append(reward)             episode_reward += reward              if done:                 break          # Update running reward to check condition for solving         running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward          # Calculate expected value from rewards         # - At each timestep what was the total reward received after that timestep         # - Rewards in the past are discounted by multiplying them with gamma         # - These are the labels for our critic         returns = []         discounted_sum = 0         for r in rewards_history[::-1]:             discounted_sum = r + gamma * discounted_sum             returns.insert(0, discounted_sum)          # Normalize         returns = np.array(returns)         returns = (returns - np.mean(returns)) / (np.std(returns) + eps)         returns = returns.tolist()          # Calculating loss values to update our network         history = zip(action_probs_history, critic_value_history, returns)         actor_losses = []         critic_losses = []         for log_prob, value, ret in history:             # At this point in history, the critic estimated that we would get a             # total reward = `value` in the future. We took an action with log probability             # of `log_prob` and ended up recieving a total reward = `ret`.             # The actor must be updated so that it predicts an action that leads to             # high rewards (compared to critic's estimate) with high probability.             diff = ret - value             actor_losses.append(-log_prob * diff)  # actor loss              # The critic must be updated so that it predicts a better estimate of             # the future rewards.             critic_losses.append(                 huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))             )          # Backpropagation         loss_value = sum(actor_losses) + sum(critic_losses)         grads = tape.gradient(loss_value, model.trainable_variables)         optimizer.apply_gradients(zip(grads, model.trainable_variables))          # Clear the loss and reward history         action_probs_history.clear()         critic_value_history.clear()         rewards_history.clear()      # Log details     episode_count += 1     if episode_count % 10 == 0:         template = \"running reward: {:.2f} at episode {}\"         print(template.format(running_reward, episode_count))      if running_reward > 195:  # Condition to consider the task solved         print(\"Solved at episode {}!\".format(episode_count))         break"},{"path":"https://keras.posit.co/articles/examples/actor_critic_cartpole.html","id":"visualizations","dir":"Articles > Examples","previous_headings":"","what":"Visualizations","title":"Actor Critic Method","text":"early stages training: later stages training:","code":""},{"path":"https://keras.posit.co/articles/examples/addition_rnn.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Sequence to sequence learning for performing number addition","text":"example, train model learn add two numbers, provided strings. Example: Input: “535+61” Output: “596” Input may optionally reversed, shown increase performance many tasks : Learning Execute Sequence Sequence Learning Neural Networks. Theoretically, sequence order inversion introduces shorter term dependencies source target problem. Results: two digits (reversed): One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy 55 epochs Three digits (reversed): One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy 100 epochs Four digits (reversed): One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy 20 epochs Five digits (reversed): One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy 30 epochs","code":""},{"path":"https://keras.posit.co/articles/examples/addition_rnn.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Sequence to sequence learning for performing number addition","text":"","code":"import keras as keras from keras import layers import numpy as np  # Parameters for the model and dataset. TRAINING_SIZE = 50000 DIGITS = 3 REVERSE = True  # Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of # int is DIGITS. MAXLEN = DIGITS + 1 + DIGITS"},{"path":"https://keras.posit.co/articles/examples/addition_rnn.html","id":"generate-the-data","dir":"Articles > Examples","previous_headings":"","what":"Generate the data","title":"Sequence to sequence learning for performing number addition","text":"","code":"class CharacterTable:     \"\"\"Given a set of characters:     + Encode them to a one-hot integer representation     + Decode the one-hot or integer representation to their character output     + Decode a vector of probabilities to their character output     \"\"\"      def __init__(self, chars):         \"\"\"Initialize character table.         # Arguments             chars: Characters that can appear in the input.         \"\"\"         self.chars = sorted(set(chars))         self.char_indices = dict((c, i) for i, c in enumerate(self.chars))         self.indices_char = dict((i, c) for i, c in enumerate(self.chars))      def encode(self, C, num_rows):         \"\"\"One-hot encode given string C.         # Arguments             C: string, to be encoded.             num_rows: Number of rows in the returned one-hot encoding. This is                 used to keep the # of rows for each data the same.         \"\"\"         x = np.zeros((num_rows, len(self.chars)))         for i, c in enumerate(C):             x[i, self.char_indices[c]] = 1         return x      def decode(self, x, calc_argmax=True):         \"\"\"Decode the given vector or 2D array to their character output.         # Arguments             x: A vector or a 2D array of probabilities or one-hot representations;                 or a vector of character indices (used with `calc_argmax=False`).             calc_argmax: Whether to find the character index with maximum                 probability, defaults to `True`.         \"\"\"         if calc_argmax:             x = x.argmax(axis=-1)         return \"\".join(self.indices_char[x] for x in x)   # All the numbers, plus sign and space for padding. chars = \"0123456789+ \" ctable = CharacterTable(chars)  questions = [] expected = [] seen = set() print(\"Generating data...\") while len(questions) < TRAINING_SIZE:     f = lambda: int(         \"\".join(             np.random.choice(list(\"0123456789\"))             for i in range(np.random.randint(1, DIGITS + 1))         )     )     a, b = f(), f()     # Skip any addition questions we've already seen     # Also skip any such that x+Y == Y+x (hence the sorting).     key = tuple(sorted((a, b)))     if key in seen:         continue     seen.add(key)     # Pad the data with spaces such that it is always MAXLEN.     q = \"{}+{}\".format(a, b)     query = q + \" \" * (MAXLEN - len(q))     ans = str(a + b)     # Answers can be of maximum size DIGITS + 1.     ans += \" \" * (DIGITS + 1 - len(ans))     if REVERSE:         # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the         # space used for padding.)         query = query[::-1]     questions.append(query)     expected.append(ans) print(\"Total questions:\", len(questions))"},{"path":"https://keras.posit.co/articles/examples/addition_rnn.html","id":"vectorize-the-data","dir":"Articles > Examples","previous_headings":"","what":"Vectorize the data","title":"Sequence to sequence learning for performing number addition","text":"","code":"print(\"Vectorization...\") x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=bool) y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=bool) for i, sentence in enumerate(questions):     x[i] = ctable.encode(sentence, MAXLEN) for i, sentence in enumerate(expected):     y[i] = ctable.encode(sentence, DIGITS + 1)  # Shuffle (x, y) in unison as the later parts of x will almost all be larger # digits. indices = np.arange(len(y)) np.random.shuffle(indices) x = x[indices] y = y[indices]  # Explicitly set apart 10% for validation data that we never train over. split_at = len(x) - len(x) // 10 (x_train, x_val) = x[:split_at], x[split_at:] (y_train, y_val) = y[:split_at], y[split_at:]  print(\"Training Data:\") print(x_train.shape) print(y_train.shape)  print(\"Validation Data:\") print(x_val.shape) print(y_val.shape)"},{"path":"https://keras.posit.co/articles/examples/addition_rnn.html","id":"build-the-model","dir":"Articles > Examples","previous_headings":"","what":"Build the model","title":"Sequence to sequence learning for performing number addition","text":"","code":"print(\"Build model...\") num_layers = 1  # Try to add more LSTM layers!  model = keras.Sequential() # \"Encode\" the input sequence using a LSTM, producing an output of size 128. # Note: In a situation where your input sequences have a variable length, # use input_shape=(None, num_feature). model.add(layers.Input((MAXLEN, len(chars)))) model.add(layers.LSTM(128)) # As the decoder RNN's input, repeatedly provide with the last output of # RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum # length of output, e.g., when DIGITS=3, max output is 999+999=1998. model.add(layers.RepeatVector(DIGITS + 1)) # The decoder RNN could be multiple layers stacked or a single layer. for _ in range(num_layers):     # By setting return_sequences to True, return not only the last output but     # all the outputs so far in the form of (num_samples, timesteps,     # output_dim). This is necessary as TimeDistributed in the below expects     # the first dimension to be the timesteps.     model.add(layers.LSTM(128, return_sequences=True))  # Apply a dense layer to the every temporal slice of an input. For each of step # of the output sequence, decide which character should be chosen. model.add(layers.Dense(len(chars), activation=\"softmax\")) model.compile(     loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"] ) model.summary()"},{"path":"https://keras.posit.co/articles/examples/addition_rnn.html","id":"train-the-model","dir":"Articles > Examples","previous_headings":"","what":"Train the model","title":"Sequence to sequence learning for performing number addition","text":"’ll get 99+% validation accuracy ~30 epochs. Example available HuggingFace.","code":"epochs = 30 batch_size = 32   # Train the model each generation and show predictions against the validation # dataset. for epoch in range(1, epochs):     print()     print(\"Iteration\", epoch)     model.fit(         x_train,         y_train,         batch_size=batch_size,         epochs=1,         validation_data=(x_val, y_val),     )     # Select 10 samples from the validation set at random so we can visualize     # errors.     for i in range(10):         ind = np.random.randint(0, len(x_val))         rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]         preds = np.argmax(model.predict(rowx), axis=-1)         q = ctable.decode(rowx[0])         correct = ctable.decode(rowy[0])         guess = ctable.decode(preds[0], calc_argmax=False)         print(\"Q\", q[::-1] if REVERSE else q, end=\" \")         print(\"T\", correct, end=\" \")         if correct == guess:             print(\"☑ \" + guess)         else:             print(\"☒ \" + guess)"},{"path":"https://keras.posit.co/articles/examples/antirectifier.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Simple custom layer example: Antirectifier","text":"example shows create custom layers, using Antirectifier layer (originally proposed Keras example script January 2016), alternative ReLU. Instead zeroing-negative part input, splits negative positive parts returns concatenation absolute value . avoids loss information, cost increase dimensionality. fix dimensionality increase, linearly combine features back space original size.","code":""},{"path":"https://keras.posit.co/articles/examples/antirectifier.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Simple custom layer example: Antirectifier","text":"","code":"import tensorflow as tf import keras as keras from keras import layers"},{"path":"https://keras.posit.co/articles/examples/antirectifier.html","id":"the-antirectifier-layer","dir":"Articles > Examples","previous_headings":"","what":"The Antirectifier layer","title":"Simple custom layer example: Antirectifier","text":"","code":"class Antirectifier(layers.Layer):     def __init__(self, initializer=\"he_normal\", **kwargs):         super().__init__(**kwargs)         self.initializer = keras.initializers.get(initializer)      def build(self, input_shape):         output_dim = input_shape[-1]         self.kernel = self.add_weight(             shape=(output_dim * 2, output_dim),             initializer=self.initializer,             name=\"kernel\",             trainable=True,         )      def call(self, inputs):         inputs -= tf.reduce_mean(inputs, axis=-1, keepdims=True)         pos = tf.nn.relu(inputs)         neg = tf.nn.relu(-inputs)         concatenated = tf.concat([pos, neg], axis=-1)         mixed = tf.matmul(concatenated, self.kernel)         return mixed      def get_config(self):         # Implement get_config to enable serialization. This is optional.         base_config = super().get_config()         config = {\"initializer\": keras.initializers.serialize(self.initializer)}         return dict(list(base_config.items()) + list(config.items()))"},{"path":"https://keras.posit.co/articles/examples/antirectifier.html","id":"lets-test-drive-it-on-mnist","dir":"Articles > Examples","previous_headings":"","what":"Let’s test-drive it on MNIST","title":"Simple custom layer example: Antirectifier","text":"","code":"# Training parameters batch_size = 128 num_classes = 10 epochs = 20  # The data, split between train and test sets (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()  x_train = x_train.reshape(-1, 784) x_test = x_test.reshape(-1, 784) x_train = x_train.astype(\"float32\") x_test = x_test.astype(\"float32\") x_train /= 255 x_test /= 255 print(x_train.shape[0], \"train samples\") print(x_test.shape[0], \"test samples\")  # Build the model model = keras.Sequential(     [         keras.Input(shape=(784,)),         layers.Dense(256),         Antirectifier(),         layers.Dense(256),         Antirectifier(),         layers.Dropout(0.5),         layers.Dense(10),     ] )  # Compile the model model.compile(     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),     optimizer=keras.optimizers.RMSprop(),     metrics=[keras.metrics.SparseCategoricalAccuracy()], )  # Train the model model.fit(     x_train,     y_train,     batch_size=batch_size,     epochs=epochs,     validation_split=0.15, )  # Test the model model.evaluate(x_test, y_test)"},{"path":[]},{"path":"https://keras.posit.co/articles/examples/attention_mil_classification.html","id":"what-is-multiple-instance-learning-mil","dir":"Articles > Examples","previous_headings":"Introduction","what":"What is Multiple Instance Learning (MIL)?","title":"Classification using Attention-based Deep Multiple Instance Learning (MIL).","text":"Usually, supervised learning algorithms, learner receives labels set instances. case MIL, learner receives labels set bags, contains set instances. bag labeled positive contains least one positive instance, negative contain .","code":""},{"path":"https://keras.posit.co/articles/examples/attention_mil_classification.html","id":"motivation","dir":"Articles > Examples","previous_headings":"Introduction","what":"Motivation","title":"Classification using Attention-based Deep Multiple Instance Learning (MIL).","text":"often assumed image classification tasks image clearly represents class label. medical imaging (e.g. computational pathology, etc.) entire image represented single class label (cancerous/non-cancerous) region interest given. However, one interested knowing patterns image actually causing belong class. context, image(s) divided subimages form bag instances. Therefore, goals : Learn model predict class label bag instances. Find instances within bag caused position class label prediction.","code":""},{"path":"https://keras.posit.co/articles/examples/attention_mil_classification.html","id":"implementation","dir":"Articles > Examples","previous_headings":"Introduction","what":"Implementation","title":"Classification using Attention-based Deep Multiple Instance Learning (MIL).","text":"following steps describe model works: feature extractor layers extract feature embeddings. embeddings fed MIL attention layer get attention scores. layer designed permutation-invariant. Input features corresponding attention scores multiplied together. resulting output passed softmax function classification.","code":""},{"path":"https://keras.posit.co/articles/examples/attention_mil_classification.html","id":"references","dir":"Articles > Examples","previous_headings":"Introduction","what":"References","title":"Classification using Attention-based Deep Multiple Instance Learning (MIL).","text":"Attention-based Deep Multiple Instance Learning. attention operator code implementation inspired https://github.com/utayao/Atten_Deep_MIL. Imbalanced data tutorial TensorFlow.","code":""},{"path":"https://keras.posit.co/articles/examples/attention_mil_classification.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Classification using Attention-based Deep Multiple Instance Learning (MIL).","text":"","code":"import numpy as np import keras as keras from keras import layers from keras import ops from tqdm import tqdm from matplotlib import pyplot as plt  plt.style.use(\"ggplot\")"},{"path":"https://keras.posit.co/articles/examples/attention_mil_classification.html","id":"create-dataset","dir":"Articles > Examples","previous_headings":"","what":"Create dataset","title":"Classification using Attention-based Deep Multiple Instance Learning (MIL).","text":"create set bags assign labels according contents. least one positive instance available bag, bag considered positive bag. contain positive instance, bag considered negative.","code":""},{"path":"https://keras.posit.co/articles/examples/attention_mil_classification.html","id":"configuration-parameters","dir":"Articles > Examples","previous_headings":"Create dataset","what":"Configuration parameters","title":"Classification using Attention-based Deep Multiple Instance Learning (MIL).","text":"POSITIVE_CLASS: desired class kept positive bag. BAG_COUNT: number training bags. VAL_BAG_COUNT: number validation bags. BAG_SIZE: number instances bag. PLOT_SIZE: number bags plot. ENSEMBLE_AVG_COUNT: number models create average together. (Optional: often results better performance - set 1 single model)","code":"POSITIVE_CLASS = 1 BAG_COUNT = 1000 VAL_BAG_COUNT = 300 BAG_SIZE = 3 PLOT_SIZE = 3 ENSEMBLE_AVG_COUNT = 1"},{"path":"https://keras.posit.co/articles/examples/attention_mil_classification.html","id":"prepare-bags","dir":"Articles > Examples","previous_headings":"Create dataset","what":"Prepare bags","title":"Classification using Attention-based Deep Multiple Instance Learning (MIL).","text":"Since attention operator permutation-invariant operator, instance positive class label randomly placed among instances positive bag.","code":"def create_bags(     input_data, input_labels, positive_class, bag_count, instance_count ):     # Set up bags.     bags = []     bag_labels = []      # Normalize input data.     input_data = np.divide(input_data, 255.0)      # Count positive samples.     count = 0      for _ in range(bag_count):         # Pick a fixed size random subset of samples.         index = np.random.choice(             input_data.shape[0], instance_count, replace=False         )         instances_data = input_data[index]         instances_labels = input_labels[index]          # By default, all bags are labeled as 0.         bag_label = 0          # Check if there is at least a positive class in the bag.         if positive_class in instances_labels:             # Positive bag will be labeled as 1.             bag_label = 1             count += 1          bags.append(instances_data)         bag_labels.append(np.array([bag_label]))      print(f\"Positive bags: {count}\")     print(f\"Negative bags: {bag_count - count}\")      return (list(np.swapaxes(bags, 0, 1)), np.array(bag_labels))   # Load the MNIST dataset. (x_train, y_train), (x_val, y_val) = keras.datasets.mnist.load_data()  # Create training data. train_data, train_labels = create_bags(     x_train, y_train, POSITIVE_CLASS, BAG_COUNT, BAG_SIZE )  # Create validation data. val_data, val_labels = create_bags(     x_val, y_val, POSITIVE_CLASS, VAL_BAG_COUNT, BAG_SIZE )"},{"path":"https://keras.posit.co/articles/examples/attention_mil_classification.html","id":"create-the-model","dir":"Articles > Examples","previous_headings":"","what":"Create the model","title":"Classification using Attention-based Deep Multiple Instance Learning (MIL).","text":"now build attention layer, prepare utilities, build train entire model.","code":""},{"path":"https://keras.posit.co/articles/examples/attention_mil_classification.html","id":"attention-operator-implementation","dir":"Articles > Examples","previous_headings":"Create the model","what":"Attention operator implementation","title":"Classification using Attention-based Deep Multiple Instance Learning (MIL).","text":"output size layer decided size single bag. attention mechanism uses weighted average instances bag, sum weights must equal 1 (invariant bag size). weight matrices (parameters) w v. include positive negative values, hyperbolic tangent element-wise non-linearity utilized. Gated attention mechanism can used deal complex relations. Another weight matrix, u, added computation. sigmoid non-linearity used overcome approximately linear behavior x ∈ [−1, 1] hyperbolic tangent non-linearity.","code":"class MILAttentionLayer(layers.Layer):     \"\"\"Implementation of the attention-based Deep MIL layer.      Args:       weight_params_dim: Positive Integer. Dimension of the weight matrix.       kernel_initializer: Initializer for the `kernel` matrix.       kernel_regularizer: Regularizer function applied to the `kernel` matrix.       use_gated: Boolean, whether or not to use the gated mechanism.      Returns:       List of 2D tensors with BAG_SIZE length.       The tensors are the attention scores after softmax with shape `(batch_size, 1)`.     \"\"\"      def __init__(         self,         weight_params_dim,         kernel_initializer=\"glorot_uniform\",         kernel_regularizer=None,         use_gated=False,         **kwargs,     ):         super().__init__(**kwargs)          self.weight_params_dim = weight_params_dim         self.use_gated = use_gated          self.kernel_initializer = keras.initializers.get(kernel_initializer)         self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)          self.v_init = self.kernel_initializer         self.w_init = self.kernel_initializer         self.u_init = self.kernel_initializer          self.v_regularizer = self.kernel_regularizer         self.w_regularizer = self.kernel_regularizer         self.u_regularizer = self.kernel_regularizer      def build(self, input_shape):         # Input shape.         # List of 2D tensors with shape: (batch_size, input_dim).         input_dim = input_shape[0][1]          self.v_weight_params = self.add_weight(             shape=(input_dim, self.weight_params_dim),             initializer=self.v_init,             name=\"v\",             regularizer=self.v_regularizer,             trainable=True,         )          self.w_weight_params = self.add_weight(             shape=(self.weight_params_dim, 1),             initializer=self.w_init,             name=\"w\",             regularizer=self.w_regularizer,             trainable=True,         )          if self.use_gated:             self.u_weight_params = self.add_weight(                 shape=(input_dim, self.weight_params_dim),                 initializer=self.u_init,                 name=\"u\",                 regularizer=self.u_regularizer,                 trainable=True,             )         else:             self.u_weight_params = None          self.input_built = True      def call(self, inputs):         # Assigning variables from the number of inputs.         instances = [             self.compute_attention_scores(instance) for instance in inputs         ]          # Stack instances into a single tensor.         instances = ops.stack(instances)          # Apply softmax over instances such that the output summation is equal to 1.         alpha = ops.softmax(instances, axis=0)          # Split to recreate the same array of tensors we had as inputs.         return [alpha[i] for i in range(alpha.shape[0])]      def compute_attention_scores(self, instance):         # Reserve in-case \"gated mechanism\" used.         original_instance = instance          # tanh(v*h_k^T)         instance = ops.tanh(             ops.tensordot(instance, self.v_weight_params, axes=1)         )          # for learning non-linear relations efficiently.         if self.use_gated:             instance = instance * ops.sigmoid(                 ops.tensordot(original_instance, self.u_weight_params, axes=1)             )          # w^T*(tanh(v*h_k^T)) / w^T*(tanh(v*h_k^T)*sigmoid(u*h_k^T))         return ops.tensordot(instance, self.w_weight_params, axes=1)"},{"path":"https://keras.posit.co/articles/examples/attention_mil_classification.html","id":"visualizer-tool","dir":"Articles > Examples","previous_headings":"","what":"Visualizer tool","title":"Classification using Attention-based Deep Multiple Instance Learning (MIL).","text":"Plot number bags (given PLOT_SIZE) respect class. Moreover, activated, class label prediction associated instance score bag (model trained) can seen.","code":"def plot(data, labels, bag_class, predictions=None, attention_weights=None):     \"\"\" \"Utility for plotting bags and attention weights.      Args:       data: Input data that contains the bags of instances.       labels: The associated bag labels of the input data.       bag_class: String name of the desired bag class.         The options are: \"positive\" or \"negative\".       predictions: Class labels model predictions.       If you don't specify anything, ground truth labels will be used.       attention_weights: Attention weights for each instance within the input data.       If you don't specify anything, the values won't be displayed.     \"\"\"     return  ## TODO     labels = np.array(labels).reshape(-1)      if bag_class == \"positive\":         if predictions is not None:             labels = np.where(predictions.argmax(1) == 1)[0]             bags = np.array(data)[:, labels[0:PLOT_SIZE]]          else:             labels = np.where(labels == 1)[0]             bags = np.array(data)[:, labels[0:PLOT_SIZE]]      elif bag_class == \"negative\":         if predictions is not None:             labels = np.where(predictions.argmax(1) == 0)[0]             bags = np.array(data)[:, labels[0:PLOT_SIZE]]         else:             labels = np.where(labels == 0)[0]             bags = np.array(data)[:, labels[0:PLOT_SIZE]]      else:         print(f\"There is no class {bag_class}\")         return      print(f\"The bag class label is {bag_class}\")     for i in range(PLOT_SIZE):         figure = plt.figure(figsize=(8, 8))         print(f\"Bag number: {labels[i]}\")         for j in range(BAG_SIZE):             image = bags[j][i]             figure.add_subplot(1, BAG_SIZE, j + 1)             plt.grid(False)             if attention_weights is not None:                 plt.title(np.around(attention_weights[labels[i]][j], 2))             plt.imshow(image)         plt.show()   # Plot some of validation data bags per class. plot(val_data, val_labels, \"positive\") plot(val_data, val_labels, \"negative\")"},{"path":"https://keras.posit.co/articles/examples/attention_mil_classification.html","id":"create-model","dir":"Articles > Examples","previous_headings":"","what":"Create model","title":"Classification using Attention-based Deep Multiple Instance Learning (MIL).","text":"First create embeddings per instance, invoke attention operator use softmax function output class probabilities.","code":"def create_model(instance_shape):     # Extract features from inputs.     inputs, embeddings = [], []     shared_dense_layer_1 = layers.Dense(128, activation=\"relu\")     shared_dense_layer_2 = layers.Dense(64, activation=\"relu\")     for _ in range(BAG_SIZE):         inp = layers.Input(instance_shape)         flatten = layers.Flatten()(inp)         dense_1 = shared_dense_layer_1(flatten)         dense_2 = shared_dense_layer_2(dense_1)         inputs.append(inp)         embeddings.append(dense_2)      # Invoke the attention layer.     alpha = MILAttentionLayer(         weight_params_dim=256,         kernel_regularizer=keras.regularizers.L2(0.01),         use_gated=True,         name=\"alpha\",     )(embeddings)      # Multiply attention weights with the input layers.     multiply_layers = [         layers.multiply([alpha[i], embeddings[i]]) for i in range(len(alpha))     ]      # Concatenate layers.     concat = layers.concatenate(multiply_layers, axis=1)      # Classification output node.     output = layers.Dense(2, activation=\"softmax\")(concat)      return keras.Model(inputs, output)"},{"path":"https://keras.posit.co/articles/examples/attention_mil_classification.html","id":"class-weights","dir":"Articles > Examples","previous_headings":"","what":"Class weights","title":"Classification using Attention-based Deep Multiple Instance Learning (MIL).","text":"Since kind problem simply turn imbalanced data classification problem, class weighting considered. Let’s say 1000 bags. often cases ~90 % bags contain positive label ~10 % . data can referred Imbalanced data. Using class weights, model tend give higher weight rare class.","code":"def compute_class_weights(labels):     # Count number of postive and negative bags.     negative_count = len(np.where(labels == 0)[0])     positive_count = len(np.where(labels == 1)[0])     total_count = negative_count + positive_count      # Build class weight dictionary.     return {         0: (1 / negative_count) * (total_count / 2),         1: (1 / positive_count) * (total_count / 2),     }"},{"path":"https://keras.posit.co/articles/examples/attention_mil_classification.html","id":"build-and-train-model","dir":"Articles > Examples","previous_headings":"","what":"Build and train model","title":"Classification using Attention-based Deep Multiple Instance Learning (MIL).","text":"model built trained section.","code":"def train(train_data, train_labels, val_data, val_labels, model):     # Train model.     # Prepare callbacks.     # Path where to save best weights.      # Take the file name from the wrapper.     file_path = \"/tmp/best_model.weights.h5\"      # Initialize model checkpoint callback.     model_checkpoint = keras.callbacks.ModelCheckpoint(         file_path,         monitor=\"val_loss\",         verbose=0,         mode=\"min\",         save_best_only=True,         save_weights_only=True,     )      # Initialize early stopping callback.     # The model performance is monitored across the validation data and stops training     # when the generalization error cease to decrease.     early_stopping = keras.callbacks.EarlyStopping(         monitor=\"val_loss\", patience=10, mode=\"min\"     )      # Compile model.     model.compile(         optimizer=\"adam\",         loss=\"sparse_categorical_crossentropy\",         metrics=[\"accuracy\"],     )      # Fit model.     model.fit(         train_data,         train_labels,         validation_data=(val_data, val_labels),         epochs=20,         class_weight=compute_class_weights(train_labels),         batch_size=1,         callbacks=[early_stopping, model_checkpoint],         verbose=0,     )      # Load best weights.     model.load_weights(file_path)      return model   # Building model(s). instance_shape = train_data[0][0].shape models = [create_model(instance_shape) for _ in range(ENSEMBLE_AVG_COUNT)]  # Show single model architecture. print(models[0].summary())  # Training model(s). trained_models = [     train(train_data, train_labels, val_data, val_labels, model)     for model in tqdm(models) ]"},{"path":"https://keras.posit.co/articles/examples/attention_mil_classification.html","id":"model-evaluation","dir":"Articles > Examples","previous_headings":"","what":"Model evaluation","title":"Classification using Attention-based Deep Multiple Instance Learning (MIL).","text":"models now ready evaluation. model also create associated intermediate model get weights attention layer. compute prediction ENSEMBLE_AVG_COUNT models, average together final prediction.","code":"def predict(data, labels, trained_models):     # Collect info per model.     models_predictions = []     models_attention_weights = []     models_losses = []     models_accuracies = []      for model in trained_models:         # Predict output classes on data.         predictions = model.predict(data)         models_predictions.append(predictions)          # Create intermediate model to get MIL attention layer weights.         intermediate_model = keras.Model(             model.input, model.get_layer(\"alpha\").output         )          # Predict MIL attention layer weights.         intermediate_predictions = intermediate_model.predict(data)          attention_weights = np.squeeze(             np.swapaxes(intermediate_predictions, 1, 0)         )         models_attention_weights.append(attention_weights)          loss, accuracy = model.evaluate(data, labels, verbose=0)         models_losses.append(loss)         models_accuracies.append(accuracy)      print(         f\"The average loss and accuracy are {np.sum(models_losses, axis=0) / ENSEMBLE_AVG_COUNT:.2f}\"         f\" and {100 * np.sum(models_accuracies, axis=0) / ENSEMBLE_AVG_COUNT:.2f} % resp.\"     )      return (         np.sum(models_predictions, axis=0) / ENSEMBLE_AVG_COUNT,         np.sum(models_attention_weights, axis=0) / ENSEMBLE_AVG_COUNT,     )   # Evaluate and predict classes and attention scores on validation data. class_predictions, attention_params = predict(     val_data, val_labels, trained_models )  # Plot some results from our validation data. plot(     val_data,     val_labels,     \"positive\",     predictions=class_predictions,     attention_weights=attention_params, ) plot(     val_data,     val_labels,     \"negative\",     predictions=class_predictions,     attention_weights=attention_params, )"},{"path":"https://keras.posit.co/articles/examples/attention_mil_classification.html","id":"conclusion","dir":"Articles > Examples","previous_headings":"","what":"Conclusion","title":"Classification using Attention-based Deep Multiple Instance Learning (MIL).","text":"plot, can notice weights always sum 1. positively predict bag, instance resulted positive labeling substantially higher attention score rest bag. However, negatively predicted bag, two cases: instances approximately similar scores. instance relatively higher score (high positive instance). feature space instance close positive instance.","code":""},{"path":"https://keras.posit.co/articles/examples/attention_mil_classification.html","id":"remarks","dir":"Articles > Examples","previous_headings":"","what":"Remarks","title":"Classification using Attention-based Deep Multiple Instance Learning (MIL).","text":"model overfit, weights equally distributed bags. Hence, regularization techniques necessary. paper, bag sizes can differ one bag another. simplicity, bag sizes fixed . order rely random initial weights single model, averaging ensemble methods considered. Example available HuggingFace.","code":""},{"path":"https://keras.posit.co/articles/examples/autoencoder.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Convolutional autoencoder for image denoising","text":"example demonstrates implement deep convolutional autoencoder image denoising, mapping noisy digits images MNIST dataset clean digits images. implementation based original blog post titled Building Autoencoders Keras François Chollet.","code":""},{"path":"https://keras.posit.co/articles/examples/autoencoder.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Convolutional autoencoder for image denoising","text":"","code":"library(keras3)  # Normalizes the supplied array and reshapes it. preprocess <- function(array) {   array_reshape(array/255, c(dim(array)[1], 28, 28, 1)) }  # Adds random noise to each image in the supplied array. noise <- function(array) {   noise_factor <- 0.4   noisy_array <- array + noise_factor * random_normal(dim(array))   op_clip(noisy_array, 0.0, 1.0) }  display <- function(array1, array2) {   n <- 2   indices <- sample.int(dim(array1)[1], n)   images1 <- as.array(array1)[indices,,,]   images2 <- as.array(array2)[indices,,,]    par(mfrow = c(2, n), mar = c(0,0,0,0))   for (i in seq_len(n)) {     plot(as.raster(images1[i,,]))     plot(as.raster(images2[i,, ]))   } }"},{"path":"https://keras.posit.co/articles/examples/autoencoder.html","id":"prepare-the-data","dir":"Articles > Examples","previous_headings":"","what":"Prepare the data","title":"Convolutional autoencoder for image denoising","text":"","code":"# Since we only need images from the dataset to encode and decode, we # won't use the labels. c(c(train_data, .), c(test_data, .)) %<-% dataset_mnist()  # Normalize and reshape the data train_data <- preprocess(train_data) test_data <- preprocess(test_data)  # Create a copy of the data with added noise noisy_train_data <- noise(train_data) noisy_test_data <- noise(test_data)  # Display the train data and a version of it with added noise display(train_data, noisy_train_data)"},{"path":"https://keras.posit.co/articles/examples/autoencoder.html","id":"build-the-autoencoder","dir":"Articles > Examples","previous_headings":"","what":"Build the autoencoder","title":"Convolutional autoencoder for image denoising","text":"going use Functional API build convolutional autoencoder. Now can train autoencoder using train_data input data target. Notice setting validation data using format. Let’s predict test dataset display original image together prediction autoencoder. Notice predictions pretty close original images, although quite . Now know autoencoder works, let’s retrain using noisy data input clean data target. want autoencoder learn denoise images. Let’s now predict noisy data display results autoencoder. Notice autoencoder amazing job removing noise input images.","code":"input <- layer_input(shape=c(28, 28, 1))  # Encoder enc <- input %>%   layer_conv_2d(filters=32, kernel_size=c(3, 3), activation=\"relu\", padding=\"same\") %>%   layer_max_pooling_2d(pool_size=c(2, 2), padding=\"same\") %>%   layer_conv_2d(filters=32, kernel_size=c(3, 3), activation=\"relu\", padding=\"same\") %>%   layer_max_pooling_2d(pool_size=c(2, 2), padding=\"same\")  # Decoder dec <- enc %>%   layer_conv_2d_transpose(     filters=32, kernel_size=c(3, 3), strides = 2, activation=\"relu\", padding=\"same\") %>%   layer_conv_2d_transpose(     filters=32, kernel_size=c(3, 3), strides = 2, activation=\"relu\", padding=\"same\") %>%   layer_conv_2d(filters=1, kernel_size=c(3, 3), activation=\"sigmoid\", padding=\"same\")  # Autoencoder autoencoder <- keras_model(input, dec) autoencoder %>% compile(optimizer=\"adam\", loss=\"binary_crossentropy\") autoencoder %>% summary() ## [1mModel: \"functional_1\"[0m ## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓ ## ┃[1m [0m[1mLayer (type)                   [0m[1m [0m┃[1m [0m[1mOutput Shape             [0m[1m [0m┃[1m [0m[1m   Param #[0m[1m [0m┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩ ## │ input_layer ([38;5;33mInputLayer[0m)        │ ([38;5;45mNone[0m, [38;5;34m28[0m, [38;5;34m28[0m, [38;5;34m1[0m)         │          [38;5;34m0[0m │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ conv2d_1 ([38;5;33mConv2D[0m)               │ ([38;5;45mNone[0m, [38;5;34m28[0m, [38;5;34m28[0m, [38;5;34m32[0m)        │        [38;5;34m320[0m │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ max_pooling2d_1 ([38;5;33mMaxPooling2D[0m)  │ ([38;5;45mNone[0m, [38;5;34m14[0m, [38;5;34m14[0m, [38;5;34m32[0m)        │          [38;5;34m0[0m │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ conv2d ([38;5;33mConv2D[0m)                 │ ([38;5;45mNone[0m, [38;5;34m14[0m, [38;5;34m14[0m, [38;5;34m32[0m)        │      [38;5;34m9,248[0m │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ max_pooling2d ([38;5;33mMaxPooling2D[0m)    │ ([38;5;45mNone[0m, [38;5;34m7[0m, [38;5;34m7[0m, [38;5;34m32[0m)          │          [38;5;34m0[0m │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ conv2d_transpose_1              │ ([38;5;45mNone[0m, [38;5;34m14[0m, [38;5;34m14[0m, [38;5;34m32[0m)        │      [38;5;34m9,248[0m │ ## │ ([38;5;33mConv2DTranspose[0m)               │                           │            │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ conv2d_transpose                │ ([38;5;45mNone[0m, [38;5;34m28[0m, [38;5;34m28[0m, [38;5;34m32[0m)        │      [38;5;34m9,248[0m │ ## │ ([38;5;33mConv2DTranspose[0m)               │                           │            │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ conv2d_2 ([38;5;33mConv2D[0m)               │ ([38;5;45mNone[0m, [38;5;34m28[0m, [38;5;34m28[0m, [38;5;34m1[0m)         │        [38;5;34m289[0m │ ## └─────────────────────────────────┴───────────────────────────┴────────────┘ ## [1m Total params: [0m[38;5;34m28,353[0m (110.75 KB) ## [1m Trainable params: [0m[38;5;34m28,353[0m (110.75 KB) ## [1m Non-trainable params: [0m[38;5;34m0[0m (0.00 B) autoencoder %>% fit(     x=train_data,     y=train_data,     epochs=50,     batch_size=128,     shuffle=TRUE,     validation_data=list(test_data, test_data), ) ## Epoch 1/50 ## 469/469 - 16s - 34ms/step - loss: 0.1322 - val_loss: 0.0736 ## Epoch 2/50 ## 469/469 - 15s - 33ms/step - loss: 0.0721 - val_loss: 0.0698 ## Epoch 3/50 ## 469/469 - 16s - 34ms/step - loss: 0.0695 - val_loss: 0.0681 ## Epoch 4/50 ## 469/469 - 16s - 35ms/step - loss: 0.0682 - val_loss: 0.0672 ## Epoch 5/50 ## 469/469 - 17s - 35ms/step - loss: 0.0674 - val_loss: 0.0665 ## Epoch 6/50 ## 469/469 - 17s - 35ms/step - loss: 0.0668 - val_loss: 0.0661 ## Epoch 7/50 ## 469/469 - 17s - 35ms/step - loss: 0.0663 - val_loss: 0.0656 ## Epoch 8/50 ## 469/469 - 17s - 36ms/step - loss: 0.0659 - val_loss: 0.0653 ## Epoch 9/50 ## 469/469 - 17s - 35ms/step - loss: 0.0656 - val_loss: 0.0650 ## Epoch 10/50 ## 469/469 - 17s - 35ms/step - loss: 0.0653 - val_loss: 0.0647 ## Epoch 11/50 ## 469/469 - 17s - 36ms/step - loss: 0.0650 - val_loss: 0.0645 ## Epoch 12/50 ## 469/469 - 17s - 36ms/step - loss: 0.0648 - val_loss: 0.0642 ## Epoch 13/50 ## 469/469 - 16s - 35ms/step - loss: 0.0646 - val_loss: 0.0640 ## Epoch 14/50 ## 469/469 - 17s - 35ms/step - loss: 0.0644 - val_loss: 0.0638 ## Epoch 15/50 ## 469/469 - 17s - 36ms/step - loss: 0.0642 - val_loss: 0.0637 ## Epoch 16/50 ## 469/469 - 17s - 35ms/step - loss: 0.0641 - val_loss: 0.0635 ## Epoch 17/50 ## 469/469 - 17s - 36ms/step - loss: 0.0639 - val_loss: 0.0634 ## Epoch 18/50 ## 469/469 - 17s - 36ms/step - loss: 0.0638 - val_loss: 0.0633 ## Epoch 19/50 ## 469/469 - 17s - 36ms/step - loss: 0.0637 - val_loss: 0.0632 ## Epoch 20/50 ## 469/469 - 17s - 36ms/step - loss: 0.0636 - val_loss: 0.0631 ## Epoch 21/50 ## 469/469 - 17s - 36ms/step - loss: 0.0635 - val_loss: 0.0630 ## Epoch 22/50 ## 469/469 - 17s - 37ms/step - loss: 0.0634 - val_loss: 0.0629 ## Epoch 23/50 ## 469/469 - 17s - 37ms/step - loss: 0.0633 - val_loss: 0.0628 ## Epoch 24/50 ## 469/469 - 17s - 37ms/step - loss: 0.0632 - val_loss: 0.0628 ## Epoch 25/50 ## 469/469 - 17s - 36ms/step - loss: 0.0632 - val_loss: 0.0627 ## Epoch 26/50 ## 469/469 - 17s - 37ms/step - loss: 0.0631 - val_loss: 0.0626 ## Epoch 27/50 ## 469/469 - 17s - 36ms/step - loss: 0.0630 - val_loss: 0.0626 ## Epoch 28/50 ## 469/469 - 17s - 37ms/step - loss: 0.0630 - val_loss: 0.0625 ## Epoch 29/50 ## 469/469 - 17s - 36ms/step - loss: 0.0629 - val_loss: 0.0625 ## Epoch 30/50 ## 469/469 - 17s - 37ms/step - loss: 0.0629 - val_loss: 0.0624 ## Epoch 31/50 ## 469/469 - 17s - 36ms/step - loss: 0.0628 - val_loss: 0.0624 ## Epoch 32/50 ## 469/469 - 17s - 37ms/step - loss: 0.0628 - val_loss: 0.0623 ## Epoch 33/50 ## 469/469 - 17s - 37ms/step - loss: 0.0627 - val_loss: 0.0623 ## Epoch 34/50 ## 469/469 - 17s - 37ms/step - loss: 0.0627 - val_loss: 0.0623 ## Epoch 35/50 ## 469/469 - 17s - 37ms/step - loss: 0.0626 - val_loss: 0.0622 ## Epoch 36/50 ## 469/469 - 17s - 37ms/step - loss: 0.0626 - val_loss: 0.0622 ## Epoch 37/50 ## 469/469 - 17s - 36ms/step - loss: 0.0626 - val_loss: 0.0622 ## Epoch 38/50 ## 469/469 - 17s - 36ms/step - loss: 0.0625 - val_loss: 0.0621 ## Epoch 39/50 ## 469/469 - 17s - 36ms/step - loss: 0.0625 - val_loss: 0.0621 ## Epoch 40/50 ## 469/469 - 17s - 36ms/step - loss: 0.0625 - val_loss: 0.0621 ## Epoch 41/50 ## 469/469 - 17s - 36ms/step - loss: 0.0624 - val_loss: 0.0621 ## Epoch 42/50 ## 469/469 - 17s - 36ms/step - loss: 0.0624 - val_loss: 0.0620 ## Epoch 43/50 ## 469/469 - 17s - 35ms/step - loss: 0.0624 - val_loss: 0.0620 ## Epoch 44/50 ## 469/469 - 17s - 35ms/step - loss: 0.0624 - val_loss: 0.0620 ## Epoch 45/50 ## 469/469 - 17s - 36ms/step - loss: 0.0623 - val_loss: 0.0620 ## Epoch 46/50 ## 469/469 - 17s - 36ms/step - loss: 0.0623 - val_loss: 0.0620 ## Epoch 47/50 ## 469/469 - 17s - 36ms/step - loss: 0.0623 - val_loss: 0.0619 ## Epoch 48/50 ## 469/469 - 17s - 37ms/step - loss: 0.0623 - val_loss: 0.0619 ## Epoch 49/50 ## 469/469 - 17s - 36ms/step - loss: 0.0623 - val_loss: 0.0619 ## Epoch 50/50 ## 469/469 - 17s - 36ms/step - loss: 0.0622 - val_loss: 0.0619 predictions <- autoencoder %>% predict(test_data) ## 313/313 - 2s - 6ms/step display(test_data, predictions) autoencoder %>% fit(     x=noisy_train_data,     y=train_data,     epochs=100,     batch_size=128,     shuffle=TRUE,     validation_data=list(noisy_test_data, test_data), ) ## Epoch 1/100 ## 469/469 - 16s - 34ms/step - loss: 0.1001 - val_loss: 0.0927 ## Epoch 2/100 ## 469/469 - 16s - 35ms/step - loss: 0.0923 - val_loss: 0.0905 ## Epoch 3/100 ## 469/469 - 17s - 35ms/step - loss: 0.0908 - val_loss: 0.0894 ## Epoch 4/100 ## 469/469 - 17s - 35ms/step - loss: 0.0898 - val_loss: 0.0887 ## Epoch 5/100 ## 469/469 - 17s - 35ms/step - loss: 0.0891 - val_loss: 0.0881 ## Epoch 6/100 ## 469/469 - 17s - 36ms/step - loss: 0.0886 - val_loss: 0.0876 ## Epoch 7/100 ## 469/469 - 17s - 35ms/step - loss: 0.0882 - val_loss: 0.0872 ## Epoch 8/100 ## 469/469 - 17s - 36ms/step - loss: 0.0878 - val_loss: 0.0869 ## Epoch 9/100 ## 469/469 - 16s - 35ms/step - loss: 0.0875 - val_loss: 0.0867 ## Epoch 10/100 ## 469/469 - 16s - 35ms/step - loss: 0.0873 - val_loss: 0.0865 ## Epoch 11/100 ## 469/469 - 16s - 35ms/step - loss: 0.0870 - val_loss: 0.0863 ## Epoch 12/100 ## 469/469 - 16s - 35ms/step - loss: 0.0868 - val_loss: 0.0862 ## Epoch 13/100 ## 469/469 - 16s - 35ms/step - loss: 0.0866 - val_loss: 0.0860 ## Epoch 14/100 ## 469/469 - 17s - 35ms/step - loss: 0.0865 - val_loss: 0.0858 ## Epoch 15/100 ## 469/469 - 16s - 35ms/step - loss: 0.0863 - val_loss: 0.0857 ## Epoch 16/100 ## 469/469 - 16s - 35ms/step - loss: 0.0862 - val_loss: 0.0856 ## Epoch 17/100 ## 469/469 - 17s - 36ms/step - loss: 0.0861 - val_loss: 0.0855 ## Epoch 18/100 ## 469/469 - 17s - 36ms/step - loss: 0.0860 - val_loss: 0.0854 ## Epoch 19/100 ## 469/469 - 17s - 36ms/step - loss: 0.0859 - val_loss: 0.0853 ## Epoch 20/100 ## 469/469 - 17s - 36ms/step - loss: 0.0858 - val_loss: 0.0852 ## Epoch 21/100 ## 469/469 - 17s - 36ms/step - loss: 0.0857 - val_loss: 0.0851 ## Epoch 22/100 ## 469/469 - 17s - 36ms/step - loss: 0.0856 - val_loss: 0.0851 ## Epoch 23/100 ## 469/469 - 17s - 37ms/step - loss: 0.0855 - val_loss: 0.0850 ## Epoch 24/100 ## 469/469 - 17s - 36ms/step - loss: 0.0855 - val_loss: 0.0849 ## Epoch 25/100 ## 469/469 - 17s - 36ms/step - loss: 0.0854 - val_loss: 0.0849 ## Epoch 26/100 ## 469/469 - 17s - 36ms/step - loss: 0.0853 - val_loss: 0.0848 ## Epoch 27/100 ## 469/469 - 17s - 37ms/step - loss: 0.0853 - val_loss: 0.0848 ## Epoch 28/100 ## 469/469 - 17s - 36ms/step - loss: 0.0852 - val_loss: 0.0847 ## Epoch 29/100 ## 469/469 - 17s - 36ms/step - loss: 0.0852 - val_loss: 0.0847 ## Epoch 30/100 ## 469/469 - 18s - 38ms/step - loss: 0.0851 - val_loss: 0.0847 ## Epoch 31/100 ## 469/469 - 19s - 40ms/step - loss: 0.0851 - val_loss: 0.0846 ## Epoch 32/100 ## 469/469 - 18s - 39ms/step - loss: 0.0850 - val_loss: 0.0846 ## Epoch 33/100 ## 469/469 - 17s - 35ms/step - loss: 0.0850 - val_loss: 0.0846 ## Epoch 34/100 ## 469/469 - 16s - 35ms/step - loss: 0.0850 - val_loss: 0.0845 ## Epoch 35/100 ## 469/469 - 16s - 35ms/step - loss: 0.0849 - val_loss: 0.0845 ## Epoch 36/100 ## 469/469 - 16s - 35ms/step - loss: 0.0849 - val_loss: 0.0845 ## Epoch 37/100 ## 469/469 - 16s - 35ms/step - loss: 0.0849 - val_loss: 0.0845 ## Epoch 38/100 ## 469/469 - 17s - 36ms/step - loss: 0.0848 - val_loss: 0.0844 ## Epoch 39/100 ## 469/469 - 17s - 36ms/step - loss: 0.0848 - val_loss: 0.0844 ## Epoch 40/100 ## 469/469 - 17s - 36ms/step - loss: 0.0848 - val_loss: 0.0844 ## Epoch 41/100 ## 469/469 - 17s - 36ms/step - loss: 0.0847 - val_loss: 0.0844 ## Epoch 42/100 ## 469/469 - 17s - 36ms/step - loss: 0.0847 - val_loss: 0.0844 ## Epoch 43/100 ## 469/469 - 17s - 36ms/step - loss: 0.0847 - val_loss: 0.0843 ## Epoch 44/100 ## 469/469 - 17s - 36ms/step - loss: 0.0847 - val_loss: 0.0843 ## Epoch 45/100 ## 469/469 - 17s - 36ms/step - loss: 0.0846 - val_loss: 0.0843 ## Epoch 46/100 ## 469/469 - 17s - 37ms/step - loss: 0.0846 - val_loss: 0.0843 ## Epoch 47/100 ## 469/469 - 17s - 36ms/step - loss: 0.0846 - val_loss: 0.0842 ## Epoch 48/100 ## 469/469 - 17s - 37ms/step - loss: 0.0846 - val_loss: 0.0842 ## Epoch 49/100 ## 469/469 - 17s - 36ms/step - loss: 0.0845 - val_loss: 0.0842 ## Epoch 50/100 ## 469/469 - 17s - 36ms/step - loss: 0.0845 - val_loss: 0.0842 ## Epoch 51/100 ## 469/469 - 17s - 36ms/step - loss: 0.0845 - val_loss: 0.0842 ## Epoch 52/100 ## 469/469 - 17s - 37ms/step - loss: 0.0845 - val_loss: 0.0841 ## Epoch 53/100 ## 469/469 - 17s - 37ms/step - loss: 0.0845 - val_loss: 0.0841 ## Epoch 54/100 ## 469/469 - 17s - 36ms/step - loss: 0.0844 - val_loss: 0.0841 ## Epoch 55/100 ## 469/469 - 17s - 37ms/step - loss: 0.0844 - val_loss: 0.0841 ## Epoch 56/100 ## 469/469 - 17s - 37ms/step - loss: 0.0844 - val_loss: 0.0841 ## Epoch 57/100 ## 469/469 - 17s - 37ms/step - loss: 0.0844 - val_loss: 0.0841 ## Epoch 58/100 ## 469/469 - 17s - 36ms/step - loss: 0.0844 - val_loss: 0.0841 ## Epoch 59/100 ## 469/469 - 17s - 37ms/step - loss: 0.0844 - val_loss: 0.0841 ## Epoch 60/100 ## 469/469 - 17s - 37ms/step - loss: 0.0844 - val_loss: 0.0841 ## Epoch 61/100 ## 469/469 - 17s - 37ms/step - loss: 0.0843 - val_loss: 0.0840 ## Epoch 62/100 ## 469/469 - 17s - 36ms/step - loss: 0.0843 - val_loss: 0.0840 ## Epoch 63/100 ## 469/469 - 17s - 37ms/step - loss: 0.0843 - val_loss: 0.0840 ## Epoch 64/100 ## 469/469 - 17s - 37ms/step - loss: 0.0843 - val_loss: 0.0840 ## Epoch 65/100 ## 469/469 - 17s - 36ms/step - loss: 0.0843 - val_loss: 0.0840 ## Epoch 66/100 ## 469/469 - 17s - 35ms/step - loss: 0.0843 - val_loss: 0.0840 ## Epoch 67/100 ## 469/469 - 17s - 36ms/step - loss: 0.0843 - val_loss: 0.0840 ## Epoch 68/100 ## 469/469 - 17s - 36ms/step - loss: 0.0843 - val_loss: 0.0840 ## Epoch 69/100 ## 469/469 - 17s - 37ms/step - loss: 0.0842 - val_loss: 0.0840 ## Epoch 70/100 ## 469/469 - 17s - 37ms/step - loss: 0.0842 - val_loss: 0.0840 ## Epoch 71/100 ## 469/469 - 17s - 36ms/step - loss: 0.0842 - val_loss: 0.0839 ## Epoch 72/100 ## 469/469 - 17s - 37ms/step - loss: 0.0842 - val_loss: 0.0839 ## Epoch 73/100 ## 469/469 - 17s - 37ms/step - loss: 0.0842 - val_loss: 0.0839 ## Epoch 74/100 ## 469/469 - 18s - 38ms/step - loss: 0.0842 - val_loss: 0.0839 ## Epoch 75/100 ## 469/469 - 17s - 36ms/step - loss: 0.0842 - val_loss: 0.0839 ## Epoch 76/100 ## 469/469 - 17s - 36ms/step - loss: 0.0842 - val_loss: 0.0839 ## Epoch 77/100 ## 469/469 - 17s - 36ms/step - loss: 0.0842 - val_loss: 0.0839 ## Epoch 78/100 ## 469/469 - 17s - 36ms/step - loss: 0.0842 - val_loss: 0.0839 ## Epoch 79/100 ## 469/469 - 17s - 36ms/step - loss: 0.0841 - val_loss: 0.0839 ## Epoch 80/100 ## 469/469 - 17s - 36ms/step - loss: 0.0841 - val_loss: 0.0839 ## Epoch 81/100 ## 469/469 - 16s - 34ms/step - loss: 0.0841 - val_loss: 0.0839 ## Epoch 82/100 ## 469/469 - 16s - 35ms/step - loss: 0.0841 - val_loss: 0.0839 ## Epoch 83/100 ## 469/469 - 16s - 35ms/step - loss: 0.0841 - val_loss: 0.0839 ## Epoch 84/100 ## 469/469 - 16s - 35ms/step - loss: 0.0841 - val_loss: 0.0839 ## Epoch 85/100 ## 469/469 - 16s - 35ms/step - loss: 0.0841 - val_loss: 0.0839 ## Epoch 86/100 ## 469/469 - 17s - 36ms/step - loss: 0.0841 - val_loss: 0.0839 ## Epoch 87/100 ## 469/469 - 17s - 35ms/step - loss: 0.0841 - val_loss: 0.0839 ## Epoch 88/100 ## 469/469 - 17s - 35ms/step - loss: 0.0841 - val_loss: 0.0839 ## Epoch 89/100 ## 469/469 - 17s - 36ms/step - loss: 0.0841 - val_loss: 0.0839 ## Epoch 90/100 ## 469/469 - 17s - 36ms/step - loss: 0.0841 - val_loss: 0.0839 ## Epoch 91/100 ## 469/469 - 17s - 36ms/step - loss: 0.0841 - val_loss: 0.0838 ## Epoch 92/100 ## 469/469 - 17s - 36ms/step - loss: 0.0841 - val_loss: 0.0838 ## Epoch 93/100 ## 469/469 - 17s - 36ms/step - loss: 0.0840 - val_loss: 0.0838 ## Epoch 94/100 ## 469/469 - 17s - 36ms/step - loss: 0.0840 - val_loss: 0.0838 ## Epoch 95/100 ## 469/469 - 17s - 36ms/step - loss: 0.0840 - val_loss: 0.0838 ## Epoch 96/100 ## 469/469 - 17s - 36ms/step - loss: 0.0840 - val_loss: 0.0838 ## Epoch 97/100 ## 469/469 - 17s - 36ms/step - loss: 0.0840 - val_loss: 0.0838 ## Epoch 98/100 ## 469/469 - 17s - 36ms/step - loss: 0.0840 - val_loss: 0.0838 ## Epoch 99/100 ## 469/469 - 17s - 36ms/step - loss: 0.0840 - val_loss: 0.0838 ## Epoch 100/100 ## 469/469 - 17s - 36ms/step - loss: 0.0840 - val_loss: 0.0838 predictions <- autoencoder %>% predict(noisy_test_data) ## 313/313 - 2s - 6ms/step display(noisy_test_data, predictions)"},{"path":"https://keras.posit.co/articles/examples/bidirectional_lstm_imdb.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Bidirectional LSTM on IMDB","text":"","code":"import numpy as np import keras as keras from keras import layers  max_features = 20000  # Only consider the top 20k words maxlen = 200  # Only consider the first 200 words of each movie review"},{"path":"https://keras.posit.co/articles/examples/bidirectional_lstm_imdb.html","id":"build-the-model","dir":"Articles > Examples","previous_headings":"","what":"Build the model","title":"Bidirectional LSTM on IMDB","text":"","code":"# Input for variable-length sequences of integers inputs = keras.Input(shape=(None,), dtype=\"int32\") # Embed each integer in a 128-dimensional vector x = layers.Embedding(max_features, 128)(inputs) # Add 2 bidirectional LSTMs x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x) x = layers.Bidirectional(layers.LSTM(64))(x) # Add a classifier outputs = layers.Dense(1, activation=\"sigmoid\")(x) model = keras.Model(inputs, outputs) model.summary()"},{"path":"https://keras.posit.co/articles/examples/bidirectional_lstm_imdb.html","id":"load-the-imdb-movie-review-sentiment-data","dir":"Articles > Examples","previous_headings":"","what":"Load the IMDB movie review sentiment data","title":"Bidirectional LSTM on IMDB","text":"","code":"(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(     num_words=max_features ) print(len(x_train), \"Training sequences\") print(len(x_val), \"Validation sequences\") # Use pad_sequence to standardize sequence length: # this will truncate sequences longer than 200 words and zero-pad sequences shorter than 200 words. x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen) x_val = keras.utils.pad_sequences(x_val, maxlen=maxlen)"},{"path":"https://keras.posit.co/articles/examples/bidirectional_lstm_imdb.html","id":"train-and-evaluate-the-model","dir":"Articles > Examples","previous_headings":"","what":"Train and evaluate the model","title":"Bidirectional LSTM on IMDB","text":"can use trained model hosted Hugging Face Hub try demo Hugging Face Spaces.","code":"model.compile(     optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"] ) model.fit(     x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val) )"},{"path":"https://keras.posit.co/articles/examples/bit.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Image Classification using BigTransfer (BiT)","text":"BigTransfer (also known BiT) state---art transfer learning method image classification. Transfer pre-trained representations improves sample efficiency simplifies hyperparameter tuning training deep neural networks vision. BiT revisit paradigm pre-training large supervised datasets fine-tuning model target task. importance appropriately choosing normalization layers scaling architecture capacity amount pre-training data increases. BigTransfer(BiT) trained public datasets, along code TF2, Jax Pytorch. help anyone reach state art performance task interest, even just handful labeled images per class. can find BiT models pre-trained ImageNet ImageNet-21k TFHub TensorFlow2 SavedModels can use easily Keras Layers. variety sizes ranging standard ResNet50 ResNet152x4 (152 layers deep, 4x wider typical ResNet50) users larger computational memory budgets higher accuracy requirements. Figure: x-axis shows number images used per class, ranging 1 full dataset. plots left, curve blue BiT-L model, whereas curve ResNet-50 pre-trained ImageNet (ILSVRC-2012).","code":""},{"path":"https://keras.posit.co/articles/examples/bit.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Image Classification using BigTransfer (BiT)","text":"","code":"import os  os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  import keras as keras import numpy as np import matplotlib.pyplot as plt  import tensorflow as tf import tensorflow_hub as hub import tensorflow_datasets as tfds  tfds.disable_progress_bar()  SEEDS = 42  np.random.seed(SEEDS) keras.utils.set_random_seed(SEEDS)"},{"path":"https://keras.posit.co/articles/examples/bit.html","id":"gather-flower-dataset","dir":"Articles > Examples","previous_headings":"","what":"Gather Flower Dataset","title":"Image Classification using BigTransfer (BiT)","text":"","code":"train_ds, validation_ds = tfds.load(     \"tf_flowers\",     split=[\"train[:85%]\", \"train[85%:]\"],     as_supervised=True, )"},{"path":"https://keras.posit.co/articles/examples/bit.html","id":"visualise-the-dataset","dir":"Articles > Examples","previous_headings":"","what":"Visualise the dataset","title":"Image Classification using BigTransfer (BiT)","text":"","code":"plt.figure(figsize=(10, 10)) for i, (image, label) in enumerate(train_ds.take(9)):     ax = plt.subplot(3, 3, i + 1)     plt.imshow(image)     plt.title(int(label))     plt.axis(\"off\")"},{"path":"https://keras.posit.co/articles/examples/bit.html","id":"define-hyperparameters","dir":"Articles > Examples","previous_headings":"","what":"Define hyperparameters","title":"Image Classification using BigTransfer (BiT)","text":"hyperparamteres like SCHEDULE_LENGTH SCHEDULE_BOUNDARIES determined based empirical results. method explained original paper Google AI Blog Post. SCHEDULE_LENGTH aslo determined whether use MixUp Augmentation . can also find easy MixUp Implementation Keras Coding Examples.","code":"RESIZE_TO = 384 CROP_TO = 224 BATCH_SIZE = 64 STEPS_PER_EPOCH = 10 AUTO = tf.data.AUTOTUNE  # optimise the pipeline performance NUM_CLASSES = 5  # number of classes SCHEDULE_LENGTH = 500  # we will train on lower resolution images and will still attain good results SCHEDULE_BOUNDARIES = [     200,     300,     400, ]  # more the dataset size the schedule length increase"},{"path":"https://keras.posit.co/articles/examples/bit.html","id":"define-preprocessing-helper-functions","dir":"Articles > Examples","previous_headings":"","what":"Define preprocessing helper functions","title":"Image Classification using BigTransfer (BiT)","text":"","code":"SCHEDULE_LENGTH = SCHEDULE_LENGTH * 512 / BATCH_SIZE   @tf.function def preprocess_train(image, label):     image = tf.image.random_flip_left_right(image)     image = tf.image.resize(image, (RESIZE_TO, RESIZE_TO))     image = tf.image.random_crop(image, (CROP_TO, CROP_TO, 3))     image = image / 255.0     return (image, label)   @tf.function def preprocess_test(image, label):     image = tf.image.resize(image, (RESIZE_TO, RESIZE_TO))     image = image / 255.0     return (image, label)   DATASET_NUM_TRAIN_EXAMPLES = train_ds.cardinality().numpy()  repeat_count = int(     SCHEDULE_LENGTH * BATCH_SIZE / DATASET_NUM_TRAIN_EXAMPLES * STEPS_PER_EPOCH ) repeat_count += 50 + 1  # To ensure at least there are 50 epochs of training"},{"path":"https://keras.posit.co/articles/examples/bit.html","id":"define-the-data-pipeline","dir":"Articles > Examples","previous_headings":"","what":"Define the data pipeline","title":"Image Classification using BigTransfer (BiT)","text":"","code":"# Training pipeline pipeline_train = (     train_ds.shuffle(10000)     .repeat(repeat_count)  # Repeat dataset_size / num_steps     .map(preprocess_train, num_parallel_calls=AUTO)     .batch(BATCH_SIZE)     .prefetch(AUTO) )  # Validation pipeline pipeline_validation = (     validation_ds.map(preprocess_test, num_parallel_calls=AUTO)     .batch(BATCH_SIZE)     .prefetch(AUTO) )"},{"path":"https://keras.posit.co/articles/examples/bit.html","id":"visualise-the-training-samples","dir":"Articles > Examples","previous_headings":"","what":"Visualise the training samples","title":"Image Classification using BigTransfer (BiT)","text":"","code":"image_batch, label_batch = next(iter(pipeline_train))  plt.figure(figsize=(10, 10)) for n in range(25):     ax = plt.subplot(5, 5, n + 1)     plt.imshow(image_batch[n])     plt.title(label_batch[n].numpy())     plt.axis(\"off\")"},{"path":"https://keras.posit.co/articles/examples/bit.html","id":"load-pretrained-tf-hub-model-into-a-keraslayer","dir":"Articles > Examples","previous_headings":"","what":"Load pretrained TF-Hub model into a KerasLayer","title":"Image Classification using BigTransfer (BiT)","text":"","code":"bit_model_url = \"https://tfhub.dev/google/bit/m-r50x1/1\" bit_module = hub.KerasLayer(bit_model_url)"},{"path":"https://keras.posit.co/articles/examples/bit.html","id":"create-bigtransfer-bit-model","dir":"Articles > Examples","previous_headings":"","what":"Create BigTransfer (BiT) model","title":"Image Classification using BigTransfer (BiT)","text":"create new model, : Cut BiT model’s original head. leaves us “pre-logits” output. use ‘feature extractor’ models (.e. subdirectories titled feature_vectors), since models head already cut . Add new head number outputs equal number classes new task. Note important initialise head zeroes.","code":"class MyBiTModel(keras.Model):     def __init__(self, num_classes, module, **kwargs):         super().__init__(**kwargs)          self.num_classes = num_classes         self.head = keras.layers.Dense(num_classes, kernel_initializer=\"zeros\")         self.bit_model = module      def call(self, images):         bit_embedding = self.bit_model(images)         return self.head(bit_embedding)   model = MyBiTModel(num_classes=NUM_CLASSES, module=bit_module)"},{"path":"https://keras.posit.co/articles/examples/bit.html","id":"define-optimizer-and-loss","dir":"Articles > Examples","previous_headings":"","what":"Define optimizer and loss","title":"Image Classification using BigTransfer (BiT)","text":"","code":"learning_rate = 0.003 * BATCH_SIZE / 512  # Decay learning rate by a factor of 10 at SCHEDULE_BOUNDARIES. lr_schedule = keras.optimizers.schedules.PiecewiseConstantDecay(     boundaries=SCHEDULE_BOUNDARIES,     values=[         learning_rate,         learning_rate * 0.1,         learning_rate * 0.01,         learning_rate * 0.001,     ], ) optimizer = keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9)  loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)"},{"path":"https://keras.posit.co/articles/examples/bit.html","id":"compile-the-model","dir":"Articles > Examples","previous_headings":"","what":"Compile the model","title":"Image Classification using BigTransfer (BiT)","text":"","code":"model.compile(optimizer=optimizer, loss=loss_fn, metrics=[\"accuracy\"])"},{"path":"https://keras.posit.co/articles/examples/bit.html","id":"set-up-callbacks","dir":"Articles > Examples","previous_headings":"","what":"Set up callbacks","title":"Image Classification using BigTransfer (BiT)","text":"","code":"train_callbacks = [     keras.callbacks.EarlyStopping(         monitor=\"val_accuracy\", patience=2, restore_best_weights=True     ) ]"},{"path":"https://keras.posit.co/articles/examples/bit.html","id":"train-the-model","dir":"Articles > Examples","previous_headings":"","what":"Train the model","title":"Image Classification using BigTransfer (BiT)","text":"","code":"history = model.fit(     pipeline_train,     batch_size=BATCH_SIZE,     epochs=int(SCHEDULE_LENGTH / STEPS_PER_EPOCH),     steps_per_epoch=STEPS_PER_EPOCH,     validation_data=pipeline_validation,     callbacks=train_callbacks, )"},{"path":"https://keras.posit.co/articles/examples/bit.html","id":"plot-the-training-and-validation-metrics","dir":"Articles > Examples","previous_headings":"","what":"Plot the training and validation metrics","title":"Image Classification using BigTransfer (BiT)","text":"","code":"def plot_hist(hist):     plt.plot(hist.history[\"accuracy\"])     plt.plot(hist.history[\"val_accuracy\"])     plt.plot(hist.history[\"loss\"])     plt.plot(hist.history[\"val_loss\"])     plt.title(\"Training Progress\")     plt.ylabel(\"Accuracy/Loss\")     plt.xlabel(\"Epochs\")     plt.legend(         [\"train_acc\", \"val_acc\", \"train_loss\", \"val_loss\"], loc=\"upper left\"     )     plt.show()   plot_hist(history)"},{"path":"https://keras.posit.co/articles/examples/bit.html","id":"evaluate-the-model","dir":"Articles > Examples","previous_headings":"","what":"Evaluate the model","title":"Image Classification using BigTransfer (BiT)","text":"","code":"accuracy = model.evaluate(pipeline_validation)[1] * 100 print(\"Accuracy: {:.2f}%\".format(accuracy))"},{"path":"https://keras.posit.co/articles/examples/bit.html","id":"conclusion","dir":"Articles > Examples","previous_headings":"","what":"Conclusion","title":"Image Classification using BigTransfer (BiT)","text":"BiT performs well across surprisingly wide range data regimes – 1 example per class 1M total examples. BiT achieves 87.5% top-1 accuracy ILSVRC-2012, 99.4% CIFAR-10, 76.3% 19 task Visual Task Adaptation Benchmark (VTAB). small datasets, BiT attains 76.8% ILSVRC-2012 10 examples per class, 97.0% CIFAR-10 10 examples per class.  can experiment BigTransfer Method following original paper. Example available HuggingFace | Trained Model | Demo | | :–: | :–: | |  |  |","code":""},{"path":"https://keras.posit.co/articles/examples/captcha_ocr.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"OCR model for reading Captchas","text":"example demonstrates simple OCR model built Functional API. Apart combining CNN RNN, also illustrates can instantiate new layer use “Endpoint layer” implementing CTC loss. detailed guide layer subclassing, please check page developer guides.","code":""},{"path":"https://keras.posit.co/articles/examples/captcha_ocr.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"OCR model for reading Captchas","text":"","code":"import os import numpy as np import matplotlib.pyplot as plt  from pathlib import Path from collections import Counter  import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers"},{"path":"https://keras.posit.co/articles/examples/captcha_ocr.html","id":"load-the-data-captcha-images","dir":"Articles > Examples","previous_headings":"","what":"Load the data: Captcha Images","title":"OCR model for reading Captchas","text":"Let’s download data. curl -LO https://github.com/AakashKumarNain/CaptchaCracker/raw/master/captcha_images_v2.zip unzip -qq captcha_images_v2.zip dataset contains 1040 captcha files png images. label sample string, name file (minus file extension). map character string integer training model. Similary, need map predictions model back strings. purpose maintain two dictionaries, mapping characters integers, integers characters, respectively.","code":"# Path to the data directory data_dir = Path(\"./captcha_images_v2/\")  # Get list of all the images images = sorted(list(map(str, list(data_dir.glob(\"*.png\"))))) labels = [img.split(os.path.sep)[-1].split(\".png\")[0] for img in images] characters = set(char for label in labels for char in label) characters = sorted(list(characters))  print(\"Number of images found: \", len(images)) print(\"Number of labels found: \", len(labels)) print(\"Number of unique characters: \", len(characters)) print(\"Characters present: \", characters)  # Batch size for training and validation batch_size = 16  # Desired image dimensions img_width = 200 img_height = 50  # Factor by which the image is going to be downsampled # by the convolutional blocks. We will be using two # convolution blocks and each block will have # a pooling layer which downsample the features by a factor of 2. # Hence total downsampling factor would be 4. downsample_factor = 4  # Maximum length of any captcha in the dataset max_length = max([len(label) for label in labels])"},{"path":"https://keras.posit.co/articles/examples/captcha_ocr.html","id":"preprocessing","dir":"Articles > Examples","previous_headings":"","what":"Preprocessing","title":"OCR model for reading Captchas","text":"","code":"# Mapping characters to integers char_to_num = layers.StringLookup(vocabulary=list(characters), mask_token=None)  # Mapping integers back to original characters num_to_char = layers.StringLookup(     vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True )   def split_data(images, labels, train_size=0.9, shuffle=True):     # 1. Get the total size of the dataset     size = len(images)     # 2. Make an indices array and shuffle it, if required     indices = np.arange(size)     if shuffle:         np.random.shuffle(indices)     # 3. Get the size of training samples     train_samples = int(size * train_size)     # 4. Split data into training and validation sets     x_train, y_train = (         images[indices[:train_samples]],         labels[indices[:train_samples]],     )     x_valid, y_valid = (         images[indices[train_samples:]],         labels[indices[train_samples:]],     )     return x_train, x_valid, y_train, y_valid   # Splitting data into training and validation sets x_train, x_valid, y_train, y_valid = split_data(     np.array(images), np.array(labels) )   def encode_single_sample(img_path, label):     # 1. Read image     img = tf.io.read_file(img_path)     # 2. Decode and convert to grayscale     img = tf.io.decode_png(img, channels=1)     # 3. Convert to float32 in [0, 1] range     img = tf.image.convert_image_dtype(img, tf.float32)     # 4. Resize to the desired size     img = tf.image.resize(img, [img_height, img_width])     # 5. Transpose the image because we want the time     # dimension to correspond to the width of the image.     img = tf.transpose(img, perm=[1, 0, 2])     # 6. Map the characters in label to numbers     label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))     # 7. Return a dict as our model is expecting two inputs     return {\"image\": img, \"label\": label}"},{"path":"https://keras.posit.co/articles/examples/captcha_ocr.html","id":"create-dataset-objects","dir":"Articles > Examples","previous_headings":"","what":"Create Dataset objects","title":"OCR model for reading Captchas","text":"","code":"train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)) train_dataset = (     train_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)     .batch(batch_size)     .prefetch(buffer_size=tf.data.AUTOTUNE) )  validation_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid)) validation_dataset = (     validation_dataset.map(         encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE     )     .batch(batch_size)     .prefetch(buffer_size=tf.data.AUTOTUNE) )"},{"path":"https://keras.posit.co/articles/examples/captcha_ocr.html","id":"visualize-the-data","dir":"Articles > Examples","previous_headings":"","what":"Visualize the data","title":"OCR model for reading Captchas","text":"","code":"_, ax = plt.subplots(4, 4, figsize=(10, 5)) for batch in train_dataset.take(1):     images = batch[\"image\"]     labels = batch[\"label\"]     for i in range(16):         img = (images[i] * 255).numpy().astype(\"uint8\")         label = (             tf.strings.reduce_join(num_to_char(labels[i]))             .numpy()             .decode(\"utf-8\")         )         ax[i // 4, i % 4].imshow(img[:, :, 0].T, cmap=\"gray\")         ax[i // 4, i % 4].set_title(label)         ax[i // 4, i % 4].axis(\"off\") plt.show()"},{"path":"https://keras.posit.co/articles/examples/captcha_ocr.html","id":"model","dir":"Articles > Examples","previous_headings":"","what":"Model","title":"OCR model for reading Captchas","text":"","code":"class CTCLayer(layers.Layer):     def __init__(self, name=None):         super().__init__(name=name)         self.loss_fn = keras.backend.ctc_batch_cost      def call(self, y_true, y_pred):         # Compute the training-time loss value and add it         # to the layer using `self.add_loss()`.         batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")         input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")         label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")          input_length = input_length * tf.ones(             shape=(batch_len, 1), dtype=\"int64\"         )         label_length = label_length * tf.ones(             shape=(batch_len, 1), dtype=\"int64\"         )          loss = self.loss_fn(y_true, y_pred, input_length, label_length)         self.add_loss(loss)          # At test time, just return the computed predictions         return y_pred   def build_model():     # Inputs to the model     input_img = layers.Input(         shape=(img_width, img_height, 1), name=\"image\", dtype=\"float32\"     )     labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")      # First conv block     x = layers.Conv2D(         32,         (3, 3),         activation=\"relu\",         kernel_initializer=\"he_normal\",         padding=\"same\",         name=\"Conv1\",     )(input_img)     x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)      # Second conv block     x = layers.Conv2D(         64,         (3, 3),         activation=\"relu\",         kernel_initializer=\"he_normal\",         padding=\"same\",         name=\"Conv2\",     )(x)     x = layers.MaxPooling2D((2, 2), name=\"pool2\")(x)      # We have used two max pool with pool size and strides 2.     # Hence, downsampled feature maps are 4x smaller. The number of     # filters in the last layer is 64. Reshape accordingly before     # passing the output to the RNN part of the model     new_shape = ((img_width // 4), (img_height // 4) * 64)     x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)     x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)     x = layers.Dropout(0.2)(x)      # RNNs     x = layers.Bidirectional(         layers.LSTM(128, return_sequences=True, dropout=0.25)     )(x)     x = layers.Bidirectional(         layers.LSTM(64, return_sequences=True, dropout=0.25)     )(x)      # Output layer     x = layers.Dense(         len(char_to_num.get_vocabulary()) + 1,         activation=\"softmax\",         name=\"dense2\",     )(x)      # Add CTC layer for calculating CTC loss at each step     output = CTCLayer(name=\"ctc_loss\")(labels, x)      # Define the model     model = keras.models.Model(         inputs=[input_img, labels], outputs=output, name=\"ocr_model_v1\"     )     # Optimizer     opt = keras.optimizers.Adam()     # Compile the model and return     model.compile(optimizer=opt)     return model   # Get the model model = build_model() model.summary()"},{"path":"https://keras.posit.co/articles/examples/captcha_ocr.html","id":"training","dir":"Articles > Examples","previous_headings":"","what":"Training","title":"OCR model for reading Captchas","text":"","code":"epochs = 1 early_stopping_patience = 10 # Add early stopping early_stopping = keras.callbacks.EarlyStopping(     monitor=\"val_loss\",     patience=early_stopping_patience,     restore_best_weights=True, )  # Train the model history = model.fit(     train_dataset,     validation_data=validation_dataset,     epochs=epochs,     callbacks=[early_stopping], )"},{"path":"https://keras.posit.co/articles/examples/captcha_ocr.html","id":"inference","dir":"Articles > Examples","previous_headings":"","what":"Inference","title":"OCR model for reading Captchas","text":"can use trained model hosted Hugging Face Hub try demo Hugging Face Spaces.","code":"# Get the prediction model by extracting layers till the output layer prediction_model = keras.models.Model(     model.get_layer(name=\"image\").input, model.get_layer(name=\"dense2\").output ) prediction_model.summary()   # A utility function to decode the output of the network def decode_batch_predictions(pred):     input_len = np.ones(pred.shape[0]) * pred.shape[1]     # Use greedy search. For complex tasks, you can use beam search     results = keras.backend.ctc_decode(         pred, input_length=input_len, greedy=True     )[0][0][:, :max_length]     # Iterate over the results and get back the text     output_text = []     for res in results:         res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")         output_text.append(res)     return output_text   #  Let's check results on some validation samples for batch in validation_dataset.take(1):     batch_images = batch[\"image\"]     batch_labels = batch[\"label\"]      preds = prediction_model.predict(batch_images)     pred_texts = decode_batch_predictions(preds)      orig_texts = []     for label in batch_labels:         label = (             tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")         )         orig_texts.append(label)      _, ax = plt.subplots(4, 4, figsize=(15, 5))     for i in range(len(pred_texts)):         img = (batch_images[i, :, :, 0] * 255).numpy().astype(np.uint8)         img = img.T         title = f\"Prediction: {pred_texts[i]}\"         ax[i // 4, i % 4].imshow(img, cmap=\"gray\")         ax[i // 4, i % 4].set_title(title)         ax[i // 4, i % 4].axis(\"off\") plt.show()"},{"path":"https://keras.posit.co/articles/examples/cct.html","id":"imports","dir":"Articles > Examples","previous_headings":"","what":"Imports","title":"Compact Convolutional Transformers","text":"","code":"from keras import layers import keras as keras  import matplotlib.pyplot as plt import numpy as np"},{"path":"https://keras.posit.co/articles/examples/cct.html","id":"hyperparameters-and-constants","dir":"Articles > Examples","previous_headings":"","what":"Hyperparameters and constants","title":"Compact Convolutional Transformers","text":"","code":"positional_emb = True conv_layers = 2 projection_dim = 128  num_heads = 2 transformer_units = [     projection_dim,     projection_dim, ] transformer_layers = 2 stochastic_depth_rate = 0.1  learning_rate = 0.001 weight_decay = 0.0001 batch_size = 128 num_epochs = 30 image_size = 32"},{"path":"https://keras.posit.co/articles/examples/cct.html","id":"load-cifar-10-dataset","dir":"Articles > Examples","previous_headings":"","what":"Load CIFAR-10 dataset","title":"Compact Convolutional Transformers","text":"","code":"num_classes = 10 input_shape = (32, 32, 3)  (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()  y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes)  print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\") print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"},{"path":"https://keras.posit.co/articles/examples/cct.html","id":"the-cct-tokenizer","dir":"Articles > Examples","previous_headings":"","what":"The CCT tokenizer","title":"Compact Convolutional Transformers","text":"first recipe introduced CCT authors tokenizer processing images. standard ViT, images organized uniform non-overlapping patches. eliminates boundary-level information present different patches. important neural network effectively exploit locality information. figure presents illustration images organized patches.  already know convolutions quite good exploiting locality information. , based , authors introduce -convolution mini-network produce image patches. Positional embeddings optional CCT. want use , can use Layer defined .","code":"class CCTTokenizer(layers.Layer):     def __init__(         self,         kernel_size=3,         stride=1,         padding=1,         pooling_kernel_size=3,         pooling_stride=2,         num_conv_layers=conv_layers,         num_output_channels=[64, 128],         positional_emb=positional_emb,         **kwargs,     ):         super().__init__(**kwargs)          # This is our tokenizer.         self.conv_model = keras.Sequential()         for i in range(num_conv_layers):             self.conv_model.add(                 layers.Conv2D(                     num_output_channels[i],                     kernel_size,                     stride,                     padding=\"valid\",                     use_bias=False,                     activation=\"relu\",                     kernel_initializer=\"he_normal\",                 )             )             self.conv_model.add(layers.ZeroPadding2D(padding))             self.conv_model.add(                 layers.MaxPooling2D(pooling_kernel_size, pooling_stride, \"same\")             )          self.positional_emb = positional_emb      def call(self, images):         outputs = self.conv_model(images)         # After passing the images through our mini-network the spatial dimensions         # are flattened to form sequences.         reshaped = keras.ops.reshape(             outputs,             (                 -1,                 keras.ops.shape(outputs)[1] * keras.ops.shape(outputs)[2],                 keras.ops.shape(outputs)[-1],             ),         )         return reshaped class PositionEmbedding(keras.layers.Layer):     def __init__(         self,         sequence_length,         initializer=\"glorot_uniform\",         **kwargs,     ):         super().__init__(**kwargs)         if sequence_length is None:             raise ValueError(                 \"`sequence_length` must be an Integer, received `None`.\"             )         self.sequence_length = int(sequence_length)         self.initializer = keras.initializers.get(initializer)      def get_config(self):         config = super().get_config()         config.update(             {                 \"sequence_length\": self.sequence_length,                 \"initializer\": keras.initializers.serialize(self.initializer),             }         )         return config      def build(self, input_shape):         feature_size = input_shape[-1]         self.position_embeddings = self.add_weight(             name=\"embeddings\",             shape=[self.sequence_length, feature_size],             initializer=self.initializer,             trainable=True,         )          super().build(input_shape)      def call(self, inputs, start_index=0):         shape = keras.ops.shape(inputs)         feature_length = shape[-1]         sequence_length = shape[-2]         # trim to match the length of the input sequence, which might be less         # than the sequence_length of the layer.         position_embeddings = keras.ops.convert_to_tensor(             self.position_embeddings         )         position_embeddings = keras.ops.slice(             position_embeddings,             (start_index, 0),             (sequence_length, feature_length),         )         return keras.ops.broadcast_to(position_embeddings, shape)      def compute_output_shape(self, input_shape):         return input_shape"},{"path":"https://keras.posit.co/articles/examples/cct.html","id":"sequence-pooling","dir":"Articles > Examples","previous_headings":"","what":"Sequence Pooling","title":"Compact Convolutional Transformers","text":"Another recipe introduced CCT attention pooling sequence pooling. ViT, feature map corresponding class token pooled used subsequent classification task (downstream task).","code":"class SequencePooling(layers.Layer):     def __init__(self):         super().__init__()         self.attention = layers.Dense(1)      def call(self, x):         attention_weights = keras.ops.softmax(self.attention(x), axis=1)         attention_weights = keras.ops.transpose(             attention_weights, axes=(0, 2, 1)         )         weighted_representation = keras.ops.matmul(attention_weights, x)         return keras.ops.squeeze(weighted_representation, -2)"},{"path":"https://keras.posit.co/articles/examples/cct.html","id":"stochastic-depth-for-regularization","dir":"Articles > Examples","previous_headings":"","what":"Stochastic depth for regularization","title":"Compact Convolutional Transformers","text":"Stochastic depth regularization technique randomly drops set layers. inference, layers kept . much similar Dropout operates block layers rather individual nodes present inside layer. CCT, stochastic depth used just residual blocks Transformers encoder.","code":"# Referred from: github.com:rwightman/pytorch-image-models. class StochasticDepth(layers.Layer):     def __init__(self, drop_prop, **kwargs):         super().__init__(**kwargs)         self.drop_prob = drop_prop      def call(self, x, training=None):         if training:             keep_prob = 1 - self.drop_prob             shape = (keras.ops.shape(x)[0],) + (1,) * (len(x.shape) - 1)             random_tensor = keep_prob + keras.random.uniform(shape, 0, 1)             random_tensor = keras.ops.floor(random_tensor)             return (x / keep_prob) * random_tensor         return x"},{"path":"https://keras.posit.co/articles/examples/cct.html","id":"mlp-for-the-transformers-encoder","dir":"Articles > Examples","previous_headings":"","what":"MLP for the Transformers encoder","title":"Compact Convolutional Transformers","text":"","code":"def mlp(x, hidden_units, dropout_rate):     for units in hidden_units:         x = layers.Dense(units, activation=keras.ops.gelu)(x)         x = layers.Dropout(dropout_rate)(x)     return x"},{"path":"https://keras.posit.co/articles/examples/cct.html","id":"data-augmentation","dir":"Articles > Examples","previous_headings":"","what":"Data augmentation","title":"Compact Convolutional Transformers","text":"original paper, authors use AutoAugment induce stronger regularization. example, using standard geometric augmentations like random cropping flipping.","code":"# Note the rescaling layer. These layers have pre-defined inference behavior. data_augmentation = keras.Sequential(     [         layers.Rescaling(scale=1.0 / 255),         layers.RandomCrop(image_size, image_size),         layers.RandomFlip(\"horizontal\"),     ],     name=\"data_augmentation\", )"},{"path":"https://keras.posit.co/articles/examples/cct.html","id":"the-final-cct-model","dir":"Articles > Examples","previous_headings":"","what":"The final CCT model","title":"Compact Convolutional Transformers","text":"CCT, outputs Transformers encoder weighted passed final task-specific layer (example, classification).","code":"def create_cct_model(     image_size=image_size,     input_shape=input_shape,     num_heads=num_heads,     projection_dim=projection_dim,     transformer_units=transformer_units, ):     inputs = layers.Input(input_shape)      # Augment data.     augmented = data_augmentation(inputs)      # Encode patches.     cct_tokenizer = CCTTokenizer()     encoded_patches = cct_tokenizer(augmented)      # Apply positional embedding.     if positional_emb:         sequence_length = encoded_patches.shape[1]         encoded_patches += PositionEmbedding(sequence_length=sequence_length)(             encoded_patches         )      # Calculate Stochastic Depth probabilities.     dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]      # Create multiple layers of the Transformer block.     for i in range(transformer_layers):         # Layer normalization 1.         x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)          # Create a multi-head attention layer.         attention_output = layers.MultiHeadAttention(             num_heads=num_heads, key_dim=projection_dim, dropout=0.1         )(x1, x1)          # Skip connection 1.         attention_output = StochasticDepth(dpr[i])(attention_output)         x2 = layers.Add()([attention_output, encoded_patches])          # Layer normalization 2.         x3 = layers.LayerNormalization(epsilon=1e-5)(x2)          # MLP.         x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)          # Skip connection 2.         x3 = StochasticDepth(dpr[i])(x3)         encoded_patches = layers.Add()([x3, x2])      # Apply sequence pooling.     representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)     weighted_representation = SequencePooling()(representation)      # Classify outputs.     logits = layers.Dense(num_classes)(weighted_representation)     # Create the Keras model.     model = keras.Model(inputs=inputs, outputs=logits)     return model"},{"path":"https://keras.posit.co/articles/examples/cct.html","id":"model-training-and-evaluation","dir":"Articles > Examples","previous_headings":"","what":"Model training and evaluation","title":"Compact Convolutional Transformers","text":"Let’s now visualize training progress model. CCT model just trained just 0.4 million parameters, gets us ~79% top-1 accuracy within 30 epochs. plot shows signs overfitting well. means can train network longer (perhaps bit regularization) may obtain even better performance. performance can improved additional recipes like cosine decay learning rate schedule, data augmentation techniques like AutoAugment, MixUp Cutmix. modifications, authors present 95.1% top-1 accuracy CIFAR-10 dataset. authors also present number experiments study number convolution blocks, Transformers layers, etc. affect final performance CCTs. comparison, ViT model takes 4.7 million parameters 100 epochs training reach top-1 accuracy 78.22% CIFAR-10 dataset. can refer notebook know experimental setup. authors also demonstrate performance Compact Convolutional Transformers NLP tasks report competitive results .","code":"def run_experiment(model):     optimizer = keras.optimizers.AdamW(learning_rate=0.001, weight_decay=0.0001)      model.compile(         optimizer=optimizer,         loss=keras.losses.CategoricalCrossentropy(             from_logits=True, label_smoothing=0.1         ),         metrics=[             keras.metrics.CategoricalAccuracy(name=\"accuracy\"),             keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),         ],     )      checkpoint_filepath = \"/tmp/checkpoint.weights.h5\"     checkpoint_callback = keras.callbacks.ModelCheckpoint(         checkpoint_filepath,         monitor=\"val_accuracy\",         save_best_only=True,         save_weights_only=True,     )      history = model.fit(         x=x_train,         y=y_train,         batch_size=batch_size,         epochs=num_epochs,         validation_split=0.1,         callbacks=[checkpoint_callback],     )      model.load_weights(checkpoint_filepath)     _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)     print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")     print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")      return history   cct_model = create_cct_model() history = run_experiment(cct_model) plt.plot(history.history[\"loss\"], label=\"train_loss\") plt.plot(history.history[\"val_loss\"], label=\"val_loss\") plt.xlabel(\"Epochs\") plt.ylabel(\"Loss\") plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14) plt.legend() plt.grid() plt.show()"},{"path":"https://keras.posit.co/articles/examples/collaborative_filtering_movielens.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Collaborative Filtering for Movie Recommendations","text":"example demonstrates Collaborative filtering using Movielens dataset recommend movies users. MovieLens ratings dataset lists ratings given set users set movies. goal able predict ratings movies user yet watched. movies highest predicted ratings can recommended user. steps model follows: Map user ID “user vector” via embedding matrix Map movie ID “movie vector” via embedding matrix Compute dot product user vector movie vector, obtain match score user movie (predicted rating). Train embeddings via gradient descent using known user-movie pairs. References: Collaborative Filtering Neural Collaborative Filtering","code":"import pandas as pd from pathlib import Path import matplotlib.pyplot as plt import numpy as np from zipfile import ZipFile  import keras as keras from keras import layers from keras import ops"},{"path":"https://keras.posit.co/articles/examples/collaborative_filtering_movielens.html","id":"first-load-the-data-and-apply-preprocessing","dir":"Articles > Examples","previous_headings":"","what":"First, load the data and apply preprocessing","title":"Collaborative Filtering for Movie Recommendations","text":"First, need perform preprocessing encode users movies integer indices.","code":"# Download the actual data from http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\" # Use the ratings.csv file movielens_data_file_url = (     \"http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\" ) movielens_zipped_file = keras.utils.get_file(     \"ml-latest-small.zip\", movielens_data_file_url, extract=False ) keras_datasets_path = Path(movielens_zipped_file).parents[0] movielens_dir = keras_datasets_path / \"ml-latest-small\"  # Only extract the data the first time the script is run. if not movielens_dir.exists():     with ZipFile(movielens_zipped_file, \"r\") as zip:         # Extract files         print(\"Extracting all the files now...\")         zip.extractall(path=keras_datasets_path)         print(\"Done!\")  ratings_file = movielens_dir / \"ratings.csv\" df = pd.read_csv(ratings_file) user_ids = df[\"userId\"].unique().tolist() user2user_encoded = {x: i for i, x in enumerate(user_ids)} userencoded2user = {i: x for i, x in enumerate(user_ids)} movie_ids = df[\"movieId\"].unique().tolist() movie2movie_encoded = {x: i for i, x in enumerate(movie_ids)} movie_encoded2movie = {i: x for i, x in enumerate(movie_ids)} df[\"user\"] = df[\"userId\"].map(user2user_encoded) df[\"movie\"] = df[\"movieId\"].map(movie2movie_encoded)  num_users = len(user2user_encoded) num_movies = len(movie_encoded2movie) df[\"rating\"] = df[\"rating\"].values.astype(np.float32) # min and max ratings will be used to normalize the ratings later min_rating = min(df[\"rating\"]) max_rating = max(df[\"rating\"])  print(     \"Number of users: {}, Number of Movies: {}, Min rating: {}, Max rating: {}\".format(         num_users, num_movies, min_rating, max_rating     ) )"},{"path":"https://keras.posit.co/articles/examples/collaborative_filtering_movielens.html","id":"prepare-training-and-validation-data","dir":"Articles > Examples","previous_headings":"","what":"Prepare training and validation data","title":"Collaborative Filtering for Movie Recommendations","text":"","code":"df = df.sample(frac=1, random_state=42) x = df[[\"user\", \"movie\"]].values # Normalize the targets between 0 and 1. Makes it easy to train. y = (     df[\"rating\"]     .apply(lambda x: (x - min_rating) / (max_rating - min_rating))     .values ) # Assuming training on 90% of the data and validating on 10%. train_indices = int(0.9 * df.shape[0]) x_train, x_val, y_train, y_val = (     x[:train_indices],     x[train_indices:],     y[:train_indices],     y[train_indices:], )"},{"path":"https://keras.posit.co/articles/examples/collaborative_filtering_movielens.html","id":"create-the-model","dir":"Articles > Examples","previous_headings":"","what":"Create the model","title":"Collaborative Filtering for Movie Recommendations","text":"embed users movies 50-dimensional vectors. model computes match score user movie embeddings via dot product, adds per-movie per-user bias. match score scaled [0, 1] interval via sigmoid (since ratings normalized range).","code":"EMBEDDING_SIZE = 50   class RecommenderNet(keras.Model):     def __init__(self, num_users, num_movies, embedding_size, **kwargs):         super().__init__(**kwargs)         self.num_users = num_users         self.num_movies = num_movies         self.embedding_size = embedding_size         self.user_embedding = layers.Embedding(             num_users,             embedding_size,             embeddings_initializer=\"he_normal\",             embeddings_regularizer=keras.regularizers.l2(1e-6),         )         self.user_bias = layers.Embedding(num_users, 1)         self.movie_embedding = layers.Embedding(             num_movies,             embedding_size,             embeddings_initializer=\"he_normal\",             embeddings_regularizer=keras.regularizers.l2(1e-6),         )         self.movie_bias = layers.Embedding(num_movies, 1)      def call(self, inputs):         user_vector = self.user_embedding(inputs[:, 0])         user_bias = self.user_bias(inputs[:, 0])         movie_vector = self.movie_embedding(inputs[:, 1])         movie_bias = self.movie_bias(inputs[:, 1])         dot_user_movie = ops.tensordot(user_vector, movie_vector, 2)         # Add all the components (including bias)         x = dot_user_movie + user_bias + movie_bias         # The sigmoid activation forces the rating to between 0 and 1         return ops.nn.sigmoid(x)   model = RecommenderNet(num_users, num_movies, EMBEDDING_SIZE) model.compile(     loss=keras.losses.BinaryCrossentropy(),     optimizer=keras.optimizers.Adam(learning_rate=0.001), )"},{"path":"https://keras.posit.co/articles/examples/collaborative_filtering_movielens.html","id":"train-the-model-based-on-the-data-split","dir":"Articles > Examples","previous_headings":"","what":"Train the model based on the data split","title":"Collaborative Filtering for Movie Recommendations","text":"","code":"history = model.fit(     x=x_train,     y=y_train,     batch_size=64,     epochs=5,     verbose=1,     validation_data=(x_val, y_val), )"},{"path":"https://keras.posit.co/articles/examples/collaborative_filtering_movielens.html","id":"plot-training-and-validation-loss","dir":"Articles > Examples","previous_headings":"","what":"Plot training and validation loss","title":"Collaborative Filtering for Movie Recommendations","text":"","code":"plt.plot(history.history[\"loss\"]) plt.plot(history.history[\"val_loss\"]) plt.title(\"model loss\") plt.ylabel(\"loss\") plt.xlabel(\"epoch\") plt.legend([\"train\", \"test\"], loc=\"upper left\") plt.show()"},{"path":"https://keras.posit.co/articles/examples/collaborative_filtering_movielens.html","id":"show-top-10-movie-recommendations-to-a-user","dir":"Articles > Examples","previous_headings":"","what":"Show top 10 movie recommendations to a user","title":"Collaborative Filtering for Movie Recommendations","text":"Example available HuggingFace","code":"movie_df = pd.read_csv(movielens_dir / \"movies.csv\")  # Let us get a user and see the top recommendations. user_id = df.userId.sample(1).iloc[0] movies_watched_by_user = df[df.userId == user_id] movies_not_watched = movie_df[     ~movie_df[\"movieId\"].isin(movies_watched_by_user.movieId.values) ][\"movieId\"] movies_not_watched = list(     set(movies_not_watched).intersection(set(movie2movie_encoded.keys())) ) movies_not_watched = [[movie2movie_encoded.get(x)] for x in movies_not_watched] user_encoder = user2user_encoded.get(user_id) user_movie_array = np.hstack(     ([[user_encoder]] * len(movies_not_watched), movies_not_watched) ) ratings = model.predict(user_movie_array).flatten() top_ratings_indices = ratings.argsort()[-10:][::-1] recommended_movie_ids = [     movie_encoded2movie.get(movies_not_watched[x][0])     for x in top_ratings_indices ]  print(\"Showing recommendations for user: {}\".format(user_id)) print(\"====\" * 9) print(\"Movies with high ratings from user\") print(\"----\" * 8) top_movies_user = (     movies_watched_by_user.sort_values(by=\"rating\", ascending=False)     .head(5)     .movieId.values ) movie_df_rows = movie_df[movie_df[\"movieId\"].isin(top_movies_user)] for row in movie_df_rows.itertuples():     print(row.title, \":\", row.genres)  print(\"----\" * 8) print(\"Top 10 movie recommendations\") print(\"----\" * 8) recommended_movies = movie_df[movie_df[\"movieId\"].isin(recommended_movie_ids)] for row in recommended_movies.itertuples():     print(row.title, \":\", row.genres)"},{"path":"https://keras.posit.co/articles/examples/conv_lstm.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Next-Frame Video Prediction with Convolutional LSTMs","text":"Convolutional LSTM architectures bring together time series processing computer vision introducing convolutional recurrent cell LSTM layer. example, explore Convolutional LSTM model application next-frame prediction, process predicting video frames come next given series past frames.","code":""},{"path":"https://keras.posit.co/articles/examples/conv_lstm.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Next-Frame Video Prediction with Convolutional LSTMs","text":"","code":"import numpy as np import matplotlib.pyplot as plt  import keras as keras from keras import layers  import io import imageio from IPython.display import Image, display from ipywidgets import widgets, Layout, HBox"},{"path":"https://keras.posit.co/articles/examples/conv_lstm.html","id":"dataset-construction","dir":"Articles > Examples","previous_headings":"","what":"Dataset Construction","title":"Next-Frame Video Prediction with Convolutional LSTMs","text":"example, using Moving MNIST dataset. download dataset construct preprocess training validation sets. next-frame prediction, model using previous frame, ’ll call f_n, predict new frame, called f_(n + 1). allow model create predictions, ’ll need process data “shifted” inputs outputs, input data frame x_n, used predict frame y_(n + 1).","code":"# Download and load the dataset. fpath = keras.utils.get_file(     \"moving_mnist.npy\",     \"http://www.cs.toronto.edu/~nitish/unsupervised_video/mnist_test_seq.npy\", ) dataset = np.load(fpath)  # Swap the axes representing the number of frames and number of data samples. dataset = np.swapaxes(dataset, 0, 1) # We'll pick out 1000 of the 10000 total examples and use those. dataset = dataset[:1000, ...] # Add a channel dimension since the images are grayscale. dataset = np.expand_dims(dataset, axis=-1)  # Split into train and validation sets using indexing to optimize memory. indexes = np.arange(dataset.shape[0]) np.random.shuffle(indexes) train_index = indexes[: int(0.9 * dataset.shape[0])] val_index = indexes[int(0.9 * dataset.shape[0]) :] train_dataset = dataset[train_index] val_dataset = dataset[val_index]  # Normalize the data to the 0-1 range. train_dataset = train_dataset / 255 val_dataset = val_dataset / 255   # We'll define a helper function to shift the frames, where # `x` is frames 0 to n - 1, and `y` is frames 1 to n. def create_shifted_frames(data):     x = data[:, 0 : data.shape[1] - 1, :, :]     y = data[:, 1 : data.shape[1], :, :]     return x, y   # Apply the processing function to the datasets. x_train, y_train = create_shifted_frames(train_dataset) x_val, y_val = create_shifted_frames(val_dataset)  # Inspect the dataset. print(     \"Training Dataset Shapes: \" + str(x_train.shape) + \", \" + str(y_train.shape) ) print(     \"Validation Dataset Shapes: \" + str(x_val.shape) + \", \" + str(y_val.shape) )"},{"path":"https://keras.posit.co/articles/examples/conv_lstm.html","id":"data-visualization","dir":"Articles > Examples","previous_headings":"","what":"Data Visualization","title":"Next-Frame Video Prediction with Convolutional LSTMs","text":"data consists sequences frames, used predict upcoming frame. Let’s take look sequential frames.","code":"# Construct a figure on which we will visualize the images. fig, axes = plt.subplots(4, 5, figsize=(10, 8))  # Plot each of the sequential images for one random data example. data_choice = np.random.choice(range(len(train_dataset)), size=1)[0] for idx, ax in enumerate(axes.flat):     ax.imshow(np.squeeze(train_dataset[data_choice][idx]), cmap=\"gray\")     ax.set_title(f\"Frame {idx + 1}\")     ax.axis(\"off\")  # Print information and display the figure. print(f\"Displaying frames for example {data_choice}.\") plt.show()"},{"path":"https://keras.posit.co/articles/examples/conv_lstm.html","id":"model-construction","dir":"Articles > Examples","previous_headings":"","what":"Model Construction","title":"Next-Frame Video Prediction with Convolutional LSTMs","text":"build Convolutional LSTM model, use ConvLSTM2D layer, accept inputs shape (batch_size, num_frames, width, height, channels), return prediction movie shape.","code":"# Construct the input layer with no definite frame size. inp = layers.Input(shape=(None, *x_train.shape[2:]))  # We will construct 3 `ConvLSTM2D` layers with batch normalization, # followed by a `Conv3D` layer for the spatiotemporal outputs. x = layers.ConvLSTM2D(     filters=64,     kernel_size=(5, 5),     padding=\"same\",     return_sequences=True,     activation=\"relu\", )(inp) x = layers.BatchNormalization()(x) x = layers.ConvLSTM2D(     filters=64,     kernel_size=(3, 3),     padding=\"same\",     return_sequences=True,     activation=\"relu\", )(x) x = layers.BatchNormalization()(x) x = layers.ConvLSTM2D(     filters=64,     kernel_size=(1, 1),     padding=\"same\",     return_sequences=True,     activation=\"relu\", )(x) x = layers.Conv3D(     filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\" )(x)  # Next, we will build the complete model and compile it. model = keras.models.Model(inp, x) model.compile(     loss=keras.losses.binary_crossentropy,     optimizer=keras.optimizers.Adam(), )"},{"path":"https://keras.posit.co/articles/examples/conv_lstm.html","id":"model-training","dir":"Articles > Examples","previous_headings":"","what":"Model Training","title":"Next-Frame Video Prediction with Convolutional LSTMs","text":"model data constructed, can now train model.","code":"# Define some callbacks to improve training. early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10) reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5)  # Define modifiable training hyperparameters. epochs = 20 batch_size = 5  # Fit the model to the training data. model.fit(     x_train,     y_train,     batch_size=batch_size,     epochs=epochs,     validation_data=(x_val, y_val),     callbacks=[early_stopping, reduce_lr], )"},{"path":"https://keras.posit.co/articles/examples/conv_lstm.html","id":"frame-prediction-visualizations","dir":"Articles > Examples","previous_headings":"","what":"Frame Prediction Visualizations","title":"Next-Frame Video Prediction with Convolutional LSTMs","text":"model now constructed trained, can generate example frame predictions based new video. ’ll pick random example validation set choose first ten frames . , can allow model predict 10 new frames, can compare ground truth frame predictions.","code":"# Select a random example from the validation dataset. example = val_dataset[np.random.choice(range(len(val_dataset)), size=1)[0]]  # Pick the first/last ten frames from the example. frames = example[:10, ...] original_frames = example[10:, ...]  # Predict a new set of 10 frames. for _ in range(10):     # Extract the model's prediction and post-process it.     new_prediction = model.predict(np.expand_dims(frames, axis=0))     new_prediction = np.squeeze(new_prediction, axis=0)     predicted_frame = np.expand_dims(new_prediction[-1, ...], axis=0)      # Extend the set of prediction frames.     frames = np.concatenate((frames, predicted_frame), axis=0)  # Construct a figure for the original and new frames. fig, axes = plt.subplots(2, 10, figsize=(20, 4))  # Plot the original frames. for idx, ax in enumerate(axes[0]):     ax.imshow(np.squeeze(original_frames[idx]), cmap=\"gray\")     ax.set_title(f\"Frame {idx + 11}\")     ax.axis(\"off\")  # Plot the new frames. new_frames = frames[10:, ...] for idx, ax in enumerate(axes[1]):     ax.imshow(np.squeeze(new_frames[idx]), cmap=\"gray\")     ax.set_title(f\"Frame {idx + 11}\")     ax.axis(\"off\")  # Display the figure. plt.show()"},{"path":"https://keras.posit.co/articles/examples/conv_lstm.html","id":"predicted-videos","dir":"Articles > Examples","previous_headings":"","what":"Predicted Videos","title":"Next-Frame Video Prediction with Convolutional LSTMs","text":"Finally, ’ll pick examples validation set construct GIFs see model’s predicted videos. can use trained model hosted Hugging Face Hub try demo Hugging Face Spaces.","code":"# Select a few random examples from the dataset. examples = val_dataset[np.random.choice(range(len(val_dataset)), size=5)]  # Iterate over the examples and predict the frames. predicted_videos = [] for example in examples:     # Pick the first/last ten frames from the example.     frames = example[:10, ...]     original_frames = example[10:, ...]     new_predictions = np.zeros(shape=(10, *frames[0].shape))      # Predict a new set of 10 frames.     for i in range(10):         # Extract the model's prediction and post-process it.         frames = example[: 10 + i + 1, ...]         new_prediction = model.predict(np.expand_dims(frames, axis=0))         new_prediction = np.squeeze(new_prediction, axis=0)         predicted_frame = np.expand_dims(new_prediction[-1, ...], axis=0)          # Extend the set of prediction frames.         new_predictions[i] = predicted_frame      # Create and save GIFs for each of the ground truth/prediction images.     for frame_set in [original_frames, new_predictions]:         # Construct a GIF from the selected video frames.         current_frames = np.squeeze(frame_set)         current_frames = current_frames[..., np.newaxis] * np.ones(3)         current_frames = (current_frames * 255).astype(np.uint8)         current_frames = list(current_frames)          # Construct a GIF from the frames.         with io.BytesIO() as gif:             imageio.mimsave(gif, current_frames, \"GIF\", duration=200)             predicted_videos.append(gif.getvalue())  # Display the videos. print(\" Truth\\tPrediction\") for i in range(0, len(predicted_videos), 2):     # Construct and display an `HBox` with the ground truth and prediction.     box = HBox(         [             widgets.Image(value=predicted_videos[i]),             widgets.Image(value=predicted_videos[i + 1]),         ]     )     display(box)"},{"path":"https://keras.posit.co/articles/examples/convmixer.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Image classification with ConvMixer","text":"Vision Transformers (ViT; Dosovitskiy et al.) extract small patches input images, linearly project , apply Transformer (Vaswani et al.) blocks. application ViTs image recognition tasks quickly becoming promising area research, ViTs eliminate need strong inductive biases (convolutions) modeling locality. presents general computation primititive capable learning just training data minimal inductive priors possible. ViTs yield great downstream performance trained proper regularization, data augmentation, relatively large datasets. Patches Need paper (note: time writing, submission ICLR 2022 conference), authors extend idea using patches train -convolutional network demonstrate competitive results. architecture namely ConvMixer uses recipes recent isotrophic architectures like ViT, MLP-Mixer (Tolstikhin et al.), using depth resolution across different layers network, residual connections, . example, implement ConvMixer model demonstrate performance CIFAR-10 dataset.","code":""},{"path":"https://keras.posit.co/articles/examples/convmixer.html","id":"imports","dir":"Articles > Examples","previous_headings":"","what":"Imports","title":"Image classification with ConvMixer","text":"","code":"import os  os.environ[\"KERAS_BACKEND\"] = \"jax\"  import keras from keras import layers  import matplotlib.pyplot as plt import tensorflow as tf import numpy as np"},{"path":"https://keras.posit.co/articles/examples/convmixer.html","id":"hyperparameters","dir":"Articles > Examples","previous_headings":"","what":"Hyperparameters","title":"Image classification with ConvMixer","text":"keep run time short, train model 10 epochs. focus core ideas ConvMixer, use training-specific elements like RandAugment (Cubuk et al.). interested learning details, please refer original paper.","code":"learning_rate = 0.001 weight_decay = 0.0001 batch_size = 128 num_epochs = 10"},{"path":"https://keras.posit.co/articles/examples/convmixer.html","id":"load-the-cifar-10-dataset","dir":"Articles > Examples","previous_headings":"","what":"Load the CIFAR-10 dataset","title":"Image classification with ConvMixer","text":"","code":"(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data() val_split = 0.1  val_indices = int(len(x_train) * val_split) new_x_train, new_y_train = x_train[val_indices:], y_train[val_indices:] x_val, y_val = x_train[:val_indices], y_train[:val_indices]  print(f\"Training data samples: {len(new_x_train)}\") print(f\"Validation data samples: {len(x_val)}\") print(f\"Test data samples: {len(x_test)}\")"},{"path":"https://keras.posit.co/articles/examples/convmixer.html","id":"prepare-tf-data-dataset-objects","dir":"Articles > Examples","previous_headings":"","what":"Prepare tf.data.Dataset objects","title":"Image classification with ConvMixer","text":"data augmentation pipeline different authors used CIFAR-10 dataset, fine purpose example. Note , ’s ok use TF APIs data /O preprocessing backends (jax, torch) feature-complete framework comes data preprocessing.","code":"image_size = 32 auto = tf.data.AUTOTUNE  augmentation_layers = [     keras.layers.RandomCrop(image_size, image_size),     keras.layers.RandomFlip(\"horizontal\") ]   def augment_images(images):     for layer in augmentation_layers:         images = layer(images, training=True)     return images   def make_datasets(images, labels, is_train=False):     dataset = tf.data.Dataset.from_tensor_slices((images, labels))     if is_train:         dataset = dataset.shuffle(batch_size * 10)     dataset = dataset.batch(batch_size)     if is_train:         dataset = dataset.map(             lambda x, y: (augment_images(x), y), num_parallel_calls=auto         )     return dataset.prefetch(auto)   train_dataset = make_datasets(new_x_train, new_y_train, is_train=True) val_dataset = make_datasets(x_val, y_val) test_dataset = make_datasets(x_test, y_test)"},{"path":"https://keras.posit.co/articles/examples/convmixer.html","id":"convmixer-utilities","dir":"Articles > Examples","previous_headings":"","what":"ConvMixer utilities","title":"Image classification with ConvMixer","text":"following figure (taken original paper) depicts ConvMixer model:  ConvMixer similar MLP-Mixer, model following key differences: Instead using fully-connected layers, uses standard convolution layers. Instead LayerNorm (typical ViTs MLP-Mixers), uses BatchNorm. Two types convolution layers used ConvMixer. (1): Depthwise convolutions, mixing spatial locations images, (2): Pointwise convolutions (follow depthwise convolutions), mixing channel-wise information across patches. Another keypoint use larger kernel sizes allow larger receptive field. model used experiment termed ConvMixer-256/8 256 denotes number channels 8 denotes depth. resulting model 0.8 million parameters.","code":"def activation_block(x):     x = layers.Activation(\"gelu\")(x)     return layers.BatchNormalization()(x)   def conv_stem(x, filters: int, patch_size: int):     x = layers.Conv2D(filters, kernel_size=patch_size, strides=patch_size)(x)     return activation_block(x)   def conv_mixer_block(x, filters: int, kernel_size: int):     # Depthwise convolution.     x0 = x     x = layers.DepthwiseConv2D(kernel_size=kernel_size, padding=\"same\")(x)     x = layers.Add()([activation_block(x), x0])  # Residual.      # Pointwise convolution.     x = layers.Conv2D(filters, kernel_size=1)(x)     x = activation_block(x)      return x   def get_conv_mixer_256_8(     image_size=32, filters=256, depth=8, kernel_size=5, patch_size=2, num_classes=10 ):     \"\"\"ConvMixer-256/8: https://openreview.net/pdf?id=TVHS5Y4dNvM.     The hyperparameter values are taken from the paper.     \"\"\"     inputs = keras.Input((image_size, image_size, 3))     x = layers.Rescaling(scale=1.0 / 255)(inputs)      # Extract patch embeddings.     x = conv_stem(x, filters, patch_size)      # ConvMixer blocks.     for _ in range(depth):         x = conv_mixer_block(x, filters, kernel_size)      # Classification block.     x = layers.GlobalAvgPool2D()(x)     outputs = layers.Dense(num_classes, activation=\"softmax\")(x)      return keras.Model(inputs, outputs)"},{"path":"https://keras.posit.co/articles/examples/convmixer.html","id":"model-training-and-evaluation-utility","dir":"Articles > Examples","previous_headings":"","what":"Model training and evaluation utility","title":"Image classification with ConvMixer","text":"","code":"# Code reference: # https://keras.io/examples/vision/image_classification_with_vision_transformer/.   def run_experiment(model):     optimizer = keras.optimizers.AdamW(         learning_rate=learning_rate, weight_decay=weight_decay     )      model.compile(         optimizer=optimizer,         loss=\"sparse_categorical_crossentropy\",         metrics=[\"accuracy\"],     )      checkpoint_filepath = \"/tmp/checkpoint.keras\"     checkpoint_callback = keras.callbacks.ModelCheckpoint(         checkpoint_filepath,         monitor=\"val_accuracy\",         save_best_only=True,         save_weights_only=False,     )      history = model.fit(         train_dataset,         validation_data=val_dataset,         epochs=num_epochs,         callbacks=[checkpoint_callback],     )      model.load_weights(checkpoint_filepath)     _, accuracy = model.evaluate(test_dataset)     print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")      return history, model"},{"path":"https://keras.posit.co/articles/examples/convmixer.html","id":"train-and-evaluate-model","dir":"Articles > Examples","previous_headings":"","what":"Train and evaluate model","title":"Image classification with ConvMixer","text":"gap training validation performance can mitigated using additional regularization techniques. Nevertheless, able get ~83% accuracy within 10 epochs 0.8 million parameters strong result.","code":"conv_mixer_model = get_conv_mixer_256_8() history, conv_mixer_model = run_experiment(conv_mixer_model)"},{"path":"https://keras.posit.co/articles/examples/convmixer.html","id":"visualizing-the-internals-of-convmixer","dir":"Articles > Examples","previous_headings":"","what":"Visualizing the internals of ConvMixer","title":"Image classification with ConvMixer","text":"can visualize patch embeddings learned convolution filters. Recall patch embedding intermediate feature map number channels (256 case). make visualization utility easier implement. Even though train network convergence, can notice different patches show different patterns. share similarity others different. visualizations salient larger image sizes. Similarly, can visualize raw convolution kernels. can help us understand patterns given kernel receptive. see different filters kernel different locality spans, pattern likely evolve training.","code":"# Code reference: https://bit.ly/3awIRbP.   def visualization_plot(weights, idx=1):     # First, apply min-max normalization to the     # given weights to avoid isotrophic scaling.     p_min, p_max = weights.min(), weights.max()     weights = (weights - p_min) / (p_max - p_min)      # Visualize all the filters.     num_filters = 256     plt.figure(figsize=(8, 8))      for i in range(num_filters):         current_weight = weights[:, :, :, i]         if current_weight.shape[-1] == 1:             current_weight = current_weight.squeeze()         ax = plt.subplot(16, 16, idx)         ax.set_xticks([])         ax.set_yticks([])         plt.imshow(current_weight)         idx += 1   # We first visualize the learned patch embeddings. patch_embeddings = conv_mixer_model.layers[2].get_weights()[0] visualization_plot(patch_embeddings) # First, print the indices of the convolution layers that are not # pointwise convolutions. for i, layer in enumerate(conv_mixer_model.layers):     if isinstance(layer, layers.DepthwiseConv2D):         if layer.get_config()[\"kernel_size\"] == (5, 5):             print(i, layer)  idx = 26  # Taking a kernel from the middle of the network.  kernel = conv_mixer_model.layers[idx].get_weights()[0] kernel = np.expand_dims(kernel.squeeze(), axis=2) visualization_plot(kernel)"},{"path":"https://keras.posit.co/articles/examples/convmixer.html","id":"final-notes","dir":"Articles > Examples","previous_headings":"","what":"Final notes","title":"Image classification with ConvMixer","text":"’s recent trend fusing convolutions data-agnostic operations like self-attention. Following works along line research: ConViT (d’Ascoli et al.) CCT (Hassani et al.) CoAtNet (Dai et al.)","code":""},{"path":"https://keras.posit.co/articles/examples/cutmix.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"CutMix data augmentation for image classification","text":"CutMix data augmentation technique addresses issue information loss inefficiency present regional dropout strategies. Instead removing pixels filling black grey pixels Gaussian noise, replace removed regions patch another image, ground truth labels mixed proportionally number pixels combined images. CutMix proposed CutMix: Regularization Strategy Train Strong Classifiers Localizable Features (Yun et al., 2019) ’s implemented via following formulas:  M binary mask indicates cutout fill-regions two randomly drawn images λ ([0, 1]) drawn Beta(α, α) distribution coordinates bounding boxes :  indicates cutout fill-regions case images. bounding box sampling represented :  rx, ry randomly drawn uniform distribution upper bound.","code":""},{"path":"https://keras.posit.co/articles/examples/cutmix.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"CutMix data augmentation for image classification","text":"","code":"import numpy as np import pandas as pd import keras as keras import matplotlib.pyplot as plt  from keras import layers  # TF imports related to tf.data preprocessing from tensorflow import clip_by_value from tensorflow import data as tf_data from tensorflow import image as tf_image from tensorflow.random import gamma as tf_random_gamma  keras.utils.set_random_seed(42)"},{"path":"https://keras.posit.co/articles/examples/cutmix.html","id":"load-the-cifar-10-dataset","dir":"Articles > Examples","previous_headings":"","what":"Load the CIFAR-10 dataset","title":"CutMix data augmentation for image classification","text":"example, use CIFAR-10 image classification dataset.","code":"(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data() y_train = keras.utils.to_categorical(y_train, num_classes=10) y_test = keras.utils.to_categorical(y_test, num_classes=10)  print(x_train.shape) print(y_train.shape) print(x_test.shape) print(y_test.shape)  class_names = [     \"Airplane\",     \"Automobile\",     \"Bird\",     \"Cat\",     \"Deer\",     \"Dog\",     \"Frog\",     \"Horse\",     \"Ship\",     \"Truck\", ]"},{"path":"https://keras.posit.co/articles/examples/cutmix.html","id":"define-hyperparameters","dir":"Articles > Examples","previous_headings":"","what":"Define hyperparameters","title":"CutMix data augmentation for image classification","text":"","code":"AUTO = tf_data.AUTOTUNE BATCH_SIZE = 32 IMG_SIZE = 32"},{"path":"https://keras.posit.co/articles/examples/cutmix.html","id":"define-the-image-preprocessing-function","dir":"Articles > Examples","previous_headings":"","what":"Define the image preprocessing function","title":"CutMix data augmentation for image classification","text":"","code":"def preprocess_image(image, label):     image = tf_image.resize(image, (IMG_SIZE, IMG_SIZE))     image = tf_image.convert_image_dtype(image, \"float32\") / 255.0     label = keras.backend.cast(label, dtype=\"float32\")     return image, label"},{"path":"https://keras.posit.co/articles/examples/cutmix.html","id":"convert-the-data-into-tensorflow-dataset-objects","dir":"Articles > Examples","previous_headings":"","what":"Convert the data into TensorFlow Dataset objects","title":"CutMix data augmentation for image classification","text":"","code":"train_ds_one = (     tf_data.Dataset.from_tensor_slices((x_train, y_train))     .shuffle(1024)     .map(preprocess_image, num_parallel_calls=AUTO) ) train_ds_two = (     tf_data.Dataset.from_tensor_slices((x_train, y_train))     .shuffle(1024)     .map(preprocess_image, num_parallel_calls=AUTO) )  train_ds_simple = tf_data.Dataset.from_tensor_slices((x_train, y_train))  test_ds = tf_data.Dataset.from_tensor_slices((x_test, y_test))  train_ds_simple = (     train_ds_simple.map(preprocess_image, num_parallel_calls=AUTO)     .batch(BATCH_SIZE)     .prefetch(AUTO) )  # Combine two shuffled datasets from the same training data. train_ds = tf_data.Dataset.zip((train_ds_one, train_ds_two))  test_ds = (     test_ds.map(preprocess_image, num_parallel_calls=AUTO)     .batch(BATCH_SIZE)     .prefetch(AUTO) )"},{"path":"https://keras.posit.co/articles/examples/cutmix.html","id":"define-the-cutmix-data-augmentation-function","dir":"Articles > Examples","previous_headings":"","what":"Define the CutMix data augmentation function","title":"CutMix data augmentation for image classification","text":"CutMix function takes two image label pairs perform augmentation. samples λ(l) Beta distribution returns bounding box get_box function. crop second image (image2) pad image final padded image location. Note: combining two images create single one.","code":"def sample_beta_distribution(size, concentration_0=0.2, concentration_1=0.2):     gamma_1_sample = tf_random_gamma(shape=[size], alpha=concentration_1)     gamma_2_sample = tf_random_gamma(shape=[size], alpha=concentration_0)     return gamma_1_sample / (gamma_1_sample + gamma_2_sample)   def get_box(lambda_value):     cut_rat = keras.ops.sqrt(1.0 - lambda_value)      cut_w = IMG_SIZE * cut_rat  # rw     cut_w = keras.backend.cast(cut_w, \"int32\")      cut_h = IMG_SIZE * cut_rat  # rh     cut_h = keras.backend.cast(cut_h, \"int32\")      cut_x = keras.random.random.uniform((1,), minval=0, maxval=IMG_SIZE)  # rx     cut_x = keras.backend.cast(cut_x, \"int32\")     cut_y = keras.random.random.uniform((1,), minval=0, maxval=IMG_SIZE)  # ry     cut_y = keras.backend.cast(cut_y, \"int32\")      boundaryx1 = clip_by_value(cut_x[0] - cut_w // 2, 0, IMG_SIZE)     boundaryy1 = clip_by_value(cut_y[0] - cut_h // 2, 0, IMG_SIZE)     bbx2 = clip_by_value(cut_x[0] + cut_w // 2, 0, IMG_SIZE)     bby2 = clip_by_value(cut_y[0] + cut_h // 2, 0, IMG_SIZE)      target_h = bby2 - boundaryy1     if target_h == 0:         target_h += 1      target_w = bbx2 - boundaryx1     if target_w == 0:         target_w += 1      return boundaryx1, boundaryy1, target_h, target_w   def cutmix(train_ds_one, train_ds_two):     (image1, label1), (image2, label2) = train_ds_one, train_ds_two      alpha = [0.25]     beta = [0.25]      # Get a sample from the Beta distribution     lambda_value = sample_beta_distribution(1, alpha, beta)      # Define Lambda     lambda_value = lambda_value[0][0]      # Get the bounding box offsets, heights and widths     boundaryx1, boundaryy1, target_h, target_w = get_box(lambda_value)      # Get a patch from the second image (`image2`)     crop2 = tf_image.crop_to_bounding_box(         image2, boundaryy1, boundaryx1, target_h, target_w     )     # Pad the `image2` patch (`crop2`) with the same offset     image2 = tf_image.pad_to_bounding_box(         crop2, boundaryy1, boundaryx1, IMG_SIZE, IMG_SIZE     )     # Get a patch from the first image (`image1`)     crop1 = tf_image.crop_to_bounding_box(         image1, boundaryy1, boundaryx1, target_h, target_w     )     # Pad the `image1` patch (`crop1`) with the same offset     img1 = tf_image.pad_to_bounding_box(         crop1, boundaryy1, boundaryx1, IMG_SIZE, IMG_SIZE     )      # Modify the first image by subtracting the patch from `image1`     # (before applying the `image2` patch)     image1 = image1 - img1     # Add the modified `image1` and `image2`  together to get the CutMix image     image = image1 + image2      # Adjust Lambda in accordance to the pixel ration     lambda_value = 1 - (target_w * target_h) / (IMG_SIZE * IMG_SIZE)     lambda_value = keras.backend.cast(lambda_value, \"float32\")      # Combine the labels of both images     label = lambda_value * label1 + (1 - lambda_value) * label2     return image, label"},{"path":"https://keras.posit.co/articles/examples/cutmix.html","id":"visualize-the-new-dataset-after-applying-the-cutmix-augmentation","dir":"Articles > Examples","previous_headings":"","what":"Visualize the new dataset after applying the CutMix augmentation","title":"CutMix data augmentation for image classification","text":"","code":"# Create the new dataset using our `cutmix` utility train_ds_cmu = (     train_ds.shuffle(1024)     .map(cutmix, num_parallel_calls=AUTO)     .batch(BATCH_SIZE)     .prefetch(AUTO) )  # Let's preview 9 samples from the dataset image_batch, label_batch = next(iter(train_ds_cmu)) plt.figure(figsize=(10, 10)) for i in range(9):     ax = plt.subplot(3, 3, i + 1)     plt.title(class_names[np.argmax(label_batch[i])])     plt.imshow(image_batch[i])     plt.axis(\"off\")"},{"path":"https://keras.posit.co/articles/examples/cutmix.html","id":"define-a-resnet-20-model","dir":"Articles > Examples","previous_headings":"","what":"Define a ResNet-20 model","title":"CutMix data augmentation for image classification","text":"","code":"def resnet_layer(     inputs,     num_filters=16,     kernel_size=3,     strides=1,     activation=\"relu\",     batch_normalization=True,     conv_first=True, ):     conv = layers.Conv2D(         num_filters,         kernel_size=kernel_size,         strides=strides,         padding=\"same\",         kernel_initializer=\"he_normal\",         kernel_regularizer=keras.regularizers.L2(1e-4),     )     x = inputs     if conv_first:         x = conv(x)         if batch_normalization:             x = layers.BatchNormalization()(x)         if activation is not None:             x = layers.Activation(activation)(x)     else:         if batch_normalization:             x = layers.BatchNormalization()(x)         if activation is not None:             x = layers.Activation(activation)(x)         x = conv(x)     return x   def resnet_v20(input_shape, depth, num_classes=10):     if (depth - 2) % 6 != 0:         raise ValueError(\"depth should be 6n+2 (eg 20, 32, 44 in [a])\")     # Start model definition.     num_filters = 16     num_res_blocks = int((depth - 2) / 6)      inputs = layers.Input(shape=input_shape)     x = resnet_layer(inputs=inputs)     # Instantiate the stack of residual units     for stack in range(3):         for res_block in range(num_res_blocks):             strides = 1             if stack > 0 and res_block == 0:  # first layer but not first stack                 strides = 2  # downsample             y = resnet_layer(inputs=x, num_filters=num_filters, strides=strides)             y = resnet_layer(inputs=y, num_filters=num_filters, activation=None)             if stack > 0 and res_block == 0:  # first layer but not first stack                 # linear projection residual shortcut connection to match                 # changed dims                 x = resnet_layer(                     inputs=x,                     num_filters=num_filters,                     kernel_size=1,                     strides=strides,                     activation=None,                     batch_normalization=False,                 )             x = layers.add([x, y])             x = layers.Activation(\"relu\")(x)         num_filters *= 2      # Add classifier on top.     # v1 does not use BN after last shortcut connection-ReLU     x = layers.AveragePooling2D(pool_size=8)(x)     y = layers.Flatten()(x)     outputs = layers.Dense(         num_classes, activation=\"softmax\", kernel_initializer=\"he_normal\"     )(y)      # Instantiate model.     model = keras.Model(inputs=inputs, outputs=outputs)     return model   def training_model():     return resnet_v20((32, 32, 3), 20)   initial_model = training_model() initial_model.save_weights(\"initial_weights.weights.h5\")"},{"path":"https://keras.posit.co/articles/examples/cutmix.html","id":"train-the-model-with-the-dataset-augmented-by-cutmix","dir":"Articles > Examples","previous_headings":"","what":"Train the model with the dataset augmented by CutMix","title":"CutMix data augmentation for image classification","text":"","code":"model = training_model() model.load_weights(\"initial_weights.weights.h5\")  model.compile(     loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"] ) model.fit(train_ds_cmu, validation_data=test_ds, epochs=15)  test_loss, test_accuracy = model.evaluate(test_ds) print(\"Test accuracy: {:.2f}%\".format(test_accuracy * 100))"},{"path":"https://keras.posit.co/articles/examples/cutmix.html","id":"train-the-model-using-the-original-non-augmented-dataset","dir":"Articles > Examples","previous_headings":"","what":"Train the model using the original non-augmented dataset","title":"CutMix data augmentation for image classification","text":"","code":"model = training_model() model.load_weights(\"initial_weights.weights.h5\") model.compile(     loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"] ) model.fit(train_ds_simple, validation_data=test_ds, epochs=15)  test_loss, test_accuracy = model.evaluate(test_ds) print(\"Test accuracy: {:.2f}%\".format(test_accuracy * 100))"},{"path":"https://keras.posit.co/articles/examples/cutmix.html","id":"notes","dir":"Articles > Examples","previous_headings":"","what":"Notes","title":"CutMix data augmentation for image classification","text":"example, trained model 15 epochs. experiment, model CutMix achieves better accuracy CIFAR-10 dataset (77.34% experiment) compared model doesn’t use augmentation (66.90%). may notice takes less time train model CutMix augmentation. can experiment CutMix technique following original paper.","code":""},{"path":"https://keras.posit.co/articles/examples/cyclegan.html","id":"cyclegan","dir":"Articles > Examples","previous_headings":"","what":"CycleGAN","title":"CycleGAN","text":"CycleGAN model aims solve image--image translation problem. goal image--image translation problem learn mapping input image output image using training set aligned image pairs. However, obtaining paired examples isn’t always feasible. CycleGAN tries learn mapping without requiring paired input-output images, using cycle-consistent adversarial networks. Paper Original implementation","code":""},{"path":"https://keras.posit.co/articles/examples/cyclegan.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"CycleGAN","text":"","code":"import numpy as np import matplotlib.pyplot as plt import keras as keras from keras import layers from keras import ops  import tensorflow as tf import tensorflow_datasets as tfds  tfds.disable_progress_bar() autotune = tf.data.AUTOTUNE"},{"path":"https://keras.posit.co/articles/examples/cyclegan.html","id":"prepare-the-dataset","dir":"Articles > Examples","previous_headings":"","what":"Prepare the dataset","title":"CycleGAN","text":"example, using horse zebra dataset.","code":"# Load the horse-zebra dataset using tensorflow-datasets. dataset, _ = tfds.load(     \"cycle_gan/horse2zebra\", with_info=True, as_supervised=True ) train_horses, train_zebras = dataset[\"trainA\"], dataset[\"trainB\"] test_horses, test_zebras = dataset[\"testA\"], dataset[\"testB\"]  # Define the standard image size. orig_img_size = (286, 286) # Size of the random crops to be used during training. input_img_size = (256, 256, 3) # Weights initializer for the layers. kernel_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02) # Gamma initializer for instance normalization. gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)  buffer_size = 256 batch_size = 1   def normalize_img(img):     img = ops.cast(img, dtype=\"float32\")     # Map values in the range [-1, 1]     return (img / 127.5) - 1.0   def preprocess_train_image(img, label):     # Random flip     img = tf.image.random_flip_left_right(img)     # Resize to the original size first     img = tf.image.resize(img, [*orig_img_size])     # Random crop to 256X256     img = tf.image.random_crop(img, size=[*input_img_size])     # Normalize the pixel values in the range [-1, 1]     img = normalize_img(img)     return img   def preprocess_test_image(img, label):     # Only resizing and normalization for the test images.     img = tf.image.resize(img, [input_img_size[0], input_img_size[1]])     img = normalize_img(img)     return img"},{"path":"https://keras.posit.co/articles/examples/cyclegan.html","id":"create-dataset-objects","dir":"Articles > Examples","previous_headings":"","what":"Create Dataset objects","title":"CycleGAN","text":"","code":"# Apply the preprocessing operations to the training data train_horses = (     train_horses.map(preprocess_train_image, num_parallel_calls=autotune)     .cache()     .shuffle(buffer_size)     .batch(batch_size) ) train_zebras = (     train_zebras.map(preprocess_train_image, num_parallel_calls=autotune)     .cache()     .shuffle(buffer_size)     .batch(batch_size) )  # Apply the preprocessing operations to the test data test_horses = (     test_horses.map(preprocess_test_image, num_parallel_calls=autotune)     .cache()     .shuffle(buffer_size)     .batch(batch_size) ) test_zebras = (     test_zebras.map(preprocess_test_image, num_parallel_calls=autotune)     .cache()     .shuffle(buffer_size)     .batch(batch_size) )"},{"path":"https://keras.posit.co/articles/examples/cyclegan.html","id":"visualize-some-samples","dir":"Articles > Examples","previous_headings":"","what":"Visualize some samples","title":"CycleGAN","text":"","code":"_, ax = plt.subplots(4, 2, figsize=(10, 15)) for i, samples in enumerate(zip(train_horses.take(4), train_zebras.take(4))):     horse = (((samples[0][0] * 127.5) + 127.5).numpy()).astype(np.uint8)     zebra = (((samples[1][0] * 127.5) + 127.5).numpy()).astype(np.uint8)     ax[i, 0].imshow(horse)     ax[i, 1].imshow(zebra) plt.show()"},{"path":"https://keras.posit.co/articles/examples/cyclegan.html","id":"building-blocks-used-in-the-cyclegan-generators-and-discriminators","dir":"Articles > Examples","previous_headings":"","what":"Building blocks used in the CycleGAN generators and discriminators","title":"CycleGAN","text":"","code":"class ReflectionPadding2D(layers.Layer):     \"\"\"Implements Reflection Padding as a layer.      Args:         padding(tuple): Amount of padding for the         spatial dimensions.      Returns:         A padded tensor with the same type as the input tensor.     \"\"\"      def __init__(self, padding=(1, 1), **kwargs):         self.padding = tuple(padding)         super().__init__(**kwargs)      def call(self, input_tensor, mask=None):         padding_width, padding_height = self.padding         padding_tensor = [             [0, 0],             [padding_height, padding_height],             [padding_width, padding_width],             [0, 0],         ]         return ops.pad(             input_tensor, ops.convert_to_tensor(padding_tensor), mode=\"reflect\"         )   def residual_block(     x,     activation,     kernel_initializer=kernel_init,     kernel_size=(3, 3),     strides=(1, 1),     padding=\"valid\",     gamma_initializer=gamma_init,     use_bias=False, ):     dim = x.shape[-1]     input_tensor = x      x = ReflectionPadding2D()(input_tensor)     x = layers.Conv2D(         dim,         kernel_size,         strides=strides,         kernel_initializer=kernel_initializer,         padding=padding,         use_bias=use_bias,     )(x)     x = layers.GroupNormalization(         groups=x.shape[-1], gamma_initializer=gamma_initializer     )(x)     x = activation(x)      x = ReflectionPadding2D()(x)     x = layers.Conv2D(         dim,         kernel_size,         strides=strides,         kernel_initializer=kernel_initializer,         padding=padding,         use_bias=use_bias,     )(x)     x = layers.GroupNormalization(         groups=x.shape[-1], gamma_initializer=gamma_initializer     )(x)     x = layers.add([input_tensor, x])     return x   def downsample(     x,     filters,     activation,     kernel_initializer=kernel_init,     kernel_size=(3, 3),     strides=(2, 2),     padding=\"same\",     gamma_initializer=gamma_init,     use_bias=False, ):     x = layers.Conv2D(         filters,         kernel_size,         strides=strides,         kernel_initializer=kernel_initializer,         padding=padding,         use_bias=use_bias,     )(x)     x = layers.GroupNormalization(         groups=x.shape[-1], gamma_initializer=gamma_initializer     )(x)     if activation:         x = activation(x)     return x   def upsample(     x,     filters,     activation,     kernel_size=(3, 3),     strides=(2, 2),     padding=\"same\",     kernel_initializer=kernel_init,     gamma_initializer=gamma_init,     use_bias=False, ):     x = layers.Conv2DTranspose(         filters,         kernel_size,         strides=strides,         padding=padding,         kernel_initializer=kernel_initializer,         use_bias=use_bias,     )(x)     x = layers.GroupNormalization(         groups=x.shape[-1], gamma_initializer=gamma_initializer     )(x)     if activation:         x = activation(x)     return x"},{"path":"https://keras.posit.co/articles/examples/cyclegan.html","id":"build-the-generators","dir":"Articles > Examples","previous_headings":"","what":"Build the generators","title":"CycleGAN","text":"generator consists downsampling blocks: nine residual blocks upsampling blocks. structure generator following:","code":"c7s1-64 ==> Conv block with `relu` activation, filter size of 7 d128 ====|          |-> 2 downsampling blocks d256 ====| R256 ====| R256     | R256     | R256     | R256     |-> 9 residual blocks R256     | R256     | R256     | R256 ====| u128 ====|          |-> 2 upsampling blocks u64  ====| c7s1-3 => Last conv block with `tanh` activation, filter size of 7. def get_resnet_generator(     filters=64,     num_downsampling_blocks=2,     num_residual_blocks=9,     num_upsample_blocks=2,     gamma_initializer=gamma_init,     name=None, ):     img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")     x = ReflectionPadding2D(padding=(3, 3))(img_input)     x = layers.Conv2D(         filters, (7, 7), kernel_initializer=kernel_init, use_bias=False     )(x)     x = layers.GroupNormalization(         groups=x.shape[-1], gamma_initializer=gamma_initializer     )(x)     x = layers.Activation(\"relu\")(x)      # Downsampling     for _ in range(num_downsampling_blocks):         filters *= 2         x = downsample(x, filters=filters, activation=layers.Activation(\"relu\"))      # Residual blocks     for _ in range(num_residual_blocks):         x = residual_block(x, activation=layers.Activation(\"relu\"))      # Upsampling     for _ in range(num_upsample_blocks):         filters //= 2         x = upsample(x, filters, activation=layers.Activation(\"relu\"))      # Final block     x = ReflectionPadding2D(padding=(3, 3))(x)     x = layers.Conv2D(3, (7, 7), padding=\"valid\")(x)     x = layers.Activation(\"tanh\")(x)      model = keras.models.Model(img_input, x, name=name)     return model"},{"path":"https://keras.posit.co/articles/examples/cyclegan.html","id":"build-the-discriminators","dir":"Articles > Examples","previous_headings":"","what":"Build the discriminators","title":"CycleGAN","text":"discriminators implement following architecture: C64->C128->C256->C512","code":"def get_discriminator(     filters=64, kernel_initializer=kernel_init, num_downsampling=3, name=None ):     img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")     x = layers.Conv2D(         filters,         (4, 4),         strides=(2, 2),         padding=\"same\",         kernel_initializer=kernel_initializer,     )(img_input)     x = layers.LeakyReLU(0.2)(x)      num_filters = filters     for num_downsample_block in range(3):         num_filters *= 2         if num_downsample_block < 2:             x = downsample(                 x,                 filters=num_filters,                 activation=layers.LeakyReLU(0.2),                 kernel_size=(4, 4),                 strides=(2, 2),             )         else:             x = downsample(                 x,                 filters=num_filters,                 activation=layers.LeakyReLU(0.2),                 kernel_size=(4, 4),                 strides=(1, 1),             )      x = layers.Conv2D(         1,         (4, 4),         strides=(1, 1),         padding=\"same\",         kernel_initializer=kernel_initializer,     )(x)      model = keras.models.Model(inputs=img_input, outputs=x, name=name)     return model   # Get the generators gen_G = get_resnet_generator(name=\"generator_G\") gen_F = get_resnet_generator(name=\"generator_F\")  # Get the discriminators disc_X = get_discriminator(name=\"discriminator_X\") disc_Y = get_discriminator(name=\"discriminator_Y\")"},{"path":"https://keras.posit.co/articles/examples/cyclegan.html","id":"build-the-cyclegan-model","dir":"Articles > Examples","previous_headings":"","what":"Build the CycleGAN model","title":"CycleGAN","text":"override train_step() method Model class training via fit().","code":"class CycleGan(keras.Model):     def __init__(         self,         generator_G,         generator_F,         discriminator_X,         discriminator_Y,         lambda_cycle=10.0,         lambda_identity=0.5,     ):         super().__init__()         self.gen_G = generator_G         self.gen_F = generator_F         self.disc_X = discriminator_X         self.disc_Y = discriminator_Y         self.lambda_cycle = lambda_cycle         self.lambda_identity = lambda_identity      def call(self, inputs):         return (             self.disc_X(inputs),             self.disc_Y(inputs),             self.gen_G(inputs),             self.gen_F(inputs),         )      def compile(         self,         gen_G_optimizer,         gen_F_optimizer,         disc_X_optimizer,         disc_Y_optimizer,         gen_loss_fn,         disc_loss_fn,     ):         super().compile()         self.gen_G_optimizer = gen_G_optimizer         self.gen_F_optimizer = gen_F_optimizer         self.disc_X_optimizer = disc_X_optimizer         self.disc_Y_optimizer = disc_Y_optimizer         self.generator_loss_fn = gen_loss_fn         self.discriminator_loss_fn = disc_loss_fn         self.cycle_loss_fn = keras.losses.MeanAbsoluteError()         self.identity_loss_fn = keras.losses.MeanAbsoluteError()      def train_step(self, batch_data):         # x is Horse and y is zebra         real_x, real_y = batch_data          # For CycleGAN, we need to calculate different         # kinds of losses for the generators and discriminators.         # We will perform the following steps here:         #         # 1. Pass real images through the generators and get the generated images         # 2. Pass the generated images back to the generators to check if we         #    can predict the original image from the generated image.         # 3. Do an identity mapping of the real images using the generators.         # 4. Pass the generated images in 1) to the corresponding discriminators.         # 5. Calculate the generators total loss (adversarial + cycle + identity)         # 6. Calculate the discriminators loss         # 7. Update the weights of the generators         # 8. Update the weights of the discriminators         # 9. Return the losses in a dictionary          with tf.GradientTape(persistent=True) as tape:             # Horse to fake zebra             fake_y = self.gen_G(real_x, training=True)             # Zebra to fake horse -> y2x             fake_x = self.gen_F(real_y, training=True)              # Cycle (Horse to fake zebra to fake horse): x -> y -> x             cycled_x = self.gen_F(fake_y, training=True)             # Cycle (Zebra to fake horse to fake zebra) y -> x -> y             cycled_y = self.gen_G(fake_x, training=True)              # Identity mapping             same_x = self.gen_F(real_x, training=True)             same_y = self.gen_G(real_y, training=True)              # Discriminator output             disc_real_x = self.disc_X(real_x, training=True)             disc_fake_x = self.disc_X(fake_x, training=True)              disc_real_y = self.disc_Y(real_y, training=True)             disc_fake_y = self.disc_Y(fake_y, training=True)              # Generator adversarial loss             gen_G_loss = self.generator_loss_fn(disc_fake_y)             gen_F_loss = self.generator_loss_fn(disc_fake_x)              # Generator cycle loss             cycle_loss_G = (                 self.cycle_loss_fn(real_y, cycled_y) * self.lambda_cycle             )             cycle_loss_F = (                 self.cycle_loss_fn(real_x, cycled_x) * self.lambda_cycle             )              # Generator identity loss             id_loss_G = (                 self.identity_loss_fn(real_y, same_y)                 * self.lambda_cycle                 * self.lambda_identity             )             id_loss_F = (                 self.identity_loss_fn(real_x, same_x)                 * self.lambda_cycle                 * self.lambda_identity             )              # Total generator loss             total_loss_G = gen_G_loss + cycle_loss_G + id_loss_G             total_loss_F = gen_F_loss + cycle_loss_F + id_loss_F              # Discriminator loss             disc_X_loss = self.discriminator_loss_fn(disc_real_x, disc_fake_x)             disc_Y_loss = self.discriminator_loss_fn(disc_real_y, disc_fake_y)          # Get the gradients for the generators         grads_G = tape.gradient(total_loss_G, self.gen_G.trainable_variables)         grads_F = tape.gradient(total_loss_F, self.gen_F.trainable_variables)          # Get the gradients for the discriminators         disc_X_grads = tape.gradient(             disc_X_loss, self.disc_X.trainable_variables         )         disc_Y_grads = tape.gradient(             disc_Y_loss, self.disc_Y.trainable_variables         )          # Update the weights of the generators         self.gen_G_optimizer.apply_gradients(             zip(grads_G, self.gen_G.trainable_variables)         )         self.gen_F_optimizer.apply_gradients(             zip(grads_F, self.gen_F.trainable_variables)         )          # Update the weights of the discriminators         self.disc_X_optimizer.apply_gradients(             zip(disc_X_grads, self.disc_X.trainable_variables)         )         self.disc_Y_optimizer.apply_gradients(             zip(disc_Y_grads, self.disc_Y.trainable_variables)         )          return {             \"G_loss\": total_loss_G,             \"F_loss\": total_loss_F,             \"D_X_loss\": disc_X_loss,             \"D_Y_loss\": disc_Y_loss,         }"},{"path":"https://keras.posit.co/articles/examples/cyclegan.html","id":"create-a-callback-that-periodically-saves-generated-images","dir":"Articles > Examples","previous_headings":"","what":"Create a callback that periodically saves generated images","title":"CycleGAN","text":"","code":"class GANMonitor(keras.callbacks.Callback):     \"\"\"A callback to generate and save images after each epoch\"\"\"      def __init__(self, num_img=4):         self.num_img = num_img      def on_epoch_end(self, epoch, logs=None):         _, ax = plt.subplots(4, 2, figsize=(12, 12))         for i, img in enumerate(test_horses.take(self.num_img)):             prediction = self.model.gen_G(img)[0].numpy()             prediction = (prediction * 127.5 + 127.5).astype(np.uint8)             img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)              ax[i, 0].imshow(img)             ax[i, 1].imshow(prediction)             ax[i, 0].set_title(\"Input image\")             ax[i, 1].set_title(\"Translated image\")             ax[i, 0].axis(\"off\")             ax[i, 1].axis(\"off\")              prediction = keras.utils.array_to_img(prediction)             prediction.save(                 \"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch + 1)             )         plt.show()         plt.close()"},{"path":"https://keras.posit.co/articles/examples/cyclegan.html","id":"train-the-end-to-end-model","dir":"Articles > Examples","previous_headings":"","what":"Train the end-to-end model","title":"CycleGAN","text":"Test performance model. can use trained model hosted Hugging Face Hub try demo Hugging Face Spaces. curl -LO https://github.com/freedomtan/cyclegan-keras/archive/refs/tags/2.0.zip unzip -qq 2.0.zip","code":"# Loss function for evaluating adversarial loss adv_loss_fn = keras.losses.MeanSquaredError()  # Define the loss function for the generators   def generator_loss_fn(fake):     fake_loss = adv_loss_fn(ops.ones_like(fake), fake)     return fake_loss   # Define the loss function for the discriminators def discriminator_loss_fn(real, fake):     real_loss = adv_loss_fn(ops.ones_like(real), real)     fake_loss = adv_loss_fn(ops.zeros_like(fake), fake)     return (real_loss + fake_loss) * 0.5   # Create cycle gan model cycle_gan_model = CycleGan(     generator_G=gen_G,     generator_F=gen_F,     discriminator_X=disc_X,     discriminator_Y=disc_Y, )  # Compile the model cycle_gan_model.compile(     gen_G_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),     gen_F_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),     disc_X_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),     disc_Y_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),     gen_loss_fn=generator_loss_fn,     disc_loss_fn=discriminator_loss_fn, ) # Callbacks plotter = GANMonitor() checkpoint_filepath = (     \"./model_checkpoints/cyclegan_checkpoints.{epoch:03d}.weights.h5\" ) model_checkpoint_callback = keras.callbacks.ModelCheckpoint(     filepath=checkpoint_filepath, save_weights_only=True )  # Here we will train the model for just one epoch as each epoch takes around # 7 minutes on a single P100 backed machine. cycle_gan_model.fit(     tf.data.Dataset.zip((train_horses, train_zebras)),     epochs=1,     callbacks=[plotter, model_checkpoint_callback], ) # This model was trained for 90 epochs. We will be loading those weights # here. Once the weights are loaded, we will take a few samples from the test # data and check the model's performance. # Load the checkpoints weight_file = \"./cyclegan-keras-2.0/model_checkpoints/cyclegan_checkpoints.090.weights.h5\" cycle_gan_model.load_weights(weight_file) print(\"Weights loaded successfully\")  _, ax = plt.subplots(4, 2, figsize=(10, 15)) for i, img in enumerate(test_horses.take(4)):     prediction = cycle_gan_model.gen_G(img, training=False)[0].numpy()     prediction = (prediction * 127.5 + 127.5).astype(np.uint8)     img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)      ax[i, 0].imshow(img)     ax[i, 1].imshow(prediction)     ax[i, 0].set_title(\"Input image\")     ax[i, 0].set_title(\"Input image\")     ax[i, 1].set_title(\"Translated image\")     ax[i, 0].axis(\"off\")     ax[i, 1].axis(\"off\")      prediction = keras.utils.array_to_img(prediction)     prediction.save(\"predicted_img_{i}.png\".format(i=i)) plt.tight_layout() plt.show()"},{"path":"https://keras.posit.co/articles/examples/dcgan_overriding_train_step.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"DCGAN to generate face images","text":"","code":"import tensorflow as tf import keras as keras from keras import layers import matplotlib.pyplot as plt import os import gdown from zipfile import ZipFile"},{"path":"https://keras.posit.co/articles/examples/dcgan_overriding_train_step.html","id":"prepare-celeba-data","dir":"Articles > Examples","previous_headings":"","what":"Prepare CelebA data","title":"DCGAN to generate face images","text":"’ll use face images CelebA dataset, resized 64x64. Create dataset folder, rescale images [0-1] range: Let’s display sample image:","code":"os.makedirs(\"celeba_gan\")  url = \"https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684\" output = \"celeba_gan/data.zip\" gdown.download(url, output, quiet=True)  with ZipFile(\"celeba_gan/data.zip\", \"r\") as zipobj:     zipobj.extractall(\"celeba_gan\") dataset = keras.utils.image_dataset_from_directory(     \"celeba_gan\", label_mode=None, image_size=(64, 64), batch_size=32 ) dataset = dataset.map(lambda x: x / 255.0) for x in dataset:     plt.axis(\"off\")     plt.imshow((x.numpy() * 255).astype(\"int32\")[0])     break"},{"path":"https://keras.posit.co/articles/examples/dcgan_overriding_train_step.html","id":"create-the-discriminator","dir":"Articles > Examples","previous_headings":"","what":"Create the discriminator","title":"DCGAN to generate face images","text":"maps 64x64 image binary classification score.","code":"discriminator = keras.Sequential(     [         keras.Input(shape=(64, 64, 3)),         layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),         layers.LeakyReLU(alpha=0.2),         layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),         layers.LeakyReLU(alpha=0.2),         layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),         layers.LeakyReLU(alpha=0.2),         layers.Flatten(),         layers.Dropout(0.2),         layers.Dense(1, activation=\"sigmoid\"),     ],     name=\"discriminator\", ) discriminator.summary()"},{"path":"https://keras.posit.co/articles/examples/dcgan_overriding_train_step.html","id":"create-the-generator","dir":"Articles > Examples","previous_headings":"","what":"Create the generator","title":"DCGAN to generate face images","text":"mirrors discriminator, replacing Conv2D layers Conv2DTranspose layers.","code":"latent_dim = 128  generator = keras.Sequential(     [         keras.Input(shape=(latent_dim,)),         layers.Dense(8 * 8 * 128),         layers.Reshape((8, 8, 128)),         layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),         layers.LeakyReLU(alpha=0.2),         layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),         layers.LeakyReLU(alpha=0.2),         layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),         layers.LeakyReLU(alpha=0.2),         layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),     ],     name=\"generator\", ) generator.summary()"},{"path":"https://keras.posit.co/articles/examples/dcgan_overriding_train_step.html","id":"override-train_step","dir":"Articles > Examples","previous_headings":"","what":"Override train_step","title":"DCGAN to generate face images","text":"","code":"class GAN(keras.Model):     def __init__(self, discriminator, generator, latent_dim):         super().__init__()         self.discriminator = discriminator         self.generator = generator         self.latent_dim = latent_dim      def compile(self, d_optimizer, g_optimizer, loss_fn):         super().compile()         self.d_optimizer = d_optimizer         self.g_optimizer = g_optimizer         self.loss_fn = loss_fn         self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")         self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")      @property     def metrics(self):         return [self.d_loss_metric, self.g_loss_metric]      def train_step(self, real_images):         # Sample random points in the latent space         batch_size = tf.shape(real_images)[0]         random_latent_vectors = tf.random.normal(             shape=(batch_size, self.latent_dim)         )          # Decode them to fake images         generated_images = self.generator(random_latent_vectors)          # Combine them with real images         combined_images = tf.concat([generated_images, real_images], axis=0)          # Assemble labels discriminating real from fake images         labels = tf.concat(             [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0         )         # Add random noise to the labels - important trick!         labels += 0.05 * tf.random.uniform(tf.shape(labels))          # Train the discriminator         with tf.GradientTape() as tape:             predictions = self.discriminator(combined_images)             d_loss = self.loss_fn(labels, predictions)         grads = tape.gradient(d_loss, self.discriminator.trainable_weights)         self.d_optimizer.apply_gradients(             zip(grads, self.discriminator.trainable_weights)         )          # Sample random points in the latent space         random_latent_vectors = tf.random.normal(             shape=(batch_size, self.latent_dim)         )          # Assemble labels that say \"all real images\"         misleading_labels = tf.zeros((batch_size, 1))          # Train the generator (note that we should *not* update the weights         # of the discriminator)!         with tf.GradientTape() as tape:             predictions = self.discriminator(                 self.generator(random_latent_vectors)             )             g_loss = self.loss_fn(misleading_labels, predictions)         grads = tape.gradient(g_loss, self.generator.trainable_weights)         self.g_optimizer.apply_gradients(             zip(grads, self.generator.trainable_weights)         )          # Update metrics         self.d_loss_metric.update_state(d_loss)         self.g_loss_metric.update_state(g_loss)         return {             \"d_loss\": self.d_loss_metric.result(),             \"g_loss\": self.g_loss_metric.result(),         }"},{"path":"https://keras.posit.co/articles/examples/dcgan_overriding_train_step.html","id":"create-a-callback-that-periodically-saves-generated-images","dir":"Articles > Examples","previous_headings":"","what":"Create a callback that periodically saves generated images","title":"DCGAN to generate face images","text":"","code":"class GANMonitor(keras.callbacks.Callback):     def __init__(self, num_img=3, latent_dim=128):         self.num_img = num_img         self.latent_dim = latent_dim      def on_epoch_end(self, epoch, logs=None):         random_latent_vectors = tf.random.normal(             shape=(self.num_img, self.latent_dim)         )         generated_images = self.model.generator(random_latent_vectors)         generated_images *= 255         generated_images.numpy()         for i in range(self.num_img):             img = keras.utils.array_to_img(generated_images[i])             img.save(\"generated_img_%03d_%d.png\" % (epoch, i))"},{"path":"https://keras.posit.co/articles/examples/dcgan_overriding_train_step.html","id":"train-the-end-to-end-model","dir":"Articles > Examples","previous_headings":"","what":"Train the end-to-end model","title":"DCGAN to generate face images","text":"last generated images around epoch 30 (results keep improving ):","code":"epochs = 1  # In practice, use ~100 epochs  gan = GAN(     discriminator=discriminator, generator=generator, latent_dim=latent_dim ) gan.compile(     d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),     g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),     loss_fn=keras.losses.BinaryCrossentropy(), )  gan.fit(     dataset,     epochs=epochs,     callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)], )"},{"path":[]},{"path":"https://keras.posit.co/articles/examples/ddim.html","id":"what-are-diffusion-models","dir":"Articles > Examples","previous_headings":"Introduction","what":"What are diffusion models?","title":"Denoising Diffusion Implicit Models","text":"Recently, denoising diffusion models, including score-based generative models, gained popularity powerful class generative models, can rival even generative adversarial networks (GANs) image synthesis quality. tend generate diverse samples, stable train easy scale. Recent large diffusion models, DALL-E 2 Imagen, shown incredible text--image generation capability. One drawbacks however, slower sample , require multiple forward passes generating image. Diffusion refers process turning structured signal (image) noise step--step. simulating diffusion, can generate noisy images training images, can train neural network try denoise . Using trained network can simulate opposite diffusion, reverse diffusion, process image emerging noise. One-sentence summary: diffusion models trained denoise noisy images, can generate images iteratively denoising pure noise.","code":""},{"path":"https://keras.posit.co/articles/examples/ddim.html","id":"goal-of-this-example","dir":"Articles > Examples","previous_headings":"Introduction","what":"Goal of this example","title":"Denoising Diffusion Implicit Models","text":"code example intends minimal feature-complete (generation quality metric) implementation diffusion models, modest compute requirements reasonable performance. implementation choices hyperparameter tuning done goals mind. Since currently literature diffusion models mathematically quite complex multiple theoretical frameworks (score matching, differential equations, Markov chains) sometimes even conflicting notations (see Appendix C.2), can daunting trying understand . view models example learn separate noisy image image Gaussian noise components. example made effort break long mathematical expressions digestible pieces gave variables explanatory names. also included numerous links relevant literature help interested readers dive deeper topic, hope code example become good starting point practitioners learning diffusion models. following sections, implement continuous time version Denoising Diffusion Implicit Models (DDIMs) deterministic sampling.","code":""},{"path":"https://keras.posit.co/articles/examples/ddim.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Denoising Diffusion Implicit Models","text":"","code":"import math import matplotlib.pyplot as plt import tensorflow as tf import tensorflow_datasets as tfds  import keras as keras from keras import layers from keras import ops"},{"path":"https://keras.posit.co/articles/examples/ddim.html","id":"hyperparameters","dir":"Articles > Examples","previous_headings":"","what":"Hyperparameters","title":"Denoising Diffusion Implicit Models","text":"","code":"# data dataset_name = \"oxford_flowers102\" dataset_repetitions = 5 num_epochs = 1  # train for at least 50 epochs for good results image_size = 64 # KID = Kernel Inception Distance, see related section kid_image_size = 75 kid_diffusion_steps = 5 plot_diffusion_steps = 20  # sampling min_signal_rate = 0.02 max_signal_rate = 0.95  # architecture embedding_dims = 32 embedding_max_frequency = 1000.0 widths = [32, 64, 96, 128] block_depth = 2  # optimization batch_size = 64 ema = 0.999 learning_rate = 1e-3 weight_decay = 1e-4"},{"path":"https://keras.posit.co/articles/examples/ddim.html","id":"data-pipeline","dir":"Articles > Examples","previous_headings":"","what":"Data pipeline","title":"Denoising Diffusion Implicit Models","text":"use Oxford Flowers 102 dataset generating images flowers, diverse natural dataset containing around 8,000 images. Unfortunately official splits imbalanced, images contained test split. create new splits (80% train, 20% validation) using Tensorflow Datasets slicing API. apply center crops preprocessing, repeat dataset multiple times (reason given next section).","code":"def preprocess_image(data):     # center crop image     height = ops.shape(data[\"image\"])[0]     width = ops.shape(data[\"image\"])[1]     crop_size = ops.minimum(height, width)     image = tf.image.crop_to_bounding_box(         data[\"image\"],         (height - crop_size) // 2,         (width - crop_size) // 2,         crop_size,         crop_size,     )      # resize and clip     # for image downsampling it is important to turn on antialiasing     image = tf.image.resize(         image, size=[image_size, image_size], antialias=True     )     return ops.clip(image / 255.0, 0.0, 1.0)   def prepare_dataset(split):     # the validation dataset is shuffled as well, because data order matters     # for the KID estimation     return (         tfds.load(dataset_name, split=split, shuffle_files=True)         .map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)         .cache()         .repeat(dataset_repetitions)         .shuffle(10 * batch_size)         .batch(batch_size, drop_remainder=True)         .prefetch(buffer_size=tf.data.AUTOTUNE)     )   # load dataset train_dataset = prepare_dataset(\"train[:80%]+validation[:80%]+test[:80%]\") val_dataset = prepare_dataset(\"train[80%:]+validation[80%:]+test[80%:]\")"},{"path":"https://keras.posit.co/articles/examples/ddim.html","id":"kernel-inception-distance","dir":"Articles > Examples","previous_headings":"","what":"Kernel inception distance","title":"Denoising Diffusion Implicit Models","text":"Kernel Inception Distance (KID) image quality metric proposed replacement popular Frechet Inception Distance (FID). prefer KID FID simpler implement, can estimated per-batch, computationally lighter. details . example, images evaluated minimal possible resolution Inception network (75x75 instead 299x299), metric measured validation set computational efficiency. also limit number sampling steps evaluation 5 reason. Since dataset relatively small, go train validation splits multiple times per epoch, KID estimation noisy compute-intensive, want evaluate many iterations, many iterations.","code":"@keras.saving.register_keras_serializable() class KID(keras.metrics.Metric):     def __init__(self, name, **kwargs):         super().__init__(name=name, **kwargs)          # KID is estimated per batch and is averaged across batches         self.kid_tracker = keras.metrics.Mean(name=\"kid_tracker\")          # a pretrained InceptionV3 is used without its classification layer         # transform the pixel values to the 0-255 range, then use the same         # preprocessing as during pretraining         self.encoder = keras.Sequential(             [                 keras.Input(shape=(image_size, image_size, 3)),                 layers.Rescaling(255.0),                 layers.Resizing(height=kid_image_size, width=kid_image_size),                 layers.Lambda(keras.applications.inception_v3.preprocess_input),                 keras.applications.InceptionV3(                     include_top=False,                     input_shape=(kid_image_size, kid_image_size, 3),                     weights=\"imagenet\",                 ),                 layers.GlobalAveragePooling2D(),             ],             name=\"inception_encoder\",         )      def polynomial_kernel(self, features_1, features_2):         feature_dimensions = ops.cast(ops.shape(features_1)[1], dtype=\"float32\")         return (             features_1 @ ops.transpose(features_2) / feature_dimensions + 1.0         ) ** 3.0      def update_state(self, real_images, generated_images, sample_weight=None):         real_features = self.encoder(real_images, training=False)         generated_features = self.encoder(generated_images, training=False)          # compute polynomial kernels using the two sets of features         kernel_real = self.polynomial_kernel(real_features, real_features)         kernel_generated = self.polynomial_kernel(             generated_features, generated_features         )         kernel_cross = self.polynomial_kernel(real_features, generated_features)          # estimate the squared maximum mean discrepancy using the average kernel values         batch_size = real_features.shape[0]         batch_size_f = ops.cast(batch_size, dtype=\"float32\")         mean_kernel_real = ops.sum(             kernel_real * (1.0 - ops.eye(batch_size))         ) / (batch_size_f * (batch_size_f - 1.0))         mean_kernel_generated = ops.sum(             kernel_generated * (1.0 - ops.eye(batch_size))         ) / (batch_size_f * (batch_size_f - 1.0))         mean_kernel_cross = ops.mean(kernel_cross)         kid = mean_kernel_real + mean_kernel_generated - 2.0 * mean_kernel_cross          # update the average KID estimate         self.kid_tracker.update_state(kid)      def result(self):         return self.kid_tracker.result()      def reset_state(self):         self.kid_tracker.reset_state()"},{"path":"https://keras.posit.co/articles/examples/ddim.html","id":"network-architecture","dir":"Articles > Examples","previous_headings":"","what":"Network architecture","title":"Denoising Diffusion Implicit Models","text":"specify architecture neural network use denoising. build U-Net identical input output dimensions. U-Net popular semantic segmentation architecture, whose main idea progressively downsamples upsamples input image, adds skip connections layers resolution. help gradient flow avoid introducing representation bottleneck, unlike usual autoencoders. Based , one can view diffusion models denoising autoencoders without bottleneck. network takes two inputs, noisy images variances noise components. latter required since denoising signal requires different operations different levels noise. transform noise variances using sinusoidal embeddings, similarly positional encodings used transformers NeRF. helps network highly sensitive noise level, crucial good performance. implement sinusoidal embeddings using Lambda layer. considerations: build network using Keras Functional API, use closures build blocks layers consistent style. Diffusion models embed index timestep diffusion process instead noise variance, score-based models (Table 1) usually use function noise level. prefer latter can change sampling schedule inference time, without retraining network. Diffusion models input embedding convolution block separately. input start network simplicity, experience barely decreases performance, skip residual connections help information propagate network properly. literature common use attention layers lower resolutions better global coherence. omitted simplicity. disable learnable center scale parameters batch normalization layers, since following convolution layers make redundant. initialize last convolution’s kernel zeros good practice, making network predict zeros initialization, mean targets. improve behaviour start training make mean squared error loss start exactly 1. showcases power Functional API. Note built relatively complex U-Net skip connections, residual blocks, multiple inputs, sinusoidal embeddings 80 lines code!","code":"@keras.saving.register_keras_serializable() def sinusoidal_embedding(x):     embedding_min_frequency = 1.0     frequencies = ops.exp(         ops.linspace(             ops.log(embedding_min_frequency),             ops.log(embedding_max_frequency),             embedding_dims // 2,         )     )     angular_speeds = ops.cast(2.0 * math.pi * frequencies, \"float32\")     embeddings = ops.concatenate(         [ops.sin(angular_speeds * x), ops.cos(angular_speeds * x)], axis=3     )     return embeddings   def ResidualBlock(width):     def apply(x):         input_width = x.shape[3]         if input_width == width:             residual = x         else:             residual = layers.Conv2D(width, kernel_size=1)(x)         x = layers.BatchNormalization(center=False, scale=False)(x)         x = layers.Conv2D(             width, kernel_size=3, padding=\"same\", activation=\"swish\"         )(x)         x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)         x = layers.Add()([x, residual])         return x      return apply   def DownBlock(width, block_depth):     def apply(x):         x, skips = x         for _ in range(block_depth):             x = ResidualBlock(width)(x)             skips.append(x)         x = layers.AveragePooling2D(pool_size=2)(x)         return x      return apply   def UpBlock(width, block_depth):     def apply(x):         x, skips = x         x = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(x)         for _ in range(block_depth):             x = layers.Concatenate()([x, skips.pop()])             x = ResidualBlock(width)(x)         return x      return apply   def get_network(image_size, widths, block_depth):     noisy_images = keras.Input(shape=(image_size, image_size, 3))     noise_variances = keras.Input(shape=(1, 1, 1))      e = layers.Lambda(sinusoidal_embedding, output_shape=(1, 1, 32))(         noise_variances     )     e = layers.UpSampling2D(size=image_size, interpolation=\"nearest\")(e)      x = layers.Conv2D(widths[0], kernel_size=1)(noisy_images)     x = layers.Concatenate()([x, e])      skips = []     for width in widths[:-1]:         x = DownBlock(width, block_depth)([x, skips])      for _ in range(block_depth):         x = ResidualBlock(widths[-1])(x)      for width in reversed(widths[:-1]):         x = UpBlock(width, block_depth)([x, skips])      x = layers.Conv2D(3, kernel_size=1, kernel_initializer=\"zeros\")(x)      return keras.Model([noisy_images, noise_variances], x, name=\"residual_unet\")"},{"path":[]},{"path":"https://keras.posit.co/articles/examples/ddim.html","id":"diffusion-schedule","dir":"Articles > Examples","previous_headings":"Diffusion model","what":"Diffusion schedule","title":"Denoising Diffusion Implicit Models","text":"Let us say, diffusion process starts time = 0, ends time = 1. variable called diffusion time, can either discrete (common diffusion models) continuous (common score-based models). choose latter, number sampling steps can changed inference time. need function tells us point diffusion process noise levels signal levels noisy image corresponding actual diffusion time. called diffusion schedule (see diffusion_schedule()). schedule outputs two quantities: noise_rate signal_rate (corresponding sqrt(1 - alpha) sqrt(alpha) DDIM paper, respectively). generate noisy image weighting random noise training image corresponding rates adding together. Since (standard normal) random noises (normalized) images zero mean unit variance, noise rate signal rate can interpreted standard deviation components noisy image, squares rates can interpreted variance (power signal processing sense). rates always set squared sum 1, meaning noisy images always unit variance, just like unscaled components. use simplified, continuous version cosine schedule (Section 3.2), quite commonly used literature. schedule symmetric, slow towards start end diffusion process, also nice geometric interpretation, using trigonometric properties unit circle:","code":""},{"path":"https://keras.posit.co/articles/examples/ddim.html","id":"training-process","dir":"Articles > Examples","previous_headings":"Diffusion model","what":"Training process","title":"Denoising Diffusion Implicit Models","text":"training procedure (see train_step() denoise()) denoising diffusion models following: sample random diffusion times uniformly, mix training images random gaussian noises rates corresponding diffusion times. , train model separate noisy image two components. Usually, neural network trained predict unscaled noise component, predicted image component can calculated using signal noise rates. Pixelwise mean squared error used theoretically, however recommend using mean absolute error instead (similarly implementation), produces better results dataset.","code":""},{"path":"https://keras.posit.co/articles/examples/ddim.html","id":"sampling-reverse-diffusion","dir":"Articles > Examples","previous_headings":"Diffusion model","what":"Sampling (reverse diffusion)","title":"Denoising Diffusion Implicit Models","text":"sampling (see reverse_diffusion()), step take previous estimate noisy image separate image noise using network. recombine components using signal noise rate following step. Though similar view shown Equation 12 DDIMs, believe explanation sampling equation widely known. example implements deterministic sampling procedure DDIM, corresponds eta = 0 paper. One can also use stochastic sampling (case model becomes Denoising Diffusion Probabilistic Model (DDPM)), part predicted noise replaced larger amount random noise (see Equation 16 ). Stochastic sampling can used without retraining network (since models trained way), can improve sample quality, hand requiring sampling steps usually.","code":"@keras.saving.register_keras_serializable() class DiffusionModel(keras.Model):     def __init__(self, image_size, widths, block_depth):         super().__init__()          self.normalizer = layers.Normalization()         self.network = get_network(image_size, widths, block_depth)         self.ema_network = keras.models.clone_model(self.network)      def compile(self, **kwargs):         super().compile(**kwargs)          self.noise_loss_tracker = keras.metrics.Mean(name=\"n_loss\")         self.image_loss_tracker = keras.metrics.Mean(name=\"i_loss\")         self.kid = KID(name=\"kid\")      @property     def metrics(self):         return [self.noise_loss_tracker, self.image_loss_tracker, self.kid]      def denormalize(self, images):         # convert the pixel values back to 0-1 range         images = self.normalizer.mean + images * self.normalizer.variance**0.5         return ops.clip(images, 0.0, 1.0)      def diffusion_schedule(self, diffusion_times):         # diffusion times -> angles         start_angle = ops.cast(ops.arccos(max_signal_rate), \"float32\")         end_angle = ops.cast(ops.arccos(min_signal_rate), \"float32\")          diffusion_angles = start_angle + diffusion_times * (             end_angle - start_angle         )          # angles -> signal and noise rates         signal_rates = ops.cos(diffusion_angles)         noise_rates = ops.sin(diffusion_angles)         # note that their squared sum is always: sin^2(x) + cos^2(x) = 1          return noise_rates, signal_rates      def denoise(self, noisy_images, noise_rates, signal_rates, training):         # the exponential moving average weights are used at evaluation         if training:             network = self.network         else:             network = self.ema_network          # predict noise component and calculate the image component using it         pred_noises = network(             [noisy_images, noise_rates**2], training=training         )         pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates          return pred_noises, pred_images      def reverse_diffusion(self, initial_noise, diffusion_steps):         # reverse diffusion = sampling         num_images = initial_noise.shape[0]         step_size = 1.0 / diffusion_steps          # important line:         # at the first sampling step, the \"noisy image\" is pure noise         # but its signal rate is assumed to be nonzero (min_signal_rate)         next_noisy_images = initial_noise         for step in range(diffusion_steps):             noisy_images = next_noisy_images              # separate the current noisy image to its components             diffusion_times = ops.ones((num_images, 1, 1, 1)) - step * step_size             noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)             pred_noises, pred_images = self.denoise(                 noisy_images, noise_rates, signal_rates, training=False             )             # network used in eval mode              # remix the predicted components using the next signal and noise rates             next_diffusion_times = diffusion_times - step_size             next_noise_rates, next_signal_rates = self.diffusion_schedule(                 next_diffusion_times             )             next_noisy_images = (                 next_signal_rates * pred_images + next_noise_rates * pred_noises             )             # this new noisy image will be used in the next step          return pred_images      def generate(self, num_images, diffusion_steps):         # noise -> images -> denormalized images         initial_noise = keras.random.normal(             shape=(num_images, image_size, image_size, 3)         )         generated_images = self.reverse_diffusion(             initial_noise, diffusion_steps         )         generated_images = self.denormalize(generated_images)         return generated_images      def train_step(self, images):         # normalize images to have standard deviation of 1, like the noises         images = self.normalizer(images, training=True)         noises = keras.random.normal(             shape=(batch_size, image_size, image_size, 3)         )          # sample uniform random diffusion times         diffusion_times = keras.random.uniform(             shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0         )         noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)         # mix the images with noises accordingly         noisy_images = signal_rates * images + noise_rates * noises          with tf.GradientTape() as tape:             # train the network to separate noisy images to their components             pred_noises, pred_images = self.denoise(                 noisy_images, noise_rates, signal_rates, training=True             )              noise_loss = self.loss(noises, pred_noises)  # used for training             image_loss = self.loss(images, pred_images)  # only used as metric          gradients = tape.gradient(noise_loss, self.network.trainable_weights)         self.optimizer.apply_gradients(             zip(gradients, self.network.trainable_weights)         )          self.noise_loss_tracker.update_state(noise_loss)         self.image_loss_tracker.update_state(image_loss)          # track the exponential moving averages of weights         for weight, ema_weight in zip(             self.network.weights, self.ema_network.weights         ):             ema_weight.assign(ema * ema_weight + (1 - ema) * weight)          # KID is not measured during the training phase for computational efficiency         return {m.name: m.result() for m in self.metrics[:-1]}      def test_step(self, images):         # normalize images to have standard deviation of 1, like the noises         images = self.normalizer(images, training=False)         noises = keras.random.normal(             shape=(batch_size, image_size, image_size, 3)         )          # sample uniform random diffusion times         diffusion_times = keras.random.uniform(             shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0         )         noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)         # mix the images with noises accordingly         noisy_images = signal_rates * images + noise_rates * noises          # use the network to separate noisy images to their components         pred_noises, pred_images = self.denoise(             noisy_images, noise_rates, signal_rates, training=False         )          noise_loss = self.loss(noises, pred_noises)         image_loss = self.loss(images, pred_images)          self.image_loss_tracker.update_state(image_loss)         self.noise_loss_tracker.update_state(noise_loss)          # measure KID between real and generated images         # this is computationally demanding, kid_diffusion_steps has to be small         images = self.denormalize(images)         generated_images = self.generate(             num_images=batch_size, diffusion_steps=kid_diffusion_steps         )         self.kid.update_state(images, generated_images)          return {m.name: m.result() for m in self.metrics}      def plot_images(self, epoch=None, logs=None, num_rows=3, num_cols=6):         # plot random generated images for visual evaluation of generation quality         generated_images = self.generate(             num_images=num_rows * num_cols,             diffusion_steps=plot_diffusion_steps,         )          plt.figure(figsize=(num_cols * 2.0, num_rows * 2.0))         for row in range(num_rows):             for col in range(num_cols):                 index = row * num_cols + col                 plt.subplot(num_rows, num_cols, index + 1)                 plt.imshow(generated_images[index])                 plt.axis(\"off\")         plt.tight_layout()         plt.show()         plt.close()"},{"path":"https://keras.posit.co/articles/examples/ddim.html","id":"training","dir":"Articles > Examples","previous_headings":"","what":"Training","title":"Denoising Diffusion Implicit Models","text":"","code":"# create and compile the model model = DiffusionModel(image_size, widths, block_depth) # below tensorflow 2.9: # pip install tensorflow_addons # import tensorflow_addons as tfa # optimizer=tfa.optimizers.AdamW model.compile(     optimizer=keras.optimizers.AdamW(         learning_rate=learning_rate, weight_decay=weight_decay     ),     loss=keras.losses.mean_absolute_error, ) # pixelwise mean absolute error is used as loss  # save the best model based on the validation KID metric checkpoint_path = \"checkpoints/diffusion_model.weights.h5\" checkpoint_callback = keras.callbacks.ModelCheckpoint(     filepath=checkpoint_path,     save_weights_only=True,     monitor=\"val_kid\",     mode=\"min\",     save_best_only=True, )  # calculate mean and variance of training dataset for normalization model.normalizer.adapt(train_dataset)  # run training and plot generated images periodically model.fit(     train_dataset,     epochs=num_epochs,     validation_data=val_dataset,     callbacks=[         keras.callbacks.LambdaCallback(on_epoch_end=model.plot_images),         checkpoint_callback,     ], )"},{"path":"https://keras.posit.co/articles/examples/ddim.html","id":"inference","dir":"Articles > Examples","previous_headings":"","what":"Inference","title":"Denoising Diffusion Implicit Models","text":"","code":"# load the best model and generate images model.load_weights(checkpoint_path) model.plot_images()"},{"path":"https://keras.posit.co/articles/examples/ddim.html","id":"results","dir":"Articles > Examples","previous_headings":"","what":"Results","title":"Denoising Diffusion Implicit Models","text":"running training least 50 epochs (takes 2 hours T4 GPU 30 minutes A100 GPU), one can get high quality image generations using code example. evolution batch images 80 epoch training (color artifacts due GIF compression): Images generated using 1 20 sampling steps initial noise: Interpolation (spherical) initial noise samples: Deterministic sampling process (noisy images top, predicted images bottom, 40 steps): Stochastic sampling process (noisy images top, predicted images bottom, 80 steps): Trained model demo available HuggingFace:","code":""},{"path":"https://keras.posit.co/articles/examples/ddim.html","id":"lessons-learned","dir":"Articles > Examples","previous_headings":"","what":"Lessons learned","title":"Denoising Diffusion Implicit Models","text":"preparation code example run numerous experiments using repository. section list lessons learned recommendations subjective order importance.","code":""},{"path":"https://keras.posit.co/articles/examples/ddim.html","id":"algorithmic-tips","dir":"Articles > Examples","previous_headings":"Lessons learned","what":"Algorithmic tips","title":"Denoising Diffusion Implicit Models","text":"min. max. signal rates: found min. signal rate important hyperparameter. Setting low make generated images oversaturated, setting high make undersaturated. recommend tuning carefully. Also, setting 0 lead division zero error. max. signal rate can set 1, found setting lower slightly improves generation quality. loss function: large models tend use mean squared error (MSE) loss, recommend using mean absolute error (MAE) dataset. experience MSE loss generates diverse samples (also seems hallucinate Section 3), MAE loss leads smoother images. recommend trying . weight decay: occasionally run diverged trainings scaling model, found weight decay helps avoiding instabilities low performance cost. use AdamW instead Adam example. exponential moving average weights: helps reduce variance KID metric, helps averaging short-term changes training. image augmentations: Though use image augmentations example, experience adding horizontal flips training increases generation performance, random crops . Since use supervised denoising loss, overfitting can issue, image augmentations might important small datasets. One also careful use leaky augmentations, can done following method (end Section 5) instance. data normalization: literature pixel values images usually converted -1 1 range. theoretical correctness, normalize images zero mean unit variance instead, exactly like random noises. noise level input: chose input noise variance network, symmetrical sampling schedule. One also input noise rate (similar performance), signal rate (lower performance), even log-signal--noise ratio (Appendix B.1) (try, range highly dependent min. max. signal rates, require adjusting min. embedding frequency accordingly). gradient clipping: Using global gradient clipping value 1 can help training stability large models, decreased performance significantly experience. residual connection downscaling: deeper models (Appendix B), scaling residual connections 1/sqrt(2) can helpful, help case. learning rate: , Adam optimizer’s default learning rate 1e-3 worked well, lower learning rates common literature (Tables 11-13).","code":""},{"path":"https://keras.posit.co/articles/examples/ddim.html","id":"architectural-tips","dir":"Articles > Examples","previous_headings":"Lessons learned","what":"Architectural tips","title":"Denoising Diffusion Implicit Models","text":"sinusoidal embedding: Using sinusoidal embeddings noise level input network crucial good performance. recommend setting min. embedding frequency reciprocal range input, since use noise variance example, can left always 1. max. embedding frequency controls smallest change noise variance network sensitive , embedding dimensions set number frequency components embedding. experience performance sensitive values. skip connections: Using skip connections network architecture absolutely critical, without model fail learn denoise good performance. residual connections: experience residual connections also significantly improve performance, might due fact input noise level embeddings first layer network instead . normalization: scaling model, occasionally encounter diverged trainings, using normalization layers helped mitigate issue. literature common use GroupNormalization (8 groups example) LayerNormalization network, however chose use BatchNormalization, gave similar benefits experiments computationally lighter. activations: choice activation functions larger effect generation quality expected. experiments using non-monotonic activation functions outperformed monotonic ones (ReLU), Swish performing best (also Imagen uses, page 41). attention: mentioned earlier, common literature use attention layers low resolutions better global coherence. omitted simplicity. upsampling: Bilinear nearest neighbour upsampling network performed similarly, however try transposed convolutions. similar list GANs check Keras tutorial.","code":""},{"path":"https://keras.posit.co/articles/examples/ddim.html","id":"what-to-try-next","dir":"Articles > Examples","previous_headings":"","what":"What to try next?","title":"Denoising Diffusion Implicit Models","text":"like dive deeper topic, recommend checking repository created preparation code example, implements wider range features similar style, : stochastic sampling second-order sampling based differential equation view DDIMs (Equation 13) diffusion schedules network output types: predicting image velocity (Appendix D) instead noise datasets","code":""},{"path":"https://keras.posit.co/articles/examples/ddim.html","id":"related-works","dir":"Articles > Examples","previous_headings":"","what":"Related works","title":"Denoising Diffusion Implicit Models","text":"Score-based generative modeling (blogpost) diffusion models? (blogpost) Annotated diffusion model (blogpost) CVPR 2022 tutorial diffusion models (slides available) Elucidating Design Space Diffusion-Based Generative Models: attempts unifying diffusion methods common framework High-level video overviews: 1, 2 Detailed technical videos: 1, 2 Score-based generative models: NCSN, NCSN+, NCSN++ Denoising diffusion models: DDPM, DDIM, DDPM+, DDPM++ Large diffusion models: GLIDE, DALL-E 2, Imagen","code":""},{"path":"https://keras.posit.co/articles/examples/ddpm.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Denoising Diffusion Probabilistic Model","text":"Generative modeling experienced tremendous growth last five years. Models like VAEs, GANs, flow-based models proved great success generating high-quality content, especially images. Diffusion models new type generative model proven better previous approaches. Diffusion models inspired non-equilibrium thermodynamics, learn generate denoising. Learning denoising consists two processes, Markov Chain. : forward process: forward process, slowly add random noise data series time steps (t1, t2, ..., tn ). Samples current time step drawn Gaussian distribution mean distribution conditioned sample previous time step, variance distribution follows fixed schedule. end forward process, samples end pure noise distribution. reverse process: reverse process, try undo added noise every time step. start pure noise distribution (last step forward process) try denoise samples backward direction (tn, tn-1, ..., t1). implement Denoising Diffusion Probabilistic Models paper DDPMs short code example. first paper demonstrating use diffusion models generating high-quality images. authors proved certain parameterization diffusion models reveals equivalence denoising score matching multiple noise levels training annealed Langevin dynamics sampling generates best quality results. paper replicates Markov chains (forward process reverse process) involved diffusion process images. forward process fixed gradually adds Gaussian noise images according fixed variance schedule denoted beta paper. diffusion process looks like case images: (image -> noise::noise -> image) paper describes two algorithms, one training model, sampling trained model. Training performed optimizing usual variational bound negative log-likelihood. objective function simplified, network treated noise prediction network. optimized, can sample network generate new images noise samples. overview algorithms presented paper: Note: DDPM just one way implementing diffusion model. Also, sampling algorithm DDPM replicates complete Markov chain. Hence ’s slow generating new samples compared generative models like GANs. Lots research efforts made address issue. One example Denoising Diffusion Implicit Models, DDIM short, authors replaced Markov chain non-Markovian process sample faster. can find code example DDIM Implementing DDPM model simple. define model takes two inputs: Images randomly sampled time steps. training step, perform following operations train model: Sample random noise added inputs. Apply forward process diffuse inputs sampled noise. model takes noisy samples inputs outputs noise prediction time step. Given true noise predicted noise, calculate loss values calculate gradients update model weights. Given model knows denoise noisy sample given time step, can leverage idea generate new samples, starting pure noise distribution.","code":""},{"path":"https://keras.posit.co/articles/examples/ddpm.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Denoising Diffusion Probabilistic Model","text":"","code":"import math import numpy as np import matplotlib.pyplot as plt  # Requires TensorFlow >=2.11 for the GroupNormalization layer. import tensorflow as tf import keras as keras from keras import layers import tensorflow_datasets as tfds"},{"path":"https://keras.posit.co/articles/examples/ddpm.html","id":"hyperparameters","dir":"Articles > Examples","previous_headings":"","what":"Hyperparameters","title":"Denoising Diffusion Probabilistic Model","text":"","code":"batch_size = 32 num_epochs = 1  # Just for the sake of demonstration total_timesteps = 1000 norm_groups = 8  # Number of groups used in GroupNormalization layer learning_rate = 2e-4  img_size = 64 img_channels = 3 clip_min = -1.0 clip_max = 1.0  first_conv_channels = 64 channel_multiplier = [1, 2, 4, 8] widths = [first_conv_channels * mult for mult in channel_multiplier] has_attention = [False, False, True, True] num_res_blocks = 2  # Number of residual blocks  dataset_name = \"oxford_flowers102\" splits = [\"train\"]"},{"path":"https://keras.posit.co/articles/examples/ddpm.html","id":"dataset","dir":"Articles > Examples","previous_headings":"","what":"Dataset","title":"Denoising Diffusion Probabilistic Model","text":"use Oxford Flowers 102 dataset generating images flowers. terms preprocessing, use center cropping resizing images desired image size, rescale pixel values range [-1.0, 1.0]. line range pixel values applied authors DDPMs paper. augmenting training data, randomly flip images left/right.","code":"# Load the dataset (ds,) = tfds.load(     dataset_name, split=splits, with_info=False, shuffle_files=True )   def augment(img):     \"\"\"Flips an image left/right randomly.\"\"\"     return tf.image.random_flip_left_right(img)   def resize_and_rescale(img, size):     \"\"\"Resize the image to the desired size first and then     rescale the pixel values in the range [-1.0, 1.0].      Args:         img: Image tensor         size: Desired image size for resizing     Returns:         Resized and rescaled image tensor     \"\"\"      height = tf.shape(img)[0]     width = tf.shape(img)[1]     crop_size = tf.minimum(height, width)      img = tf.image.crop_to_bounding_box(         img,         (height - crop_size) // 2,         (width - crop_size) // 2,         crop_size,         crop_size,     )      # Resize     img = tf.cast(img, dtype=tf.float32)     img = tf.image.resize(img, size=size, antialias=True)      # Rescale the pixel values     img = img / 127.5 - 1.0     img = tf.clip_by_value(img, clip_min, clip_max)     return img   def train_preprocessing(x):     img = x[\"image\"]     img = resize_and_rescale(img, size=(img_size, img_size))     img = augment(img)     return img   train_ds = (     ds.map(train_preprocessing, num_parallel_calls=tf.data.AUTOTUNE)     .batch(batch_size, drop_remainder=True)     .shuffle(batch_size * 2)     .prefetch(tf.data.AUTOTUNE) )"},{"path":"https://keras.posit.co/articles/examples/ddpm.html","id":"gaussian-diffusion-utilities","dir":"Articles > Examples","previous_headings":"","what":"Gaussian diffusion utilities","title":"Denoising Diffusion Probabilistic Model","text":"define forward process reverse process separate utility. code utility borrowed original implementation slight modifications.","code":"class GaussianDiffusion:     \"\"\"Gaussian diffusion utility.      Args:         beta_start: Start value of the scheduled variance         beta_end: End value of the scheduled variance         timesteps: Number of time steps in the forward process     \"\"\"      def __init__(         self,         beta_start=1e-4,         beta_end=0.02,         timesteps=1000,         clip_min=-1.0,         clip_max=1.0,     ):         self.beta_start = beta_start         self.beta_end = beta_end         self.timesteps = timesteps         self.clip_min = clip_min         self.clip_max = clip_max          # Define the linear variance schedule         self.betas = betas = np.linspace(             beta_start,             beta_end,             timesteps,             dtype=np.float64,  # Using float64 for better precision         )         self.num_timesteps = int(timesteps)          alphas = 1.0 - betas         alphas_cumprod = np.cumprod(alphas, axis=0)         alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])          self.betas = tf.constant(betas, dtype=tf.float32)         self.alphas_cumprod = tf.constant(alphas_cumprod, dtype=tf.float32)         self.alphas_cumprod_prev = tf.constant(             alphas_cumprod_prev, dtype=tf.float32         )          # Calculations for diffusion q(x_t | x_{t-1}) and others         self.sqrt_alphas_cumprod = tf.constant(             np.sqrt(alphas_cumprod), dtype=tf.float32         )          self.sqrt_one_minus_alphas_cumprod = tf.constant(             np.sqrt(1.0 - alphas_cumprod), dtype=tf.float32         )          self.log_one_minus_alphas_cumprod = tf.constant(             np.log(1.0 - alphas_cumprod), dtype=tf.float32         )          self.sqrt_recip_alphas_cumprod = tf.constant(             np.sqrt(1.0 / alphas_cumprod), dtype=tf.float32         )         self.sqrt_recipm1_alphas_cumprod = tf.constant(             np.sqrt(1.0 / alphas_cumprod - 1), dtype=tf.float32         )          # Calculations for posterior q(x_{t-1} | x_t, x_0)         posterior_variance = (             betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)         )         self.posterior_variance = tf.constant(             posterior_variance, dtype=tf.float32         )          # Log calculation clipped because the posterior variance is 0 at the beginning         # of the diffusion chain         self.posterior_log_variance_clipped = tf.constant(             np.log(np.maximum(posterior_variance, 1e-20)), dtype=tf.float32         )          self.posterior_mean_coef1 = tf.constant(             betas * np.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod),             dtype=tf.float32,         )          self.posterior_mean_coef2 = tf.constant(             (1.0 - alphas_cumprod_prev)             * np.sqrt(alphas)             / (1.0 - alphas_cumprod),             dtype=tf.float32,         )      def _extract(self, a, t, x_shape):         \"\"\"Extract some coefficients at specified timesteps,         then reshape to [batch_size, 1, 1, 1, 1, ...] for broadcasting purposes.          Args:             a: Tensor to extract from             t: Timestep for which the coefficients are to be extracted             x_shape: Shape of the current batched samples         \"\"\"         batch_size = x_shape[0]         out = tf.gather(a, t)         return tf.reshape(out, [batch_size, 1, 1, 1])      def q_mean_variance(self, x_start, t):         \"\"\"Extracts the mean, and the variance at current timestep.          Args:             x_start: Initial sample (before the first diffusion step)             t: Current timestep         \"\"\"         x_start_shape = tf.shape(x_start)         mean = (             self._extract(self.sqrt_alphas_cumprod, t, x_start_shape) * x_start         )         variance = self._extract(1.0 - self.alphas_cumprod, t, x_start_shape)         log_variance = self._extract(             self.log_one_minus_alphas_cumprod, t, x_start_shape         )         return mean, variance, log_variance      def q_sample(self, x_start, t, noise):         \"\"\"Diffuse the data.          Args:             x_start: Initial sample (before the first diffusion step)             t: Current timestep             noise: Gaussian noise to be added at the current timestep         Returns:             Diffused samples at timestep `t`         \"\"\"         x_start_shape = tf.shape(x_start)         return (             self._extract(self.sqrt_alphas_cumprod, t, tf.shape(x_start))             * x_start             + self._extract(                 self.sqrt_one_minus_alphas_cumprod, t, x_start_shape             )             * noise         )      def predict_start_from_noise(self, x_t, t, noise):         x_t_shape = tf.shape(x_t)         return (             self._extract(self.sqrt_recip_alphas_cumprod, t, x_t_shape) * x_t             - self._extract(self.sqrt_recipm1_alphas_cumprod, t, x_t_shape)             * noise         )      def q_posterior(self, x_start, x_t, t):         \"\"\"Compute the mean and variance of the diffusion         posterior q(x_{t-1} | x_t, x_0).          Args:             x_start: Stating point(sample) for the posterior computation             x_t: Sample at timestep `t`             t: Current timestep         Returns:             Posterior mean and variance at current timestep         \"\"\"          x_t_shape = tf.shape(x_t)         posterior_mean = (             self._extract(self.posterior_mean_coef1, t, x_t_shape) * x_start             + self._extract(self.posterior_mean_coef2, t, x_t_shape) * x_t         )         posterior_variance = self._extract(             self.posterior_variance, t, x_t_shape         )         posterior_log_variance_clipped = self._extract(             self.posterior_log_variance_clipped, t, x_t_shape         )         return (             posterior_mean,             posterior_variance,             posterior_log_variance_clipped,         )      def p_mean_variance(self, pred_noise, x, t, clip_denoised=True):         x_recon = self.predict_start_from_noise(x, t=t, noise=pred_noise)         if clip_denoised:             x_recon = tf.clip_by_value(x_recon, self.clip_min, self.clip_max)          (             model_mean,             posterior_variance,             posterior_log_variance,         ) = self.q_posterior(x_start=x_recon, x_t=x, t=t)         return model_mean, posterior_variance, posterior_log_variance      def p_sample(self, pred_noise, x, t, clip_denoised=True):         \"\"\"Sample from the diffuison model.          Args:             pred_noise: Noise predicted by the diffusion model             x: Samples at a given timestep for which the noise was predicted             t: Current timestep             clip_denoised (bool): Whether to clip the predicted noise                 within the specified range or not.         \"\"\"         model_mean, _, model_log_variance = self.p_mean_variance(             pred_noise, x=x, t=t, clip_denoised=clip_denoised         )         noise = tf.random.normal(shape=x.shape, dtype=x.dtype)         # No noise when t == 0         nonzero_mask = tf.reshape(             1 - tf.cast(tf.equal(t, 0), tf.float32), [tf.shape(x)[0], 1, 1, 1]         )         return (             model_mean + nonzero_mask * tf.exp(0.5 * model_log_variance) * noise         )"},{"path":"https://keras.posit.co/articles/examples/ddpm.html","id":"network-architecture","dir":"Articles > Examples","previous_headings":"","what":"Network architecture","title":"Denoising Diffusion Probabilistic Model","text":"U-Net, originally developed semantic segmentation, architecture widely used implementing diffusion models slight modifications: network accepts two inputs: Image time step Self-attention convolution blocks reach specific resolution (16x16 paper) Group Normalization instead weight normalization implement things used original paper. use swish activation function throughout network. use variance scaling kernel initializer. difference number groups used GroupNormalization layer. flowers dataset, found value groups=8 produces better results compared default value groups=32. Dropout optional used chances fitting high. paper, authors used dropout training CIFAR10.","code":"# Kernel initializer to use def kernel_init(scale):     scale = max(scale, 1e-10)     return keras.initializers.VarianceScaling(         scale, mode=\"fan_avg\", distribution=\"uniform\"     )   class AttentionBlock(layers.Layer):     \"\"\"Applies self-attention.      Args:         units: Number of units in the dense layers         groups: Number of groups to be used for GroupNormalization layer     \"\"\"      def __init__(self, units, groups=8, **kwargs):         self.units = units         self.groups = groups         super().__init__(**kwargs)          self.norm = layers.GroupNormalization(groups=groups)         self.query = layers.Dense(units, kernel_initializer=kernel_init(1.0))         self.key = layers.Dense(units, kernel_initializer=kernel_init(1.0))         self.value = layers.Dense(units, kernel_initializer=kernel_init(1.0))         self.proj = layers.Dense(units, kernel_initializer=kernel_init(0.0))      def call(self, inputs):         batch_size = tf.shape(inputs)[0]         height = tf.shape(inputs)[1]         width = tf.shape(inputs)[2]         scale = tf.cast(self.units, tf.float32) ** (-0.5)          inputs = self.norm(inputs)         q = self.query(inputs)         k = self.key(inputs)         v = self.value(inputs)          attn_score = tf.einsum(\"bhwc, bHWc->bhwHW\", q, k) * scale         attn_score = tf.reshape(             attn_score, [batch_size, height, width, height * width]         )          attn_score = tf.nn.softmax(attn_score, -1)         attn_score = tf.reshape(             attn_score, [batch_size, height, width, height, width]         )          proj = tf.einsum(\"bhwHW,bHWc->bhwc\", attn_score, v)         proj = self.proj(proj)         return inputs + proj   class TimeEmbedding(layers.Layer):     def __init__(self, dim, **kwargs):         super().__init__(**kwargs)         self.dim = dim         self.half_dim = dim // 2         self.emb = math.log(10000) / (self.half_dim - 1)         self.emb = tf.exp(tf.range(self.half_dim, dtype=tf.float32) * -self.emb)      def call(self, inputs):         inputs = tf.cast(inputs, dtype=tf.float32)         emb = inputs[:, None] * self.emb[None, :]         emb = tf.concat([tf.sin(emb), tf.cos(emb)], axis=-1)         return emb   def ResidualBlock(width, groups=8, activation_fn=keras.activations.swish):     def apply(inputs):         x, t = inputs         input_width = x.shape[3]          if input_width == width:             residual = x         else:             residual = layers.Conv2D(                 width, kernel_size=1, kernel_initializer=kernel_init(1.0)             )(x)          temb = activation_fn(t)         temb = layers.Dense(width, kernel_initializer=kernel_init(1.0))(temb)[             :, None, None, :         ]          x = layers.GroupNormalization(groups=groups)(x)         x = activation_fn(x)         x = layers.Conv2D(             width,             kernel_size=3,             padding=\"same\",             kernel_initializer=kernel_init(1.0),         )(x)          x = layers.Add()([x, temb])         x = layers.GroupNormalization(groups=groups)(x)         x = activation_fn(x)          x = layers.Conv2D(             width,             kernel_size=3,             padding=\"same\",             kernel_initializer=kernel_init(0.0),         )(x)         x = layers.Add()([x, residual])         return x      return apply   def DownSample(width):     def apply(x):         x = layers.Conv2D(             width,             kernel_size=3,             strides=2,             padding=\"same\",             kernel_initializer=kernel_init(1.0),         )(x)         return x      return apply   def UpSample(width, interpolation=\"nearest\"):     def apply(x):         x = layers.UpSampling2D(size=2, interpolation=interpolation)(x)         x = layers.Conv2D(             width,             kernel_size=3,             padding=\"same\",             kernel_initializer=kernel_init(1.0),         )(x)         return x      return apply   def TimeMLP(units, activation_fn=keras.activations.swish):     def apply(inputs):         temb = layers.Dense(             units, activation=activation_fn, kernel_initializer=kernel_init(1.0)         )(inputs)         temb = layers.Dense(units, kernel_initializer=kernel_init(1.0))(temb)         return temb      return apply   def build_model(     img_size,     img_channels,     widths,     has_attention,     num_res_blocks=2,     norm_groups=8,     interpolation=\"nearest\",     activation_fn=keras.activations.swish, ):     image_input = layers.Input(         shape=(img_size, img_size, img_channels), name=\"image_input\"     )     time_input = keras.Input(shape=(), dtype=tf.int64, name=\"time_input\")      x = layers.Conv2D(         first_conv_channels,         kernel_size=(3, 3),         padding=\"same\",         kernel_initializer=kernel_init(1.0),     )(image_input)      temb = TimeEmbedding(dim=first_conv_channels * 4)(time_input)     temb = TimeMLP(units=first_conv_channels * 4, activation_fn=activation_fn)(         temb     )      skips = [x]      # DownBlock     for i in range(len(widths)):         for _ in range(num_res_blocks):             x = ResidualBlock(                 widths[i], groups=norm_groups, activation_fn=activation_fn             )([x, temb])             if has_attention[i]:                 x = AttentionBlock(widths[i], groups=norm_groups)(x)             skips.append(x)          if widths[i] != widths[-1]:             x = DownSample(widths[i])(x)             skips.append(x)      # MiddleBlock     x = ResidualBlock(         widths[-1], groups=norm_groups, activation_fn=activation_fn     )([x, temb])     x = AttentionBlock(widths[-1], groups=norm_groups)(x)     x = ResidualBlock(         widths[-1], groups=norm_groups, activation_fn=activation_fn     )([x, temb])      # UpBlock     for i in reversed(range(len(widths))):         for _ in range(num_res_blocks + 1):             x = layers.Concatenate(axis=-1)([x, skips.pop()])             x = ResidualBlock(                 widths[i], groups=norm_groups, activation_fn=activation_fn             )([x, temb])             if has_attention[i]:                 x = AttentionBlock(widths[i], groups=norm_groups)(x)          if i != 0:             x = UpSample(widths[i], interpolation=interpolation)(x)      # End block     x = layers.GroupNormalization(groups=norm_groups)(x)     x = activation_fn(x)     x = layers.Conv2D(         3, (3, 3), padding=\"same\", kernel_initializer=kernel_init(0.0)     )(x)     return keras.Model([image_input, time_input], x, name=\"unet\")"},{"path":"https://keras.posit.co/articles/examples/ddpm.html","id":"training","dir":"Articles > Examples","previous_headings":"","what":"Training","title":"Denoising Diffusion Probabilistic Model","text":"follow setup training diffusion model described paper. use Adam optimizer learning rate 2e-4. use EMA model parameters decay factor 0.999. treat model noise prediction network .e. every training step, input batch images corresponding time steps UNet, network outputs noise predictions. difference aren’t using Kernel Inception Distance (KID) Frechet Inception Distance (FID) evaluating quality generated samples training. metrics compute heavy skipped brevity implementation. Note:  using mean squared error loss function aligned paper, theoretically makes sense. practice, though, also common use mean absolute error Huber loss loss function.","code":"class DiffusionModel(keras.Model):     def __init__(self, network, ema_network, timesteps, gdf_util, ema=0.999):         super().__init__()         self.network = network         self.ema_network = ema_network         self.timesteps = timesteps         self.gdf_util = gdf_util         self.ema = ema      def train_step(self, images):         # 1. Get the batch size         batch_size = tf.shape(images)[0]          # 2. Sample timesteps uniformly         t = tf.random.uniform(             minval=0, maxval=self.timesteps, shape=(batch_size,), dtype=tf.int64         )          with tf.GradientTape() as tape:             # 3. Sample random noise to be added to the images in the batch             noise = tf.random.normal(shape=tf.shape(images), dtype=images.dtype)              # 4. Diffuse the images with noise             images_t = self.gdf_util.q_sample(images, t, noise)              # 5. Pass the diffused images and time steps to the network             pred_noise = self.network([images_t, t], training=True)              # 6. Calculate the loss             loss = self.loss(noise, pred_noise)          # 7. Get the gradients         gradients = tape.gradient(loss, self.network.trainable_weights)          # 8. Update the weights of the network         self.optimizer.apply_gradients(             zip(gradients, self.network.trainable_weights)         )          # 9. Updates the weight values for the network with EMA weights         for weight, ema_weight in zip(             self.network.weights, self.ema_network.weights         ):             ema_weight.assign(self.ema * ema_weight + (1 - self.ema) * weight)          # 10. Return loss values         return {\"loss\": loss}      def generate_images(self, num_images=16):         # 1. Randomly sample noise (starting point for reverse process)         samples = tf.random.normal(             shape=(num_images, img_size, img_size, img_channels),             dtype=tf.float32,         )         # 2. Sample from the model iteratively         for t in reversed(range(0, self.timesteps)):             tt = tf.cast(tf.fill(num_images, t), dtype=tf.int64)             pred_noise = self.ema_network.predict(                 [samples, tt], verbose=0, batch_size=num_images             )             samples = self.gdf_util.p_sample(                 pred_noise, samples, tt, clip_denoised=True             )         # 3. Return generated samples         return samples      def plot_images(         self, epoch=None, logs=None, num_rows=2, num_cols=8, figsize=(12, 5)     ):         \"\"\"Utility to plot images using the diffusion model during training.\"\"\"         generated_samples = self.generate_images(num_images=num_rows * num_cols)         generated_samples = (             tf.clip_by_value(generated_samples * 127.5 + 127.5, 0.0, 255.0)             .numpy()             .astype(np.uint8)         )          _, ax = plt.subplots(num_rows, num_cols, figsize=figsize)         for i, image in enumerate(generated_samples):             if num_rows == 1:                 ax[i].imshow(image)                 ax[i].axis(\"off\")             else:                 ax[i // num_cols, i % num_cols].imshow(image)                 ax[i // num_cols, i % num_cols].axis(\"off\")          plt.tight_layout()         plt.show()   # Build the unet model network = build_model(     img_size=img_size,     img_channels=img_channels,     widths=widths,     has_attention=has_attention,     num_res_blocks=num_res_blocks,     norm_groups=norm_groups,     activation_fn=keras.activations.swish, ) ema_network = build_model(     img_size=img_size,     img_channels=img_channels,     widths=widths,     has_attention=has_attention,     num_res_blocks=num_res_blocks,     norm_groups=norm_groups,     activation_fn=keras.activations.swish, ) ema_network.set_weights(     network.get_weights() )  # Initially the weights are the same  # Get an instance of the Gaussian Diffusion utilities gdf_util = GaussianDiffusion(timesteps=total_timesteps)  # Get the model model = DiffusionModel(     network=network,     ema_network=ema_network,     gdf_util=gdf_util,     timesteps=total_timesteps, )  # Compile the model model.compile(     loss=keras.losses.MeanSquaredError(),     optimizer=keras.optimizers.Adam(learning_rate=learning_rate),     jit_compile=False, )  # Train the model model.fit(     train_ds,     epochs=num_epochs,     batch_size=batch_size,     callbacks=[keras.callbacks.LambdaCallback(on_epoch_end=model.plot_images)], )"},{"path":"https://keras.posit.co/articles/examples/ddpm.html","id":"results","dir":"Articles > Examples","previous_headings":"","what":"Results","title":"Denoising Diffusion Probabilistic Model","text":"trained model 800 epochs V100 GPU, epoch took almost 8 seconds finish. load weights , generate samples starting pure noise. curl -LO https://github.com/AakashKumarNain/ddpms/releases/download/v3.0.0/checkpoints.zip unzip -qq checkpoints.zip","code":"# Load the model weights model.ema_network.load_weights(\"checkpoints/diffusion_model_checkpoint\")  # Generate and plot some samples model.plot_images(num_rows=4, num_cols=8)"},{"path":"https://keras.posit.co/articles/examples/ddpm.html","id":"conclusion","dir":"Articles > Examples","previous_headings":"","what":"Conclusion","title":"Denoising Diffusion Probabilistic Model","text":"successfully implemented trained diffusion model exactly fashion implemented authors DDPMs paper. can find original implementation . things can try improve model: Increasing width block. bigger model can learn denoise fewer epochs, though may take care overfitting. implemented linear schedule variance scheduling. can implement schemes like cosine scheduling compare performance.","code":""},{"path":"https://keras.posit.co/articles/examples/ddpm.html","id":"references","dir":"Articles > Examples","previous_headings":"","what":"References","title":"Denoising Diffusion Probabilistic Model","text":"Denoising Diffusion Probabilistic Models Author’s implementation deep dive DDPMs Denoising Diffusion Implicit Models Annotated Diffusion Model AIAIART","code":""},{"path":"https://keras.posit.co/articles/examples/deep_dream.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Deep Dream","text":"“Deep dream” image-filtering technique consists taking image classification model, running gradient ascent input image try maximize activations specific layers (sometimes, specific units specific layers) input. produces hallucination-like visuals. first introduced Alexander Mordvintsev Google July 2015. Process: Load original image. Define number processing scales (“octaves”), smallest largest. Resize original image smallest scale. Run gradient ascent Upscale image next scale Reinject detail lost upscaling time Stop back original size. obtain detail lost upscaling, simply take original image, shrink , upscale , compare result (resized) original image.","code":""},{"path":"https://keras.posit.co/articles/examples/deep_dream.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Deep Dream","text":"base image: Let’s set image preprocessing/deprocessing utilities:","code":"import numpy as np import tensorflow as tf import keras as keras from keras.applications import inception_v3  base_image_path = keras.utils.get_file(     \"sky.jpg\", \"https://i.imgur.com/aGBdQyK.jpg\" ) result_prefix = \"sky_dream\"  # These are the names of the layers # for which we try to maximize activation, # as well as their weight in the final loss # we try to maximize. # You can tweak these setting to obtain new visual effects. layer_settings = {     \"mixed4\": 1.0,     \"mixed5\": 1.5,     \"mixed6\": 2.0,     \"mixed7\": 2.5, }  # Playing with these hyperparameters will also allow you to achieve new effects step = 0.01  # Gradient ascent step size num_octave = 3  # Number of scales at which to run gradient ascent octave_scale = 1.4  # Size ratio between scales iterations = 20  # Number of ascent steps per scale max_loss = 15.0 from IPython.display import Image, display  display(Image(base_image_path)) def preprocess_image(image_path):     # Util function to open, resize and format pictures     # into appropriate arrays.     img = keras.utils.load_img(image_path)     img = keras.utils.img_to_array(img)     img = np.expand_dims(img, axis=0)     img = inception_v3.preprocess_input(img)     return img   def deprocess_image(x):     # Util function to convert a NumPy array into a valid image.     x = x.reshape((x.shape[1], x.shape[2], 3))     # Undo inception v3 preprocessing     x /= 2.0     x += 0.5     x *= 255.0     # Convert to uint8 and clip to the valid range [0, 255]     x = np.clip(x, 0, 255).astype(\"uint8\")     return x"},{"path":"https://keras.posit.co/articles/examples/deep_dream.html","id":"compute-the-deep-dream-loss","dir":"Articles > Examples","previous_headings":"","what":"Compute the Deep Dream loss","title":"Deep Dream","text":"First, build feature extraction model retrieve activations target layers given input image. actual loss computation simple:","code":"# Build an InceptionV3 model loaded with pre-trained ImageNet weights model = inception_v3.InceptionV3(weights=\"imagenet\", include_top=False)  # Get the symbolic outputs of each \"key\" layer (we gave them unique names). outputs_dict = dict(     [         (layer.name, layer.output)         for layer in [model.get_layer(name) for name in layer_settings.keys()]     ] )  # Set up a model that returns the activation values for every target layer # (as a dict) feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict) def compute_loss(input_image):     features = feature_extractor(input_image)     # Initialize the loss     loss = tf.zeros(shape=())     for name in features.keys():         coeff = layer_settings[name]         activation = features[name]         # We avoid border artifacts by only involving non-border pixels in the loss.         scaling = tf.reduce_prod(tf.cast(tf.shape(activation), \"float32\"))         loss += (             coeff             * tf.reduce_sum(tf.square(activation[:, 2:-2, 2:-2, :]))             / scaling         )     return loss"},{"path":"https://keras.posit.co/articles/examples/deep_dream.html","id":"set-up-the-gradient-ascent-loop-for-one-octave","dir":"Articles > Examples","previous_headings":"","what":"Set up the gradient ascent loop for one octave","title":"Deep Dream","text":"","code":"@tf.function def gradient_ascent_step(img, learning_rate):     with tf.GradientTape() as tape:         tape.watch(img)         loss = compute_loss(img)     # Compute gradients.     grads = tape.gradient(loss, img)     # Normalize gradients.     grads /= tf.maximum(tf.reduce_mean(tf.abs(grads)), 1e-6)     img += learning_rate * grads     return loss, img   def gradient_ascent_loop(img, iterations, learning_rate, max_loss=None):     for i in range(iterations):         loss, img = gradient_ascent_step(img, learning_rate)         if max_loss is not None and loss > max_loss:             break         print(\"... Loss value at step %d: %.2f\" % (i, loss))     return img"},{"path":"https://keras.posit.co/articles/examples/deep_dream.html","id":"run-the-training-loop-iterating-over-different-octaves","dir":"Articles > Examples","previous_headings":"","what":"Run the training loop, iterating over different octaves","title":"Deep Dream","text":"Display result. can use trained model hosted Hugging Face Hub try demo Hugging Face Spaces.","code":"original_img = preprocess_image(base_image_path) original_shape = original_img.shape[1:3]  successive_shapes = [original_shape] for i in range(1, num_octave):     shape = tuple([int(dim / (octave_scale**i)) for dim in original_shape])     successive_shapes.append(shape) successive_shapes = successive_shapes[::-1] shrunk_original_img = tf.image.resize(original_img, successive_shapes[0])  img = tf.identity(original_img)  # Make a copy for i, shape in enumerate(successive_shapes):     print(\"Processing octave %d with shape %s\" % (i, shape))     img = tf.image.resize(img, shape)     img = gradient_ascent_loop(         img, iterations=iterations, learning_rate=step, max_loss=max_loss     )     upscaled_shrunk_original_img = tf.image.resize(shrunk_original_img, shape)     same_size_original = tf.image.resize(original_img, shape)     lost_detail = same_size_original - upscaled_shrunk_original_img      img += lost_detail     shrunk_original_img = tf.image.resize(original_img, shape)  keras.utils.save_img(result_prefix + \".png\", deprocess_image(img.numpy())) display(Image(result_prefix + \".png\"))"},{"path":"https://keras.posit.co/articles/examples/deep_neural_decision_forests.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Classification with Neural Decision Forests","text":"example provides implementation Deep Neural Decision Forest model introduced P. Kontschieder et al. structured data classification. demonstrates build stochastic differentiable decision tree model, train end--end, unify decision trees deep representation learning.","code":""},{"path":"https://keras.posit.co/articles/examples/deep_neural_decision_forests.html","id":"the-dataset","dir":"Articles > Examples","previous_headings":"","what":"The dataset","title":"Classification with Neural Decision Forests","text":"example uses United States Census Income Dataset provided UC Irvine Machine Learning Repository. task binary classification predict whether person likely making USD 50,000 year. dataset includes 48,842 instances 14 input features (age, work class, education, occupation, ): 5 numerical features 9 categorical features.","code":""},{"path":"https://keras.posit.co/articles/examples/deep_neural_decision_forests.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Classification with Neural Decision Forests","text":"","code":"import keras as keras from keras import layers from keras.layers import StringLookup from keras import ops   from tensorflow import data as tf_data import numpy as np import pandas as pd  import math   _dtype = \"float32\""},{"path":"https://keras.posit.co/articles/examples/deep_neural_decision_forests.html","id":"prepare-the-data","dir":"Articles > Examples","previous_headings":"","what":"Prepare the data","title":"Classification with Neural Decision Forests","text":"Remove first record (valid data example) trailing ‘dot’ class labels. store training test data splits locally CSV files.","code":"CSV_HEADER = [     \"age\",     \"workclass\",     \"fnlwgt\",     \"education\",     \"education_num\",     \"marital_status\",     \"occupation\",     \"relationship\",     \"race\",     \"gender\",     \"capital_gain\",     \"capital_loss\",     \"hours_per_week\",     \"native_country\",     \"income_bracket\", ]  train_data_url = (     \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\" ) train_data = pd.read_csv(train_data_url, header=None, names=CSV_HEADER)  test_data_url = (     \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\" ) test_data = pd.read_csv(test_data_url, header=None, names=CSV_HEADER)  print(f\"Train dataset shape: {train_data.shape}\") print(f\"Test dataset shape: {test_data.shape}\") test_data = test_data[1:] test_data.income_bracket = test_data.income_bracket.apply(     lambda value: value.replace(\".\", \"\") ) train_data_file = \"train_data.csv\" test_data_file = \"test_data.csv\"  train_data.to_csv(train_data_file, index=False, header=False) test_data.to_csv(test_data_file, index=False, header=False)"},{"path":"https://keras.posit.co/articles/examples/deep_neural_decision_forests.html","id":"define-dataset-metadata","dir":"Articles > Examples","previous_headings":"","what":"Define dataset metadata","title":"Classification with Neural Decision Forests","text":", define metadata dataset useful reading parsing encoding input features.","code":"# A list of the numerical feature names. NUMERIC_FEATURE_NAMES = [     \"age\",     \"education_num\",     \"capital_gain\",     \"capital_loss\",     \"hours_per_week\", ] # A dictionary of the categorical features and their vocabulary. CATEGORICAL_FEATURES_WITH_VOCABULARY = {     \"workclass\": sorted(list(train_data[\"workclass\"].unique())),     \"education\": sorted(list(train_data[\"education\"].unique())),     \"marital_status\": sorted(list(train_data[\"marital_status\"].unique())),     \"occupation\": sorted(list(train_data[\"occupation\"].unique())),     \"relationship\": sorted(list(train_data[\"relationship\"].unique())),     \"race\": sorted(list(train_data[\"race\"].unique())),     \"gender\": sorted(list(train_data[\"gender\"].unique())),     \"native_country\": sorted(list(train_data[\"native_country\"].unique())), } # A list of the columns to ignore from the dataset. IGNORE_COLUMN_NAMES = [\"fnlwgt\"] # A list of the categorical feature names. CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys()) # A list of all the input features. FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES # A list of column default values for each feature. COLUMN_DEFAULTS = [     [0.0]     if feature_name in NUMERIC_FEATURE_NAMES + IGNORE_COLUMN_NAMES     else [\"NA\"]     for feature_name in CSV_HEADER ] # The name of the target feature. TARGET_FEATURE_NAME = \"income_bracket\" # A list of the labels of the target features. TARGET_LABELS = [\" <=50K\", \" >50K\"]"},{"path":"https://keras.posit.co/articles/examples/deep_neural_decision_forests.html","id":"create-tf_data-dataset-objects-for-training-and-validation","dir":"Articles > Examples","previous_headings":"","what":"Create tf_data.Dataset objects for training and validation","title":"Classification with Neural Decision Forests","text":"create input function read parse file, convert features labels tf_data.Dataset training validation. also preprocess input mapping target label index.","code":"target_label_lookup = StringLookup(     vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0 )   lookup_dict = {} for feature_name in CATEGORICAL_FEATURE_NAMES:     vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]     # Create a lookup to convert a string values to an integer indices.     # Since we are not using a mask token, nor expecting any out of vocabulary     # (oov) token, we set mask_token to None and num_oov_indices to 0.     lookup = StringLookup(         vocabulary=vocabulary, mask_token=None, num_oov_indices=0     )     lookup_dict[feature_name] = lookup   def encode_categorical(batch_x, batch_y):     for feature_name in CATEGORICAL_FEATURE_NAMES:         batch_x[feature_name] = lookup_dict[feature_name](batch_x[feature_name])      return batch_x, batch_y   def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):     dataset = (         tf_data.experimental.make_csv_dataset(             csv_file_path,             batch_size=batch_size,             column_names=CSV_HEADER,             column_defaults=COLUMN_DEFAULTS,             label_name=TARGET_FEATURE_NAME,             num_epochs=1,             header=False,             na_value=\"?\",             shuffle=shuffle,         )         .map(lambda features, target: (features, target_label_lookup(target)))         .map(encode_categorical)     )      return dataset.cache()"},{"path":"https://keras.posit.co/articles/examples/deep_neural_decision_forests.html","id":"create-model-inputs","dir":"Articles > Examples","previous_headings":"","what":"Create model inputs","title":"Classification with Neural Decision Forests","text":"","code":"def create_model_inputs():     inputs = {}     for feature_name in FEATURE_NAMES:         if feature_name in NUMERIC_FEATURE_NAMES:             inputs[feature_name] = layers.Input(                 name=feature_name, shape=(), dtype=_dtype             )         else:             inputs[feature_name] = layers.Input(                 name=feature_name, shape=(), dtype=\"int32\"             )     return inputs"},{"path":"https://keras.posit.co/articles/examples/deep_neural_decision_forests.html","id":"encode-input-features","dir":"Articles > Examples","previous_headings":"","what":"Encode input features","title":"Classification with Neural Decision Forests","text":"","code":"def encode_inputs(inputs):     encoded_features = []     for feature_name in inputs:         if feature_name in CATEGORICAL_FEATURE_NAMES:             vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]             # Create a lookup to convert a string values to an integer indices.             # Since we are not using a mask token, nor expecting any out of vocabulary             # (oov) token, we set mask_token to None and num_oov_indices to 0.             value_index = inputs[feature_name]             embedding_dims = int(math.sqrt(lookup.vocabulary_size()))             # Create an embedding layer with the specified dimensions.             embedding = layers.Embedding(                 input_dim=lookup.vocabulary_size(), output_dim=embedding_dims             )             # Convert the index values to embedding representations.             encoded_feature = embedding(value_index)         else:             # Use the numerical features as-is.             encoded_feature = inputs[feature_name]             if inputs[feature_name].shape[-1] is None:                 encoded_feature = keras.ops.expand_dims(encoded_feature, -1)          encoded_features.append(encoded_feature)      encoded_features = layers.concatenate(encoded_features)     return encoded_features"},{"path":"https://keras.posit.co/articles/examples/deep_neural_decision_forests.html","id":"deep-neural-decision-tree","dir":"Articles > Examples","previous_headings":"","what":"Deep Neural Decision Tree","title":"Classification with Neural Decision Forests","text":"neural decision tree model two sets weights learn. first set pi, represents probability distribution classes tree leaves. second set weights routing layer decision_fn, represents probability going leave. forward pass model works follows: model expects input features single vector encoding features instance batch. vector can generated Convolution Neural Network (CNN) applied images dense transformations applied structured data features. model first applies used_features_mask randomly select subset input features use. , model computes probabilities (mu) input instances reach tree leaves iteratively performing stochastic routing throughout tree levels. Finally, probabilities reaching leaves combined class probabilities leaves produce final outputs.","code":"class NeuralDecisionTree(keras.Model):     def __init__(self, depth, num_features, used_features_rate, num_classes):         super().__init__()         self.depth = depth         self.num_leaves = 2**depth         self.num_classes = num_classes          # Create a mask for the randomly selected features.         num_used_features = int(num_features * used_features_rate)         one_hot = np.eye(num_features)         sampled_feature_indices = np.random.choice(             np.arange(num_features), num_used_features, replace=False         )         self.used_features_mask = ops.convert_to_tensor(             one_hot[sampled_feature_indices], dtype=_dtype         )          # Initialize the weights of the classes in leaves.         self.pi = self.add_weight(             initializer=\"random_normal\",             shape=[self.num_leaves, self.num_classes],             dtype=_dtype,             trainable=True,         )          # Initialize the stochastic routing layer.         self.decision_fn = layers.Dense(             units=self.num_leaves, activation=\"sigmoid\", name=\"decision\"         )      def call(self, features):         batch_size = ops.shape(features)[0]          # Apply the feature mask to the input features.         features = ops.matmul(             features, ops.transpose(self.used_features_mask)         )  # [batch_size, num_used_features]         # Compute the routing probabilities.         decisions = ops.expand_dims(             self.decision_fn(features), axis=2         )  # [batch_size, num_leaves, 1]         # Concatenate the routing probabilities with their complements.         decisions = layers.concatenate(             [decisions, 1 - decisions], axis=2         )  # [batch_size, num_leaves, 2]          mu = ops.ones([batch_size, 1, 1])          begin_idx = 1         end_idx = 2         # Traverse the tree in breadth-first order.         for level in range(self.depth):             mu = ops.reshape(                 mu, [batch_size, -1, 1]             )  # [batch_size, 2 ** level, 1]             mu = ops.tile(mu, (1, 1, 2))  # [batch_size, 2 ** level, 2]             level_decisions = decisions[                 :, begin_idx:end_idx, :             ]  # [batch_size, 2 ** level, 2]             mu = mu * level_decisions  # [batch_size, 2**level, 2]             begin_idx = end_idx             end_idx = begin_idx + 2 ** (level + 1)          mu = ops.reshape(             mu, [batch_size, self.num_leaves]         )  # [batch_size, num_leaves]         probabilities = keras.activations.softmax(             self.pi         )  # [num_leaves, num_classes]         outputs = ops.matmul(mu, probabilities)  # [batch_size, num_classes]         return outputs"},{"path":"https://keras.posit.co/articles/examples/deep_neural_decision_forests.html","id":"deep-neural-decision-forest","dir":"Articles > Examples","previous_headings":"","what":"Deep Neural Decision Forest","title":"Classification with Neural Decision Forests","text":"neural decision forest model consists set neural decision trees trained simultaneously. output forest model average outputs trees. Finally, let’s set code train evaluate model.","code":"class NeuralDecisionForest(keras.Model):     def __init__(         self, num_trees, depth, num_features, used_features_rate, num_classes     ):         super().__init__()         self.ensemble = []         # Initialize the ensemble by adding NeuralDecisionTree instances.         # Each tree will have its own randomly selected input features to use.         for _ in range(num_trees):             self.ensemble.append(                 NeuralDecisionTree(                     depth, num_features, used_features_rate, num_classes                 )             )      def call(self, inputs):         # Initialize the outputs: a [batch_size, num_classes] matrix of zeros.         batch_size = ops.shape(inputs)[0]         outputs = ops.zeros([batch_size, num_classes])          # Aggregate the outputs of trees in the ensemble.         for tree in self.ensemble:             outputs += tree(inputs)         # Divide the outputs by the ensemble size to get the average.         outputs /= len(self.ensemble)         return outputs learning_rate = 0.01 batch_size = 265 num_epochs = 10   def run_experiment(model):     model.compile(         optimizer=keras.optimizers.Adam(learning_rate=learning_rate),         loss=keras.losses.SparseCategoricalCrossentropy(),         metrics=[keras.metrics.SparseCategoricalAccuracy()],     )      print(\"Start training the model...\")     train_dataset = get_dataset_from_csv(         train_data_file, shuffle=True, batch_size=batch_size     )      model.fit(train_dataset, epochs=num_epochs)     print(\"Model training finished\")      print(\"Evaluating the model on the test data...\")     test_dataset = get_dataset_from_csv(test_data_file, batch_size=batch_size)      _, accuracy = model.evaluate(test_dataset)     print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"},{"path":"https://keras.posit.co/articles/examples/deep_neural_decision_forests.html","id":"experiment-1-train-a-decision-tree-model","dir":"Articles > Examples","previous_headings":"","what":"Experiment 1: train a decision tree model","title":"Classification with Neural Decision Forests","text":"experiment, train single neural decision tree model use input features.","code":"num_trees = 10 depth = 10 used_features_rate = 1.0 num_classes = len(TARGET_LABELS)   def create_tree_model():     inputs = create_model_inputs()     features = encode_inputs(inputs)     features = layers.BatchNormalization()(features)     num_features = features.shape[1]      tree = NeuralDecisionTree(         depth, num_features, used_features_rate, num_classes     )      outputs = tree(features)     model = keras.Model(inputs=inputs, outputs=outputs)     return model   tree_model = create_tree_model() run_experiment(tree_model)"},{"path":"https://keras.posit.co/articles/examples/deep_neural_decision_forests.html","id":"experiment-2-train-a-forest-model","dir":"Articles > Examples","previous_headings":"","what":"Experiment 2: train a forest model","title":"Classification with Neural Decision Forests","text":"experiment, train neural decision forest num_trees trees tree uses randomly selected 50% input features. can control number features used tree setting used_features_rate variable. addition, set depth 5 instead 10 compared previous experiment. can use trained model hosted Hugging Face Hub try demo Hugging Face Spaces.","code":"num_trees = 25 depth = 5 used_features_rate = 0.5   def create_forest_model():     inputs = create_model_inputs()     features = encode_inputs(inputs)     features = layers.BatchNormalization()(features)     num_features = features.shape[1]      forest_model = NeuralDecisionForest(         num_trees, depth, num_features, used_features_rate, num_classes     )      outputs = forest_model(features)     model = keras.Model(inputs=inputs, outputs=outputs)     return model   forest_model = create_forest_model()  run_experiment(forest_model)"},{"path":"https://keras.posit.co/articles/examples/deeplabv3_plus.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Multiclass semantic segmentation using DeepLabV3+","text":"Semantic segmentation, goal assign semantic labels every pixel image, essential computer vision task. example, implement DeepLabV3+ model multi-class semantic segmentation, fully-convolutional architecture performs well semantic segmentation benchmarks.","code":""},{"path":"https://keras.posit.co/articles/examples/deeplabv3_plus.html","id":"references","dir":"Articles > Examples","previous_headings":"Introduction","what":"References:","title":"Multiclass semantic segmentation using DeepLabV3+","text":"Encoder-Decoder Atrous Separable Convolution Semantic Image Segmentation Rethinking Atrous Convolution Semantic Image Segmentation DeepLab: Semantic Image Segmentation Deep Convolutional Nets, Atrous Convolution, Fully Connected CRFs","code":""},{"path":"https://keras.posit.co/articles/examples/deeplabv3_plus.html","id":"downloading-the-data","dir":"Articles > Examples","previous_headings":"","what":"Downloading the data","title":"Multiclass semantic segmentation using DeepLabV3+","text":"use Crowd Instance-level Human Parsing Dataset training model. Crowd Instance-level Human Parsing (CIHP) dataset 38,280 diverse human images. image CIHP labeled pixel-wise annotations 20 categories, well instance-level identification. dataset can used “human part segmentation” task. gdown “1B9A9UCJYMwTL4oBEo4RZfbMZMaZhKJaz&confirm=t” unzip -q instance-level-human-parsing.zip","code":"import keras as keras from keras import layers from keras import ops  import cv2 import numpy as np from glob import glob from scipy.io import loadmat import matplotlib.pyplot as plt from tensorflow import image as tf_image from tensorflow import data as tf_data from tensorflow import io as tf_io"},{"path":"https://keras.posit.co/articles/examples/deeplabv3_plus.html","id":"creating-a-tensorflow-dataset","dir":"Articles > Examples","previous_headings":"","what":"Creating a TensorFlow Dataset","title":"Multiclass semantic segmentation using DeepLabV3+","text":"Training entire CIHP dataset 38,280 images takes lot time, hence using smaller subset 200 images training model example.","code":"IMAGE_SIZE = 512 BATCH_SIZE = 4 NUM_CLASSES = 20 DATA_DIR = (     \"./instance-level_human_parsing/instance-level_human_parsing/Training\" ) NUM_TRAIN_IMAGES = 1000 NUM_VAL_IMAGES = 50  train_images = sorted(glob(os.path.join(DATA_DIR, \"Images/*\")))[     :NUM_TRAIN_IMAGES ] train_masks = sorted(glob(os.path.join(DATA_DIR, \"Category_ids/*\")))[     :NUM_TRAIN_IMAGES ] val_images = sorted(glob(os.path.join(DATA_DIR, \"Images/*\")))[     NUM_TRAIN_IMAGES : NUM_VAL_IMAGES + NUM_TRAIN_IMAGES ] val_masks = sorted(glob(os.path.join(DATA_DIR, \"Category_ids/*\")))[     NUM_TRAIN_IMAGES : NUM_VAL_IMAGES + NUM_TRAIN_IMAGES ]   def read_image(image_path, mask=False):     image = tf_io.read_file(image_path)     if mask:         image = tf_image.decode_png(image, channels=1)         image.set_shape([None, None, 1])         image = tf_image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])     else:         image = tf_image.decode_png(image, channels=3)         image.set_shape([None, None, 3])         image = tf_image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])     return image   def load_data(image_list, mask_list):     image = read_image(image_list)     mask = read_image(mask_list, mask=True)     return image, mask   def data_generator(image_list, mask_list):     dataset = tf_data.Dataset.from_tensor_slices((image_list, mask_list))     dataset = dataset.map(load_data, num_parallel_calls=tf_data.AUTOTUNE)     dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)     return dataset   train_dataset = data_generator(train_images, train_masks) val_dataset = data_generator(val_images, val_masks)  print(\"Train Dataset:\", train_dataset) print(\"Val Dataset:\", val_dataset)"},{"path":"https://keras.posit.co/articles/examples/deeplabv3_plus.html","id":"building-the-deeplabv3-model","dir":"Articles > Examples","previous_headings":"","what":"Building the DeepLabV3+ model","title":"Multiclass semantic segmentation using DeepLabV3+","text":"DeepLabv3+ extends DeepLabv3 adding encoder-decoder structure. encoder module processes multiscale contextual information applying dilated convolution multiple scales, decoder module refines segmentation results along object boundaries.  Dilated convolution: dilated convolution, go deeper network, can keep stride constant larger field--view without increasing number parameters amount computation. Besides, enables larger output feature maps, useful semantic segmentation. reason using Dilated Spatial Pyramid Pooling shown sampling rate becomes larger, number valid filter weights (.e., weights applied valid feature region, instead padded zeros) becomes smaller. encoder features first bilinearly upsampled factor 4, concatenated corresponding low-level features network backbone spatial resolution. example, use ResNet50 pretrained ImageNet backbone model, use low-level features conv4_block6_2_relu block backbone.","code":"def convolution_block(     block_input,     num_filters=256,     kernel_size=3,     dilation_rate=1,     padding=\"same\",     use_bias=False, ):     x = layers.Conv2D(         num_filters,         kernel_size=kernel_size,         dilation_rate=dilation_rate,         padding=\"same\",         use_bias=use_bias,         kernel_initializer=keras.initializers.HeNormal(),     )(block_input)     x = layers.BatchNormalization()(x)     return ops.nn.relu(x)   def DilatedSpatialPyramidPooling(dspp_input):     dims = dspp_input.shape     x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)     x = convolution_block(x, kernel_size=1, use_bias=True)     out_pool = layers.UpSampling2D(         size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]),         interpolation=\"bilinear\",     )(x)      out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)     out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)     out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)     out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)      x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])     output = convolution_block(x, kernel_size=1)     return output def DeeplabV3Plus(image_size, num_classes):     model_input = keras.Input(shape=(image_size, image_size, 3))     preprocessed = keras.applications.resnet50.preprocess_input(model_input)     resnet50 = keras.applications.ResNet50(         weights=\"imagenet\", include_top=False, input_tensor=preprocessed     )     x = resnet50.get_layer(\"conv4_block6_2_relu\").output     x = DilatedSpatialPyramidPooling(x)      input_a = layers.UpSampling2D(         size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),         interpolation=\"bilinear\",     )(x)     input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output     input_b = convolution_block(input_b, num_filters=48, kernel_size=1)      x = layers.Concatenate(axis=-1)([input_a, input_b])     x = convolution_block(x)     x = convolution_block(x)     x = layers.UpSampling2D(         size=(image_size // x.shape[1], image_size // x.shape[2]),         interpolation=\"bilinear\",     )(x)     model_output = layers.Conv2D(         num_classes, kernel_size=(1, 1), padding=\"same\"     )(x)     return keras.Model(inputs=model_input, outputs=model_output)   model = DeeplabV3Plus(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES) model.summary()"},{"path":"https://keras.posit.co/articles/examples/deeplabv3_plus.html","id":"training","dir":"Articles > Examples","previous_headings":"","what":"Training","title":"Multiclass semantic segmentation using DeepLabV3+","text":"train model using sparse categorical crossentropy loss function, Adam optimizer.","code":"loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True) model.compile(     optimizer=keras.optimizers.Adam(learning_rate=0.001),     loss=loss,     metrics=[\"accuracy\"], )  history = model.fit(train_dataset, validation_data=val_dataset, epochs=25)  plt.plot(history.history[\"loss\"]) plt.title(\"Training Loss\") plt.ylabel(\"loss\") plt.xlabel(\"epoch\") plt.show()  plt.plot(history.history[\"accuracy\"]) plt.title(\"Training Accuracy\") plt.ylabel(\"accuracy\") plt.xlabel(\"epoch\") plt.show()  plt.plot(history.history[\"val_loss\"]) plt.title(\"Validation Loss\") plt.ylabel(\"val_loss\") plt.xlabel(\"epoch\") plt.show()  plt.plot(history.history[\"val_accuracy\"]) plt.title(\"Validation Accuracy\") plt.ylabel(\"val_accuracy\") plt.xlabel(\"epoch\") plt.show()"},{"path":"https://keras.posit.co/articles/examples/deeplabv3_plus.html","id":"inference-using-colormap-overlay","dir":"Articles > Examples","previous_headings":"","what":"Inference using Colormap Overlay","title":"Multiclass semantic segmentation using DeepLabV3+","text":"raw predictions model represent one-hot encoded tensor shape (N, 512, 512, 20) one 20 channels binary mask corresponding predicted label. order visualize results, plot RGB segmentation masks pixel represented unique color corresponding particular label predicted. can easily find color corresponding label human_colormap.mat file provided part dataset. also plot overlay RGB segmentation mask input image helps us identify different categories present image intuitively.","code":"# Loading the Colormap colormap = loadmat(     \"./instance-level_human_parsing/instance-level_human_parsing/human_colormap.mat\" )[\"colormap\"] colormap = colormap * 100 colormap = colormap.astype(np.uint8)   def infer(model, image_tensor):     predictions = model.predict(np.expand_dims((image_tensor), axis=0))     predictions = np.squeeze(predictions)     predictions = np.argmax(predictions, axis=2)     return predictions   def decode_segmentation_masks(mask, colormap, n_classes):     r = np.zeros_like(mask).astype(np.uint8)     g = np.zeros_like(mask).astype(np.uint8)     b = np.zeros_like(mask).astype(np.uint8)     for l in range(0, n_classes):         idx = mask == l         r[idx] = colormap[l, 0]         g[idx] = colormap[l, 1]         b[idx] = colormap[l, 2]     rgb = np.stack([r, g, b], axis=2)     return rgb   def get_overlay(image, colored_mask):     image = keras.utils.array_to_img(image)     image = np.array(image).astype(np.uint8)     overlay = cv2.addWeighted(image, 0.35, colored_mask, 0.65, 0)     return overlay   def plot_samples_matplotlib(display_list, figsize=(5, 3)):     _, axes = plt.subplots(nrows=1, ncols=len(display_list), figsize=figsize)     for i in range(len(display_list)):         if display_list[i].shape[-1] == 3:             axes[i].imshow(keras.utils.array_to_img(display_list[i]))         else:             axes[i].imshow(display_list[i])     plt.show()   def plot_predictions(images_list, colormap, model):     for image_file in images_list:         image_tensor = read_image(image_file)         prediction_mask = infer(image_tensor=image_tensor, model=model)         prediction_colormap = decode_segmentation_masks(             prediction_mask, colormap, 20         )         overlay = get_overlay(image_tensor, prediction_colormap)         plot_samples_matplotlib(             [image_tensor, overlay, prediction_colormap], figsize=(18, 14)         )"},{"path":"https://keras.posit.co/articles/examples/deeplabv3_plus.html","id":"inference-on-train-images","dir":"Articles > Examples","previous_headings":"Inference using Colormap Overlay","what":"Inference on Train Images","title":"Multiclass semantic segmentation using DeepLabV3+","text":"","code":"plot_predictions(train_images[:4], colormap, model=model)"},{"path":"https://keras.posit.co/articles/examples/deeplabv3_plus.html","id":"inference-on-validation-images","dir":"Articles > Examples","previous_headings":"Inference using Colormap Overlay","what":"Inference on Validation Images","title":"Multiclass semantic segmentation using DeepLabV3+","text":"can use trained model hosted Hugging Face Hub try demo Hugging Face Spaces.","code":"plot_predictions(val_images[:4], colormap, model=model)"},{"path":"https://keras.posit.co/articles/examples/deit.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Distilling Vision Transformers","text":"original Vision Transformers (ViT) paper (Dosovitskiy et al.), authors concluded perform par Convolutional Neural Networks (CNNs), ViTs need pre-trained larger datasets. larger better. mainly due lack inductive biases ViT architecture – unlike CNNs, don’t layers exploit locality. follow-paper (Steiner et al.), authors show possible substantially improve performance ViTs stronger regularization longer training. Many groups proposed different ways deal problem data-intensiveness ViT training. One way shown Data-efficient image Transformers, (DeiT) paper (Touvron et al.). authors introduced distillation technique specific transformer-based vision models. DeiT among first works show ’s possible train ViTs well without using larger datasets. example, implement distillation recipe proposed DeiT. requires us slightly tweak original ViT architecture write custom training loop implement distillation recipe. comfortably navigate example, ’ll expected know ViT knowledge distillation work. following good resources case needed refresher: ViT keras.io Knowledge distillation keras.io","code":""},{"path":"https://keras.posit.co/articles/examples/deit.html","id":"imports","dir":"Articles > Examples","previous_headings":"","what":"Imports","title":"Distilling Vision Transformers","text":"","code":"import os  os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  import tensorflow as tf import tensorflow_datasets as tfds import keras as keras from keras import layers  tfds.disable_progress_bar() keras.utils.set_random_seed(42)"},{"path":"https://keras.posit.co/articles/examples/deit.html","id":"constants","dir":"Articles > Examples","previous_headings":"","what":"Constants","title":"Distilling Vision Transformers","text":"probably noticed DROPOUT_RATE set 0.0. Dropout used implementation keep complete. smaller models (like one used example), don’t need , bigger models, using dropout helps.","code":"# Model MODEL_TYPE = \"deit_distilled_tiny_patch16_224\" RESOLUTION = 224 PATCH_SIZE = 16 NUM_PATCHES = (RESOLUTION // PATCH_SIZE) ** 2 LAYER_NORM_EPS = 1e-6 PROJECTION_DIM = 192 NUM_HEADS = 3 NUM_LAYERS = 12 MLP_UNITS = [     PROJECTION_DIM * 4,     PROJECTION_DIM, ] DROPOUT_RATE = 0.0 DROP_PATH_RATE = 0.1  # Training NUM_EPOCHS = 20 BASE_LR = 0.0005 WEIGHT_DECAY = 0.0001  # Data BATCH_SIZE = 256 AUTO = tf.data.AUTOTUNE NUM_CLASSES = 5"},{"path":"https://keras.posit.co/articles/examples/deit.html","id":"load-the-tf_flowers-dataset-and-prepare-preprocessing-utilities","dir":"Articles > Examples","previous_headings":"","what":"Load the tf_flowers dataset and prepare preprocessing utilities","title":"Distilling Vision Transformers","text":"authors use array different augmentation techniques, including MixUp (Zhang et al.), RandAugment (Cubuk et al.), . However, keep example simple work , ’ll discard .","code":"def preprocess_dataset(is_training=True):     def fn(image, label):         if is_training:             # Resize to a bigger spatial resolution and take the random             # crops.             image = tf.image.resize(image, (RESOLUTION + 20, RESOLUTION + 20))             image = tf.image.random_crop(image, (RESOLUTION, RESOLUTION, 3))             image = tf.image.random_flip_left_right(image)         else:             image = tf.image.resize(image, (RESOLUTION, RESOLUTION))         label = tf.one_hot(label, depth=NUM_CLASSES)         return image, label      return fn   def prepare_dataset(dataset, is_training=True):     if is_training:         dataset = dataset.shuffle(BATCH_SIZE * 10)     dataset = dataset.map(         preprocess_dataset(is_training), num_parallel_calls=AUTO     )     return dataset.batch(BATCH_SIZE).prefetch(AUTO)   train_dataset, val_dataset = tfds.load(     \"tf_flowers\", split=[\"train[:90%]\", \"train[90%:]\"], as_supervised=True ) num_train = train_dataset.cardinality() num_val = val_dataset.cardinality() print(f\"Number of training examples: {num_train}\") print(f\"Number of validation examples: {num_val}\")  train_dataset = prepare_dataset(train_dataset, is_training=True) val_dataset = prepare_dataset(val_dataset, is_training=False)"},{"path":"https://keras.posit.co/articles/examples/deit.html","id":"implementing-the-deit-variants-of-vit","dir":"Articles > Examples","previous_headings":"","what":"Implementing the DeiT variants of ViT","title":"Distilling Vision Transformers","text":"Since DeiT extension ViT ’d make sense first implement ViT extend support DeiT’s components. First, ’ll implement layer Stochastic Depth (Huang et al.) used DeiT regularization. Now, ’ll implement MLP Transformer blocks. ’ll now implement ViTClassifier class building top components just developed. ’ll following original pooling strategy used ViT paper – use class token use feature representations corresponding classification. class can used standalone ViT end--end trainable. Just remove distilled phrase MODEL_TYPE work vit_tiny = ViTClassifier(). Let’s now extend DeiT. following figure presents schematic DeiT (taken DeiT paper):  Apart class token, DeiT another token distillation. distillation, logits corresponding class token compared true labels, logits corresponding distillation token compared teacher’s predictions. Let’s verify ViTDistilled class can initialized called expected.","code":"# Referred from: github.com:rwightman/pytorch-image-models. class StochasticDepth(layers.Layer):     def __init__(self, drop_prop, **kwargs):         super().__init__(**kwargs)         self.drop_prob = drop_prop      def call(self, x, training=True):         if training:             keep_prob = 1 - self.drop_prob             shape = (tf.shape(x)[0],) + (1,) * (len(x.shape) - 1)             random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)             random_tensor = tf.floor(random_tensor)             return (x / keep_prob) * random_tensor         return x def mlp(x, dropout_rate: float, hidden_units):     \"\"\"FFN for a Transformer block.\"\"\"     # Iterate over the hidden units and     # add Dense => Dropout.     for idx, units in enumerate(hidden_units):         x = layers.Dense(             units,             activation=tf.nn.gelu if idx == 0 else None,         )(x)         x = layers.Dropout(dropout_rate)(x)     return x   def transformer(drop_prob: float, name: str) -> keras.Model:     \"\"\"Transformer block with pre-norm.\"\"\"     num_patches = (         NUM_PATCHES + 2 if \"distilled\" in MODEL_TYPE else NUM_PATCHES + 1     )     encoded_patches = layers.Input((num_patches, PROJECTION_DIM))      # Layer normalization 1.     x1 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(encoded_patches)      # Multi Head Self Attention layer 1.     attention_output = layers.MultiHeadAttention(         num_heads=NUM_HEADS,         key_dim=PROJECTION_DIM,         dropout=DROPOUT_RATE,     )(x1, x1)     attention_output = (         StochasticDepth(drop_prob)(attention_output)         if drop_prob         else attention_output     )      # Skip connection 1.     x2 = layers.Add()([attention_output, encoded_patches])      # Layer normalization 2.     x3 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(x2)      # MLP layer 1.     x4 = mlp(x3, hidden_units=MLP_UNITS, dropout_rate=DROPOUT_RATE)     x4 = StochasticDepth(drop_prob)(x4) if drop_prob else x4      # Skip connection 2.     outputs = layers.Add()([x2, x4])      return keras.Model(encoded_patches, outputs, name=name) class ViTClassifier(keras.Model):     \"\"\"Vision Transformer base class.\"\"\"      def __init__(self, **kwargs):         super().__init__(**kwargs)          # Patchify + linear projection + reshaping.         self.projection = keras.Sequential(             [                 layers.Conv2D(                     filters=PROJECTION_DIM,                     kernel_size=(PATCH_SIZE, PATCH_SIZE),                     strides=(PATCH_SIZE, PATCH_SIZE),                     padding=\"VALID\",                     name=\"conv_projection\",                 ),                 layers.Reshape(                     target_shape=(NUM_PATCHES, PROJECTION_DIM),                     name=\"flatten_projection\",                 ),             ],             name=\"projection\",         )          # Positional embedding.         init_shape = (             1,             NUM_PATCHES + 1,             PROJECTION_DIM,         )         self.positional_embedding = tf.Variable(             tf.zeros(init_shape), name=\"pos_embedding\"         )          # Transformer blocks.         dpr = [x for x in tf.linspace(0.0, DROP_PATH_RATE, NUM_LAYERS)]         self.transformer_blocks = [             transformer(drop_prob=dpr[i], name=f\"transformer_block_{i}\")             for i in range(NUM_LAYERS)         ]          # CLS token.         initial_value = tf.zeros((1, 1, PROJECTION_DIM))         self.cls_token = tf.Variable(             initial_value=initial_value, trainable=True, name=\"cls\"         )          # Other layers.         self.dropout = layers.Dropout(DROPOUT_RATE)         self.layer_norm = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)         self.head = layers.Dense(             NUM_CLASSES,             name=\"classification_head\",         )      def call(self, inputs):         n = tf.shape(inputs)[0]          # Create patches and project the patches.         projected_patches = self.projection(inputs)          # Append class token if needed.         cls_token = tf.tile(self.cls_token, (n, 1, 1))         cls_token = tf.cast(cls_token, projected_patches.dtype)         projected_patches = tf.concat([cls_token, projected_patches], axis=1)          # Add positional embeddings to the projected patches.         encoded_patches = (             self.positional_embedding + projected_patches         )  # (B, number_patches, projection_dim)         encoded_patches = self.dropout(encoded_patches)          # Iterate over the number of layers and stack up blocks of         # Transformer.         for transformer_module in self.transformer_blocks:             # Add a Transformer block.             encoded_patches = transformer_module(encoded_patches)          # Final layer normalization.         representation = self.layer_norm(encoded_patches)          # Pool representation.         encoded_patches = representation[:, 0]          # Classification head.         output = self.head(encoded_patches)         return output class ViTDistilled(ViTClassifier):     def __init__(self, regular_training=False, **kwargs):         super().__init__(**kwargs)         self.num_tokens = 2         self.regular_training = regular_training          # CLS and distillation tokens, positional embedding.         init_value = tf.zeros((1, 1, PROJECTION_DIM))         self.dist_token = tf.Variable(init_value, name=\"dist_token\")         self.positional_embedding = tf.Variable(             tf.zeros(                 (                     1,                     NUM_PATCHES + self.num_tokens,                     PROJECTION_DIM,                 )             ),             name=\"pos_embedding\",         )          # Head layers.         self.head = layers.Dense(             NUM_CLASSES,             name=\"classification_head\",         )         self.head_dist = layers.Dense(             NUM_CLASSES,             name=\"distillation_head\",         )      def call(self, inputs, training=False):         n = tf.shape(inputs)[0]          # Create patches and project the patches.         projected_patches = self.projection(inputs)          # Append the tokens.         cls_token = tf.tile(self.cls_token, (n, 1, 1))         dist_token = tf.tile(self.dist_token, (n, 1, 1))         cls_token = tf.cast(cls_token, projected_patches.dtype)         dist_token = tf.cast(dist_token, projected_patches.dtype)         projected_patches = tf.concat(             [cls_token, dist_token, projected_patches], axis=1         )          # Add positional embeddings to the projected patches.         encoded_patches = (             self.positional_embedding + projected_patches         )  # (B, number_patches, projection_dim)         encoded_patches = self.dropout(encoded_patches)          # Iterate over the number of layers and stack up blocks of         # Transformer.         for transformer_module in self.transformer_blocks:             # Add a Transformer block.             encoded_patches = transformer_module(encoded_patches)          # Final layer normalization.         representation = self.layer_norm(encoded_patches)          # Classification heads.         x, x_dist = (             self.head(representation[:, 0]),             self.head_dist(representation[:, 1]),         )          if not training or self.regular_training:             # During standard train / finetune, inference average the classifier             # predictions.             return (x + x_dist) / 2          elif training:             # Only return separate classification predictions when training in distilled             # mode.             return x, x_dist deit_tiny_distilled = ViTDistilled()  dummy_inputs = tf.ones((2, 224, 224, 3)) outputs = deit_tiny_distilled(dummy_inputs, training=False) print(f\"output_shape: {outputs.shape}\")"},{"path":"https://keras.posit.co/articles/examples/deit.html","id":"implementing-the-trainer","dir":"Articles > Examples","previous_headings":"","what":"Implementing the trainer","title":"Distilling Vision Transformers","text":"Unlike happens standard knowledge distillation (Hinton et al.), temperature-scaled softmax used well KL divergence, DeiT authors use following loss function:  , CE cross-entropy psi softmax function Z_s denotes student predictions y denotes true labels y_t denotes teacher predictions","code":"class DeiT(keras.Model):     # Reference:     # https://keras.io/examples/vision/knowledge_distillation/     def __init__(self, student, teacher, **kwargs):         super().__init__(**kwargs)         self.student = student         self.teacher = teacher         self.student_loss_tracker = keras.metrics.Mean(name=\"student_loss\")         self.distillation_loss_tracker = keras.metrics.Mean(             name=\"distillation_loss\"         )         self.accuracy = keras.metrics.CategoricalAccuracy(name=\"accuracy\")      @property     def metrics(self):         return [             self.accuracy,             self.student_loss_tracker,             self.distillation_loss_tracker,         ]      def compile(         self,         optimizer,         student_loss_fn,         distillation_loss_fn,         run_eagerly=False,         jit_compile=False,     ):         super().compile(             optimizer=optimizer,             run_eagerly=run_eagerly,             jit_compile=jit_compile,         )         self.student_loss_fn = student_loss_fn         self.distillation_loss_fn = distillation_loss_fn      def train_step(self, data):         # Unpack data.         x, y = data          # Forward pass of teacher         teacher_predictions = self.teacher(x)[\"dense\"]         teacher_predictions = tf.nn.softmax(teacher_predictions, axis=-1)          with tf.GradientTape() as tape:             # Forward pass of student.             cls_predictions, dist_predictions = self.student(                 x / 255.0, training=True             )              # Compute losses.             student_loss = self.student_loss_fn(y, cls_predictions)             distillation_loss = self.distillation_loss_fn(                 teacher_predictions, dist_predictions             )             loss = (student_loss + distillation_loss) / 2          # Compute gradients.         trainable_vars = self.student.trainable_variables         gradients = tape.gradient(loss, trainable_vars)          # Update weights.         self.optimizer.apply_gradients(zip(gradients, trainable_vars))          # Update the metrics configured in `compile()`.         student_predictions = (cls_predictions + dist_predictions) / 2         self.student_loss_tracker.update_state(student_loss)         self.distillation_loss_tracker.update_state(distillation_loss)         self.accuracy.update_state(y, student_predictions)          # Return a dict of performance.         return {m.name: m.result() for m in self.metrics}      def test_step(self, data):         # Unpack the data.         x, y = data          # Compute predictions.         y_prediction = self.student(x / 255.0)          # Calculate the loss.         student_loss = self.student_loss_fn(y, y_prediction)          # Update the metrics.         self.student_loss_tracker.update_state(student_loss)         self.accuracy.update_state(y, y_prediction)          # Return a dict of performance.         results = {m.name: m.result() for m in self.metrics}         return results      def call(self, inputs):         return self.student(inputs / 255.0)"},{"path":"https://keras.posit.co/articles/examples/deit.html","id":"load-the-teacher-model","dir":"Articles > Examples","previous_headings":"","what":"Load the teacher model","title":"Distilling Vision Transformers","text":"model based BiT family ResNets (Kolesnikov et al.) fine-tuned tf_flowers dataset. can refer notebook know training performed. teacher model 212 Million parameters 40x student. wget -q https://github.com/sayakpaul/deit-tf/releases/download/v0.1.0/bit_teacher_flowers.zip unzip -q bit_teacher_flowers.zip","code":"bit_teacher_flowers = keras.layers.TFSMLayer(     filepath=\"bit_teacher_flowers\",     call_endpoint=\"serving_default\", )"},{"path":"https://keras.posit.co/articles/examples/deit.html","id":"training-through-distillation","dir":"Articles > Examples","previous_headings":"","what":"Training through distillation","title":"Distilling Vision Transformers","text":"trained model (ViTClassifier) scratch exact hyperparameters, model scored 59% accuracy. can adapt following code reproduce result:","code":"deit_tiny = ViTDistilled() deit_distiller = DeiT(student=deit_tiny, teacher=bit_teacher_flowers)  lr_scaled = (BASE_LR / 512) * BATCH_SIZE deit_distiller.compile(     optimizer=keras.optimizers.AdamW(         weight_decay=WEIGHT_DECAY, learning_rate=lr_scaled     ),     student_loss_fn=keras.losses.CategoricalCrossentropy(         from_logits=True, label_smoothing=0.1     ),     distillation_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=True), ) _ = deit_distiller.fit(     train_dataset, validation_data=val_dataset, epochs=NUM_EPOCHS ) vit_tiny = ViTClassifier()  inputs = keras.Input((RESOLUTION, RESOLUTION, 3)) x = keras.layers.Rescaling(scale=1./255)(inputs) outputs = deit_tiny(x) model = keras.Model(inputs, outputs)  model.compile(...) model.fit(...)"},{"path":"https://keras.posit.co/articles/examples/deit.html","id":"notes","dir":"Articles > Examples","previous_headings":"","what":"Notes","title":"Distilling Vision Transformers","text":"use distillation, ’re effectively transferring inductive biases CNN-based teacher model. Interestingly enough, distillation strategy works better CNN teacher model rather Transformer shown paper. use regularization train DeiT models important. ViT models initialized combination different initializers including truncated normal, random normal, Glorot uniform, etc. ’re looking end--end reproduction original results, don’t forget initialize ViTs well. want explore pre-trained DeiT models TensorFlow Keras code fine-tuning, check models TF-Hub.","code":""},{"path":"https://keras.posit.co/articles/examples/deit.html","id":"acknowledgements","dir":"Articles > Examples","previous_headings":"","what":"Acknowledgements","title":"Distilling Vision Transformers","text":"Ross Wightman keeping timm updated readable implementations. referred implementations ViT DeiT lot implementing TensorFlow. Aritra Roy Gosthipaty implemented portions ViTClassifier another project. Google Developers Experts program supporting GCP credits used run experiments example.","code":""},{"path":"https://keras.posit.co/articles/examples/eanet.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Image classification with EANet (External Attention Transformer)","text":"example implements EANet model image classification, demonstrates CIFAR-100 dataset. EANet introduces novel attention mechanism named external attention, based two external, small, learnable, shared memories, can implemented easily simply using two cascaded linear layers two normalization layers. conveniently replaces self-attention used existing architectures. External attention linear complexity, implicitly considers correlations samples.","code":""},{"path":"https://keras.posit.co/articles/examples/eanet.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Image classification with EANet (External Attention Transformer)","text":"","code":"import keras as keras from keras import layers from keras import ops  import matplotlib.pyplot as plt"},{"path":"https://keras.posit.co/articles/examples/eanet.html","id":"prepare-the-data","dir":"Articles > Examples","previous_headings":"","what":"Prepare the data","title":"Image classification with EANet (External Attention Transformer)","text":"","code":"num_classes = 100 input_shape = (32, 32, 3)  (x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data() y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\") print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"},{"path":"https://keras.posit.co/articles/examples/eanet.html","id":"configure-the-hyperparameters","dir":"Articles > Examples","previous_headings":"","what":"Configure the hyperparameters","title":"Image classification with EANet (External Attention Transformer)","text":"","code":"weight_decay = 0.0001 learning_rate = 0.001 label_smoothing = 0.1 validation_split = 0.2 batch_size = 128 num_epochs = 50 patch_size = 2  # Size of the patches to be extracted from the input images. num_patches = (input_shape[0] // patch_size) ** 2  # Number of patch embedding_dim = 64  # Number of hidden units. mlp_dim = 64 dim_coefficient = 4 num_heads = 4 attention_dropout = 0.2 projection_dropout = 0.2 num_transformer_blocks = 8  # Number of repetitions of the transformer layer  print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \") print(f\"Patches per image: {num_patches}\")"},{"path":"https://keras.posit.co/articles/examples/eanet.html","id":"use-data-augmentation","dir":"Articles > Examples","previous_headings":"","what":"Use data augmentation","title":"Image classification with EANet (External Attention Transformer)","text":"","code":"data_augmentation = keras.Sequential(     [         layers.Normalization(),         layers.RandomFlip(\"horizontal\"),         layers.RandomRotation(factor=0.1),         layers.RandomContrast(factor=0.1),         layers.RandomZoom(height_factor=0.2, width_factor=0.2),     ],     name=\"data_augmentation\", ) # Compute the mean and the variance of the training data for normalization. data_augmentation.layers[0].adapt(x_train)"},{"path":"https://keras.posit.co/articles/examples/eanet.html","id":"implement-the-patch-extraction-and-encoding-layer","dir":"Articles > Examples","previous_headings":"","what":"Implement the patch extraction and encoding layer","title":"Image classification with EANet (External Attention Transformer)","text":"","code":"class PatchExtract(layers.Layer):     def __init__(self, patch_size, **kwargs):         super().__init__(**kwargs)         self.patch_size = patch_size      def call(self, x):         B, C = ops.shape(x)[0], ops.shape(x)[-1]         x = ops.image.extract_patches(x, self.patch_size)         x = ops.reshape(x, (B, -1, self.patch_size * self.patch_size * C))         return x   class PatchEmbedding(layers.Layer):     def __init__(self, num_patch, embed_dim, **kwargs):         super().__init__(**kwargs)         self.num_patch = num_patch         self.proj = layers.Dense(embed_dim)         self.pos_embed = layers.Embedding(             input_dim=num_patch, output_dim=embed_dim         )      def call(self, patch):         pos = ops.arange(start=0, stop=self.num_patch, step=1)         return self.proj(patch) + self.pos_embed(pos)"},{"path":"https://keras.posit.co/articles/examples/eanet.html","id":"implement-the-external-attention-block","dir":"Articles > Examples","previous_headings":"","what":"Implement the external attention block","title":"Image classification with EANet (External Attention Transformer)","text":"","code":"def external_attention(     x,     dim,     num_heads,     dim_coefficient=4,     attention_dropout=0,     projection_dropout=0, ):     _, num_patch, channel = x.shape     assert dim % num_heads == 0     num_heads = num_heads * dim_coefficient      x = layers.Dense(dim * dim_coefficient)(x)     # create tensor [batch_size, num_patches, num_heads, dim*dim_coefficient//num_heads]     x = ops.reshape(         x, (-1, num_patch, num_heads, dim * dim_coefficient // num_heads)     )     x = ops.transpose(x, axes=[0, 2, 1, 3])     # a linear layer M_k     attn = layers.Dense(dim // dim_coefficient)(x)     # normalize attention map     attn = layers.Softmax(axis=2)(attn)     # dobule-normalization     attn = layers.Lambda(         lambda attn: ops.divide(             attn,             ops.convert_to_tensor(1e-9) + ops.sum(attn, axis=-1, keepdims=True),         )     )(attn)     attn = layers.Dropout(attention_dropout)(attn)     # a linear layer M_v     x = layers.Dense(dim * dim_coefficient // num_heads)(attn)     x = ops.transpose(x, axes=[0, 2, 1, 3])     x = ops.reshape(x, [-1, num_patch, dim * dim_coefficient])     # a linear layer to project original dim     x = layers.Dense(dim)(x)     x = layers.Dropout(projection_dropout)(x)     return x"},{"path":"https://keras.posit.co/articles/examples/eanet.html","id":"implement-the-mlp-block","dir":"Articles > Examples","previous_headings":"","what":"Implement the MLP block","title":"Image classification with EANet (External Attention Transformer)","text":"","code":"def mlp(x, embedding_dim, mlp_dim, drop_rate=0.2):     x = layers.Dense(mlp_dim, activation=ops.gelu)(x)     x = layers.Dropout(drop_rate)(x)     x = layers.Dense(embedding_dim)(x)     x = layers.Dropout(drop_rate)(x)     return x"},{"path":"https://keras.posit.co/articles/examples/eanet.html","id":"implement-the-transformer-block","dir":"Articles > Examples","previous_headings":"","what":"Implement the Transformer block","title":"Image classification with EANet (External Attention Transformer)","text":"","code":"def transformer_encoder(     x,     embedding_dim,     mlp_dim,     num_heads,     dim_coefficient,     attention_dropout,     projection_dropout,     attention_type=\"external_attention\", ):     residual_1 = x     x = layers.LayerNormalization(epsilon=1e-5)(x)     if attention_type == \"external_attention\":         x = external_attention(             x,             embedding_dim,             num_heads,             dim_coefficient,             attention_dropout,             projection_dropout,         )     elif attention_type == \"self_attention\":         x = layers.MultiHeadAttention(             num_heads=num_heads,             key_dim=embedding_dim,             dropout=attention_dropout,         )(x, x)     x = layers.add([x, residual_1])     residual_2 = x     x = layers.LayerNormalization(epsilon=1e-5)(x)     x = mlp(x, embedding_dim, mlp_dim)     x = layers.add([x, residual_2])     return x"},{"path":"https://keras.posit.co/articles/examples/eanet.html","id":"implement-the-eanet-model","dir":"Articles > Examples","previous_headings":"","what":"Implement the EANet model","title":"Image classification with EANet (External Attention Transformer)","text":"EANet model leverages external attention. computational complexity traditional self attention O(d * N ** 2), d embedding size, N number patch. authors find pixels closely related just pixels, N--N attention matrix may redundant. , propose alternative external attention module computational complexity external attention O(d * S * N). d S hyper-parameters, proposed algorithm linear number pixels. fact, equivalent drop patch operation, lot information contained patch image redundant unimportant.","code":"def get_model(attention_type=\"external_attention\"):     inputs = layers.Input(shape=input_shape)     # Image augment     x = data_augmentation(inputs)     # Extract patches.     x = PatchExtract(patch_size)(x)     # Create patch embedding.     x = PatchEmbedding(num_patches, embedding_dim)(x)     # Create Transformer block.     for _ in range(num_transformer_blocks):         x = transformer_encoder(             x,             embedding_dim,             mlp_dim,             num_heads,             dim_coefficient,             attention_dropout,             projection_dropout,             attention_type,         )      x = layers.GlobalAveragePooling1D()(x)     outputs = layers.Dense(num_classes, activation=\"softmax\")(x)     model = keras.Model(inputs=inputs, outputs=outputs)     return model"},{"path":"https://keras.posit.co/articles/examples/eanet.html","id":"train-on-cifar-100","dir":"Articles > Examples","previous_headings":"","what":"Train on CIFAR-100","title":"Image classification with EANet (External Attention Transformer)","text":"","code":"model = get_model(attention_type=\"external_attention\")  model.compile(     loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),     optimizer=keras.optimizers.AdamW(         learning_rate=learning_rate, weight_decay=weight_decay     ),     metrics=[         keras.metrics.CategoricalAccuracy(name=\"accuracy\"),         keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),     ], )  history = model.fit(     x_train,     y_train,     batch_size=batch_size,     epochs=num_epochs,     validation_split=validation_split, )"},{"path":"https://keras.posit.co/articles/examples/eanet.html","id":"lets-visualize-the-training-progress-of-the-model-","dir":"Articles > Examples","previous_headings":"Train on CIFAR-100","what":"Let’s visualize the training progress of the model.","title":"Image classification with EANet (External Attention Transformer)","text":"","code":"plt.plot(history.history[\"loss\"], label=\"train_loss\") plt.plot(history.history[\"val_loss\"], label=\"val_loss\") plt.xlabel(\"Epochs\") plt.ylabel(\"Loss\") plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14) plt.legend() plt.grid() plt.show()"},{"path":"https://keras.posit.co/articles/examples/eanet.html","id":"lets-display-the-final-results-of-the-test-on-cifar-100-","dir":"Articles > Examples","previous_headings":"Train on CIFAR-100","what":"Let’s display the final results of the test on CIFAR-100.","title":"Image classification with EANet (External Attention Transformer)","text":"EANet just replaces self attention Vit external attention. traditional Vit achieved ~73% test top-5 accuracy ~41 top-1 accuracy training 50 epochs, 0.6M parameters. experimental environment hyperparameters, EANet model just trained just 0.3M parameters, gets us ~73% test top-5 accuracy ~43% top-1 accuracy. fully demonstrates effectiveness external attention. show training process EANet, can train Vit experimental conditions observe test results.","code":"loss, accuracy, top_5_accuracy = model.evaluate(x_test, y_test) print(f\"Test loss: {round(loss, 2)}\") print(f\"Test accuracy: {round(accuracy * 100, 2)}%\") print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")"},{"path":"https://keras.posit.co/articles/examples/eeg_signal_classification.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Electroencephalogram Signal Classification for action identification","text":"following example explores can make Convolution-based Neural Network perform classification Electroencephalogram signals captured subjects exposed different stimuli. train model scratch since signal-classification models fairly scarce pre-trained format. data use sourced UC Berkeley-Biosense Lab data collected 15 subjects time. process follows: Load UC Berkeley-Biosense Synchronized Brainwave Dataset Visualize random samples data Pre-process, collate scale data finally make tf.data.Dataset Prepare class weights order tackle major imbalances Create Conv1D Dense-based model perform classification Define callbacks hyperparameters Train model Plot metrics History perform evaluation example needs following external dependencies (Gdown, Scikit-learn, Pandas, Numpy, Matplotlib). can install via following commands. Gdown external package used download large files Google Drive. know , can refer PyPi page ","code":""},{"path":"https://keras.posit.co/articles/examples/eeg_signal_classification.html","id":"setup-and-data-downloads","dir":"Articles > Examples","previous_headings":"","what":"Setup and Data Downloads","title":"Electroencephalogram Signal Classification for action identification","text":"First, lets install dependencies: pip install gdown -q pip install sklearn -q pip install pandas -q pip install numpy -q pip install matplotlib -q Next, lets download dataset. gdown package makes easy download data Google Drive: gdown 1V5B7Bt6aJm0UHbR7cRKBEK8jx7lYPVuX # gdown download eeg-data.csv onto local drive use. Total size # eeg-data.csv 105.7 MB","code":"import pandas as pd import matplotlib.pyplot as plt import json import numpy as np import keras as keras from keras import layers import tensorflow as tf from sklearn import preprocessing, model_selection import random  QUALITY_THRESHOLD = 128 BATCH_SIZE = 64 SHUFFLE_BUFFER_SIZE = BATCH_SIZE * 2"},{"path":"https://keras.posit.co/articles/examples/eeg_signal_classification.html","id":"read-data-from-eeg-data-csv","dir":"Articles > Examples","previous_headings":"","what":"Read data from eeg-data.csv","title":"Electroencephalogram Signal Classification for action identification","text":"use Pandas library read eeg-data.csv file display first 5 rows using .head() command remove unlabeled samples dataset contribute model. also perform .drop() operation columns required training data preparation data, samples recorded given score 0 128 based well-calibrated sensor (0 best, 200 worst). filter values based arbitrary cutoff limit 128.","code":"eeg = pd.read_csv(\"eeg-data.csv\") unlabeled_eeg = eeg[eeg[\"label\"] == \"unlabeled\"] eeg = eeg.loc[eeg[\"label\"] != \"unlabeled\"] eeg = eeg.loc[eeg[\"label\"] != \"everyone paired\"]  eeg.drop(     [         \"indra_time\",         \"Unnamed: 0\",         \"browser_latency\",         \"reading_time\",         \"attention_esense\",         \"meditation_esense\",         \"updatedAt\",         \"createdAt\",     ],     axis=1,     inplace=True, )  eeg.reset_index(drop=True, inplace=True) eeg.head() def convert_string_data_to_values(value_string):     str_list = json.loads(value_string)     return str_list   eeg[\"raw_values\"] = eeg[\"raw_values\"].apply(convert_string_data_to_values)  eeg = eeg.loc[eeg[\"signal_quality\"] < QUALITY_THRESHOLD] print(eeg.shape) eeg.head()"},{"path":"https://keras.posit.co/articles/examples/eeg_signal_classification.html","id":"visualize-one-random-sample-from-the-data","dir":"Articles > Examples","previous_headings":"","what":"Visualize one random sample from the data","title":"Electroencephalogram Signal Classification for action identification","text":"visualize one sample data understand stimulus-induced signal looks like","code":"def view_eeg_plot(idx):     data = eeg.loc[idx, \"raw_values\"]     plt.plot(data)     plt.title(f\"Sample random plot\")     plt.show()   view_eeg_plot(7)"},{"path":"https://keras.posit.co/articles/examples/eeg_signal_classification.html","id":"pre-process-and-collate-data","dir":"Articles > Examples","previous_headings":"","what":"Pre-process and collate data","title":"Electroencephalogram Signal Classification for action identification","text":"total 67 different labels present data, numbered sub-labels. collate single label per numbering replace data . Following process, perform simple Label encoding get integer format. extract number unique classes present data now visualize number samples present class using Bar plot.","code":"print(\"Before replacing labels\") print(eeg[\"label\"].unique(), \"\\n\") print(len(eeg[\"label\"].unique()), \"\\n\")   eeg.replace(     {         \"label\": {             \"blink1\": \"blink\",             \"blink2\": \"blink\",             \"blink3\": \"blink\",             \"blink4\": \"blink\",             \"blink5\": \"blink\",             \"math1\": \"math\",             \"math2\": \"math\",             \"math3\": \"math\",             \"math4\": \"math\",             \"math5\": \"math\",             \"math6\": \"math\",             \"math7\": \"math\",             \"math8\": \"math\",             \"math9\": \"math\",             \"math10\": \"math\",             \"math11\": \"math\",             \"math12\": \"math\",             \"thinkOfItems-ver1\": \"thinkOfItems\",             \"thinkOfItems-ver2\": \"thinkOfItems\",             \"video-ver1\": \"video\",             \"video-ver2\": \"video\",             \"thinkOfItemsInstruction-ver1\": \"thinkOfItemsInstruction\",             \"thinkOfItemsInstruction-ver2\": \"thinkOfItemsInstruction\",             \"colorRound1-1\": \"colorRound1\",             \"colorRound1-2\": \"colorRound1\",             \"colorRound1-3\": \"colorRound1\",             \"colorRound1-4\": \"colorRound1\",             \"colorRound1-5\": \"colorRound1\",             \"colorRound1-6\": \"colorRound1\",             \"colorRound2-1\": \"colorRound2\",             \"colorRound2-2\": \"colorRound2\",             \"colorRound2-3\": \"colorRound2\",             \"colorRound2-4\": \"colorRound2\",             \"colorRound2-5\": \"colorRound2\",             \"colorRound2-6\": \"colorRound2\",             \"colorRound3-1\": \"colorRound3\",             \"colorRound3-2\": \"colorRound3\",             \"colorRound3-3\": \"colorRound3\",             \"colorRound3-4\": \"colorRound3\",             \"colorRound3-5\": \"colorRound3\",             \"colorRound3-6\": \"colorRound3\",             \"colorRound4-1\": \"colorRound4\",             \"colorRound4-2\": \"colorRound4\",             \"colorRound4-3\": \"colorRound4\",             \"colorRound4-4\": \"colorRound4\",             \"colorRound4-5\": \"colorRound4\",             \"colorRound4-6\": \"colorRound4\",             \"colorRound5-1\": \"colorRound5\",             \"colorRound5-2\": \"colorRound5\",             \"colorRound5-3\": \"colorRound5\",             \"colorRound5-4\": \"colorRound5\",             \"colorRound5-5\": \"colorRound5\",             \"colorRound5-6\": \"colorRound5\",             \"colorInstruction1\": \"colorInstruction\",             \"colorInstruction2\": \"colorInstruction\",             \"readyRound1\": \"readyRound\",             \"readyRound2\": \"readyRound\",             \"readyRound3\": \"readyRound\",             \"readyRound4\": \"readyRound\",             \"readyRound5\": \"readyRound\",             \"colorRound1\": \"colorRound\",             \"colorRound2\": \"colorRound\",             \"colorRound3\": \"colorRound\",             \"colorRound4\": \"colorRound\",             \"colorRound5\": \"colorRound\",         }     },     inplace=True, )  print(\"After replacing labels\") print(eeg[\"label\"].unique()) print(len(eeg[\"label\"].unique()))  le = preprocessing.LabelEncoder()  # Generates a look-up table le.fit(eeg[\"label\"]) eeg[\"label\"] = le.transform(eeg[\"label\"]) num_classes = len(eeg[\"label\"].unique()) print(num_classes) plt.bar(range(num_classes), eeg[\"label\"].value_counts()) plt.title(\"Number of samples per class\") plt.show()"},{"path":"https://keras.posit.co/articles/examples/eeg_signal_classification.html","id":"scale-and-split-data","dir":"Articles > Examples","previous_headings":"","what":"Scale and split data","title":"Electroencephalogram Signal Classification for action identification","text":"perform simple Min-Max scaling bring value-range 0 1. use Standard Scaling data follow Gaussian distribution. now create Train-test split 15% holdout set. Following , reshape data create sequence length 512. also convert labels current label-encoded form one-hot encoding enable use several different keras.metrics functions.","code":"scaler = preprocessing.MinMaxScaler() series_list = [     scaler.fit_transform(np.asarray(i).reshape(-1, 1))     for i in eeg[\"raw_values\"] ]  labels_list = [i for i in eeg[\"label\"]] x_train, x_test, y_train, y_test = model_selection.train_test_split(     series_list, labels_list, test_size=0.15, random_state=42, shuffle=True )  print(     f\"Length of x_train : {len(x_train)}\\nLength of x_test : {len(x_test)}\\nLength of y_train : {len(y_train)}\\nLength of y_test : {len(y_test)}\" )  x_train = np.asarray(x_train).astype(np.float32).reshape(-1, 512, 1) y_train = np.asarray(y_train).astype(np.float32).reshape(-1, 1) y_train = keras.utils.to_categorical(y_train)  x_test = np.asarray(x_test).astype(np.float32).reshape(-1, 512, 1) y_test = np.asarray(y_test).astype(np.float32).reshape(-1, 1) y_test = keras.utils.to_categorical(y_test)"},{"path":"https://keras.posit.co/articles/examples/eeg_signal_classification.html","id":"prepare-tf-data-dataset","dir":"Articles > Examples","previous_headings":"","what":"Prepare tf.data.Dataset","title":"Electroencephalogram Signal Classification for action identification","text":"now create tf.data.Dataset data prepare training. also shuffle batch data use later.","code":"train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)) test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))  train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE) test_dataset = test_dataset.batch(BATCH_SIZE)"},{"path":"https://keras.posit.co/articles/examples/eeg_signal_classification.html","id":"make-class-weights-using-naive-method","dir":"Articles > Examples","previous_headings":"","what":"Make Class Weights using Naive method","title":"Electroencephalogram Signal Classification for action identification","text":"can see plot number samples per class, dataset imbalanced. Hence, calculate weights class make sure model trained fair manner without preference specific class due greater number samples. use naive method calculate weights, finding inverse proportion class using weight.","code":"vals_dict = {} for i in eeg[\"label\"]:     if i in vals_dict.keys():         vals_dict[i] += 1     else:         vals_dict[i] = 1 total = sum(vals_dict.values())  # Formula used - Naive method where # weight = 1 - (no. of samples present / total no. of samples) # So more the samples, lower the weight  weight_dict = {k: (1 - (v / total)) for k, v in vals_dict.items()} print(weight_dict)"},{"path":"https://keras.posit.co/articles/examples/eeg_signal_classification.html","id":"define-simple-function-to-plot-all-the-metrics-present-in-a-keras-callbacks-history","dir":"Articles > Examples","previous_headings":"","what":"Define simple function to plot all the metrics present in a keras.callbacks.History","title":"Electroencephalogram Signal Classification for action identification","text":"object","code":"def plot_history_metrics(history: keras.callbacks.History):     total_plots = len(history.history)     cols = total_plots // 2      rows = total_plots // cols      if total_plots % cols != 0:         rows += 1      pos = range(1, total_plots + 1)     plt.figure(figsize=(15, 10))     for i, (key, value) in enumerate(history.history.items()):         plt.subplot(rows, cols, pos[i])         plt.plot(range(len(value)), value)         plt.title(str(key))     plt.show()"},{"path":"https://keras.posit.co/articles/examples/eeg_signal_classification.html","id":"define-function-to-generate-convolutional-model","dir":"Articles > Examples","previous_headings":"","what":"Define function to generate Convolutional model","title":"Electroencephalogram Signal Classification for action identification","text":"","code":"def create_model():     input_layer = keras.Input(shape=(512, 1))      x = layers.Conv1D(         filters=32, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\"     )(input_layer)     x = layers.BatchNormalization()(x)      x = layers.Conv1D(         filters=64, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\"     )(x)     x = layers.BatchNormalization()(x)      x = layers.Conv1D(         filters=128, kernel_size=5, strides=2, activation=\"relu\", padding=\"same\"     )(x)     x = layers.BatchNormalization()(x)      x = layers.Conv1D(         filters=256, kernel_size=5, strides=2, activation=\"relu\", padding=\"same\"     )(x)     x = layers.BatchNormalization()(x)      x = layers.Conv1D(         filters=512, kernel_size=7, strides=2, activation=\"relu\", padding=\"same\"     )(x)     x = layers.BatchNormalization()(x)      x = layers.Conv1D(         filters=1024,         kernel_size=7,         strides=2,         activation=\"relu\",         padding=\"same\",     )(x)     x = layers.BatchNormalization()(x)      x = layers.Dropout(0.2)(x)      x = layers.Flatten()(x)      x = layers.Dense(4096, activation=\"relu\")(x)     x = layers.Dropout(0.2)(x)      x = layers.Dense(         2048, activation=\"relu\", kernel_regularizer=keras.regularizers.L2()     )(x)     x = layers.Dropout(0.2)(x)      x = layers.Dense(         1024, activation=\"relu\", kernel_regularizer=keras.regularizers.L2()     )(x)     x = layers.Dropout(0.2)(x)     x = layers.Dense(         128, activation=\"relu\", kernel_regularizer=keras.regularizers.L2()     )(x)     output_layer = layers.Dense(num_classes, activation=\"softmax\")(x)      return keras.Model(inputs=input_layer, outputs=output_layer)"},{"path":"https://keras.posit.co/articles/examples/eeg_signal_classification.html","id":"get-model-summary","dir":"Articles > Examples","previous_headings":"","what":"Get Model summary","title":"Electroencephalogram Signal Classification for action identification","text":"","code":"conv_model = create_model() conv_model.summary()"},{"path":"https://keras.posit.co/articles/examples/eeg_signal_classification.html","id":"define-callbacks-optimizer-loss-and-metrics","dir":"Articles > Examples","previous_headings":"","what":"Define callbacks, optimizer, loss and metrics","title":"Electroencephalogram Signal Classification for action identification","text":"set number epochs 30 performing extensive experimentation. seen optimal number, performing Early-Stopping analysis well. define Model Checkpoint callback make sure get best model weights. also define ReduceLROnPlateau several cases found experimentation loss stagnated certain point. hand, direct LRScheduler found aggressive decay.","code":"epochs = 30  callbacks = [     keras.callbacks.ModelCheckpoint(         \"best_model.keras\", save_best_only=True, monitor=\"loss\"     ),     keras.callbacks.ReduceLROnPlateau(         monitor=\"val_top_k_categorical_accuracy\",         factor=0.2,         patience=2,         min_lr=0.000001,     ), ]  optimizer = keras.optimizers.Adam(amsgrad=True, learning_rate=0.001) loss = keras.losses.CategoricalCrossentropy()"},{"path":"https://keras.posit.co/articles/examples/eeg_signal_classification.html","id":"compile-model-and-call-model-fit","dir":"Articles > Examples","previous_headings":"","what":"Compile model and call model.fit()","title":"Electroencephalogram Signal Classification for action identification","text":"use Adam optimizer since commonly considered best choice preliminary training, found best optimizer. use CategoricalCrossentropy loss labels one-hot-encoded form. define TopKCategoricalAccuracy(k=3), AUC, Precision Recall metrics aid understanding model better.","code":"conv_model.compile(     optimizer=optimizer,     loss=loss,     metrics=[         keras.metrics.TopKCategoricalAccuracy(k=3),         keras.metrics.AUC(),         keras.metrics.Precision(),         keras.metrics.Recall(),     ], )  conv_model_history = conv_model.fit(     train_dataset,     epochs=epochs,     callbacks=callbacks,     validation_data=test_dataset,     class_weight=weight_dict, )"},{"path":"https://keras.posit.co/articles/examples/eeg_signal_classification.html","id":"visualize-model-metrics-during-training","dir":"Articles > Examples","previous_headings":"","what":"Visualize model metrics during training","title":"Electroencephalogram Signal Classification for action identification","text":"use function defined see model metrics training.","code":"plot_history_metrics(conv_model_history)"},{"path":"https://keras.posit.co/articles/examples/eeg_signal_classification.html","id":"evaluate-model-on-test-data","dir":"Articles > Examples","previous_headings":"","what":"Evaluate model on test data","title":"Electroencephalogram Signal Classification for action identification","text":"","code":"loss, accuracy, auc, precision, recall = conv_model.evaluate(test_dataset) print(f\"Loss : {loss}\") print(f\"Top 3 Categorical Accuracy : {accuracy}\") print(f\"Area under the Curve (ROC) : {auc}\") print(f\"Precision : {precision}\") print(f\"Recall : {recall}\")   def view_evaluated_eeg_plots(model):     start_index = random.randint(10, len(eeg))     end_index = start_index + 11     data = eeg.loc[start_index:end_index, \"raw_values\"]     data_array = [         scaler.fit_transform(np.asarray(i).reshape(-1, 1)) for i in data     ]     data_array = [np.asarray(data_array).astype(np.float32).reshape(-1, 512, 1)]     original_labels = eeg.loc[start_index:end_index, \"label\"]     predicted_labels = np.argmax(model.predict(data_array, verbose=0), axis=1)     original_labels = [         le.inverse_transform(np.array(label).reshape(-1))[0]         for label in original_labels     ]     predicted_labels = [         le.inverse_transform(np.array(label).reshape(-1))[0]         for label in predicted_labels     ]     total_plots = 12     cols = total_plots // 3     rows = total_plots // cols     if total_plots % cols != 0:         rows += 1     pos = range(1, total_plots + 1)     fig = plt.figure(figsize=(20, 10))     for i, (plot_data, og_label, pred_label) in enumerate(         zip(data, original_labels, predicted_labels)     ):         plt.subplot(rows, cols, pos[i])         plt.plot(plot_data)         plt.title(f\"Actual Label : {og_label}\\nPredicted Label : {pred_label}\")         fig.subplots_adjust(hspace=0.5)     plt.show()   view_evaluated_eeg_plots(conv_model)"},{"path":"https://keras.posit.co/articles/examples/end_to_end_mlm_with_bert.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"End-to-end Masked Language Modeling with BERT","text":"Masked Language Modeling fill---blank task, model uses context words surrounding mask token try predict masked word . input contains one mask tokens, model generate likely substitution . Example: Input: “watched [MASK] awesome.” Output: “watched movie awesome.” Masked language modeling great way train language model self-supervised setting (without human-annotated labels). model can fine-tuned accomplish various supervised NLP tasks. example teaches build BERT model scratch, train masked language modeling task, fine-tune model sentiment classification task. use Keras-Core TextVectorization MultiHeadAttention layers create BERT Transformer-Encoder network architecture. Note: tensorflow backend compatible.","code":""},{"path":"https://keras.posit.co/articles/examples/end_to_end_mlm_with_bert.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"End-to-end Masked Language Modeling with BERT","text":"","code":"import os import re import glob import numpy as np import pandas as pd from pathlib import Path from dataclasses import dataclass  import tensorflow as tf import keras as keras from keras import layers"},{"path":"https://keras.posit.co/articles/examples/end_to_end_mlm_with_bert.html","id":"configuration","dir":"Articles > Examples","previous_headings":"","what":"Configuration","title":"End-to-end Masked Language Modeling with BERT","text":"","code":"@dataclass class Config:     MAX_LEN = 256     BATCH_SIZE = 32     LR = 0.001     VOCAB_SIZE = 30000     EMBED_DIM = 128     NUM_HEAD = 8  # used in bert model     FF_DIM = 128  # used in bert model     NUM_LAYERS = 1     NUM_EPOCHS = 1     STEPS_PER_EPOCH = 2   config = Config()"},{"path":"https://keras.posit.co/articles/examples/end_to_end_mlm_with_bert.html","id":"download-the-data-imdb-movie-review-sentiment-classification","dir":"Articles > Examples","previous_headings":"","what":"Download the Data: IMDB Movie Review Sentiment Classification","title":"End-to-end Masked Language Modeling with BERT","text":"Download IMDB data load Pandas DataFrame. aclImdb folder contains train test subfolder: interested pos neg subfolders, let’s delete rest: Let’s read dataset text files DataFrame.","code":"fpath = keras.utils.get_file(     origin=\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\" ) dirpath = Path(fpath).parent.absolute() os.system(f\"tar -xf {fpath} -C {dirpath}\") os.system(f\"ls {dirpath}/aclImdb\") os.system(f\"ls {dirpath}/aclImdb/train\") os.system(f\"ls {dirpath}/aclImdb/test\") os.system(f\"rm -r {dirpath}/aclImdb/train/unsup\") os.system(f\"rm -r {dirpath}/aclImdb/train/*.feat\") os.system(f\"rm -r {dirpath}/aclImdb/train/*.txt\") os.system(f\"rm -r {dirpath}/aclImdb/test/*.feat\") os.system(f\"rm -r {dirpath}/aclImdb/test/*.txt\") def get_text_list_from_files(files):     text_list = []     for name in files:         with open(name) as f:             for line in f:                 text_list.append(line)     return text_list   def get_data_from_text_files(folder_name):      pos_files = glob.glob(f\"{dirpath}/aclImdb/\" + folder_name + \"/pos/*.txt\")     pos_texts = get_text_list_from_files(pos_files)     neg_files = glob.glob(f\"{dirpath}/aclImdb/\" + folder_name + \"/neg/*.txt\")     neg_texts = get_text_list_from_files(neg_files)     df = pd.DataFrame(         {             \"review\": pos_texts + neg_texts,             \"sentiment\": [0] * len(pos_texts) + [1] * len(neg_texts),         }     )     df = df.sample(len(df)).reset_index(drop=True)     return df  train_df = get_data_from_text_files(\"train\") test_df = get_data_from_text_files(\"test\")  all_data = pd.concat([train_df, test_df], axis=0).reset_index(drop=True) assert len(all_data) != 0, f'{all_data} is empty'"},{"path":"https://keras.posit.co/articles/examples/end_to_end_mlm_with_bert.html","id":"dataset-preparation","dir":"Articles > Examples","previous_headings":"","what":"Dataset preparation","title":"End-to-end Masked Language Modeling with BERT","text":"use TextVectorization layer vectorize text integer token ids. transforms batch strings either sequence token indices (one sample = 1D array integer token indices, order) dense representation (one sample = 1D array float values encoding unordered set tokens). , define 3 preprocessing functions. get_vectorize_layer function builds TextVectorization layer. encode function encodes raw text integer token ids. get_masked_input_and_labels function mask input token ids. masks 15% input tokens sequence random.","code":"def custom_standardization(input_data):     lowercase = tf.strings.lower(input_data)     stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")     return tf.strings.regex_replace(         stripped_html, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"), \"\"     )   def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):     \"\"\"Build Text vectorization layer      Args:       texts (list): List of string i.e input texts       vocab_size (int): vocab size       max_seq (int): Maximum sequence lenght.       special_tokens (list, optional): List of special tokens. Defaults to `['[MASK]']`.      Returns:         layers.Layer: Return TextVectorization Keras Layer     \"\"\"     vectorize_layer = layers.TextVectorization(         max_tokens=vocab_size,         output_mode=\"int\",         standardize=custom_standardization,         output_sequence_length=max_seq,     )     vectorize_layer.adapt(texts)      # Insert mask token in vocabulary     vocab = vectorize_layer.get_vocabulary()     vocab = vocab[2 : vocab_size - len(special_tokens)] + [\"[mask]\"]     vectorize_layer.set_vocabulary(vocab)     return vectorize_layer   vectorize_layer = get_vectorize_layer(     all_data.review.values.tolist(),     config.VOCAB_SIZE,     config.MAX_LEN,     special_tokens=[\"[mask]\"], )  # Get mask token id for masked language model mask_token_id = vectorize_layer([\"[mask]\"]).numpy()[0][0]   def encode(texts):     encoded_texts = vectorize_layer(texts)     return encoded_texts.numpy()   def get_masked_input_and_labels(encoded_texts):     # 15% BERT masking     inp_mask = np.random.rand(*encoded_texts.shape) < 0.15     # Do not mask special tokens     inp_mask[encoded_texts <= 2] = False     # Set targets to -1 by default, it means ignore     labels = -1 * np.ones(encoded_texts.shape, dtype=int)     # Set labels for masked tokens     labels[inp_mask] = encoded_texts[inp_mask]      # Prepare input     encoded_texts_masked = np.copy(encoded_texts)     # Set input to [MASK] which is the last token for the 90% of tokens     # This means leaving 10% unchanged     inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)     encoded_texts_masked[         inp_mask_2mask     ] = mask_token_id  # mask token is the last in the dict      # Set 10% to a random token     inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)     encoded_texts_masked[inp_mask_2random] = np.random.randint(         3, mask_token_id, inp_mask_2random.sum()     )      # Prepare sample_weights to pass to .fit() method     sample_weights = np.ones(labels.shape)     sample_weights[labels == -1] = 0      # y_labels would be same as encoded_texts i.e input tokens     y_labels = np.copy(encoded_texts)      return encoded_texts_masked, y_labels, sample_weights   # We have 25000 examples for training x_train = encode(train_df.review.values)  # encode reviews with vectorizer y_train = train_df.sentiment.values train_classifier_ds = (     tf.data.Dataset.from_tensor_slices((x_train, y_train))     .shuffle(1000)     .batch(config.BATCH_SIZE) )  # We have 25000 examples for testing x_test = encode(test_df.review.values) y_test = test_df.sentiment.values test_classifier_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(     config.BATCH_SIZE )  # Build dataset for end to end model input (will be used at the end) test_raw_classifier_ds = tf.data.Dataset.from_tensor_slices(     (test_df.review.values, y_test) ).batch(config.BATCH_SIZE)  # Prepare data for masked language model x_all_review = encode(all_data.review.values) x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(     x_all_review )  mlm_ds = tf.data.Dataset.from_tensor_slices(     (x_masked_train, y_masked_labels, sample_weights) ) mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)  id2token = dict(enumerate(vectorize_layer.get_vocabulary())) token2id = {y: x for x, y in id2token.items()}  class MaskedTextGenerator(keras.callbacks.Callback):     def __init__(self, sample_tokens, top_k=5):         self.sample_tokens = sample_tokens         self.k = top_k      def decode(self, tokens):         return \" \".join([id2token[t] for t in tokens if t != 0])      def convert_ids_to_tokens(self, id):         return id2token[id]      def on_epoch_end(self, epoch, logs=None):         prediction = self.model.predict(self.sample_tokens)          masked_index = np.where(self.sample_tokens == mask_token_id)         masked_index = masked_index[1]         mask_prediction = prediction[0][masked_index]          top_indices = mask_prediction[0].argsort()[-self.k :][::-1]         values = mask_prediction[0][top_indices]          for i in range(len(top_indices)):             p = top_indices[i]             v = values[i]             tokens = np.copy(self.sample_tokens[0])             tokens[masked_index[0]] = p             result = {                 \"input_text\": self.decode(self.sample_tokens[0]),                 \"prediction\": self.decode(tokens),                 \"probability\": v,                 \"predicted mask token\": self.convert_ids_to_tokens(p),             }  sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"]) generator_callback = MaskedTextGenerator(sample_tokens.numpy())"},{"path":"https://keras.posit.co/articles/examples/end_to_end_mlm_with_bert.html","id":"create-bert-model-pretraining-model-for-masked-language-modeling","dir":"Articles > Examples","previous_headings":"","what":"Create BERT model (Pretraining Model) for masked language modeling","title":"End-to-end Masked Language Modeling with BERT","text":"create BERT-like pretraining model architecture using MultiHeadAttention layer. take token ids inputs (including masked tokens) predict correct ids masked input tokens.","code":"def bert_module(query, key, value, layer_num):     # Multi headed self-attention     attention_output = layers.MultiHeadAttention(         num_heads=config.NUM_HEAD,         key_dim=config.EMBED_DIM // config.NUM_HEAD,         name=f\"encoder_{layer_num}_multiheadattention\",     )(query, key, value)     attention_output = layers.Dropout(0.1, name=f\"encoder_{layer_num}_att_dropout\")(         attention_output     )     attention_output = layers.LayerNormalization(         epsilon=1e-6, name=f\"encoder_{layer_num}_att_layernormalization\"     )(query + attention_output)      # Feed-forward layer     ffn = keras.Sequential(         [             layers.Dense(config.FF_DIM, activation=\"relu\"),             layers.Dense(config.EMBED_DIM),         ],         name=f\"encoder_{layer_num}_ffn\",     )     ffn_output = ffn(attention_output)     ffn_output = layers.Dropout(0.1, name=f\"encoder_{layer_num}_ffn_dropout\")(         ffn_output     )     sequence_output = layers.LayerNormalization(         epsilon=1e-6, name=f\"encoder_{layer_num}_ffn_layernormalization\"     )(attention_output + ffn_output)     return sequence_output   def get_pos_encoding_matrix(max_len, d_emb):     pos_enc = np.array(         [             [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]             if pos != 0             else np.zeros(d_emb)             for pos in range(max_len)         ]     )     pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i     pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1     return pos_enc   loss_fn = keras.losses.SparseCategoricalCrossentropy(     reduction=None ) loss_tracker = keras.metrics.Mean(name=\"loss\")   class MaskedLanguageModel(keras.Model):     def train_step(self, inputs):         if len(inputs) == 3:             features, labels, sample_weight = inputs         else:             features, labels = inputs             sample_weight = None          with tf.GradientTape() as tape:             predictions = self(features, training=True)             loss = loss_fn(labels, predictions, sample_weight=sample_weight)          # Compute gradients         trainable_vars = self.trainable_variables         gradients = tape.gradient(loss, trainable_vars)          # Update weights         self.optimizer.apply_gradients(zip(gradients, trainable_vars))          # Compute our own metrics         loss_tracker.update_state(loss, sample_weight=sample_weight)          # Return a dict mapping metric names to current value         return {\"loss\": loss_tracker.result()}      @property     def metrics(self):         # We list our `Metric` objects here so that `reset_states()` can be         # called automatically at the start of each epoch         # or at the start of `evaluate()`.         # If you don't implement this property, you have to call         # `reset_states()` yourself at the time of your choosing.         return [loss_tracker]   def create_masked_language_bert_model():     inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)      word_embeddings = layers.Embedding(         config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\"     )(inputs)      position_embeddings = layers.Embedding(         input_dim=config.MAX_LEN,         output_dim=config.EMBED_DIM,         embeddings_initializer=keras.initializers.Constant(get_pos_encoding_matrix(config.MAX_LEN, config.EMBED_DIM)),         name=\"position_embedding\",     )(tf.range(start=0, limit=config.MAX_LEN, delta=1))      embeddings = word_embeddings + position_embeddings      encoder_output = embeddings     for i in range(config.NUM_LAYERS):         encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)      mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(         encoder_output     )     mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")      optimizer = keras.optimizers.Adam(learning_rate=config.LR)     mlm_model.compile(optimizer=optimizer)     return mlm_model  bert_masked_model = create_masked_language_bert_model() bert_masked_model.summary()"},{"path":"https://keras.posit.co/articles/examples/end_to_end_mlm_with_bert.html","id":"train-and-save","dir":"Articles > Examples","previous_headings":"","what":"Train and Save","title":"End-to-end Masked Language Modeling with BERT","text":"","code":"bert_masked_model.fit(mlm_ds, epochs=Config.NUM_EPOCHS, steps_per_epoch=Config.STEPS_PER_EPOCH, callbacks=[generator_callback]) bert_masked_model.save(\"bert_mlm_imdb.keras\")"},{"path":"https://keras.posit.co/articles/examples/end_to_end_mlm_with_bert.html","id":"fine-tune-a-sentiment-classification-model","dir":"Articles > Examples","previous_headings":"","what":"Fine-tune a sentiment classification model","title":"End-to-end Masked Language Modeling with BERT","text":"fine-tune self-supervised model downstream task sentiment classification. , let’s create classifier adding pooling layer Dense layer top pretrained BERT features.","code":"# Load pretrained bert model mlm_model = keras.models.load_model(     \"bert_mlm_imdb.keras\", custom_objects={\"MaskedLanguageModel\": MaskedLanguageModel} ) pretrained_bert_model = keras.Model(     mlm_model.input, mlm_model.get_layer(\"encoder_0_ffn_layernormalization\").output )  # Freeze it pretrained_bert_model.trainable = False   def create_classifier_bert_model():     inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)     sequence_output = pretrained_bert_model(inputs)     pooled_output = layers.GlobalMaxPooling1D()(sequence_output)     hidden_layer = layers.Dense(64, activation=\"relu\")(pooled_output)     outputs = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)     classifer_model = keras.Model(inputs, outputs, name=\"classification\")     optimizer = keras.optimizers.Adam()     classifer_model.compile(         optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]     )     return classifer_model   classifer_model = create_classifier_bert_model() classifer_model.summary()  # Train the classifier with frozen BERT stage classifer_model.fit(     train_classifier_ds,     epochs=Config.NUM_EPOCHS,     steps_per_epoch=Config.STEPS_PER_EPOCH,     validation_data=test_classifier_ds, )  # Unfreeze the BERT model for fine-tuning pretrained_bert_model.trainable = True optimizer = keras.optimizers.Adam() classifer_model.compile(     optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"] ) classifer_model.fit(     train_classifier_ds,     epochs=Config.NUM_EPOCHS,     steps_per_epoch=Config.STEPS_PER_EPOCH,     validation_data=test_classifier_ds, )"},{"path":"https://keras.posit.co/articles/examples/end_to_end_mlm_with_bert.html","id":"create-an-end-to-end-model-and-evaluate-it","dir":"Articles > Examples","previous_headings":"","what":"Create an end-to-end model and evaluate it","title":"End-to-end Masked Language Modeling with BERT","text":"want deploy model, ’s best already includes preprocessing pipeline, don’t reimplement preprocessing logic production environment. Let’s create end--end model incorporates TextVectorization layer, let’s evaluate. model accept raw strings input.","code":"def get_end_to_end(model):     inputs_string = keras.Input(shape=(1,), dtype=\"string\")     indices = vectorize_layer(inputs_string)     outputs = model(indices)     end_to_end_model = keras.Model(inputs_string, outputs, name=\"end_to_end_model\")     optimizer = keras.optimizers.Adam(learning_rate=config.LR)     end_to_end_model.compile(         optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]     )     return end_to_end_model   end_to_end_classification_model = get_end_to_end(classifer_model) end_to_end_classification_model.evaluate(test_raw_classifier_ds)"},{"path":"https://keras.posit.co/articles/examples/endpoint_layer_pattern.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Endpoint layer pattern","text":"","code":"import tensorflow as tf import keras as keras import numpy as np"},{"path":"https://keras.posit.co/articles/examples/endpoint_layer_pattern.html","id":"usage-of-endpoint-layers-in-the-functional-api","dir":"Articles > Examples","previous_headings":"","what":"Usage of endpoint layers in the Functional API","title":"Endpoint layer pattern","text":"“endpoint layer” access model’s targets, creates arbitrary losses call() using self.add_loss() Metric.update_state(). enables define losses metrics don’t match usual signature fn(y_true, y_pred, sample_weight=None). Note separate metrics training eval pattern.","code":"class LogisticEndpoint(keras.layers.Layer):     def __init__(self, name=None):         super().__init__(name=name)         self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)         self.accuracy_metric = keras.metrics.BinaryAccuracy(name=\"accuracy\")      def call(self, logits, targets=None, sample_weight=None):         if targets is not None:             # Compute the training-time loss value and add it             # to the layer using `self.add_loss()`.             loss = self.loss_fn(targets, logits, sample_weight)             self.add_loss(loss)              # Log the accuracy as a metric (we could log arbitrary metrics,             # including different metrics for training and inference.)             self.accuracy_metric.update_state(targets, logits, sample_weight)          # Return the inference-time prediction tensor (for `.predict()`).         return tf.nn.softmax(logits)   inputs = keras.Input((764,), name=\"inputs\") logits = keras.layers.Dense(1)(inputs) targets = keras.Input((1,), name=\"targets\") sample_weight = keras.Input((1,), name=\"sample_weight\") preds = LogisticEndpoint()(logits, targets, sample_weight) model = keras.Model([inputs, targets, sample_weight], preds)  data = {     \"inputs\": np.random.random((1000, 764)),     \"targets\": np.random.random((1000, 1)),     \"sample_weight\": np.random.random((1000, 1)), }  model.compile(keras.optimizers.Adam(1e-3)) model.fit(data, epochs=2)"},{"path":"https://keras.posit.co/articles/examples/endpoint_layer_pattern.html","id":"exporting-an-inference-only-model","dir":"Articles > Examples","previous_headings":"","what":"Exporting an inference-only model","title":"Endpoint layer pattern","text":"Simply don’t include targets model. weights stay .","code":"inputs = keras.Input((764,), name=\"inputs\") logits = keras.layers.Dense(1)(inputs) preds = LogisticEndpoint()(logits, targets=None, sample_weight=None) inference_model = keras.Model(inputs, preds)  inference_model.set_weights(model.get_weights())  preds = inference_model.predict(np.random.random((1000, 764)))"},{"path":"https://keras.posit.co/articles/examples/endpoint_layer_pattern.html","id":"usage-of-loss-endpoint-layers-in-subclassed-models","dir":"Articles > Examples","previous_headings":"","what":"Usage of loss endpoint layers in subclassed models","title":"Endpoint layer pattern","text":"","code":"class LogReg(keras.Model):     def __init__(self):         super().__init__()         self.dense = keras.layers.Dense(1)         self.logistic_endpoint = LogisticEndpoint()      def call(self, inputs):         # Note that all inputs should be in the first argument         # since we want to be able to call `model.fit(inputs)`.         logits = self.dense(inputs[\"inputs\"])         preds = self.logistic_endpoint(             logits=logits,             targets=inputs[\"targets\"],             sample_weight=inputs[\"sample_weight\"],         )         return preds   model = LogReg() data = {     \"inputs\": np.random.random((1000, 764)),     \"targets\": np.random.random((1000, 1)),     \"sample_weight\": np.random.random((1000, 1)), }  model.compile(keras.optimizers.Adam(1e-3)) model.fit(data, epochs=2)"},{"path":"https://keras.posit.co/articles/examples/fixres.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"FixRes: Fixing train-test resolution discrepancy","text":"common practice use input image resolution training testing vision models. However, investigated Fixing train-test resolution discrepancy (Touvron et al.), practice leads suboptimal performance. Data augmentation indispensable part training process deep neural networks. vision models, typically use random resized crops training center crops inference. introduces discrepancy object sizes seen training inference. shown Touvron et al., can fix discrepancy, can significantly boost model performance. example, implement FixRes techniques introduced Touvron et al. fix discrepancy.","code":""},{"path":"https://keras.posit.co/articles/examples/fixres.html","id":"imports","dir":"Articles > Examples","previous_headings":"","what":"Imports","title":"FixRes: Fixing train-test resolution discrepancy","text":"","code":"import keras as keras from keras import layers import tensorflow as tf  # just for image processing and pipeline  import tensorflow_datasets as tfds  tfds.disable_progress_bar()  import matplotlib.pyplot as plt"},{"path":"https://keras.posit.co/articles/examples/fixres.html","id":"load-the-tf_flowers-dataset","dir":"Articles > Examples","previous_headings":"","what":"Load the tf_flowers dataset","title":"FixRes: Fixing train-test resolution discrepancy","text":"","code":"train_dataset, val_dataset = tfds.load(     \"tf_flowers\", split=[\"train[:90%]\", \"train[90%:]\"], as_supervised=True )  num_train = train_dataset.cardinality() num_val = val_dataset.cardinality() print(f\"Number of training examples: {num_train}\") print(f\"Number of validation examples: {num_val}\")"},{"path":"https://keras.posit.co/articles/examples/fixres.html","id":"data-preprocessing-utilities","dir":"Articles > Examples","previous_headings":"","what":"Data preprocessing utilities","title":"FixRes: Fixing train-test resolution discrepancy","text":"create three datasets: dataset smaller resolution - 128x128. Two datasets larger resolution - 224x224. apply different augmentation transforms larger-resolution datasets. idea FixRes first train model smaller resolution dataset fine-tune larger resolution dataset. simple yet effective recipe leads non-trivial performance improvements. Please refer original paper results. Notice augmentation transforms vary kind dataset preparing.","code":"# Reference: https://github.com/facebookresearch/FixRes/blob/main/transforms_v2.py.  batch_size = 32 auto = tf.data.AUTOTUNE smaller_size = 128 bigger_size = 224  size_for_resizing = int((bigger_size / smaller_size) * bigger_size) central_crop_layer = layers.CenterCrop(bigger_size, bigger_size)   def preprocess_initial(train, image_size):     \"\"\"Initial preprocessing function for training on smaller resolution.      For training, do random_horizontal_flip -> random_crop.     For validation, just resize.     No color-jittering has been used.     \"\"\"      def _pp(image, label, train):         if train:             channels = image.shape[-1]             begin, size, _ = tf.image.sample_distorted_bounding_box(                 tf.shape(image),                 tf.zeros([0, 0, 4], tf.float32),                 area_range=(0.05, 1.0),                 min_object_covered=0,                 use_image_if_no_bounding_boxes=True,             )             image = tf.slice(image, begin, size)              image.set_shape([None, None, channels])             image = tf.image.resize(image, [image_size, image_size])             image = tf.image.random_flip_left_right(image)         else:             image = tf.image.resize(image, [image_size, image_size])          return image, label      return _pp   def preprocess_finetune(image, label, train):     \"\"\"Preprocessing function for fine-tuning on a higher resolution.      For training, resize to a bigger resolution to maintain the ratio ->         random_horizontal_flip -> center_crop.     For validation, do the same without any horizontal flipping.     No color-jittering has been used.     \"\"\"     image = tf.image.resize(image, [size_for_resizing, size_for_resizing])     if train:         image = tf.image.random_flip_left_right(image)     image = central_crop_layer(image[None, ...])[0]      return image, label   def make_dataset(     dataset: tf.data.Dataset,     train: bool,     image_size: int = smaller_size,     fixres: bool = True,     num_parallel_calls=auto, ):     if image_size not in [smaller_size, bigger_size]:         raise ValueError(f\"{image_size} resolution is not supported.\")      # Determine which preprocessing function we are using.     if image_size == smaller_size:         preprocess_func = preprocess_initial(train, image_size)     elif not fixres and image_size == bigger_size:         preprocess_func = preprocess_initial(train, image_size)     else:         preprocess_func = preprocess_finetune      if train:         dataset = dataset.shuffle(batch_size * 10)      return (         dataset.map(             lambda x, y: preprocess_func(x, y, train),             num_parallel_calls=num_parallel_calls,         )         .batch(batch_size)         .prefetch(num_parallel_calls)     )"},{"path":"https://keras.posit.co/articles/examples/fixres.html","id":"prepare-datasets","dir":"Articles > Examples","previous_headings":"","what":"Prepare datasets","title":"FixRes: Fixing train-test resolution discrepancy","text":"","code":"initial_train_dataset = make_dataset(     train_dataset, train=True, image_size=smaller_size ) initial_val_dataset = make_dataset(     val_dataset, train=False, image_size=smaller_size )  finetune_train_dataset = make_dataset(     train_dataset, train=True, image_size=bigger_size ) finetune_val_dataset = make_dataset(     val_dataset, train=False, image_size=bigger_size )  vanilla_train_dataset = make_dataset(     train_dataset, train=True, image_size=bigger_size, fixres=False ) vanilla_val_dataset = make_dataset(     val_dataset, train=False, image_size=bigger_size, fixres=False )"},{"path":"https://keras.posit.co/articles/examples/fixres.html","id":"visualize-the-datasets","dir":"Articles > Examples","previous_headings":"","what":"Visualize the datasets","title":"FixRes: Fixing train-test resolution discrepancy","text":"","code":"def visualize_dataset(batch_images):     plt.figure(figsize=(10, 10))     for n in range(25):         ax = plt.subplot(5, 5, n + 1)         plt.imshow(batch_images[n].numpy().astype(\"int\"))         plt.axis(\"off\")     plt.show()      print(f\"Batch shape: {batch_images.shape}.\")   # Smaller resolution. initial_sample_images, _ = next(iter(initial_train_dataset)) visualize_dataset(initial_sample_images)  # Bigger resolution, only for fine-tuning. finetune_sample_images, _ = next(iter(finetune_train_dataset)) visualize_dataset(finetune_sample_images)  # Bigger resolution, with the same augmentation transforms as # the smaller resolution dataset. vanilla_sample_images, _ = next(iter(vanilla_train_dataset)) visualize_dataset(vanilla_sample_images)"},{"path":"https://keras.posit.co/articles/examples/fixres.html","id":"model-training-utilities","dir":"Articles > Examples","previous_headings":"","what":"Model training utilities","title":"FixRes: Fixing train-test resolution discrepancy","text":"train multiple variants ResNet50V2 (et al.): smaller resolution dataset (128x128). trained scratch. fine-tune model 1 larger resolution (224x224) dataset. Train another ResNet50V2 scratch larger resolution dataset. reminder, larger resolution datasets differ terms augmentation transforms.","code":"def get_training_model(num_classes=5):     inputs = layers.Input((None, None, 3))     resnet_base = keras.applications.ResNet50V2(         include_top=False, weights=None, pooling=\"avg\"     )     resnet_base.trainable = True      x = layers.Rescaling(scale=1.0 / 127.5, offset=-1)(inputs)     x = resnet_base(x)     outputs = layers.Dense(num_classes, activation=\"softmax\")(x)     return keras.Model(inputs, outputs)   def train_and_evaluate(     model,     train_ds,     val_ds,     epochs,     learning_rate=1e-3,     use_early_stopping=False, ):     optimizer = keras.optimizers.Adam(learning_rate=learning_rate)     model.compile(         optimizer=optimizer,         loss=\"sparse_categorical_crossentropy\",         metrics=[\"accuracy\"],     )      if use_early_stopping:         es_callback = keras.callbacks.EarlyStopping(patience=5)         callbacks = [es_callback]     else:         callbacks = None      model.fit(         train_ds,         validation_data=val_ds,         epochs=epochs,         callbacks=callbacks,     )      _, accuracy = model.evaluate(val_ds)     print(f\"Top-1 accuracy on the validation set: {accuracy*100:.2f}%.\")     return model"},{"path":"https://keras.posit.co/articles/examples/fixres.html","id":"experiment-1-train-on-128x128-and-then-fine-tune-on-224x224","dir":"Articles > Examples","previous_headings":"","what":"Experiment 1: Train on 128x128 and then fine-tune on 224x224","title":"FixRes: Fixing train-test resolution discrepancy","text":"","code":"epochs = 30  smaller_res_model = get_training_model() smaller_res_model = train_and_evaluate(     smaller_res_model, initial_train_dataset, initial_val_dataset, epochs )"},{"path":"https://keras.posit.co/articles/examples/fixres.html","id":"freeze-all-the-layers-except-for-the-final-batch-normalization-layer","dir":"Articles > Examples","previous_headings":"Experiment 1: Train on 128x128 and then fine-tune on 224x224","what":"Freeze all the layers except for the final Batch Normalization layer","title":"FixRes: Fixing train-test resolution discrepancy","text":"fine-tuning, train two layers: final Batch Normalization (Ioffe et al.) layer. classification layer. unfreezing final Batch Normalization layer compensate change activation statistics global average pooling layer. shown paper, unfreezing final Batch Normalization layer enough. comprehensive guide fine-tuning models Keras, refer tutorial.","code":"for layer in smaller_res_model.layers[2].layers:     layer.trainable = False  smaller_res_model.layers[2].get_layer(\"post_bn\").trainable = True  epochs = 10  # Use a lower learning rate during fine-tuning. bigger_res_model = train_and_evaluate(     smaller_res_model,     finetune_train_dataset,     finetune_val_dataset,     epochs,     learning_rate=1e-4, )"},{"path":"https://keras.posit.co/articles/examples/fixres.html","id":"experiment-2-train-a-model-on-224x224-resolution-from-scratch","dir":"Articles > Examples","previous_headings":"","what":"Experiment 2: Train a model on 224x224 resolution from scratch","title":"FixRes: Fixing train-test resolution discrepancy","text":"Now, train another model scratch larger resolution dataset. Recall augmentation transforms used dataset different . can notice cells, FixRes leads better performance. Another advantage FixRes improved total training time reduction GPU memory usage. FixRes model-agnostic, can use image classification model potentially boost performance. can find results gathered running code different random seeds.","code":"epochs = 30  vanilla_bigger_res_model = get_training_model() vanilla_bigger_res_model = train_and_evaluate(     vanilla_bigger_res_model, vanilla_train_dataset, vanilla_val_dataset, epochs )"},{"path":"https://keras.posit.co/articles/examples/gpt2_text_generation_with_kerasnlp.html","id":"before-we-begin","dir":"Articles > Examples","previous_headings":"","what":"Before we begin","title":"GPT2 Text Generation with KerasNLP","text":"Colab offers different kinds runtimes. Make sure go Runtime -> Change runtime type choose GPU Hardware Accelerator runtime (>12G host RAM ~15G GPU RAM) since finetune GPT-2 model. Running tutorial CPU runtime take hours.","code":""},{"path":"https://keras.posit.co/articles/examples/gpt2_text_generation_with_kerasnlp.html","id":"install-kerasnlp-choose-backend-and-import-dependencies","dir":"Articles > Examples","previous_headings":"","what":"Install KerasNLP, Choose Backend and Import Dependencies","title":"GPT2 Text Generation with KerasNLP","text":"examples uses Keras 3.0 work \"tensorflow\", \"jax\" \"torch\". Support Keras Core baked KerasNLP, simply change \"KERAS_BACKEND\" environment variable select backend choice. select JAX backend . pip install git+https://github.com/keras-team/keras-nlp.git -q","code":"import keras_nlp import tensorflow as tf import json import keras import os import tensorflow_datasets as tfds import time"},{"path":"https://keras.posit.co/articles/examples/gpt2_text_generation_with_kerasnlp.html","id":"introduction-to-generative-large-language-models-llms","dir":"Articles > Examples","previous_headings":"","what":"Introduction to Generative Large Language Models (LLMs)","title":"GPT2 Text Generation with KerasNLP","text":"Large language models (LLMs) type machine learning models trained large corpus text data generate outputs various natural language processing (NLP) tasks, text generation, question answering, machine translation. Generative LLMs typically based deep learning neural networks, Transformer architecture invented Google researchers 2017, trained massive amounts text data, often involving billions words. models, Google LaMDA PaLM, trained large dataset various data sources allows generate output many tasks. core Generative LLMs predicting next word sentence, often referred Causal LM Pretraining. way LLMs can generate coherent text based user prompts. pedagogical discussion language models, can refer Stanford CS324 LLM class.","code":""},{"path":"https://keras.posit.co/articles/examples/gpt2_text_generation_with_kerasnlp.html","id":"introduction-to-kerasnlp","dir":"Articles > Examples","previous_headings":"","what":"Introduction to KerasNLP","title":"GPT2 Text Generation with KerasNLP","text":"Large Language Models complex build expensive train scratch. Luckily pretrained LLMs available use right away. KerasNLP provides large number pre-trained checkpoints allow experiment SOTA models without needing train . KerasNLP natural language processing library supports users entire development cycle. KerasNLP offers pretrained models modularized building blocks, developers easily reuse pretrained models stack LLM. nutshell, generative LLM, KerasNLP offers: Pretrained models generate() method, e.g., keras_nlp.models.GPT2CausalLM keras_nlp.models.OPTCausalLM. Sampler class implements generation algorithms Top-K, Beam contrastive search. samplers can used generate text custom models.","code":""},{"path":"https://keras.posit.co/articles/examples/gpt2_text_generation_with_kerasnlp.html","id":"load-a-pre-trained-gpt-2-model-and-generate-some-text","dir":"Articles > Examples","previous_headings":"","what":"Load a pre-trained GPT-2 model and generate some text","title":"GPT2 Text Generation with KerasNLP","text":"KerasNLP provides number pre-trained models, Google Bert GPT-2. can see list models available KerasNLP repository. ’s easy load GPT-2 model can see : model loaded, can use generate text right away. Run cells give try. ’s simple calling single function generate(): Try another one: Notice much faster second call . computational graph XLA compiled 1st run re-used 2nd behind scenes. quality generated text looks OK, can improve via fine-tuning.","code":"# To speed up training and generation, we use preprocessor of length 128 # instead of full length 1024. preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(     \"gpt2_base_en\",     sequence_length=128, ) gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(     \"gpt2_base_en\", preprocessor=preprocessor ) start = time.time()  output = gpt2_lm.generate(\"My trip to Yosemite was\", max_length=200) print(\"\\nGPT-2 output:\") print(output)  end = time.time() print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\") start = time.time()  output = gpt2_lm.generate(\"That Italian restaurant is\", max_length=200) print(\"\\nGPT-2 output:\") print(output)  end = time.time() print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"},{"path":"https://keras.posit.co/articles/examples/gpt2_text_generation_with_kerasnlp.html","id":"more-on-the-gpt-2-model-from-kerasnlp","dir":"Articles > Examples","previous_headings":"","what":"More on the GPT-2 model from KerasNLP","title":"GPT2 Text Generation with KerasNLP","text":"Next , actually fine-tune model update parameters, , let’s take look full set tools working GPT2. code GPT2 can found . Conceptually GPT2CausalLM can hierarchically broken several modules KerasNLP, from_preset() function loads pretrained model: keras_nlp.models.GPT2Tokenizer: tokenizer used GPT2 model, byte-pair encoder. keras_nlp.models.GPT2CausalLMPreprocessor: preprocessor used GPT2 causal LM training. tokenization along preprocessing works creating label appending end token. keras_nlp.models.GPT2Backbone: GPT2 model, stack keras_nlp.layers.TransformerDecoder. usually just referred GPT2. keras_nlp.models.GPT2CausalLM: wraps GPT2Backbone, multiplies output GPT2Backbone embedding matrix generate logits vocab tokens.","code":""},{"path":"https://keras.posit.co/articles/examples/gpt2_text_generation_with_kerasnlp.html","id":"finetune-on-reddit-dataset","dir":"Articles > Examples","previous_headings":"","what":"Finetune on Reddit dataset","title":"GPT2 Text Generation with KerasNLP","text":"Now knowledge GPT-2 model KerasNLP, can take one step finetune model generates text specific style, short long, strict casual. tutorial, use reddit dataset example. Let’s take look inside sample data reddit TensorFlow Dataset. two features: document: text post. title: title. case, performing next word prediction language model, need ‘document’ feature. Now can finetune model using familiar fit() function. Note preprocessor automatically called inside fit method since GPT2CausalLM keras_nlp.models.Task instance. step takes quite bit GPU memory long time train way fully trained state. just use part dataset demo purposes. fine-tuning finished, can generate text using generate() function. time, text closer Reddit writing style, generated length close preset length training set.","code":"reddit_ds = tfds.load(\"reddit_tifu\", split=\"train\", as_supervised=True) for document, title in reddit_ds:     print(document.numpy())     print(title.numpy())     break train_ds = (     reddit_ds.map(lambda document, _: document)     .batch(32)     .cache()     .prefetch(tf.data.AUTOTUNE) ) train_ds = train_ds.take(500) num_epochs = 1  # Linearly decaying learning rate. learning_rate = keras.optimizers.schedules.PolynomialDecay(     5e-5,     decay_steps=train_ds.cardinality() * num_epochs,     end_learning_rate=0.0, ) loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True) gpt2_lm.compile(     optimizer=keras.optimizers.Adam(learning_rate),     loss=loss,     weighted_metrics=[\"accuracy\"], )  gpt2_lm.fit(train_ds, epochs=num_epochs) start = time.time()  output = gpt2_lm.generate(\"I like basketball\", max_length=200) print(\"\\nGPT-2 output:\") print(output)  end = time.time() print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"},{"path":"https://keras.posit.co/articles/examples/gpt2_text_generation_with_kerasnlp.html","id":"into-the-sampling-method","dir":"Articles > Examples","previous_headings":"","what":"Into the Sampling Method","title":"GPT2 Text Generation with KerasNLP","text":"KerasNLP, offer sampling methods, e.g., contrastive search, Top-K beam sampling. default, GPT2CausalLM uses Top-k search, can choose sampling method. Much like optimizer activations, two ways specify custom sampler: Use string identifier, “greedy”, using default configuration via way. Pass keras_nlp.samplers.Sampler instance, can use custom configuration via way. details KerasNLP Sampler class, can check code .","code":"# Use a string identifier. gpt2_lm.compile(sampler=\"top_k\") output = gpt2_lm.generate(\"I like basketball\", max_length=200) print(\"\\nGPT-2 output:\") print(output)  # Use a `Sampler` instance. `GreedySampler` tends to repeat itself, greedy_sampler = keras_nlp.samplers.GreedySampler() gpt2_lm.compile(sampler=greedy_sampler)  output = gpt2_lm.generate(\"I like basketball\", max_length=200) print(\"\\nGPT-2 output:\") print(output)"},{"path":"https://keras.posit.co/articles/examples/gpt2_text_generation_with_kerasnlp.html","id":"finetune-on-chinese-poem-dataset","dir":"Articles > Examples","previous_headings":"","what":"Finetune on Chinese Poem Dataset","title":"GPT2 Text Generation with KerasNLP","text":"can also finetune GPT2 non-English datasets. readers knowing Chinese, part illustrates fine-tune GPT2 Chinese poem dataset teach model become poet! GPT2 uses byte-pair encoder, original pretraining dataset contains Chinese characters, can use original vocab finetune Chinese dataset.","code":""},{"path":"https://keras.posit.co/articles/examples/gpt2_text_generation_with_kerasnlp.html","id":"load-chinese-poetry-dataset-","dir":"Articles > Examples","previous_headings":"","what":"Load chinese poetry dataset.","title":"GPT2 Text Generation with KerasNLP","text":"git clone https://github.com/chinese-poetry/chinese-poetry.git Load text json file. use《全唐诗》demo purposes. Let’s take look sample data. Similar Reddit example, convert TF dataset, use partial data train. Let’s check result! bad 😀","code":"poem_collection = [] for file in os.listdir(\"chinese-poetry/全唐诗\"):     if \".json\" not in file or \"poet\" not in file:         continue     full_filename = f\"{'chinese-poetry/全唐诗'}/{file}\"     with open(full_filename, \"r\") as f:         content = json.load(f)         poem_collection.extend(content)  paragraphs = [\"\".join(data[\"paragraphs\"]) for data in poem_collection] print(paragraphs[0]) train_ds = (     tf.data.Dataset.from_tensor_slices(paragraphs)     .batch(16)     .cache()     .prefetch(tf.data.AUTOTUNE) )  # Running through the whole dataset takes long, only take `500` and run 1 # epochs for demo purposes. train_ds = train_ds.take(500) num_epochs = 1  learning_rate = keras.optimizers.schedules.PolynomialDecay(     5e-4,     decay_steps=train_ds.cardinality() * num_epochs,     end_learning_rate=0.0, ) loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True) gpt2_lm.compile(     optimizer=keras.optimizers.Adam(learning_rate),     loss=loss,     weighted_metrics=[\"accuracy\"], )  gpt2_lm.fit(train_ds, epochs=num_epochs) output = gpt2_lm.generate(\"昨夜雨疏风骤\", max_length=200) print(output)"},{"path":"https://keras.posit.co/articles/examples/grad_cam.html","id":"configurable-parameters","dir":"Articles > Examples","previous_headings":"","what":"Configurable parameters","title":"Grad-CAM class activation visualization","text":"can change another model. get values last_conv_layer_name use model.summary() see names layers model.","code":"model_builder = keras.applications.xception.Xception img_size = (299, 299) preprocess_input = keras.applications.xception.preprocess_input decode_predictions = keras.applications.xception.decode_predictions  last_conv_layer_name = \"block14_sepconv2_act\"  # The local path to our target image img_path = keras.utils.get_file(     fname=\"african_elephant.jpg\", origin=\"https://i.imgur.com/Bvro0YD.png\" )  display(Image(img_path))"},{"path":"https://keras.posit.co/articles/examples/grad_cam.html","id":"the-grad-cam-algorithm","dir":"Articles > Examples","previous_headings":"","what":"The Grad-CAM algorithm","title":"Grad-CAM class activation visualization","text":"","code":"def get_img_array(img_path, size):     # `img` is a PIL image of size 299x299     img = keras.utils.load_img(img_path, target_size=size)     # `array` is a float32 Numpy array of shape (299, 299, 3)     array = keras.utils.img_to_array(img)     # We add a dimension to transform our array into a \"batch\"     # of size (1, 299, 299, 3)     array = np.expand_dims(array, axis=0)     return array   def make_gradcam_heatmap(     img_array, model, last_conv_layer_name, pred_index=None ):     # First, we create a model that maps the input image to the activations     # of the last conv layer as well as the output predictions     grad_model = keras.models.Model(         model.inputs,         [model.get_layer(last_conv_layer_name).output, model.output],     )      # Then, we compute the gradient of the top predicted class for our input image     # with respect to the activations of the last conv layer     with tf.GradientTape() as tape:         last_conv_layer_output, preds = grad_model(img_array)         if pred_index is None:             pred_index = tf.argmax(preds[0])         class_channel = preds[:, pred_index]      # This is the gradient of the output neuron (top predicted or chosen)     # with regard to the output feature map of the last conv layer     grads = tape.gradient(class_channel, last_conv_layer_output)      # This is a vector where each entry is the mean intensity of the gradient     # over a specific feature map channel     pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))      # We multiply each channel in the feature map array     # by \"how important this channel is\" with regard to the top predicted class     # then sum all the channels to obtain the heatmap class activation     last_conv_layer_output = last_conv_layer_output[0]     heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]     heatmap = tf.squeeze(heatmap)      # For visualization purpose, we will also normalize the heatmap between 0 & 1     heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)     return heatmap.numpy()"},{"path":"https://keras.posit.co/articles/examples/grad_cam.html","id":"lets-test-drive-it","dir":"Articles > Examples","previous_headings":"","what":"Let’s test-drive it","title":"Grad-CAM class activation visualization","text":"","code":"# Prepare image img_array = preprocess_input(get_img_array(img_path, size=img_size))  # Make model model = model_builder(weights=\"imagenet\")  # Remove last layer's softmax model.layers[-1].activation = None  # Print what the top predicted class is preds = model.predict(img_array) print(\"Predicted:\", decode_predictions(preds, top=1)[0])  # Generate class activation heatmap heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)  # Display heatmap plt.matshow(heatmap) plt.show()"},{"path":"https://keras.posit.co/articles/examples/grad_cam.html","id":"create-a-superimposed-visualization","dir":"Articles > Examples","previous_headings":"","what":"Create a superimposed visualization","title":"Grad-CAM class activation visualization","text":"","code":"def save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):     # Load the original image     img = keras.utils.load_img(img_path)     img = keras.utils.img_to_array(img)      # Rescale heatmap to a range 0-255     heatmap = np.uint8(255 * heatmap)      # Use jet colormap to colorize heatmap     jet = cm.get_cmap(\"jet\")      # Use RGB values of the colormap     jet_colors = jet(np.arange(256))[:, :3]     jet_heatmap = jet_colors[heatmap]      # Create an image with RGB colorized heatmap     jet_heatmap = keras.utils.array_to_img(jet_heatmap)     jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))     jet_heatmap = keras.utils.img_to_array(jet_heatmap)      # Superimpose the heatmap on original image     superimposed_img = jet_heatmap * alpha + img     superimposed_img = keras.utils.array_to_img(superimposed_img)      # Save the superimposed image     superimposed_img.save(cam_path)      # Display Grad CAM     display(Image(cam_path))   save_and_display_gradcam(img_path, heatmap)"},{"path":"https://keras.posit.co/articles/examples/grad_cam.html","id":"lets-try-another-image","dir":"Articles > Examples","previous_headings":"","what":"Let’s try another image","title":"Grad-CAM class activation visualization","text":"see grad cam explains model’s outputs multi-label image. Let’s try image cat dog together, see grad cam behaves. generate class activation heatmap “chow,” class index 260 generate class activation heatmap “egyptian cat,” class index 285","code":"img_path = keras.utils.get_file(     fname=\"cat_and_dog.jpg\",     origin=\"https://storage.googleapis.com/petbacker/images/blog/2017/dog-and-cat-cover.jpg\", )  display(Image(img_path))  # Prepare image img_array = preprocess_input(get_img_array(img_path, size=img_size))  # Print what the two top predicted classes are preds = model.predict(img_array) print(\"Predicted:\", decode_predictions(preds, top=2)[0]) heatmap = make_gradcam_heatmap(     img_array, model, last_conv_layer_name, pred_index=260 )  save_and_display_gradcam(img_path, heatmap) heatmap = make_gradcam_heatmap(     img_array, model, last_conv_layer_name, pred_index=285 )  save_and_display_gradcam(img_path, heatmap)"},{"path":"https://keras.posit.co/articles/examples/gradient_centralization.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Gradient Centralization for Better Training Performance","text":"example implements Gradient Centralization, new optimization technique Deep Neural Networks Yong et al., demonstrates Laurence Moroney’s Horses Humans Dataset. Gradient Centralization can speedup training process improve final generalization performance DNNs. operates directly gradients centralizing gradient vectors zero mean. Gradient Centralization morever improves Lipschitzness loss function gradient training process becomes efficient stable. example requires tensorflow_datasets can installed command:","code":"pip install tensorflow-datasets"},{"path":"https://keras.posit.co/articles/examples/gradient_centralization.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Gradient Centralization for Better Training Performance","text":"","code":"from time import time  import keras as keras from keras import layers from keras.optimizers import RMSprop from keras import ops  from tensorflow import data as tf_data import tensorflow_datasets as tfds"},{"path":"https://keras.posit.co/articles/examples/gradient_centralization.html","id":"prepare-the-data","dir":"Articles > Examples","previous_headings":"","what":"Prepare the data","title":"Gradient Centralization for Better Training Performance","text":"example, using Horses Humans dataset.","code":"num_classes = 2 input_shape = (300, 300, 3) dataset_name = \"horses_or_humans\" batch_size = 128 AUTOTUNE = tf_data.AUTOTUNE  (train_ds, test_ds), metadata = tfds.load(     name=dataset_name,     split=[tfds.Split.TRAIN, tfds.Split.TEST],     with_info=True,     as_supervised=True, )  print(f\"Image shape: {metadata.features['image'].shape}\") print(f\"Training images: {metadata.splits['train'].num_examples}\") print(f\"Test images: {metadata.splits['test'].num_examples}\")"},{"path":"https://keras.posit.co/articles/examples/gradient_centralization.html","id":"use-data-augmentation","dir":"Articles > Examples","previous_headings":"","what":"Use Data Augmentation","title":"Gradient Centralization for Better Training Performance","text":"rescale data [0, 1] perform simple augmentations data. Rescale augment data","code":"rescale = layers.Rescaling(1.0 / 255)  data_augmentation = [     layers.RandomFlip(\"horizontal_and_vertical\"),     layers.RandomRotation(0.3),     layers.RandomZoom(0.2), ]   # Helper to apply augmentation def apply_aug(x):     for aug in data_augmentation:         x = aug(x)     return x   def prepare(ds, shuffle=False, augment=False):     # Rescale dataset     ds = ds.map(lambda x, y: (rescale(x), y), num_parallel_calls=AUTOTUNE)      if shuffle:         ds = ds.shuffle(1024)      # Batch dataset     ds = ds.batch(batch_size)      # Use data augmentation only on the training set     if augment:         ds = ds.map(             lambda x, y: (apply_aug(x), y),             num_parallel_calls=AUTOTUNE,         )      # Use buffered prefecting     return ds.prefetch(buffer_size=AUTOTUNE) train_ds = prepare(train_ds, shuffle=True, augment=True) test_ds = prepare(test_ds)"},{"path":"https://keras.posit.co/articles/examples/gradient_centralization.html","id":"define-a-model","dir":"Articles > Examples","previous_headings":"","what":"Define a model","title":"Gradient Centralization for Better Training Performance","text":"section define Convolutional neural network.","code":"model = keras.Sequential(     [         layers.Conv2D(16, (3, 3), activation=\"relu\", input_shape=(300, 300, 3)),         layers.MaxPooling2D(2, 2),         layers.Conv2D(32, (3, 3), activation=\"relu\"),         layers.Dropout(0.5),         layers.MaxPooling2D(2, 2),         layers.Conv2D(64, (3, 3), activation=\"relu\"),         layers.Dropout(0.5),         layers.MaxPooling2D(2, 2),         layers.Conv2D(64, (3, 3), activation=\"relu\"),         layers.MaxPooling2D(2, 2),         layers.Conv2D(64, (3, 3), activation=\"relu\"),         layers.MaxPooling2D(2, 2),         layers.Flatten(),         layers.Dropout(0.5),         layers.Dense(512, activation=\"relu\"),         layers.Dense(1, activation=\"sigmoid\"),     ] )"},{"path":"https://keras.posit.co/articles/examples/gradient_centralization.html","id":"implement-gradient-centralization","dir":"Articles > Examples","previous_headings":"","what":"Implement Gradient Centralization","title":"Gradient Centralization for Better Training Performance","text":"now subclass RMSProp optimizer class modifying keras.optimizers.Optimizer.get_gradients() method now implement Gradient Centralization. high level idea let us say obtain gradients back propogation Dense Convolution layer compute mean column vectors weight matrix, remove mean column vector. experiments paper various applications, including general image classification, fine-grained image classification, detection segmentation Person ReID demonstrate GC can consistently improve performance DNN learning. Also, simplicity moment implementing gradient cliiping functionality, however quite easy implement. moment just creating subclass RMSProp optimizer however easily reproduce optimizer custom optimizer way. using class later section train model Gradient Centralization.","code":"class GCRMSprop(RMSprop):     def get_gradients(self, loss, params):         # We here just provide a modified get_gradients() function since we are         # trying to just compute the centralized gradients.          grads = []         gradients = super().get_gradients()         for grad in gradients:             grad_len = len(grad.shape)             if grad_len > 1:                 axis = list(range(grad_len - 1))                 grad -= ops.mean(grad, axis=axis, keep_dims=True)             grads.append(grad)          return grads   optimizer = GCRMSprop(learning_rate=1e-4)"},{"path":"https://keras.posit.co/articles/examples/gradient_centralization.html","id":"training-utilities","dir":"Articles > Examples","previous_headings":"","what":"Training utilities","title":"Gradient Centralization for Better Training Performance","text":"also create callback allows us easily measure total training time time taken epoch since interested comparing effect Gradient Centralization model built .","code":"class TimeHistory(keras.callbacks.Callback):     def on_train_begin(self, logs={}):         self.times = []      def on_epoch_begin(self, batch, logs={}):         self.epoch_time_start = time()      def on_epoch_end(self, batch, logs={}):         self.times.append(time() - self.epoch_time_start)"},{"path":"https://keras.posit.co/articles/examples/gradient_centralization.html","id":"train-the-model-without-gc","dir":"Articles > Examples","previous_headings":"","what":"Train the model without GC","title":"Gradient Centralization for Better Training Performance","text":"now train model built earlier without Gradient Centralization can compare training performance model trained Gradient Centralization. also save history since later want compare model trained trained Gradient Centralization","code":"time_callback_no_gc = TimeHistory() model.compile(     loss=\"binary_crossentropy\",     optimizer=RMSprop(learning_rate=1e-4),     metrics=[\"accuracy\"], )  model.summary() history_no_gc = model.fit(     train_ds, epochs=10, verbose=1, callbacks=[time_callback_no_gc] )"},{"path":"https://keras.posit.co/articles/examples/gradient_centralization.html","id":"train-the-model-with-gc","dir":"Articles > Examples","previous_headings":"","what":"Train the model with GC","title":"Gradient Centralization for Better Training Performance","text":"now train model, time using Gradient Centralization, notice optimizer one using Gradient Centralization time.","code":"time_callback_gc = TimeHistory() model.compile(     loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"] )  model.summary()  history_gc = model.fit(     train_ds, epochs=10, verbose=1, callbacks=[time_callback_gc] )"},{"path":"https://keras.posit.co/articles/examples/gradient_centralization.html","id":"comparing-performance","dir":"Articles > Examples","previous_headings":"","what":"Comparing performance","title":"Gradient Centralization for Better Training Performance","text":"Readers encouraged try Gradient Centralization different datasets different domains experiment ’s effect. strongly advised check original paper well - authors present several studies Gradient Centralization showing can improve general performance, generalization, training time well efficient. Many thanks Ali Mustufa Shaikh reviewing implementation.","code":"print(\"Not using Gradient Centralization\") print(f\"Loss: {history_no_gc.history['loss'][-1]}\") print(f\"Accuracy: {history_no_gc.history['accuracy'][-1]}\") print(f\"Training Time: {sum(time_callback_no_gc.times)}\")  print(\"Using Gradient Centralization\") print(f\"Loss: {history_gc.history['loss'][-1]}\") print(f\"Accuracy: {history_gc.history['accuracy'][-1]}\") print(f\"Training Time: {sum(time_callback_gc.times)}\")"},{"path":"https://keras.posit.co/articles/examples/image_captioning.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Image Captioning","text":"","code":"import os import re import numpy as np import matplotlib.pyplot as plt  import tensorflow as tf import keras as keras from keras import layers from keras.applications import efficientnet from keras.layers import TextVectorization   seed = 111 np.random.seed(seed) tf.random.set_seed(seed)"},{"path":"https://keras.posit.co/articles/examples/image_captioning.html","id":"download-the-dataset","dir":"Articles > Examples","previous_headings":"","what":"Download the dataset","title":"Image Captioning","text":"using Flickr8K dataset tutorial. dataset comprises 8,000 images, paired five different captions. wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip unzip -qq Flickr8k_Dataset.zip unzip -qq Flickr8k_text.zip rm Flickr8k_Dataset.zip Flickr8k_text.zip","code":"# Path to the images IMAGES_PATH = \"Flicker8k_Dataset\"  # Desired image dimensions IMAGE_SIZE = (299, 299)  # Vocabulary size VOCAB_SIZE = 10000  # Fixed length allowed for any sequence SEQ_LENGTH = 25  # Dimension for the image embeddings and token embeddings EMBED_DIM = 512  # Per-layer units in the feed-forward network FF_DIM = 512  # Other training parameters BATCH_SIZE = 64 EPOCHS = 30 AUTOTUNE = tf.data.AUTOTUNE"},{"path":"https://keras.posit.co/articles/examples/image_captioning.html","id":"preparing-the-dataset","dir":"Articles > Examples","previous_headings":"","what":"Preparing the dataset","title":"Image Captioning","text":"","code":"def load_captions_data(filename):     \"\"\"Loads captions (text) data and maps them to corresponding images.      Args:         filename: Path to the text file containing caption data.      Returns:         caption_mapping: Dictionary mapping image names and the corresponding captions         text_data: List containing all the available captions     \"\"\"      with open(filename) as caption_file:         caption_data = caption_file.readlines()         caption_mapping = {}         text_data = []         images_to_skip = set()          for line in caption_data:             line = line.rstrip(\"\\n\")             # Image name and captions are separated using a tab             img_name, caption = line.split(\"\\t\")              # Each image is repeated five times for the five different captions.             # Each image name has a suffix `#(caption_number)`             img_name = img_name.split(\"#\")[0]             img_name = os.path.join(IMAGES_PATH, img_name.strip())              # We will remove caption that are either too short to too long             tokens = caption.strip().split()              if len(tokens) < 5 or len(tokens) > SEQ_LENGTH:                 images_to_skip.add(img_name)                 continue              if img_name.endswith(\"jpg\") and img_name not in images_to_skip:                 # We will add a start and an end token to each caption                 caption = \"<start> \" + caption.strip() + \" <end>\"                 text_data.append(caption)                  if img_name in caption_mapping:                     caption_mapping[img_name].append(caption)                 else:                     caption_mapping[img_name] = [caption]          for img_name in images_to_skip:             if img_name in caption_mapping:                 del caption_mapping[img_name]          return caption_mapping, text_data   def train_val_split(caption_data, train_size=0.8, shuffle=True):     \"\"\"Split the captioning dataset into train and validation sets.      Args:         caption_data (dict): Dictionary containing the mapped caption data         train_size (float): Fraction of all the full dataset to use as training data         shuffle (bool): Whether to shuffle the dataset before splitting      Returns:         Traning and validation datasets as two separated dicts     \"\"\"      # 1. Get the list of all image names     all_images = list(caption_data.keys())      # 2. Shuffle if necessary     if shuffle:         np.random.shuffle(all_images)      # 3. Split into training and validation sets     train_size = int(len(caption_data) * train_size)      training_data = {         img_name: caption_data[img_name] for img_name in all_images[:train_size]     }     validation_data = {         img_name: caption_data[img_name] for img_name in all_images[train_size:]     }      # 4. Return the splits     return training_data, validation_data   # Load the dataset captions_mapping, text_data = load_captions_data(\"Flickr8k.token.txt\")  # Split the dataset into training and validation sets train_data, valid_data = train_val_split(captions_mapping) print(\"Number of training samples: \", len(train_data)) print(\"Number of validation samples: \", len(valid_data))"},{"path":"https://keras.posit.co/articles/examples/image_captioning.html","id":"vectorizing-the-text-data","dir":"Articles > Examples","previous_headings":"","what":"Vectorizing the text data","title":"Image Captioning","text":"’ll use TextVectorization layer vectorize text data, say, turn original strings integer sequences integer represents index word vocabulary. use custom string standardization scheme (strip punctuation characters except < >) default splitting scheme (split whitespace).","code":"def custom_standardization(input_string):     lowercase = tf.strings.lower(input_string)     return tf.strings.regex_replace(         lowercase, \"[%s]\" % re.escape(strip_chars), \"\"     )   strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\" strip_chars = strip_chars.replace(\"<\", \"\") strip_chars = strip_chars.replace(\">\", \"\")  vectorization = TextVectorization(     max_tokens=VOCAB_SIZE,     output_mode=\"int\",     output_sequence_length=SEQ_LENGTH,     standardize=custom_standardization, ) vectorization.adapt(text_data)  # Data augmentation for image data image_augmentation = keras.Sequential(     [         layers.RandomFlip(\"horizontal\"),         layers.RandomRotation(0.2),         layers.RandomContrast(0.3),     ] )"},{"path":"https://keras.posit.co/articles/examples/image_captioning.html","id":"building-a-tf-data-dataset-pipeline-for-training","dir":"Articles > Examples","previous_headings":"","what":"Building a tf.data.Dataset pipeline for training","title":"Image Captioning","text":"generate pairs images corresponding captions using tf.data.Dataset object. pipeline consists two steps: Read image disk Tokenize five captions corresponding image","code":"def decode_and_resize(img_path):     img = tf.io.read_file(img_path)     img = tf.image.decode_jpeg(img, channels=3)     img = tf.image.resize(img, IMAGE_SIZE)     img = tf.image.convert_image_dtype(img, tf.float32)     return img   def process_input(img_path, captions):     return decode_and_resize(img_path), vectorization(captions)   def make_dataset(images, captions):     dataset = tf.data.Dataset.from_tensor_slices((images, captions))     dataset = dataset.shuffle(BATCH_SIZE * 8)     dataset = dataset.map(process_input, num_parallel_calls=AUTOTUNE)     dataset = dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)      return dataset   # Pass the list of images and the list of corresponding captions train_dataset = make_dataset(list(train_data.keys()), list(train_data.values()))  valid_dataset = make_dataset(list(valid_data.keys()), list(valid_data.values()))"},{"path":"https://keras.posit.co/articles/examples/image_captioning.html","id":"building-the-model","dir":"Articles > Examples","previous_headings":"","what":"Building the model","title":"Image Captioning","text":"image captioning architecture consists three models: CNN: used extract image features TransformerEncoder: extracted image features passed Transformer based encoder generates new representation inputs TransformerDecoder: model takes encoder output text data (sequences) inputs tries learn generate caption.","code":"def get_cnn_model():     base_model = efficientnet.EfficientNetB0(         input_shape=(*IMAGE_SIZE, 3),         include_top=False,         weights=\"imagenet\",     )     # We freeze our feature extractor     base_model.trainable = False     base_model_out = base_model.output     base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(         base_model_out     )     cnn_model = keras.models.Model(base_model.input, base_model_out)     return cnn_model   class TransformerEncoderBlock(layers.Layer):     def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):         super().__init__(**kwargs)         self.embed_dim = embed_dim         self.dense_dim = dense_dim         self.num_heads = num_heads         self.attention_1 = layers.MultiHeadAttention(             num_heads=num_heads, key_dim=embed_dim, dropout=0.0         )         self.layernorm_1 = layers.LayerNormalization()         self.layernorm_2 = layers.LayerNormalization()         self.dense_1 = layers.Dense(embed_dim, activation=\"relu\")      def call(self, inputs, training, mask=None):         inputs = self.layernorm_1(inputs)         inputs = self.dense_1(inputs)          attention_output_1 = self.attention_1(             query=inputs,             value=inputs,             key=inputs,             attention_mask=None,             training=training,         )         out_1 = self.layernorm_2(inputs + attention_output_1)         return out_1   class PositionalEmbedding(layers.Layer):     def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):         super().__init__(**kwargs)         self.token_embeddings = layers.Embedding(             input_dim=vocab_size, output_dim=embed_dim         )         self.position_embeddings = layers.Embedding(             input_dim=sequence_length, output_dim=embed_dim         )         self.sequence_length = sequence_length         self.vocab_size = vocab_size         self.embed_dim = embed_dim         self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))      def call(self, inputs):         length = tf.shape(inputs)[-1]         positions = tf.range(start=0, limit=length, delta=1)         embedded_tokens = self.token_embeddings(inputs)         embedded_tokens = embedded_tokens * self.embed_scale         embedded_positions = self.position_embeddings(positions)         return embedded_tokens + embedded_positions      def compute_mask(self, inputs, mask=None):         return tf.math.not_equal(inputs, 0)   class TransformerDecoderBlock(layers.Layer):     def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):         super().__init__(**kwargs)         self.embed_dim = embed_dim         self.ff_dim = ff_dim         self.num_heads = num_heads         self.attention_1 = layers.MultiHeadAttention(             num_heads=num_heads, key_dim=embed_dim, dropout=0.1         )         self.attention_2 = layers.MultiHeadAttention(             num_heads=num_heads, key_dim=embed_dim, dropout=0.1         )         self.ffn_layer_1 = layers.Dense(ff_dim, activation=\"relu\")         self.ffn_layer_2 = layers.Dense(embed_dim)          self.layernorm_1 = layers.LayerNormalization()         self.layernorm_2 = layers.LayerNormalization()         self.layernorm_3 = layers.LayerNormalization()          self.embedding = PositionalEmbedding(             embed_dim=EMBED_DIM,             sequence_length=SEQ_LENGTH,             vocab_size=VOCAB_SIZE,         )         self.out = layers.Dense(VOCAB_SIZE, activation=\"softmax\")          self.dropout_1 = layers.Dropout(0.3)         self.dropout_2 = layers.Dropout(0.5)         self.supports_masking = True      def call(self, inputs, encoder_outputs, training, mask=None):         inputs = self.embedding(inputs)         causal_mask = self.get_causal_attention_mask(inputs)          if mask is not None:             padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)             combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)             combined_mask = tf.minimum(combined_mask, causal_mask)          attention_output_1 = self.attention_1(             query=inputs,             value=inputs,             key=inputs,             attention_mask=combined_mask,             training=training,         )         out_1 = self.layernorm_1(inputs + attention_output_1)          attention_output_2 = self.attention_2(             query=out_1,             value=encoder_outputs,             key=encoder_outputs,             attention_mask=padding_mask,             training=training,         )         out_2 = self.layernorm_2(out_1 + attention_output_2)          ffn_out = self.ffn_layer_1(out_2)         ffn_out = self.dropout_1(ffn_out, training=training)         ffn_out = self.ffn_layer_2(ffn_out)          ffn_out = self.layernorm_3(ffn_out + out_2, training=training)         ffn_out = self.dropout_2(ffn_out, training=training)         preds = self.out(ffn_out)         return preds      def get_causal_attention_mask(self, inputs):         input_shape = tf.shape(inputs)         batch_size, sequence_length = input_shape[0], input_shape[1]         i = tf.range(sequence_length)[:, tf.newaxis]         j = tf.range(sequence_length)         mask = tf.cast(i >= j, dtype=\"int32\")         mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))         mult = tf.concat(             [                 tf.expand_dims(batch_size, -1),                 tf.constant([1, 1], dtype=tf.int32),             ],             axis=0,         )         return tf.tile(mask, mult)   class ImageCaptioningModel(keras.Model):     def __init__(         self,         cnn_model,         encoder,         decoder,         num_captions_per_image=5,         image_aug=None,     ):         super().__init__()         self.cnn_model = cnn_model         self.encoder = encoder         self.decoder = decoder         self.loss_tracker = keras.metrics.Mean(name=\"loss\")         self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")         self.num_captions_per_image = num_captions_per_image         self.image_aug = image_aug      def calculate_loss(self, y_true, y_pred, mask):         loss = self.loss(y_true, y_pred)         mask = tf.cast(mask, dtype=loss.dtype)         loss *= mask         return tf.reduce_sum(loss) / tf.reduce_sum(mask)      def calculate_accuracy(self, y_true, y_pred, mask):         accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))         accuracy = tf.math.logical_and(mask, accuracy)         accuracy = tf.cast(accuracy, dtype=tf.float32)         mask = tf.cast(mask, dtype=tf.float32)         return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)      def _compute_caption_loss_and_acc(         self, img_embed, batch_seq, training=True     ):         encoder_out = self.encoder(img_embed, training=training)         batch_seq_inp = batch_seq[:, :-1]         batch_seq_true = batch_seq[:, 1:]         mask = tf.math.not_equal(batch_seq_true, 0)         batch_seq_pred = self.decoder(             batch_seq_inp, encoder_out, training=training, mask=mask         )         loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)         acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)         return loss, acc      def train_step(self, batch_data):         batch_img, batch_seq = batch_data         batch_loss = 0         batch_acc = 0          if self.image_aug:             batch_img = self.image_aug(batch_img)          # 1. Get image embeddings         img_embed = self.cnn_model(batch_img)          # 2. Pass each of the five captions one by one to the decoder         # along with the encoder outputs and compute the loss as well as accuracy         # for each caption.         for i in range(self.num_captions_per_image):             with tf.GradientTape() as tape:                 loss, acc = self._compute_caption_loss_and_acc(                     img_embed, batch_seq[:, i, :], training=True                 )                  # 3. Update loss and accuracy                 batch_loss += loss                 batch_acc += acc              # 4. Get the list of all the trainable weights             train_vars = (                 self.encoder.trainable_variables                 + self.decoder.trainable_variables             )              # 5. Get the gradients             grads = tape.gradient(loss, train_vars)              # 6. Update the trainable weights             self.optimizer.apply_gradients(zip(grads, train_vars))          # 7. Update the trackers         batch_acc /= float(self.num_captions_per_image)         self.loss_tracker.update_state(batch_loss)         self.acc_tracker.update_state(batch_acc)          # 8. Return the loss and accuracy values         return {             \"loss\": self.loss_tracker.result(),             \"acc\": self.acc_tracker.result(),         }      def test_step(self, batch_data):         batch_img, batch_seq = batch_data         batch_loss = 0         batch_acc = 0          # 1. Get image embeddings         img_embed = self.cnn_model(batch_img)          # 2. Pass each of the five captions one by one to the decoder         # along with the encoder outputs and compute the loss as well as accuracy         # for each caption.         for i in range(self.num_captions_per_image):             loss, acc = self._compute_caption_loss_and_acc(                 img_embed, batch_seq[:, i, :], training=False             )              # 3. Update batch loss and batch accuracy             batch_loss += loss             batch_acc += acc          batch_acc /= float(self.num_captions_per_image)          # 4. Update the trackers         self.loss_tracker.update_state(batch_loss)         self.acc_tracker.update_state(batch_acc)          # 5. Return the loss and accuracy values         return {             \"loss\": self.loss_tracker.result(),             \"acc\": self.acc_tracker.result(),         }      @property     def metrics(self):         # We need to list our metrics here so the `reset_states()` can be         # called automatically.         return [self.loss_tracker, self.acc_tracker]   cnn_model = get_cnn_model() encoder = TransformerEncoderBlock(     embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=1 ) decoder = TransformerDecoderBlock(     embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=2 ) caption_model = ImageCaptioningModel(     cnn_model=cnn_model,     encoder=encoder,     decoder=decoder,     image_aug=image_augmentation, )"},{"path":"https://keras.posit.co/articles/examples/image_captioning.html","id":"model-training","dir":"Articles > Examples","previous_headings":"","what":"Model training","title":"Image Captioning","text":"","code":"# Define the loss function cross_entropy = keras.losses.SparseCategoricalCrossentropy(     from_logits=False,     reduction=None, )  # EarlyStopping criteria early_stopping = keras.callbacks.EarlyStopping(     patience=3, restore_best_weights=True )   # Learning Rate Scheduler for the optimizer class LRSchedule(     keras.optimizers.schedules.learning_rate_schedule.LearningRateSchedule ):     def __init__(self, post_warmup_learning_rate, warmup_steps):         super().__init__()         self.post_warmup_learning_rate = post_warmup_learning_rate         self.warmup_steps = warmup_steps      def __call__(self, step):         global_step = tf.cast(step, tf.float32)         warmup_steps = tf.cast(self.warmup_steps, tf.float32)         warmup_progress = global_step / warmup_steps         warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress         return tf.cond(             global_step < warmup_steps,             lambda: warmup_learning_rate,             lambda: self.post_warmup_learning_rate,         )   # Create a learning rate schedule num_train_steps = len(train_dataset) * EPOCHS num_warmup_steps = num_train_steps // 15 lr_schedule = LRSchedule(     post_warmup_learning_rate=1e-4, warmup_steps=num_warmup_steps )  # Compile the model caption_model.compile(     optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy )  # Fit the model caption_model.fit(     train_dataset,     epochs=EPOCHS,     validation_data=valid_dataset,     callbacks=[early_stopping], )"},{"path":"https://keras.posit.co/articles/examples/image_captioning.html","id":"check-sample-predictions","dir":"Articles > Examples","previous_headings":"","what":"Check sample predictions","title":"Image Captioning","text":"","code":"vocab = vectorization.get_vocabulary() index_lookup = dict(zip(range(len(vocab)), vocab)) max_decoded_sentence_length = SEQ_LENGTH - 1 valid_images = list(valid_data.keys())   def generate_caption():     # Select a random image from the validation dataset     sample_img = np.random.choice(valid_images)      # Read the image from the disk     sample_img = decode_and_resize(sample_img)     img = sample_img.numpy().clip(0, 255).astype(np.uint8)     plt.imshow(img)     plt.show()      # Pass the image to the CNN     img = tf.expand_dims(sample_img, 0)     img = caption_model.cnn_model(img)      # Pass the image features to the Transformer encoder     encoded_img = caption_model.encoder(img, training=False)      # Generate the caption using the Transformer decoder     decoded_caption = \"<start> \"     for i in range(max_decoded_sentence_length):         tokenized_caption = vectorization([decoded_caption])[:, :-1]         mask = tf.math.not_equal(tokenized_caption, 0)         predictions = caption_model.decoder(             tokenized_caption, encoded_img, training=False, mask=mask         )         sampled_token_index = np.argmax(predictions[0, i, :])         sampled_token = index_lookup[sampled_token_index]         if sampled_token == \"<end>\":             break         decoded_caption += \" \" + sampled_token      decoded_caption = decoded_caption.replace(\"<start> \", \"\")     decoded_caption = decoded_caption.replace(\" <end>\", \"\").strip()     print(\"Predicted Caption: \", decoded_caption)   # Check predictions for a few samples generate_caption() generate_caption() generate_caption()"},{"path":"https://keras.posit.co/articles/examples/image_captioning.html","id":"end-notes","dir":"Articles > Examples","previous_headings":"","what":"End Notes","title":"Image Captioning","text":"saw model starts generate reasonable captions epochs. keep example easily runnable, trained constraints, like minimal number attention heads. improve predictions, can try changing training settings find good model use case.","code":""},{"path":"https://keras.posit.co/articles/examples/image_classification_from_scratch.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Image classification from scratch","text":"example shows image classification scratch, starting JPEG image files disk, without leveraging pre-trained weights pre-made Keras Application model. demonstrate workflow Kaggle Cats vs Dogs binary classification dataset. use image_dataset_from_directory utility generate datasets, use Keras image preprocessing layers image standardization data augmentation.","code":""},{"path":"https://keras.posit.co/articles/examples/image_classification_from_scratch.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Image classification from scratch","text":"","code":"import tensorflow as tf import keras as keras from keras import layers import os from pathlib import Path import matplotlib.pyplot as plt"},{"path":[]},{"path":"https://keras.posit.co/articles/examples/image_classification_from_scratch.html","id":"raw-data-download","dir":"Articles > Examples","previous_headings":"Load the data: the Cats vs Dogs dataset","what":"Raw data download","title":"Image classification from scratch","text":"First, let’s download 786M ZIP archive raw data: Now PetImages folder contain two subfolders, Cat Dog. subfolder contains image files category.","code":"fpath = keras.utils.get_file(     origin=\"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\" ) dirpath = Path(fpath).parent.absolute() os.system(f\"unzip -q {fpath} -d {dirpath}\") os.system(f\"ls {dirpath}/PetImages/\")"},{"path":"https://keras.posit.co/articles/examples/image_classification_from_scratch.html","id":"filter-out-corrupted-images","dir":"Articles > Examples","previous_headings":"Load the data: the Cats vs Dogs dataset","what":"Filter out corrupted images","title":"Image classification from scratch","text":"working lots real-world image data, corrupted images common occurence. Let’s filter badly-encoded images feature string “JFIF” header.","code":"import os  num_skipped = 0 for folder_name in (\"Cat\", \"Dog\"):     folder_path = os.path.join(dirpath, \"PetImages\", folder_name)     for fname in os.listdir(folder_path):         fpath = os.path.join(folder_path, fname)         try:             fobj = open(fpath, \"rb\")             is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)         finally:             fobj.close()          if not is_jfif:             num_skipped += 1             # Delete corrupted image             os.remove(fpath)  print(f\"Deleted {num_skipped} images\")"},{"path":"https://keras.posit.co/articles/examples/image_classification_from_scratch.html","id":"generate-a-dataset","dir":"Articles > Examples","previous_headings":"","what":"Generate a Dataset","title":"Image classification from scratch","text":"","code":"image_size = (180, 180) batch_size = 128  train_ds, val_ds = keras.utils.image_dataset_from_directory(     os.path.join(dirpath, \"PetImages\"),     validation_split=0.2,     subset=\"both\",     seed=1337,     image_size=image_size,     batch_size=batch_size, )"},{"path":"https://keras.posit.co/articles/examples/image_classification_from_scratch.html","id":"visualize-the-data","dir":"Articles > Examples","previous_headings":"","what":"Visualize the data","title":"Image classification from scratch","text":"first 9 images training dataset. can see, label 1 “dog” label 0 “cat”.","code":"plt.figure(figsize=(10, 10)) for images, labels in train_ds.take(1):     for i in range(9):         ax = plt.subplot(3, 3, i + 1)         plt.imshow(images[i].numpy().astype(\"uint8\"))         plt.title(int(labels[i]))         plt.axis(\"off\")"},{"path":"https://keras.posit.co/articles/examples/image_classification_from_scratch.html","id":"using-image-data-augmentation","dir":"Articles > Examples","previous_headings":"","what":"Using image data augmentation","title":"Image classification from scratch","text":"don’t large image dataset, ’s good practice artificially introduce sample diversity applying random yet realistic transformations training images, random horizontal flipping small random rotations. helps expose model different aspects training data slowing overfitting. Let’s visualize augmented samples look like, applying data_augmentation repeatedly first image dataset:","code":"data_augmentation = keras.Sequential(     [         layers.RandomFlip(\"horizontal\"),         layers.RandomRotation(0.1),     ] ) plt.figure(figsize=(10, 10)) for images, _ in train_ds.take(1):     for i in range(9):         augmented_images = data_augmentation(images)         ax = plt.subplot(3, 3, i + 1)         plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))         plt.axis(\"off\")"},{"path":"https://keras.posit.co/articles/examples/image_classification_from_scratch.html","id":"standardizing-the-data","dir":"Articles > Examples","previous_headings":"","what":"Standardizing the data","title":"Image classification from scratch","text":"image already standard size (180x180), yielded contiguous float32 batches dataset. However, RGB channel values [0, 255] range. ideal neural network; general seek make input values small. , standardize values [0, 1] using Rescaling layer start model.","code":""},{"path":"https://keras.posit.co/articles/examples/image_classification_from_scratch.html","id":"two-options-to-preprocess-the-data","dir":"Articles > Examples","previous_headings":"","what":"Two options to preprocess the data","title":"Image classification from scratch","text":"two ways using data_augmentation preprocessor: Option 1: Make part model, like : option, data augmentation happen device, synchronously rest model execution, meaning benefit GPU acceleration. Note data augmentation inactive test time, input samples augmented fit(), calling evaluate() predict(). ’re training GPU, may good option. Option 2: apply dataset, obtain dataset yields batches augmented images, like : option, data augmentation happen CPU, asynchronously, buffered going model. ’re training CPU, better option, since makes data augmentation asynchronous non-blocking. case, ’ll go second option. ’re sure one pick, second option (asynchronous preprocessing) always solid choice.","code":"inputs = keras.Input(shape=input_shape) x = data_augmentation(inputs) x = layers.Rescaling(1./255)(x) ...  # Rest of the model augmented_train_ds = train_ds.map(     lambda x, y: (data_augmentation(x, training=True), y))"},{"path":"https://keras.posit.co/articles/examples/image_classification_from_scratch.html","id":"configure-the-dataset-for-performance","dir":"Articles > Examples","previous_headings":"","what":"Configure the dataset for performance","title":"Image classification from scratch","text":"Let’s apply data augmentation training dataset, let’s make sure use buffered prefetching can yield data disk without /O becoming blocking:","code":"print(\"Make datasets\") # Apply `data_augmentation` to the training images. train_ds = train_ds.map(     lambda img, label: (data_augmentation(img), label),     num_parallel_calls=tf.data.AUTOTUNE, ) # Prefetching samples in GPU memory helps maximize GPU utilization. train_ds = train_ds.prefetch(tf.data.AUTOTUNE) val_ds = val_ds.prefetch(tf.data.AUTOTUNE)"},{"path":"https://keras.posit.co/articles/examples/image_classification_from_scratch.html","id":"build-a-model","dir":"Articles > Examples","previous_headings":"","what":"Build a model","title":"Image classification from scratch","text":"’ll build small version Xception network. haven’t particularly tried optimize architecture; want systematic search best model configuration, consider using KerasTuner. Note : start model data_augmentation preprocessor, followed Rescaling layer. include Dropout layer final classification layer.","code":"def make_model(input_shape, num_classes):     inputs = keras.Input(shape=input_shape)      # Entry block     x = layers.Rescaling(1.0 / 255)(inputs)     x = layers.Conv2D(128, 3, strides=2, padding=\"same\")(x)     x = layers.BatchNormalization()(x)     x = layers.Activation(\"relu\")(x)      previous_block_activation = x  # Set aside residual      for size in [256, 512, 728]:         x = layers.Activation(\"relu\")(x)         x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)         x = layers.BatchNormalization()(x)          x = layers.Activation(\"relu\")(x)         x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)         x = layers.BatchNormalization()(x)          x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)          # Project residual         residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(             previous_block_activation         )         x = layers.add([x, residual])  # Add back residual         previous_block_activation = x  # Set aside next residual      x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)     x = layers.BatchNormalization()(x)     x = layers.Activation(\"relu\")(x)      x = layers.GlobalAveragePooling2D()(x)     if num_classes == 2:         activation = \"sigmoid\"         units = 1     else:         activation = \"softmax\"         units = num_classes      x = layers.Dropout(0.5)(x)     outputs = layers.Dense(units, activation=activation)(x)     return keras.Model(inputs, outputs)   model = make_model(input_shape=image_size + (3,), num_classes=2) keras.utils.plot_model(model, show_shapes=True)"},{"path":"https://keras.posit.co/articles/examples/image_classification_from_scratch.html","id":"train-the-model","dir":"Articles > Examples","previous_headings":"","what":"Train the model","title":"Image classification from scratch","text":"get >90% validation accuracy training 25 epochs full dataset (practice, can train 50+ epochs validation performance starts degrading).","code":"epochs = 25  callbacks = [     keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.keras\"), ]  model.compile(     optimizer=keras.optimizers.Adam(1e-3),     loss=\"binary_crossentropy\",     metrics=[\"accuracy\"], ) model.fit(     train_ds,     epochs=epochs,     callbacks=callbacks,     validation_data=val_ds, )"},{"path":"https://keras.posit.co/articles/examples/image_classification_from_scratch.html","id":"run-inference-on-new-data","dir":"Articles > Examples","previous_headings":"","what":"Run inference on new data","title":"Image classification from scratch","text":"Note data augmentation dropout inactive inference time.","code":"img = keras.utils.load_img(     f\"{dirpath}/PetImages/Cat/6779.jpg\", target_size=image_size ) img_array = keras.utils.img_to_array(img) img_array = tf.expand_dims(img_array, 0)  # Create batch axis  predictions = model.predict(img_array) score = float(predictions[0]) print(f\"This image is {100 * (1 - score):.2f}% cat and {100 * score:.2f}% dog.\")"},{"path":"https://keras.posit.co/articles/examples/image_classification_with_vision_transformer.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Image classification with Vision Transformer","text":"example implements Vision Transformer (ViT) model Alexey Dosovitskiy et al. image classification, demonstrates CIFAR-100 dataset. ViT model applies Transformer architecture self-attention sequences image patches, without using convolution layers.","code":""},{"path":"https://keras.posit.co/articles/examples/image_classification_with_vision_transformer.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Image classification with Vision Transformer","text":"","code":"import os  os.environ[\"KERAS_BACKEND\"] = \"jax\"  # @param [\"tensorflow\", \"jax\", \"torch\"]  import keras as keras from keras import layers from keras import ops  import numpy as np import matplotlib.pyplot as plt"},{"path":"https://keras.posit.co/articles/examples/image_classification_with_vision_transformer.html","id":"prepare-the-data","dir":"Articles > Examples","previous_headings":"","what":"Prepare the data","title":"Image classification with Vision Transformer","text":"","code":"num_classes = 100 input_shape = (32, 32, 3)  (x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()  print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\") print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"},{"path":"https://keras.posit.co/articles/examples/image_classification_with_vision_transformer.html","id":"configure-the-hyperparameters","dir":"Articles > Examples","previous_headings":"","what":"Configure the hyperparameters","title":"Image classification with Vision Transformer","text":"","code":"learning_rate = 0.001 weight_decay = 0.0001 batch_size = 256 num_epochs = 100 image_size = 72  # We'll resize input images to this size patch_size = 6  # Size of the patches to be extract from the input images num_patches = (image_size // patch_size) ** 2 projection_dim = 64 num_heads = 4 transformer_units = [     projection_dim * 2,     projection_dim, ]  # Size of the transformer layers transformer_layers = 8 mlp_head_units = [     2048,     1024, ]  # Size of the dense layers of the final classifier"},{"path":"https://keras.posit.co/articles/examples/image_classification_with_vision_transformer.html","id":"use-data-augmentation","dir":"Articles > Examples","previous_headings":"","what":"Use data augmentation","title":"Image classification with Vision Transformer","text":"","code":"data_augmentation = keras.Sequential(     [         layers.Normalization(),         layers.Resizing(image_size, image_size),         layers.RandomFlip(\"horizontal\"),         layers.RandomRotation(factor=0.02),         layers.RandomZoom(height_factor=0.2, width_factor=0.2),     ],     name=\"data_augmentation\", ) # Compute the mean and the variance of the training data for normalization. data_augmentation.layers[0].adapt(x_train)"},{"path":"https://keras.posit.co/articles/examples/image_classification_with_vision_transformer.html","id":"implement-multilayer-perceptron-mlp","dir":"Articles > Examples","previous_headings":"","what":"Implement multilayer perceptron (MLP)","title":"Image classification with Vision Transformer","text":"","code":"def mlp(x, hidden_units, dropout_rate):     for units in hidden_units:         x = layers.Dense(units, activation=keras.activations.gelu)(x)         x = layers.Dropout(dropout_rate)(x)     return x"},{"path":"https://keras.posit.co/articles/examples/image_classification_with_vision_transformer.html","id":"implement-patch-creation-as-a-layer","dir":"Articles > Examples","previous_headings":"","what":"Implement patch creation as a layer","title":"Image classification with Vision Transformer","text":"Let’s display patches sample image","code":"class Patches(layers.Layer):     def __init__(self, patch_size):         super().__init__()         self.patch_size = patch_size      def call(self, images):         input_shape = ops.backend.shape(images)         batch_size = input_shape[0]         height = input_shape[1]         width = input_shape[2]         channels = input_shape[3]         num_patches_h = height // self.patch_size         num_patches_w = width // self.patch_size         patches = keras.ops.image.extract_patches(images, size=self.patch_size)         patches = ops.reshape(             patches,             (                 batch_size,                 num_patches_h * num_patches_w,                 self.patch_size * self.patch_size * channels,             ),         )         return patches      def get_config(self):         config = super().get_config()         config.update({\"patch_size\": self.patch_size})         return config plt.figure(figsize=(4, 4)) image = x_train[np.random.choice(range(x_train.shape[0]))] plt.imshow(image.astype(\"uint8\")) plt.axis(\"off\")  resized_image = ops.image.resize(     ops.convert_to_tensor([image]), size=(image_size, image_size) ) patches = Patches(patch_size)(resized_image) print(f\"Image size: {image_size} X {image_size}\") print(f\"Patch size: {patch_size} X {patch_size}\") print(f\"Patches per image: {patches.shape[1]}\") print(f\"Elements per patch: {patches.shape[-1]}\")  n = int(np.sqrt(patches.shape[1])) plt.figure(figsize=(4, 4)) for i, patch in enumerate(patches[0]):     ax = plt.subplot(n, n, i + 1)     patch_img = ops.reshape(patch, (patch_size, patch_size, 3))     plt.imshow(ops.convert_to_numpy(patch_img).astype(\"uint8\"))     plt.axis(\"off\")"},{"path":"https://keras.posit.co/articles/examples/image_classification_with_vision_transformer.html","id":"implement-the-patch-encoding-layer","dir":"Articles > Examples","previous_headings":"","what":"Implement the patch encoding layer","title":"Image classification with Vision Transformer","text":"PatchEncoder layer linearly transform patch projecting vector size projection_dim. addition, adds learnable position embedding projected vector.","code":"class PatchEncoder(layers.Layer):     def __init__(self, num_patches, projection_dim):         super().__init__()         self.num_patches = num_patches         self.projection = layers.Dense(units=projection_dim)         self.position_embedding = layers.Embedding(             input_dim=num_patches, output_dim=projection_dim         )      def call(self, patch):         positions = ops.expand_dims(             ops.arange(start=0, stop=self.num_patches, step=1), axis=0         )         projected_patches = self.projection(patch)         encoded = projected_patches + self.position_embedding(positions)         return encoded      def get_config(self):         config = super().get_config()         config.update({\"num_patches\": self.num_patches})         return config"},{"path":"https://keras.posit.co/articles/examples/image_classification_with_vision_transformer.html","id":"build-the-vit-model","dir":"Articles > Examples","previous_headings":"","what":"Build the ViT model","title":"Image classification with Vision Transformer","text":"ViT model consists multiple Transformer blocks, use layers.MultiHeadAttention layer self-attention mechanism applied sequence patches. Transformer blocks produce [batch_size, num_patches, projection_dim] tensor, processed via classifier head softmax produce final class probabilities output. Unlike technique described paper, prepends learnable embedding sequence encoded patches serve image representation, outputs final Transformer block reshaped layers.Flatten() used image representation input classifier head. Note layers.GlobalAveragePooling1D layer also used instead aggregate outputs Transformer block, especially number patches projection dimensions large.","code":"def create_vit_classifier():     inputs = keras.Input(shape=input_shape)     # Augment data.     augmented = data_augmentation(inputs)     # Create patches.     patches = Patches(patch_size)(augmented)     # Encode patches.     encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)      # Create multiple layers of the Transformer block.     for _ in range(transformer_layers):         # Layer normalization 1.         x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)         # Create a multi-head attention layer.         attention_output = layers.MultiHeadAttention(             num_heads=num_heads, key_dim=projection_dim, dropout=0.1         )(x1, x1)         # Skip connection 1.         x2 = layers.Add()([attention_output, encoded_patches])         # Layer normalization 2.         x3 = layers.LayerNormalization(epsilon=1e-6)(x2)         # MLP.         x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)         # Skip connection 2.         encoded_patches = layers.Add()([x3, x2])      # Create a [batch_size, projection_dim] tensor.     representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)     representation = layers.Flatten()(representation)     representation = layers.Dropout(0.5)(representation)     # Add MLP.     features = mlp(         representation, hidden_units=mlp_head_units, dropout_rate=0.5     )     # Classify outputs.     logits = layers.Dense(num_classes)(features)     # Create the Keras model.     model = keras.Model(inputs=inputs, outputs=logits)     return model"},{"path":"https://keras.posit.co/articles/examples/image_classification_with_vision_transformer.html","id":"compile-train-and-evaluate-the-mode","dir":"Articles > Examples","previous_headings":"","what":"Compile, train, and evaluate the mode","title":"Image classification with Vision Transformer","text":"100 epochs, ViT model achieves around 55% accuracy 82% top-5 accuracy test data. competitive results CIFAR-100 dataset, ResNet50V2 trained scratch data can achieve 67% accuracy. Note state art results reported paper achieved pre-training ViT model using JFT-300M dataset, fine-tuning target dataset. improve model quality without pre-training, can try train model epochs, use larger number Transformer layers, resize input images, change patch size, increase projection dimensions. Besides, mentioned paper, quality model affected architecture choices, also parameters learning rate schedule, optimizer, weight decay, etc. practice, ’s recommended fine-tune ViT model pre-trained using large, high-resolution dataset.","code":"def run_experiment(model):     optimizer = keras.optimizers.AdamW(         learning_rate=learning_rate, weight_decay=weight_decay     )      model.compile(         optimizer=optimizer,         loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),         metrics=[             keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),             keras.metrics.SparseTopKCategoricalAccuracy(                 5, name=\"top-5-accuracy\"             ),         ],     )      checkpoint_filepath = \"/tmp/checkpoint\"     checkpoint_callback = keras.callbacks.ModelCheckpoint(         checkpoint_filepath,         monitor=\"val_accuracy\",         save_best_only=True,         save_weights_only=True,     )      history = model.fit(         x=x_train,         y=y_train,         batch_size=batch_size,         epochs=num_epochs,         validation_split=0.1,         callbacks=[checkpoint_callback],     )      model.load_weights(checkpoint_filepath)     _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)     print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")     print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")      return history   vit_classifier = create_vit_classifier() history = run_experiment(vit_classifier)   def plot_history(item):     plt.plot(history.history[item], label=item)     plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)     plt.xlabel(\"Epochs\")     plt.ylabel(item)     plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)     plt.legend()     plt.grid()     plt.show()   plot_history(\"loss\") plot_history(\"top-5-accuracy\")"},{"path":"https://keras.posit.co/articles/examples/image_classifier.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"","text":"Classification process predicting categorical label given input image. classification relatively straightforward computer vision task, modern approaches still built several complex components. Luckily, KerasCV provides APIs construct commonly used components. guide demonstrates KerasCV’s modular approach solving image classification problems three levels complexity: Inference pretrained classifier Fine-tuning pretrained backbone Training image classifier scratch","code":""},{"path":"https://keras.posit.co/articles/examples/image_classifier.html","id":"multi-backend-support","dir":"Articles > Examples","previous_headings":"","what":"Multi-Backend Support","title":"","text":"KerasCV’s ImageClassifier model supports several backends like JAX, PyTorch, TensorFlow help keras. enable multi-backend support KerasCV, set KERAS_CV_MULTI_BACKEND environment variable. can switch different backends setting KERAS_BACKEND environment variable. Currently, \"tensorflow\", \"jax\", \"torch\" supported. demonstration uses Jax backend. Let’s get started simplest KerasCV API: pretrained classifier. example, construct classifier pretrained ImageNet dataset. ’ll use model solve age old “Cat Dog” problem. highest level module KerasCV task. task keras.Model consisting (generally pretrained) backbone model task-specific layers. ’s example using keras_cv.models.ImageClassifier EfficientNetV2B0 Backbone. EfficientNetV2B0 great starting model constructing image classification pipeline. architecture manages achieve high accuracy, using parameter count 7M. EfficientNetV2B0 powerful enough task hoping solve, sure check KerasCV’s available Backbones! construct class EfficientNetV2B0(weights=\"imagenet\"). old API great classification, scale effectively use cases required complex architectures, like object deteciton semantic segmentation. Now classifier built, let’s apply cute cat picture! predictions = classifier.predict(np.expand_dims(image, axis=0)) Predictions come form softmax-ed category rankings. can find index top classes using simple argsort function: top_classes = predictions[0].argsort(axis=-1) order decode class mappings, can construct mapping category indices ImageNet class names. convenience, stored ImageNet class mapping GitHub gist. download load now. classes = keras.utils.get_file( origin=“https://gist.githubusercontent.com/LukeWood/62eebcd5c5c4a4d0e0b7845780f76d55/raw/fde63e5e4c09e2fa0a3436680f436bdcb8325aac/ImagenetClassnames.json” ) open(classes, “rb”) f: classes = json.load(f) Now can simply look class names via index:\"\"\" top_two = [classes[str()] top_classes[-2:]] print(\"Top two classes :\", top_two) However, one classes “Velvet”. ’re trying classify Cats VS Dogs. don’t care velvet blanket! Ideally, ’d classifier performs computation determine image cat dog, resources dedicated task. can solved fine tuning classifier.","code":"import os  os.environ[\"KERAS_CV_MULTI_BACKEND\"] = \"1\" os.environ[\"KERAS_BACKEND\"] = \"jax\"  import json import math import keras_cv import keras as keras from keras import ops from keras import losses from keras import optimizers from keras.optimizers import schedules from keras import metrics import tensorflow as tf from tensorflow import data as tf_data import tensorflow_datasets as tfds import numpy as np classifier = keras_cv.models.ImageClassifier.from_preset(     \"efficientnetv2_b0_imagenet_classifier\" ) filepath = keras.utils.get_file(origin=\"https://i.imgur.com/9i63gLN.jpg\") image = keras.utils.load_img(filepath) image = np.array(image) keras_cv.visualization.plot_image_gallery(     image[None, ...], rows=1, cols=1, value_range=(0, 255), show=True, scale=4 )"},{"path":"https://keras.posit.co/articles/examples/image_classifier.html","id":"fine-tuning-a-pretrained-classifier","dir":"Articles > Examples","previous_headings":"","what":"Fine tuning a pretrained classifier","title":"","text":"labeled images specific task available, fine-tuning custom classifier can improve performance. want train Cats vs Dogs Classifier, using explicitly labeled Cat vs Dog data perform better generic classifier! many tasks, relevant pretrained model available (e.g., categorizing images specific application). First, let’s get started loading data: Next let’s construct model. use imagenet preset name indicates backbone pretrained ImageNet dataset. Pretrained backbones extract information labeled examples leveraging patterns extracted potentially much larger datasets. Next lets put together classifier: left call model.fit(): predictions = model.predict(np.expand_dims(image, axis=0)) classes = {0: “cat”, 1: “dog”} print(“Top class :”, classes[predictions[0].argmax()]) ```Awesome - looks like model correctly classified image. # Train Classifier Scratch Now ’ve gotten hands dirty classification, let’s take one last task: training classification model scratch! standard benchmark image classification ImageNet dataset, however due licensing constraints use CalTech 101 image classification dataset tutorial. use simpler CalTech 101 dataset guide, training template may used ImageNet achieve near state---art scores. Let’s start tackling data loading: train_ds = train_ds.ragged_batch(BATCH_SIZE) eval_ds = eval_ds.ragged_batch(BATCH_SIZE) batch = next(iter(train_ds.take(1))) image_batch = batch[“images”] label_batch = batch[“labels”] keras_cv.visualization.plot_image_gallery( image_batch.to_tensor(), rows=3, cols=3, value_range=(0, 255), show=True, ) ```## Data Augmentation previous finetuning exmaple, performed static resizing operation utilize image augmentation. single pass training set sufficient achieve decent results. training solve difficult task, ’ll want include data augmentation data pipeline. Data augmentation technique make model robust changes input data lighting, cropping, orientation. KerasCV includes useful augmentations keras_cv.layers API. Creating optimal pipeline augmentations art, section guide ’ll offer tips best practices classification. One caveat aware image data augmentation must careful shift augmented data distribution far original data distribution. goal prevent overfitting increase generalization, samples lie completely data distribution simply add noise training process. first augmentation ’ll use RandomFlip. augmentation behaves less ’d expect: either flips image . augmentation useful CalTech101 ImageNet, noted used tasks data distribution vertical mirror invariant. example dataset occurs MNIST hand written digits. Flipping 6 vertical axis make digit appear like 7 6, label still show 6. crop_and_resize = keras_cv.layers.RandomCropAndResize( target_size=IMAGE_SIZE, crop_area_factor=(0.8, 1.0), aspect_ratio_factor=(0.9, 1.1), ) augmenters += [crop_and_resize] image_batch = crop_and_resize(image_batch) keras_cv.visualization.plot_image_gallery( image_batch, rows=3, cols=3, value_range=(0, 255), show=True, ) ```Great! now working batch dense images. Next , lets include spatial color-based jitter training set. allow us produce classifier robust lighting flickers, shadows, . limitless ways augment image altering color spatial features, perhaps battle tested technique RandAugment. RandAugment actually set 10 different augmentations: AutoContrast, Equalize, Solarize, RandomColorJitter, RandomContrast, RandomBrightness, ShearX, ShearY, TranslateX TranslateY. inference time, num_augmentations augmenters sampled image, random magnitude factors sampled . augmentations applied sequentially. KerasCV makes tuning parameters easy using augmentations_per_image magnitude parameters! Let’s take spin: random_cutout = keras_cv.layers.RandomCutout( width_factor=0.4, height_factor=0.4 ) keras_cv.visualization.plot_image_gallery( random_cutout(image_batch), rows=3, cols=3, value_range=(0, 255), show=True, ) ```tackles problem reasonably well, can cause classifier develop responses borders features black pixel areas caused cutout. CutMix solves issue using complex (effective) technique. Instead replacing cut-areas black pixels, CutMix replaces regions regions images sampled within training set! Following replacement, image’s classification label updated blend original mixed image’s class label. look like practice? Let’s check : mix_up = keras_cv.layers.MixUp() # MixUp needs modify images labels inputs = {“images”: image_batch, “labels”: tf.cast(label_batch, “float32”)} keras_cv.visualization.plot_image_gallery( mix_up(inputs)[“images”], rows=3, cols=3, value_range=(0, 255), show=True, ) look closely, see images blended together. Instead applying `CutMix()` `MixUp()` every image, instead pick one apply batch. can expressed using `keras_cv.layers.RandomChoice()` cut_mix_or_mix_up = keras_cv.layers.RandomChoice( [cut_mix, mix_up], batchwise=True ) augmenters += [cut_mix_or_mix_up] ```Now let’s apply final augmenter training data:““” augmenter = keras_cv.layers.Augmenter(augmenters) train_ds = train_ds.map(augmenter, num_parallel_calls=tf_data.AUTOTUNE) image_batch = next(iter(train_ds.take(1)))[“images”] keras_cv.visualization.plot_image_gallery( image_batch, rows=3, cols=3, value_range=(0, 255), show=True, ) model.fit(), accepts tuple (images, labels). classifier. Congratulations making far!","code":"BATCH_SIZE = 32 IMAGE_SIZE = (224, 224) AUTOTUNE = tf_data.AUTOTUNE tfds.disable_progress_bar()  data, dataset_info = tfds.load(     \"cats_vs_dogs\",     with_info=True,     as_supervised=True ) train_steps_per_epoch = (     dataset_info.splits[\"train\"].num_examples // BATCH_SIZE ) train_dataset = data[\"train\"]  num_classes = dataset_info.features[\"label\"].num_classes  resizing = keras_cv.layers.Resizing(     IMAGE_SIZE[0], IMAGE_SIZE[1], crop_to_aspect_ratio=True ) encoder = keras.layers.CategoryEncoding(num_classes, \"one_hot\", dtype=\"int32\")   def preprocess_inputs(image, label):     # Staticly resize images as we only iterate the dataset once.     return resizing(image), encoder(label)   # Shuffle the dataset to increase diversity of batches. # 10*BATCH_SIZE follows the assumption that bigger machines can handle bigger # shuffle buffers. train_dataset = train_dataset.shuffle(     10 * BATCH_SIZE, reshuffle_each_iteration=True ).map(preprocess_inputs, num_parallel_calls=AUTOTUNE) train_dataset = train_dataset.batch(BATCH_SIZE)  images = next(iter(train_dataset.take(1)))[0] keras_cv.visualization.plot_image_gallery(images, value_range=(0, 255)) model = keras_cv.models.ImageClassifier.from_preset(     \"efficientnetv2_b0_imagenet\", num_classes=2 ) model.compile(     loss=\"categorical_crossentropy\",     optimizer=keras.optimizers.SGD(learning_rate=0.01),     metrics=[\"accuracy\"], ) model.fit(train_dataset) NUM_CLASSES = 101 # Change epochs to 100~ to fully train. EPOCHS = 1  encoder = keras.layers.CategoryEncoding(NUM_CLASSES, \"one_hot\", dtype=\"int32\")  def package_inputs(image, label):     return {\"images\": image, \"labels\": encoder(label)}  train_ds, eval_ds = tfds.load(     \"caltech101\", split=[\"train\", \"test\"], as_supervised=\"true\" ) train_ds = train_ds.map(package_inputs, num_parallel_calls=tf_data.AUTOTUNE) eval_ds = eval_ds.map(package_inputs, num_parallel_calls=tf_data.AUTOTUNE)  train_ds = train_ds.shuffle(BATCH_SIZE * 16)  ```The CalTech101 dataset has different sizes for every image, so we use the `ragged_batch()` API to batch them together while maintaining each individual image's shape information. random_flip = keras_cv.layers.RandomFlip() augmenters = [random_flip]  image_batch = random_flip(image_batch) keras_cv.visualization.plot_image_gallery(     image_batch.to_tensor(),     rows=3,     cols=3,     value_range=(0, 255),     show=True, )  ```Half of the images have been flipped! The next augmentation we'll use is `RandomCropAndResize`. This operation selects a random subset of the image, then resizes it to the provided target size. By using this augmentation, we force our classifier to become spatially invariant. Additionally, this layer accepts an `aspect_ratio_factor` which can be used to distort the aspect ratio of the image. While this can improve model performance, it should be used with caution. It is very easy for an aspect ratio distortion to shift a sample too far from the original training set's data distribution. Remember - the goal of data augmentation is to produce more training samples that align with the data distribution of your training set!  `RandomCropAndResize` also can handle `tf.RaggedTensor` inputs.  In the CalTech101 image dataset images come in a wide variety of sizes. As such they cannot easily be batched together into a dense training batch. Luckily, `RandomCropAndResize` handles the Ragged -> Dense conversion process for you!  Let's add a `RandomCropAndResize` to our set of augmentations: rand_augment = keras_cv.layers.RandAugment(     augmentations_per_image=3,     magnitude=0.3,     value_range=(0, 255), ) augmenters += [rand_augment]  image_batch = rand_augment(image_batch) keras_cv.visualization.plot_image_gallery(     image_batch,     rows=3,     cols=3,     value_range=(0, 255),     show=True, )  ```Looks great; but we're not done yet! What if an image is missing one critical feature of a class?  For example, what if a leaf is blocking the view of a cat's ear, but our classifier learned to classify cats simply by observing their ears?  One easy approach to tackling this is to use `RandomCutout`, which randomly strips out a sub-section of the image: cut_mix = keras_cv.layers.CutMix() # CutMix needs to modify both images and labels inputs = {\"images\": image_batch, \"labels\": tf.cast(label_batch, \"float32\")}  keras_cv.visualization.plot_image_gallery(     cut_mix(inputs)[\"images\"],     rows=3,     cols=3,     value_range=(0, 255),     show=True, )  ```Let's hold off from adding it to our augmenter for a minute - more on that soon!  Next, let's look into `MixUp()`. Unfortunately, while `MixUp()` has been empirically shown to *substantially* improve both the robustness and the generalization of the trained model, it is not well-understood why such improvement occurs... but a little alchemy never hurt anyone!  `MixUp()` works by sampling two images from a batch, then proceeding to literally blend together their pixel intensities as well as their classification labels.  Let's see it in action: size expected by our model. We use the deterministic `keras_cv.layers.Resizing` in this case to avoid adding noise to our evaluation metric.  ```python inference_resizing = keras_cv.layers.Resizing(     IMAGE_SIZE[0], IMAGE_SIZE[1], crop_to_aspect_ratio=True ) eval_ds = eval_ds.map(inference_resizing, num_parallel_calls=tf_data.AUTOTUNE)  inference_resizing = keras_cv.layers.Resizing(     IMAGE_SIZE[0], IMAGE_SIZE[1], crop_to_aspect_ratio=True ) eval_ds = eval_ds.map(inference_resizing, num_parallel_calls=tf_data.AUTOTUNE)  image_batch = next(iter(eval_ds.take(1)))[\"images\"] keras_cv.visualization.plot_image_gallery(     image_batch,     rows=3,     cols=3,     value_range=(0, 255),     show=True, ) def unpackage_dict(inputs):     return inputs[\"images\"], inputs[\"labels\"]   train_ds = train_ds.map(unpackage_dict, num_parallel_calls=tf_data.AUTOTUNE) eval_ds = eval_ds.map(unpackage_dict, num_parallel_calls=tf_data.AUTOTUNE)"},{"path":"https://keras.posit.co/articles/examples/image_classifier.html","id":"optimizer-tuning","dir":"Articles > Examples","previous_headings":"Fine tuning a pretrained classifier","what":"Optimizer Tuning","title":"","text":"achieve optimal performance, need use learning rate schedule instead single learning rate. won’t go detail Cosine decay warmup schedule used , can read . schedule looks expect. Next let’s construct optimizer: keras_cv.models.EfficientNetV2B0Backbone() convenience alias keras_cv.models.EfficientNetV2Backbone.from_preset('efficientnetv2_b0'). Note preset come pretrained weights. employ label smoothing prevent model overfitting artifacts augmentation process. model.compile( loss=loss, optimizer=optimizer, metrics=[ metrics.CategoricalAccuracy(), metrics.TopKCategoricalAccuracy(k=5), ], ) finally call fit().\"\"\" model.fit(     train_ds,     epochs=EPOCHS,     validation_data=eval_ds, ) scratch KerasCV. Depending availability labeled data application, training scratch may may powerful using transfer learning addition data augmentations discussed . smaller datasets, pretrained models generally produce high accuracy faster convergence.","code":"def lr_warmup_cosine_decay(     global_step,     warmup_steps,     hold=0,     total_steps=0,     start_lr=0.0,     target_lr=1e-2, ):     # Cosine decay     learning_rate = (         0.5         * target_lr         * (             1             + ops.cos(                 math.pi                 * ops.convert_to_tensor(                     global_step - warmup_steps - hold, dtype=\"float32\"                 )                 / ops.convert_to_tensor(                     total_steps - warmup_steps - hold, dtype=\"float32\"                 )             )         )     )      warmup_lr = (target_lr * (global_step / warmup_steps))      if hold > 0:         learning_rate = ops.where(             global_step > warmup_steps + hold, learning_rate, target_lr         )      learning_rate = ops.where(         global_step < warmup_steps, warmup_lr, learning_rate     )     return learning_rate   class WarmUpCosineDecay(     schedules.LearningRateSchedule ):     def __init__(         self, warmup_steps, total_steps, hold, start_lr=0.0, target_lr=1e-2     ):         super().__init__()         self.start_lr = start_lr         self.target_lr = target_lr         self.warmup_steps = warmup_steps         self.total_steps = total_steps         self.hold = hold      def __call__(self, step):         lr = lr_warmup_cosine_decay(             global_step=step,             total_steps=self.total_steps,             warmup_steps=self.warmup_steps,             start_lr=self.start_lr,             target_lr=self.target_lr,             hold=self.hold,         )          return ops.where(step > self.total_steps, 0.0, lr) total_images = 9000 total_steps = (total_images // BATCH_SIZE) * EPOCHS warmup_steps = int(0.1 * total_steps) hold_steps = int(0.45 * total_steps) schedule = WarmUpCosineDecay(     start_lr=0.05,     target_lr=1e-2,     warmup_steps=warmup_steps,     total_steps=total_steps,     hold=hold_steps, ) optimizer = optimizers.SGD(     weight_decay=5e-4,     learning_rate=schedule,     momentum=0.9, ) backbone = keras_cv.models.ResNet18V2Backbone() model = keras.Sequential(     [         backbone,         keras.layers.GlobalMaxPooling2D(),         keras.layers.Dropout(rate=0.5),         keras.layers.Dense(101, activation=\"softmax\"),     ] ) loss = losses.CategoricalCrossentropy(label_smoothing=0.1)"},{"path":"https://keras.posit.co/articles/examples/image_classifier.html","id":"conclusions","dir":"Articles > Examples","previous_headings":"Fine tuning a pretrained classifier","what":"Conclusions","title":"","text":"image classification perhaps simplest problem computer vision, modern landscape numerous complex components. Luckily, KerasCV offers robust, production-grade APIs make assembling components possible one line code. use KerasCV’s ImageClassifier API, pretrained weights, KerasCV data augmentations can assemble everything need train powerful classifier hundred lines code! follow exercise, give following try: Fine tune KerasCV classifier dataset Learn KerasCV’s data augmentations Check train models ImageNet","code":""},{"path":"https://keras.posit.co/articles/examples/imbalanced_classification.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Imbalanced classification: credit card fraud detection","text":"example looks Kaggle Credit Card Fraud Detection dataset demonstrate train classification model data highly imbalanced classes.","code":""},{"path":"https://keras.posit.co/articles/examples/imbalanced_classification.html","id":"first-vectorize-the-csv-data","dir":"Articles > Examples","previous_headings":"","what":"First, vectorize the CSV data","title":"Imbalanced classification: credit card fraud detection","text":"","code":"import numpy as np import keras as keras  # Get the real data from https://www.kaggle.com/mlg-ulb/creditcardfraud/ fname = \"/Users/fchollet/Downloads/creditcard.csv\"  all_features = [] all_targets = [] with open(fname) as f:     for i, line in enumerate(f):         if i == 0:             print(\"HEADER:\", line.strip())             continue  # Skip header         fields = line.strip().split(\",\")         all_features.append([float(v.replace('\"', \"\")) for v in fields[:-1]])         all_targets.append([int(fields[-1].replace('\"', \"\"))])         if i == 1:             print(\"EXAMPLE FEATURES:\", all_features[-1])  features = np.array(all_features, dtype=\"float32\") targets = np.array(all_targets, dtype=\"uint8\") print(\"features.shape:\", features.shape) print(\"targets.shape:\", targets.shape)"},{"path":"https://keras.posit.co/articles/examples/imbalanced_classification.html","id":"prepare-a-validation-set","dir":"Articles > Examples","previous_headings":"","what":"Prepare a validation set","title":"Imbalanced classification: credit card fraud detection","text":"","code":"num_val_samples = int(len(features) * 0.2) train_features = features[:-num_val_samples] train_targets = targets[:-num_val_samples] val_features = features[-num_val_samples:] val_targets = targets[-num_val_samples:]  print(\"Number of training samples:\", len(train_features)) print(\"Number of validation samples:\", len(val_features))"},{"path":"https://keras.posit.co/articles/examples/imbalanced_classification.html","id":"analyze-class-imbalance-in-the-targets","dir":"Articles > Examples","previous_headings":"","what":"Analyze class imbalance in the targets","title":"Imbalanced classification: credit card fraud detection","text":"","code":"counts = np.bincount(train_targets[:, 0]) print(     \"Number of positive samples in training data: {} ({:.2f}% of total)\".format(         counts[1], 100 * float(counts[1]) / len(train_targets)     ) )  weight_for_0 = 1.0 / counts[0] weight_for_1 = 1.0 / counts[1]"},{"path":"https://keras.posit.co/articles/examples/imbalanced_classification.html","id":"normalize-the-data-using-training-set-statistics","dir":"Articles > Examples","previous_headings":"","what":"Normalize the data using training set statistics","title":"Imbalanced classification: credit card fraud detection","text":"","code":"mean = np.mean(train_features, axis=0) train_features -= mean val_features -= mean std = np.std(train_features, axis=0) train_features /= std val_features /= std"},{"path":"https://keras.posit.co/articles/examples/imbalanced_classification.html","id":"build-a-binary-classification-model","dir":"Articles > Examples","previous_headings":"","what":"Build a binary classification model","title":"Imbalanced classification: credit card fraud detection","text":"","code":"model = keras.Sequential(     [         keras.layers.Dense(             256, activation=\"relu\", input_shape=(train_features.shape[-1],)         ),         keras.layers.Dense(256, activation=\"relu\"),         keras.layers.Dropout(0.3),         keras.layers.Dense(256, activation=\"relu\"),         keras.layers.Dropout(0.3),         keras.layers.Dense(1, activation=\"sigmoid\"),     ] ) model.summary()"},{"path":"https://keras.posit.co/articles/examples/imbalanced_classification.html","id":"train-the-model-with-class_weight-argument","dir":"Articles > Examples","previous_headings":"","what":"Train the model with class_weight argument","title":"Imbalanced classification: credit card fraud detection","text":"","code":"metrics = [     keras.metrics.FalseNegatives(name=\"fn\"),     keras.metrics.FalsePositives(name=\"fp\"),     keras.metrics.TrueNegatives(name=\"tn\"),     keras.metrics.TruePositives(name=\"tp\"),     keras.metrics.Precision(name=\"precision\"),     keras.metrics.Recall(name=\"recall\"), ]  model.compile(     optimizer=keras.optimizers.Adam(1e-2),     loss=\"binary_crossentropy\",     metrics=metrics, )  callbacks = [     keras.callbacks.ModelCheckpoint(\"fraud_model_at_epoch_{epoch}.keras\") ] class_weight = {0: weight_for_0, 1: weight_for_1}  model.fit(     train_features,     train_targets,     batch_size=2048,     epochs=30,     verbose=2,     callbacks=callbacks,     validation_data=(val_features, val_targets),     class_weight=class_weight, )"},{"path":"https://keras.posit.co/articles/examples/imbalanced_classification.html","id":"conclusions","dir":"Articles > Examples","previous_headings":"","what":"Conclusions","title":"Imbalanced classification: credit card fraud detection","text":"end training, 56,961 validation transactions, : Correctly identifying 66 fraudulent Missing 9 fraudulent transactions cost incorrectly flagging 441 legitimate transactions real world, one put even higher weight class 1, reflect False Negatives costly False Positives. Next time credit card gets declined online purchase – . Example available HuggingFace.","code":""},{"path":"https://keras.posit.co/articles/examples/integrated_gradients.html","id":"integrated-gradients","dir":"Articles > Examples","previous_headings":"","what":"Integrated Gradients","title":"Model interpretability with Integrated Gradients","text":"Integrated Gradients technique attributing classification model’s prediction input features. model interpretability technique: can use visualize relationship input features model predictions. Integrated Gradients variation computing gradient prediction output regard features input. compute integrated gradients, need perform following steps: Identify input output. case, input image output last layer model (dense layer softmax activation). Compute features important neural network making prediction particular data point. identify features, need choose baseline input. baseline input can black image (pixel values set zero) random noise. shape baseline input needs input image, e.g. (299, 299, 3). Interpolate baseline given number steps. number steps represents steps need gradient approximation given input image. number steps hyperparameter. authors recommend using anywhere 20 1000 steps. Preprocess interpolated images forward pass. Get gradients interpolated images. Approximate gradients integral using trapezoidal rule. read -depth integrated gradients method works, consider reading excellent article. References: Integrated Gradients original paper Original implementation","code":""},{"path":"https://keras.posit.co/articles/examples/integrated_gradients.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Model interpretability with Integrated Gradients","text":"","code":"import numpy as np import matplotlib.pyplot as plt from scipy import ndimage from IPython.display import Image, display  import tensorflow as tf import keras as keras from keras import layers from keras.applications import xception  keras.config.disable_traceback_filtering()   # Size of the input image img_size = (299, 299, 3)  # Load Xception model with imagenet weights model = xception.Xception(weights=\"imagenet\")  # The local path to our target image img_path = keras.utils.get_file(     \"elephant.jpg\", \"https://i.imgur.com/Bvro0YD.png\" ) display(Image(img_path))"},{"path":"https://keras.posit.co/articles/examples/integrated_gradients.html","id":"integrated-gradients-algorithm","dir":"Articles > Examples","previous_headings":"","what":"Integrated Gradients algorithm","title":"Model interpretability with Integrated Gradients","text":"","code":"def get_img_array(img_path, size=(299, 299)):     # `img` is a PIL image of size 299x299     img = keras.utils.load_img(img_path, target_size=size)     # `array` is a float32 Numpy array of shape (299, 299, 3)     array = keras.utils.img_to_array(img)     # We add a dimension to transform our array into a \"batch\"     # of size (1, 299, 299, 3)     array = np.expand_dims(array, axis=0)     return array   def get_gradients(img_input, top_pred_idx):     \"\"\"Computes the gradients of outputs w.r.t input image.      Args:         img_input: 4D image tensor         top_pred_idx: Predicted label for the input image      Returns:         Gradients of the predictions w.r.t img_input     \"\"\"     images = tf.cast(img_input, tf.float32)      with tf.GradientTape() as tape:         tape.watch(images)         preds = model(images)         top_class = preds[:, top_pred_idx]      grads = tape.gradient(top_class, images)     return grads   def get_integrated_gradients(     img_input, top_pred_idx, baseline=None, num_steps=50 ):     \"\"\"Computes Integrated Gradients for a predicted label.      Args:         img_input (ndarray): Original image         top_pred_idx: Predicted label for the input image         baseline (ndarray): The baseline image to start with for interpolation         num_steps: Number of interpolation steps between the baseline             and the input used in the computation of integrated gradients. These             steps along determine the integral approximation error. By default,             num_steps is set to 50.      Returns:         Integrated gradients w.r.t input image     \"\"\"     # If baseline is not provided, start with a black image     # having same size as the input image.     if baseline is None:         baseline = np.zeros(img_size).astype(np.float32)     else:         baseline = baseline.astype(np.float32)      # 1. Do interpolation.     img_input = img_input.astype(np.float32)     interpolated_image = [         baseline + (step / num_steps) * (img_input - baseline)         for step in range(num_steps + 1)     ]     interpolated_image = np.array(interpolated_image).astype(np.float32)      # 2. Preprocess the interpolated images     interpolated_image = xception.preprocess_input(interpolated_image)      # 3. Get the gradients     grads = []     for i, img in enumerate(interpolated_image):         img = tf.expand_dims(img, axis=0)         grad = get_gradients(img, top_pred_idx=top_pred_idx)         grads.append(grad[0])     grads = tf.convert_to_tensor(grads, dtype=tf.float32)      # 4. Approximate the integral using the trapezoidal rule     grads = (grads[:-1] + grads[1:]) / 2.0     avg_grads = tf.reduce_mean(grads, axis=0)      # 5. Calculate integrated gradients and return     integrated_grads = (img_input - baseline) * avg_grads     return integrated_grads   def random_baseline_integrated_gradients(     img_input, top_pred_idx, num_steps=50, num_runs=2 ):     \"\"\"Generates a number of random baseline images.      Args:         img_input (ndarray): 3D image         top_pred_idx: Predicted label for the input image         num_steps: Number of interpolation steps between the baseline             and the input used in the computation of integrated gradients. These             steps along determine the integral approximation error. By default,             num_steps is set to 50.         num_runs: number of baseline images to generate      Returns:         Averaged integrated gradients for `num_runs` baseline images     \"\"\"     # 1. List to keep track of Integrated Gradients (IG) for all the images     integrated_grads = []      # 2. Get the integrated gradients for all the baselines     for run in range(num_runs):         baseline = np.random.random(img_size) * 255         igrads = get_integrated_gradients(             img_input=img_input,             top_pred_idx=top_pred_idx,             baseline=baseline,             num_steps=num_steps,         )         integrated_grads.append(igrads)      # 3. Return the average integrated gradients for the image     integrated_grads = tf.convert_to_tensor(integrated_grads)     return tf.reduce_mean(integrated_grads, axis=0)"},{"path":"https://keras.posit.co/articles/examples/integrated_gradients.html","id":"helper-class-for-visualizing-gradients-and-integrated-gradients","dir":"Articles > Examples","previous_headings":"","what":"Helper class for visualizing gradients and integrated gradients","title":"Model interpretability with Integrated Gradients","text":"","code":"class GradVisualizer:     \"\"\"Plot gradients of the outputs w.r.t an input image.\"\"\"      def __init__(self, positive_channel=None, negative_channel=None):         if positive_channel is None:             self.positive_channel = [0, 255, 0]         else:             self.positive_channel = positive_channel          if negative_channel is None:             self.negative_channel = [255, 0, 0]         else:             self.negative_channel = negative_channel      def apply_polarity(self, attributions, polarity):         if polarity == \"positive\":             return np.clip(attributions, 0, 1)         else:             return np.clip(attributions, -1, 0)      def apply_linear_transformation(         self,         attributions,         clip_above_percentile=99.9,         clip_below_percentile=70.0,         lower_end=0.2,     ):         # 1. Get the thresholds         m = self.get_thresholded_attributions(             attributions, percentage=100 - clip_above_percentile         )         e = self.get_thresholded_attributions(             attributions, percentage=100 - clip_below_percentile         )          # 2. Transform the attributions by a linear function f(x) = a*x + b such that         # f(m) = 1.0 and f(e) = lower_end         transformed_attributions = (1 - lower_end) * (             np.abs(attributions) - e         ) / (m - e) + lower_end          # 3. Make sure that the sign of transformed attributions is the same as original attributions         transformed_attributions *= np.sign(attributions)          # 4. Only keep values that are bigger than the lower_end         transformed_attributions *= transformed_attributions >= lower_end          # 5. Clip values and return         transformed_attributions = np.clip(transformed_attributions, 0.0, 1.0)         return transformed_attributions      def get_thresholded_attributions(self, attributions, percentage):         if percentage == 100.0:             return np.min(attributions)          # 1. Flatten the attributions         flatten_attr = attributions.flatten()          # 2. Get the sum of the attributions         total = np.sum(flatten_attr)          # 3. Sort the attributions from largest to smallest.         sorted_attributions = np.sort(np.abs(flatten_attr))[::-1]          # 4. Calculate the percentage of the total sum that each attribution         # and the values about it contribute.         cum_sum = 100.0 * np.cumsum(sorted_attributions) / total          # 5. Threshold the attributions by the percentage         indices_to_consider = np.where(cum_sum >= percentage)[0][0]          # 6. Select the desired attributions and return         attributions = sorted_attributions[indices_to_consider]         return attributions      def binarize(self, attributions, threshold=0.001):         return attributions > threshold      def morphological_cleanup_fn(self, attributions, structure=np.ones((4, 4))):         closed = ndimage.grey_closing(attributions, structure=structure)         opened = ndimage.grey_opening(closed, structure=structure)         return opened      def draw_outlines(         self,         attributions,         percentage=90,         connected_component_structure=np.ones((3, 3)),     ):         # 1. Binarize the attributions.         attributions = self.binarize(attributions)          # 2. Fill the gaps         attributions = ndimage.binary_fill_holes(attributions)          # 3. Compute connected components         connected_components, num_comp = ndimage.label(             attributions, structure=connected_component_structure         )          # 4. Sum up the attributions for each component         total = np.sum(attributions[connected_components > 0])         component_sums = []         for comp in range(1, num_comp + 1):             mask = connected_components == comp             component_sum = np.sum(attributions[mask])             component_sums.append((component_sum, mask))          # 5. Compute the percentage of top components to keep         sorted_sums_and_masks = sorted(             component_sums, key=lambda x: x[0], reverse=True         )         sorted_sums = list(zip(*sorted_sums_and_masks))[0]         cumulative_sorted_sums = np.cumsum(sorted_sums)         cutoff_threshold = percentage * total / 100         cutoff_idx = np.where(cumulative_sorted_sums >= cutoff_threshold)[0][0]         if cutoff_idx > 2:             cutoff_idx = 2          # 6. Set the values for the kept components         border_mask = np.zeros_like(attributions)         for i in range(cutoff_idx + 1):             border_mask[sorted_sums_and_masks[i][1]] = 1          # 7. Make the mask hollow and show only the border         eroded_mask = ndimage.binary_erosion(border_mask, iterations=1)         border_mask[eroded_mask] = 0          # 8. Return the outlined mask         return border_mask      def process_grads(         self,         image,         attributions,         polarity=\"positive\",         clip_above_percentile=99.9,         clip_below_percentile=0,         morphological_cleanup=False,         structure=np.ones((3, 3)),         outlines=False,         outlines_component_percentage=90,         overlay=True,     ):         if polarity not in [\"positive\", \"negative\"]:             raise ValueError(                 f\"\"\" Allowed polarity values: 'positive' or 'negative'                                     but provided {polarity}\"\"\"             )         if clip_above_percentile < 0 or clip_above_percentile > 100:             raise ValueError(\"clip_above_percentile must be in [0, 100]\")          if clip_below_percentile < 0 or clip_below_percentile > 100:             raise ValueError(\"clip_below_percentile must be in [0, 100]\")          # 1. Apply polarity         if polarity == \"positive\":             attributions = self.apply_polarity(attributions, polarity=polarity)             channel = self.positive_channel         else:             attributions = self.apply_polarity(attributions, polarity=polarity)             attributions = np.abs(attributions)             channel = self.negative_channel          # 2. Take average over the channels         attributions = np.average(attributions, axis=2)          # 3. Apply linear transformation to the attributions         attributions = self.apply_linear_transformation(             attributions,             clip_above_percentile=clip_above_percentile,             clip_below_percentile=clip_below_percentile,             lower_end=0.0,         )          # 4. Cleanup         if morphological_cleanup:             attributions = self.morphological_cleanup_fn(                 attributions, structure=structure             )         # 5. Draw the outlines         if outlines:             attributions = self.draw_outlines(                 attributions, percentage=outlines_component_percentage             )          # 6. Expand the channel axis and convert to RGB         attributions = np.expand_dims(attributions, 2) * channel          # 7.Superimpose on the original image         if overlay:             attributions = np.clip((attributions * 0.8 + image), 0, 255)         return attributions      def visualize(         self,         image,         gradients,         integrated_gradients,         polarity=\"positive\",         clip_above_percentile=99.9,         clip_below_percentile=0,         morphological_cleanup=False,         structure=np.ones((3, 3)),         outlines=False,         outlines_component_percentage=90,         overlay=True,         figsize=(15, 8),     ):         # 1. Make two copies of the original image         img1 = np.copy(image)         img2 = np.copy(image)          # 2. Process the normal gradients         grads_attr = self.process_grads(             image=img1,             attributions=gradients,             polarity=polarity,             clip_above_percentile=clip_above_percentile,             clip_below_percentile=clip_below_percentile,             morphological_cleanup=morphological_cleanup,             structure=structure,             outlines=outlines,             outlines_component_percentage=outlines_component_percentage,             overlay=overlay,         )          # 3. Process the integrated gradients         igrads_attr = self.process_grads(             image=img2,             attributions=integrated_gradients,             polarity=polarity,             clip_above_percentile=clip_above_percentile,             clip_below_percentile=clip_below_percentile,             morphological_cleanup=morphological_cleanup,             structure=structure,             outlines=outlines,             outlines_component_percentage=outlines_component_percentage,             overlay=overlay,         )          _, ax = plt.subplots(1, 3, figsize=figsize)         ax[0].imshow(image)         ax[1].imshow(grads_attr.astype(np.uint8))         ax[2].imshow(igrads_attr.astype(np.uint8))          ax[0].set_title(\"Input\")         ax[1].set_title(\"Normal gradients\")         ax[2].set_title(\"Integrated gradients\")         plt.show()"},{"path":"https://keras.posit.co/articles/examples/integrated_gradients.html","id":"lets-test-drive-it","dir":"Articles > Examples","previous_headings":"","what":"Let’s test-drive it","title":"Model interpretability with Integrated Gradients","text":"","code":"# 1. Convert the image to numpy array img = get_img_array(img_path)  # 2. Keep a copy of the original image orig_img = np.copy(img[0]).astype(np.uint8)  # 3. Preprocess the image img_processed = tf.cast(xception.preprocess_input(img), dtype=tf.float32)  # 4. Get model predictions preds = model.predict(img_processed) top_pred_idx = tf.argmax(preds[0]) print(\"Predicted:\", top_pred_idx, xception.decode_predictions(preds, top=1)[0])  # 5. Get the gradients of the last layer for the predicted label grads = get_gradients(img_processed, top_pred_idx=top_pred_idx)  # 6. Get the integrated gradients igrads = random_baseline_integrated_gradients(     np.copy(orig_img), top_pred_idx=top_pred_idx, num_steps=50, num_runs=2 )  # 7. Process the gradients and plot vis = GradVisualizer() vis.visualize(     image=orig_img,     gradients=grads[0].numpy(),     integrated_gradients=igrads.numpy(),     clip_above_percentile=99,     clip_below_percentile=0, )  vis.visualize(     image=orig_img,     gradients=grads[0].numpy(),     integrated_gradients=igrads.numpy(),     clip_above_percentile=95,     clip_below_percentile=28,     morphological_cleanup=True,     outlines=True, )"},{"path":"https://keras.posit.co/articles/examples/involution.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Involutional neural networks","text":"Convolution basis modern neural networks computer vision. convolution kernel spatial-agnostic channel-specific. , isn’t able adapt different visual patterns respect different spatial locations. Along location-related problems, receptive field convolution creates challenges regard capturing long-range spatial interactions. address issues, Li et. al. rethink properties convolution Involution: Inverting Inherence Convolution VisualRecognition. authors propose “involution kernel”, location-specific channel-agnostic. Due location-specific nature operation, authors say self-attention falls design paradigm involution. example describes involution kernel, compares two image classification models, one convolution involution, also tries drawing parallel self-attention layer.","code":""},{"path":"https://keras.posit.co/articles/examples/involution.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Involutional neural networks","text":"","code":"import tensorflow as tf import keras as keras import matplotlib.pyplot as plt  # Set seed for reproducibility. tf.random.set_seed(42)"},{"path":"https://keras.posit.co/articles/examples/involution.html","id":"convolution","dir":"Articles > Examples","previous_headings":"","what":"Convolution","title":"Involutional neural networks","text":"Convolution remains mainstay deep neural networks computer vision. understand Involution, necessary talk convolution operation. Consider input tensor X dimensions H, W C_in. take collection C_out convolution kernels shape K, K, C_in. multiply-add operation input tensor kernels obtain output tensor Y dimensions H, W, C_out. diagram C_out=3. makes output tensor shape H, W 3. One can notice convoltuion kernel depend spatial position input tensor makes location-agnostic. hand, channel output tensor based specific convolution filter makes channel-specific.","code":""},{"path":"https://keras.posit.co/articles/examples/involution.html","id":"involution","dir":"Articles > Examples","previous_headings":"","what":"Involution","title":"Involutional neural networks","text":"idea operation location-specific channel-agnostic. Trying implement specific properties poses challenge. fixed number involution kernels (spatial position) able process variable-resolution input tensors. solve problem, authors considered generating kernel conditioned specific spatial positions. method, able process variable-resolution input tensors ease. diagram provides intuition kernel generation method.","code":"class Involution(keras.layers.Layer):     def __init__(         self, channel, group_number, kernel_size, stride, reduction_ratio, name     ):         super().__init__(name=name)          # Initialize the parameters.         self.channel = channel         self.group_number = group_number         self.kernel_size = kernel_size         self.stride = stride         self.reduction_ratio = reduction_ratio      def build(self, input_shape):         # Get the shape of the input.         (_, height, width, num_channels) = input_shape          # Scale the height and width with respect to the strides.         height = height // self.stride         width = width // self.stride          # Define a layer that average pools the input tensor         # if stride is more than 1.         self.stride_layer = (             keras.layers.AveragePooling2D(                 pool_size=self.stride, strides=self.stride, padding=\"same\"             )             if self.stride > 1             else tf.identity         )         # Define the kernel generation layer.         self.kernel_gen = keras.Sequential(             [                 keras.layers.Conv2D(                     filters=self.channel // self.reduction_ratio, kernel_size=1                 ),                 keras.layers.BatchNormalization(),                 keras.layers.ReLU(),                 keras.layers.Conv2D(                     filters=self.kernel_size                     * self.kernel_size                     * self.group_number,                     kernel_size=1,                 ),             ]         )         # Define reshape layers         self.kernel_reshape = keras.layers.Reshape(             target_shape=(                 height,                 width,                 self.kernel_size * self.kernel_size,                 1,                 self.group_number,             )         )         self.input_patches_reshape = keras.layers.Reshape(             target_shape=(                 height,                 width,                 self.kernel_size * self.kernel_size,                 num_channels // self.group_number,                 self.group_number,             )         )         self.output_reshape = keras.layers.Reshape(             target_shape=(height, width, num_channels)         )      def call(self, x):         # Generate the kernel with respect to the input tensor.         # B, H, W, K*K*G         kernel_input = self.stride_layer(x)         kernel = self.kernel_gen(kernel_input)          # reshape the kerenl         # B, H, W, K*K, 1, G         kernel = self.kernel_reshape(kernel)          # Extract input patches.         # B, H, W, K*K*C         input_patches = tf.image.extract_patches(             images=x,             sizes=[1, self.kernel_size, self.kernel_size, 1],             strides=[1, self.stride, self.stride, 1],             rates=[1, 1, 1, 1],             padding=\"SAME\",         )          # Reshape the input patches to align with later operations.         # B, H, W, K*K, C//G, G         input_patches = self.input_patches_reshape(input_patches)          # Compute the multiply-add operation of kernels and patches.         # B, H, W, K*K, C//G, G         output = tf.multiply(kernel, input_patches)         # B, H, W, C//G, G         output = tf.reduce_sum(output, axis=3)          # Reshape the output kernel.         # B, H, W, C         output = self.output_reshape(output)          # Return the output tensor and the kernel.         return output, kernel"},{"path":"https://keras.posit.co/articles/examples/involution.html","id":"testing-the-involution-layer","dir":"Articles > Examples","previous_headings":"","what":"Testing the Involution layer","title":"Involutional neural networks","text":"","code":"# Define the input tensor. input_tensor = tf.random.normal((32, 256, 256, 3))  # Compute involution with stride 1. output_tensor, _ = Involution(     channel=3,     group_number=1,     kernel_size=5,     stride=1,     reduction_ratio=1,     name=\"inv_1\", )(input_tensor) print(f\"with stride 1 ouput shape: {output_tensor.shape}\")  # Compute involution with stride 2. output_tensor, _ = Involution(     channel=3,     group_number=1,     kernel_size=5,     stride=2,     reduction_ratio=1,     name=\"inv_2\", )(input_tensor) print(f\"with stride 2 ouput shape: {output_tensor.shape}\")  # Compute involution with stride 1, channel 16 and reduction ratio 2. output_tensor, _ = Involution(     channel=16,     group_number=1,     kernel_size=5,     stride=1,     reduction_ratio=2,     name=\"inv_3\", )(input_tensor) print(     \"with channel 16 and reduction ratio 2 ouput shape: {}\".format(         output_tensor.shape     ) )"},{"path":"https://keras.posit.co/articles/examples/involution.html","id":"image-classification","dir":"Articles > Examples","previous_headings":"","what":"Image Classification","title":"Involutional neural networks","text":"section, build image-classifier model. two models one convolutions involutions. image-classification model heavily inspired Convolutional Neural Network (CNN) tutorial Google.","code":""},{"path":"https://keras.posit.co/articles/examples/involution.html","id":"get-the-cifar10-dataset","dir":"Articles > Examples","previous_headings":"","what":"Get the CIFAR10 Dataset","title":"Involutional neural networks","text":"","code":"# Load the CIFAR10 dataset. print(\"loading the CIFAR10 dataset...\") (     (train_images, train_labels),     (         test_images,         test_labels,     ), ) = keras.datasets.cifar10.load_data()  # Normalize pixel values to be between 0 and 1. (train_images, test_images) = (train_images / 255.0, test_images / 255.0)  # Shuffle and batch the dataset. train_ds = (     tf.data.Dataset.from_tensor_slices((train_images, train_labels))     .shuffle(256)     .batch(256) ) test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(     256 )"},{"path":"https://keras.posit.co/articles/examples/involution.html","id":"visualise-the-data","dir":"Articles > Examples","previous_headings":"","what":"Visualise the data","title":"Involutional neural networks","text":"","code":"class_names = [     \"airplane\",     \"automobile\",     \"bird\",     \"cat\",     \"deer\",     \"dog\",     \"frog\",     \"horse\",     \"ship\",     \"truck\", ]  plt.figure(figsize=(10, 10)) for i in range(25):     plt.subplot(5, 5, i + 1)     plt.xticks([])     plt.yticks([])     plt.grid(False)     plt.imshow(train_images[i])     plt.xlabel(class_names[train_labels[i][0]]) plt.show()"},{"path":"https://keras.posit.co/articles/examples/involution.html","id":"convolutional-neural-network","dir":"Articles > Examples","previous_headings":"","what":"Convolutional Neural Network","title":"Involutional neural networks","text":"","code":"# Build the conv model. print(\"building the convolution model...\") conv_model = keras.Sequential(     [         keras.layers.Conv2D(             32, (3, 3), input_shape=(32, 32, 3), padding=\"same\"         ),         keras.layers.ReLU(name=\"relu1\"),         keras.layers.MaxPooling2D((2, 2)),         keras.layers.Conv2D(64, (3, 3), padding=\"same\"),         keras.layers.ReLU(name=\"relu2\"),         keras.layers.MaxPooling2D((2, 2)),         keras.layers.Conv2D(64, (3, 3), padding=\"same\"),         keras.layers.ReLU(name=\"relu3\"),         keras.layers.Flatten(),         keras.layers.Dense(64, activation=\"relu\"),         keras.layers.Dense(10),     ] )  # Compile the mode with the necessary loss function and optimizer. print(\"compiling the convolution model...\") conv_model.compile(     optimizer=\"adam\",     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),     metrics=[\"accuracy\"],     jit_compile=False, )  # Train the model. print(\"conv model training...\") conv_hist = conv_model.fit(train_ds, epochs=20, validation_data=test_ds)"},{"path":"https://keras.posit.co/articles/examples/involution.html","id":"involutional-neural-network","dir":"Articles > Examples","previous_headings":"","what":"Involutional Neural Network","title":"Involutional neural networks","text":"","code":"# Build the involution model. print(\"building the involution model...\")  inputs = keras.Input(shape=(32, 32, 3)) x, _ = Involution(     channel=3,     group_number=1,     kernel_size=3,     stride=1,     reduction_ratio=2,     name=\"inv_1\", )(inputs) x = keras.layers.ReLU()(x) x = keras.layers.MaxPooling2D((2, 2))(x) x, _ = Involution(     channel=3,     group_number=1,     kernel_size=3,     stride=1,     reduction_ratio=2,     name=\"inv_2\", )(x) x = keras.layers.ReLU()(x) x = keras.layers.MaxPooling2D((2, 2))(x) x, _ = Involution(     channel=3,     group_number=1,     kernel_size=3,     stride=1,     reduction_ratio=2,     name=\"inv_3\", )(x) x = keras.layers.ReLU()(x) x = keras.layers.Flatten()(x) x = keras.layers.Dense(64, activation=\"relu\")(x) outputs = keras.layers.Dense(10)(x)  inv_model = keras.Model(inputs=[inputs], outputs=[outputs], name=\"inv_model\")  # Compile the mode with the necessary loss function and optimizer. print(\"compiling the involution model...\") inv_model.compile(     optimizer=\"adam\",     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),     metrics=[\"accuracy\"],     jit_compile=False, )  # train the model print(\"inv model training...\") inv_hist = inv_model.fit(train_ds, epochs=20, validation_data=test_ds)"},{"path":"https://keras.posit.co/articles/examples/involution.html","id":"comparisons","dir":"Articles > Examples","previous_headings":"","what":"Comparisons","title":"Involutional neural networks","text":"section, looking models compare pointers.","code":""},{"path":"https://keras.posit.co/articles/examples/involution.html","id":"parameters","dir":"Articles > Examples","previous_headings":"Comparisons","what":"Parameters","title":"Involutional neural networks","text":"One can see similar architecture parameters CNN much larger INN (Involutional Neural Network).","code":"conv_model.summary()  inv_model.summary()"},{"path":"https://keras.posit.co/articles/examples/involution.html","id":"loss-and-accuracy-plots","dir":"Articles > Examples","previous_headings":"Comparisons","what":"Loss and Accuracy Plots","title":"Involutional neural networks","text":", loss accuracy plots demonstrate INNs slow learners (lower parameters).","code":"plt.figure(figsize=(20, 5))  plt.subplot(1, 2, 1) plt.title(\"Convolution Loss\") plt.plot(conv_hist.history[\"loss\"], label=\"loss\") plt.plot(conv_hist.history[\"val_loss\"], label=\"val_loss\") plt.legend()  plt.subplot(1, 2, 2) plt.title(\"Involution Loss\") plt.plot(inv_hist.history[\"loss\"], label=\"loss\") plt.plot(inv_hist.history[\"val_loss\"], label=\"val_loss\") plt.legend()  plt.show()  plt.figure(figsize=(20, 5))  plt.subplot(1, 2, 1) plt.title(\"Convolution Accuracy\") plt.plot(conv_hist.history[\"accuracy\"], label=\"accuracy\") plt.plot(conv_hist.history[\"val_accuracy\"], label=\"val_accuracy\") plt.legend()  plt.subplot(1, 2, 2) plt.title(\"Involution Accuracy\") plt.plot(inv_hist.history[\"accuracy\"], label=\"accuracy\") plt.plot(inv_hist.history[\"val_accuracy\"], label=\"val_accuracy\") plt.legend()  plt.show()"},{"path":"https://keras.posit.co/articles/examples/involution.html","id":"visualizing-involution-kernels","dir":"Articles > Examples","previous_headings":"","what":"Visualizing Involution Kernels","title":"Involutional neural networks","text":"visualize kernels, take sum K×K values involution kernel. representatives different spatial locations frame corresponding heat map. authors mention: “proposed involution reminiscent self-attention essentially become generalized version .” visualization kernel can indeed obtain attention map image. learned involution kernels provides attention individual spatial positions input tensor. location-specific property makes involution generic space models self-attention belongs.","code":"layer_names = [\"inv_1\", \"inv_2\", \"inv_3\"] # The kernel is the second output of the layer outputs = [inv_model.get_layer(name).output[1] for name in layer_names] vis_model = keras.Model(inv_model.input, outputs)  fig, axes = plt.subplots(nrows=10, ncols=4, figsize=(10, 30))  for ax, test_image in zip(axes, test_images[:10]):     inv_out = vis_model.predict(test_image[None, ...])     inv1_kernel, inv2_kernel, inv3_kernel = inv_out      inv1_kernel = tf.reduce_sum(inv1_kernel, axis=[-1, -2, -3])     inv2_kernel = tf.reduce_sum(inv2_kernel, axis=[-1, -2, -3])     inv3_kernel = tf.reduce_sum(inv3_kernel, axis=[-1, -2, -3])      ax[0].imshow(keras.utils.array_to_img(test_image))     ax[0].set_title(\"Input Image\")      ax[1].imshow(keras.utils.array_to_img(inv1_kernel[0, ..., None]))     ax[1].set_title(\"Involution Kernel 1\")      ax[2].imshow(keras.utils.array_to_img(inv2_kernel[0, ..., None]))     ax[2].set_title(\"Involution Kernel 2\")      ax[3].imshow(keras.utils.array_to_img(inv3_kernel[0, ..., None]))     ax[3].set_title(\"Involution Kernel 3\")"},{"path":"https://keras.posit.co/articles/examples/involution.html","id":"conclusions","dir":"Articles > Examples","previous_headings":"","what":"Conclusions","title":"Involutional neural networks","text":"example, main focus build Involution layer can easily reused. comparisons based specific task, feel free use layer different tasks report results. According , key take-away involution relationship self-attention. intuition behind location-specific channel-spefic processing makes sense lot tasks. Moving forward one can: Look Yannick’s video involution better understanding. Experiment various hyperparameters involution layer. Build different models involution layer. Try building different kernel generation method altogether. can use trained model hosted Hugging Face Hub try demo Hugging Face Spaces.","code":""},{"path":"https://keras.posit.co/articles/examples/keypoint_detection.html","id":"data-collection","dir":"Articles > Examples","previous_headings":"","what":"Data collection","title":"Keypoint Detection with Transfer Learning","text":"StanfordExtra dataset contains 12,000 images dogs together keypoints segmentation maps. developed Stanford dogs dataset. can downloaded command : wget -q http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar Annotations provided single JSON file StanfordExtra dataset one needs fill form get access . authors explicitly instruct users share JSON file, example respects wish: obtain JSON file . JSON file expected locally available stanfordextra_v12.zip. files downloaded, can extract archives. tar xf images.tar unzip -qq ~/stanfordextra_v12.zip","code":""},{"path":"https://keras.posit.co/articles/examples/keypoint_detection.html","id":"imports","dir":"Articles > Examples","previous_headings":"","what":"Imports","title":"Keypoint Detection with Transfer Learning","text":"","code":"from keras import layers import keras as keras  from imgaug.augmentables.kps import KeypointsOnImage from imgaug.augmentables.kps import Keypoint import imgaug.augmenters as iaa  from PIL import Image from sklearn.model_selection import train_test_split from matplotlib import pyplot as plt import pandas as pd import numpy as np import json import os"},{"path":"https://keras.posit.co/articles/examples/keypoint_detection.html","id":"define-hyperparameters","dir":"Articles > Examples","previous_headings":"","what":"Define hyperparameters","title":"Keypoint Detection with Transfer Learning","text":"","code":"IMG_SIZE = 224 BATCH_SIZE = 64 EPOCHS = 5 NUM_KEYPOINTS = 24 * 2  # 24 pairs each having x and y coordinates"},{"path":"https://keras.posit.co/articles/examples/keypoint_detection.html","id":"load-data","dir":"Articles > Examples","previous_headings":"","what":"Load data","title":"Keypoint Detection with Transfer Learning","text":"authors also provide metadata file specifies additional information keypoints, like color information, animal pose name, etc. load file pandas dataframe extract information visualization purposes. single entry json_dict looks like following: example, keys interested : img_path joints total 24 entries present inside joints. entry 3 values: x-coordinate y-coordinate visibility flag keypoints (1 indicates visibility 0 indicates non-visibility) can see joints contain multiple [0, 0, 0] entries denote keypoints labeled. example, consider non-visible well unlabeled keypoints order allow mini-batch learning.","code":"IMG_DIR = \"Images\" JSON = \"StanfordExtra_V12/StanfordExtra_v12.json\" KEYPOINT_DEF = \"https://github.com/benjiebob/StanfordExtra/raw/master/keypoint_definitions.csv\"  # Load the ground-truth annotations. with open(JSON) as infile:     json_data = json.load(infile)  # Set up a dictionary, mapping all the ground-truth information # with respect to the path of the image. json_dict = {i[\"img_path\"]: i for i in json_data} 'n02085782-Japanese_spaniel/n02085782_2886.jpg': {'img_bbox': [205, 20, 116, 201],  'img_height': 272,  'img_path': 'n02085782-Japanese_spaniel/n02085782_2886.jpg',  'img_width': 350,  'is_multiple_dogs': False,  'joints': [[108.66666666666667, 252.0, 1],             [147.66666666666666, 229.0, 1],             [163.5, 208.5, 1],             [0, 0, 0],             [0, 0, 0],             [0, 0, 0],             [54.0, 244.0, 1],             [77.33333333333333, 225.33333333333334, 1],             [79.0, 196.5, 1],             [0, 0, 0],             [0, 0, 0],             [0, 0, 0],             [0, 0, 0],             [0, 0, 0],             [150.66666666666666, 86.66666666666667, 1],             [88.66666666666667, 73.0, 1],             [116.0, 106.33333333333333, 1],             [109.0, 123.33333333333333, 1],             [0, 0, 0],             [0, 0, 0],             [0, 0, 0],             [0, 0, 0],             [0, 0, 0],             [0, 0, 0]],  'seg': ...} # Load the metdata definition file and preview it. keypoint_def = pd.read_csv(KEYPOINT_DEF) keypoint_def.head()  # Extract the colours and labels. colours = keypoint_def[\"Hex colour\"].values.tolist() colours = [\"#\" + colour for colour in colours] labels = keypoint_def[\"Name\"].values.tolist()   # Utility for reading an image and for getting its annotations. def get_dog(name):     data = json_dict[name]     img_data = plt.imread(os.path.join(IMG_DIR, data[\"img_path\"]))     # If the image is RGBA convert it to RGB.     if img_data.shape[-1] == 4:         img_data = img_data.astype(np.uint8)         img_data = Image.fromarray(img_data)         img_data = np.array(img_data.convert(\"RGB\"))     data[\"img_data\"] = img_data      return data"},{"path":"https://keras.posit.co/articles/examples/keypoint_detection.html","id":"visualize-data","dir":"Articles > Examples","previous_headings":"","what":"Visualize data","title":"Keypoint Detection with Transfer Learning","text":"Now, write utility function visualize images keypoints. plots show images non-uniform sizes, expected real-world scenarios. However, resize images uniform shape (instance (224 x 224)) ground-truth annotations also affected. applies apply geometric transformation (horizontal flip, e.g.) image. Fortunately, imgaug provides utilities can handle issue. next section, write data generator inheriting keras.utils.Sequence class applies data augmentation batches data using imgaug.","code":"# Parts of this code come from here: # https://github.com/benjiebob/StanfordExtra/blob/master/demo.ipynb def visualize_keypoints(images, keypoints):     fig, axes = plt.subplots(nrows=len(images), ncols=2, figsize=(16, 12))     [ax.axis(\"off\") for ax in np.ravel(axes)]      for (ax_orig, ax_all), image, current_keypoint in zip(         axes, images, keypoints     ):         ax_orig.imshow(image)         ax_all.imshow(image)          # If the keypoints were formed by `imgaug` then the coordinates need         # to be iterated differently.         if isinstance(current_keypoint, KeypointsOnImage):             for idx, kp in enumerate(current_keypoint.keypoints):                 ax_all.scatter(                     [kp.x],                     [kp.y],                     c=colours[idx],                     marker=\"x\",                     s=50,                     linewidths=5,                 )         else:             current_keypoint = np.array(current_keypoint)             # Since the last entry is the visibility flag, we discard it.             current_keypoint = current_keypoint[:, :2]             for idx, (x, y) in enumerate(current_keypoint):                 ax_all.scatter(                     [x], [y], c=colours[idx], marker=\"x\", s=50, linewidths=5                 )      plt.tight_layout(pad=2.0)     plt.show()   # Select four samples randomly for visualization. samples = list(json_dict.keys()) num_samples = 4 selected_samples = np.random.choice(samples, num_samples, replace=False)  images, keypoints = [], []  for sample in selected_samples:     data = get_dog(sample)     image = data[\"img_data\"]     keypoint = data[\"joints\"]      images.append(image)     keypoints.append(keypoint)  visualize_keypoints(images, keypoints)"},{"path":"https://keras.posit.co/articles/examples/keypoint_detection.html","id":"prepare-data-generator","dir":"Articles > Examples","previous_headings":"","what":"Prepare data generator","title":"Keypoint Detection with Transfer Learning","text":"know operate keypoints imgaug check document.","code":"class KeyPointsDataset(keras.utils.PyDataset):     def __init__(         self, image_keys, aug, batch_size=BATCH_SIZE, train=True, **kwargs     ):         super().__init__(**kwargs)         self.image_keys = image_keys         self.aug = aug         self.batch_size = batch_size         self.train = train         self.on_epoch_end()      def __len__(self):         return len(self.image_keys) // self.batch_size      def on_epoch_end(self):         self.indexes = np.arange(len(self.image_keys))         if self.train:             np.random.shuffle(self.indexes)      def __getitem__(self, index):         indexes = self.indexes[             index * self.batch_size : (index + 1) * self.batch_size         ]         image_keys_temp = [self.image_keys[k] for k in indexes]         (images, keypoints) = self.__data_generation(image_keys_temp)          return (images, keypoints)      def __data_generation(self, image_keys_temp):         batch_images = np.empty(             (self.batch_size, IMG_SIZE, IMG_SIZE, 3), dtype=\"int\"         )         batch_keypoints = np.empty(             (self.batch_size, 1, 1, NUM_KEYPOINTS), dtype=\"float32\"         )          for i, key in enumerate(image_keys_temp):             data = get_dog(key)             current_keypoint = np.array(data[\"joints\"])[:, :2]             kps = []              # To apply our data augmentation pipeline, we first need to             # form Keypoint objects with the original coordinates.             for j in range(0, len(current_keypoint)):                 kps.append(                     Keypoint(x=current_keypoint[j][0], y=current_keypoint[j][1])                 )              # We then project the original image and its keypoint coordinates.             current_image = data[\"img_data\"]             kps_obj = KeypointsOnImage(kps, shape=current_image.shape)              # Apply the augmentation pipeline.             (new_image, new_kps_obj) = self.aug(                 image=current_image, keypoints=kps_obj             )             batch_images[i,] = new_image              # Parse the coordinates from the new keypoint object.             kp_temp = []             for keypoint in new_kps_obj:                 kp_temp.append(np.nan_to_num(keypoint.x))                 kp_temp.append(np.nan_to_num(keypoint.y))              # More on why this reshaping later.             batch_keypoints[i,] = np.array(kp_temp).reshape(1, 1, 24 * 2)          # Scale the coordinates to [0, 1] range.         batch_keypoints = batch_keypoints / IMG_SIZE          return (batch_images, batch_keypoints)"},{"path":"https://keras.posit.co/articles/examples/keypoint_detection.html","id":"define-augmentation-transforms","dir":"Articles > Examples","previous_headings":"","what":"Define augmentation transforms","title":"Keypoint Detection with Transfer Learning","text":"","code":"train_aug = iaa.Sequential(     [         iaa.Resize(IMG_SIZE, interpolation=\"linear\"),         iaa.Fliplr(0.3),         # `Sometimes()` applies a function randomly to the inputs with         # a given probability (0.3, in this case).         iaa.Sometimes(0.3, iaa.Affine(rotate=10, scale=(0.5, 0.7))),     ] )  test_aug = iaa.Sequential([iaa.Resize(IMG_SIZE, interpolation=\"linear\")])"},{"path":"https://keras.posit.co/articles/examples/keypoint_detection.html","id":"create-training-and-validation-splits","dir":"Articles > Examples","previous_headings":"","what":"Create training and validation splits","title":"Keypoint Detection with Transfer Learning","text":"","code":"np.random.shuffle(samples) train_keys, validation_keys = (     samples[int(len(samples) * 0.15) :],     samples[: int(len(samples) * 0.15)], )"},{"path":"https://keras.posit.co/articles/examples/keypoint_detection.html","id":"data-generator-investigation","dir":"Articles > Examples","previous_headings":"","what":"Data generator investigation","title":"Keypoint Detection with Transfer Learning","text":"","code":"train_dataset = KeyPointsDataset(     train_keys, train_aug, workers=2, use_multiprocessing=True ) validation_dataset = KeyPointsDataset(     validation_keys, test_aug, train=False, workers=2, use_multiprocessing=True )  print(f\"Total batches in training set: {len(train_dataset)}\") print(f\"Total batches in validation set: {len(validation_dataset)}\")  sample_images, sample_keypoints = next(iter(train_dataset)) assert sample_keypoints.max() == 1.0 assert sample_keypoints.min() == 0.0  sample_keypoints = sample_keypoints[:4].reshape(-1, 24, 2) * IMG_SIZE visualize_keypoints(sample_images[:4], sample_keypoints)"},{"path":"https://keras.posit.co/articles/examples/keypoint_detection.html","id":"model-building","dir":"Articles > Examples","previous_headings":"","what":"Model building","title":"Keypoint Detection with Transfer Learning","text":"Stanford dogs dataset (StanfordExtra dataset based) built using ImageNet-1k dataset. , likely models pretrained ImageNet-1k dataset useful task. use MobileNetV2 pre-trained dataset backbone extract meaningful features images pass custom regression head predicting coordinates. custom network fully-convolutional makes parameter-friendly version network fully-connected dense layers. Notice output shape network: (None, 1, 1, 48). reshaped coordinates : batch_keypoints[, :] = np.array(kp_temp).reshape(1, 1, 24 * 2).","code":"def get_model():     # Load the pre-trained weights of MobileNetV2 and freeze the weights     backbone = keras.applications.MobileNetV2(         weights=\"imagenet\",         include_top=False,         input_shape=(IMG_SIZE, IMG_SIZE, 3),     )     backbone.trainable = False      inputs = layers.Input((IMG_SIZE, IMG_SIZE, 3))     x = keras.applications.mobilenet_v2.preprocess_input(inputs)     x = backbone(x)     x = layers.Dropout(0.3)(x)     x = layers.SeparableConv2D(         NUM_KEYPOINTS, kernel_size=5, strides=1, activation=\"relu\"     )(x)     outputs = layers.SeparableConv2D(         NUM_KEYPOINTS, kernel_size=3, strides=1, activation=\"sigmoid\"     )(x)      return keras.Model(inputs, outputs, name=\"keypoint_detector\") get_model().summary()"},{"path":"https://keras.posit.co/articles/examples/keypoint_detection.html","id":"model-compilation-and-training","dir":"Articles > Examples","previous_headings":"","what":"Model compilation and training","title":"Keypoint Detection with Transfer Learning","text":"example, train network five epochs.","code":"model = get_model() model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(1e-4)) model.fit(train_dataset, validation_data=validation_dataset, epochs=EPOCHS)"},{"path":"https://keras.posit.co/articles/examples/keypoint_detection.html","id":"make-predictions-and-visualize-them","dir":"Articles > Examples","previous_headings":"","what":"Make predictions and visualize them","title":"Keypoint Detection with Transfer Learning","text":"Predictions likely improve training.","code":"sample_val_images, sample_val_keypoints = next(iter(validation_dataset)) sample_val_images = sample_val_images[:4] sample_val_keypoints = sample_val_keypoints[:4].reshape(-1, 24, 2) * IMG_SIZE predictions = model.predict(sample_val_images).reshape(-1, 24, 2) * IMG_SIZE  # Ground-truth visualize_keypoints(sample_val_images, sample_val_keypoints)  # Predictions visualize_keypoints(sample_val_images, predictions)"},{"path":"https://keras.posit.co/articles/examples/keypoint_detection.html","id":"going-further","dir":"Articles > Examples","previous_headings":"","what":"Going further","title":"Keypoint Detection with Transfer Learning","text":"Try using augmentation transforms imgaug investigate changes results. , transferred features pre-trained network linearly fine-tune . encouraged fine-tune task see improves performance. can also try different architectures see affect final performance.","code":""},{"path":"https://keras.posit.co/articles/examples/knowledge_distillation.html","id":"introduction-to-knowledge-distillation","dir":"Articles > Examples","previous_headings":"","what":"Introduction to Knowledge Distillation","title":"Knowledge Distillation","text":"Knowledge Distillation procedure model compression, small (student) model trained match large pre-trained (teacher) model. Knowledge transferred teacher model student minimizing loss function, aimed matching softened teacher logits well ground-truth labels. logits softened applying “temperature” scaling function softmax, effectively smoothing probability distribution revealing inter-class relationships learned teacher. Reference: Hinton et al. (2015)","code":""},{"path":"https://keras.posit.co/articles/examples/knowledge_distillation.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Knowledge Distillation","text":"","code":"import os  os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  import keras from keras import layers from keras import ops import tensorflow as tf import numpy as np"},{"path":"https://keras.posit.co/articles/examples/knowledge_distillation.html","id":"construct-distiller-class","dir":"Articles > Examples","previous_headings":"","what":"Construct Distiller() class","title":"Knowledge Distillation","text":"custom Distiller() class, overrides Model methods compile, compute_loss, call. order use distiller, need: trained teacher model student model train student loss function difference student predictions ground-truth distillation loss function, along temperature, difference soft student predictions soft teacher labels alpha factor weight student distillation loss optimizer student (optional) metrics evaluate performance compute_loss method, perform forward pass teacher student, calculate loss weighting student_loss distillation_loss alpha 1 - alpha, respectively. Note: student weights updated.","code":"class Distiller(keras.Model):     def __init__(self, student, teacher):         super().__init__()         self.teacher = teacher         self.student = student      def compile(         self,         optimizer,         metrics,         student_loss_fn,         distillation_loss_fn,         alpha=0.1,         temperature=3,     ):         \"\"\"Configure the distiller.          Args:             optimizer: Keras optimizer for the student weights             metrics: Keras metrics for evaluation             student_loss_fn: Loss function of difference between student                 predictions and ground-truth             distillation_loss_fn: Loss function of difference between soft                 student predictions and soft teacher predictions             alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn             temperature: Temperature for softening probability distributions.                 Larger temperature gives softer distributions.         \"\"\"         super().compile(optimizer=optimizer, metrics=metrics)         self.student_loss_fn = student_loss_fn         self.distillation_loss_fn = distillation_loss_fn         self.alpha = alpha         self.temperature = temperature      def compute_loss(         self, x=None, y=None, y_pred=None, sample_weight=None, allow_empty=False     ):         teacher_pred = self.teacher(x, training=False)         student_loss = self.student_loss_fn(y, y_pred)          distillation_loss = self.distillation_loss_fn(             ops.softmax(teacher_pred / self.temperature, axis=1),             ops.softmax(y_pred / self.temperature, axis=1),         ) * (self.temperature**2)          loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss         return loss      def call(self, x):         return self.student(x)"},{"path":"https://keras.posit.co/articles/examples/knowledge_distillation.html","id":"create-student-and-teacher-models","dir":"Articles > Examples","previous_headings":"","what":"Create student and teacher models","title":"Knowledge Distillation","text":"Initialy, create teacher model smaller student model. models convolutional neural networks created using Sequential(), Keras model.","code":"# Create the teacher teacher = keras.Sequential(     [         keras.Input(shape=(28, 28, 1)),         layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),         layers.LeakyReLU(negative_slope=0.2),         layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),         layers.Conv2D(512, (3, 3), strides=(2, 2), padding=\"same\"),         layers.Flatten(),         layers.Dense(10),     ],     name=\"teacher\", )  # Create the student student = keras.Sequential(     [         keras.Input(shape=(28, 28, 1)),         layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),         layers.LeakyReLU(negative_slope=0.2),         layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),         layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),         layers.Flatten(),         layers.Dense(10),     ],     name=\"student\", )  # Clone student for later comparison student_scratch = keras.models.clone_model(student)"},{"path":"https://keras.posit.co/articles/examples/knowledge_distillation.html","id":"prepare-the-dataset","dir":"Articles > Examples","previous_headings":"","what":"Prepare the dataset","title":"Knowledge Distillation","text":"dataset used training teacher distilling teacher MNIST, procedure equivalent dataset, e.g. CIFAR-10, suitable choice models. student teacher trained training set evaluated test set.","code":"# Prepare the train and test dataset. batch_size = 64 (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()  # Normalize data x_train = x_train.astype(\"float32\") / 255.0 x_train = np.reshape(x_train, (-1, 28, 28, 1))  x_test = x_test.astype(\"float32\") / 255.0 x_test = np.reshape(x_test, (-1, 28, 28, 1))"},{"path":"https://keras.posit.co/articles/examples/knowledge_distillation.html","id":"train-the-teacher","dir":"Articles > Examples","previous_headings":"","what":"Train the teacher","title":"Knowledge Distillation","text":"knowledge distillation assume teacher trained fixed. Thus, start training teacher model training set usual way.","code":"# Train teacher as usual teacher.compile(     optimizer=keras.optimizers.Adam(),     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),     metrics=[keras.metrics.SparseCategoricalAccuracy()], )  # Train and evaluate teacher on data. teacher.fit(x_train, y_train, epochs=5) teacher.evaluate(x_test, y_test)"},{"path":"https://keras.posit.co/articles/examples/knowledge_distillation.html","id":"distill-teacher-to-student","dir":"Articles > Examples","previous_headings":"","what":"Distill teacher to student","title":"Knowledge Distillation","text":"already trained teacher model, need initialize Distiller(student, teacher) instance, compile() desired losses, hyperparameters optimizer, distill teacher student.","code":"# Initialize and compile distiller distiller = Distiller(student=student, teacher=teacher) distiller.compile(     optimizer=keras.optimizers.Adam(),     metrics=[keras.metrics.SparseCategoricalAccuracy()],     student_loss_fn=keras.losses.SparseCategoricalCrossentropy(         from_logits=True     ),     distillation_loss_fn=keras.losses.KLDivergence(),     alpha=0.1,     temperature=10, )  # Distill teacher to student distiller.fit(x_train, y_train, epochs=3)  # Evaluate student on test dataset distiller.evaluate(x_test, y_test)"},{"path":"https://keras.posit.co/articles/examples/knowledge_distillation.html","id":"train-student-from-scratch-for-comparison","dir":"Articles > Examples","previous_headings":"","what":"Train student from scratch for comparison","title":"Knowledge Distillation","text":"can also train equivalent student model scratch without teacher, order evaluate performance gain obtained knowledge distillation. teacher trained 5 full epochs student distilled teacher 3 full epochs, example experience performance boost compared training student model scratch, even compared teacher . expect teacher accuracy around 97.6%, student trained scratch around 97.6%, distilled student around 98.1%. Remove try different seeds use different weight initializations.","code":"# Train student as doen usually student_scratch.compile(     optimizer=keras.optimizers.Adam(),     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),     metrics=[keras.metrics.SparseCategoricalAccuracy()], )  # Train and evaluate student trained from scratch. student_scratch.fit(x_train, y_train, epochs=3) student_scratch.evaluate(x_test, y_test)"},{"path":"https://keras.posit.co/articles/examples/latent_walk.html","id":"overview","dir":"Articles > Examples","previous_headings":"","what":"Overview","title":"A walk through latent space with Stable Diffusion","text":"Generative image models learn “latent manifold” visual world: low-dimensional vector space point maps image. Going point manifold back displayable image called “decoding” – Stable Diffusion model, handled “decoder” model. latent manifold images continuous interpolative, meaning : Moving little manifold changes corresponding image little (continuity). two points B manifold (.e. two images), possible move B via path intermediate point also manifold (.e. also valid image). Intermediate points called “interpolations” two starting images. Stable Diffusion isn’t just image model, though, ’s also natural language model. two latent spaces: image representation space learned encoder used training, prompt latent space learned using combination pretraining training-time fine-tuning. Latent space walking, latent space exploration, process sampling point latent space incrementally changing latent representation. common application generating animations sampled point fed decoder stored frame final animation. high-quality latent representations, produces coherent-looking animations. animations can provide insight feature map latent space, can ultimately lead improvements training process. One GIF displayed : guide, show take advantage Stable Diffusion API KerasCV perform prompt interpolation circular walks Stable Diffusion’s visual latent manifold, well text encoder’s latent manifold. guide assumes reader high-level understanding Stable Diffusion. haven’t already, start reading Stable Diffusion Tutorial. start, import KerasCV load Stable Diffusion model using optimizations discussed tutorial Generate images Stable Diffusion. Note running M1 Mac GPU enable mixed precision. pip install keras-cv –upgrade –quiet","code":"import keras_cv import keras import matplotlib.pyplot as plt from keras import ops import numpy as np import math from PIL import Image  # Enable mixed precision # (only do this if you have a recent NVIDIA GPU) keras.mixed_precision.set_global_policy(\"mixed_float16\")  # Instantiate the Stable Diffusion model model = keras_cv.models.StableDiffusion(jit_compile=True)"},{"path":"https://keras.posit.co/articles/examples/latent_walk.html","id":"interpolating-between-text-prompts","dir":"Articles > Examples","previous_headings":"","what":"Interpolating between text prompts","title":"A walk through latent space with Stable Diffusion","text":"Stable Diffusion, text prompt first encoded vector, encoding used guide diffusion process. latent encoding vector shape 77x768 (’s huge!), give Stable Diffusion text prompt, ’re generating images just one point latent manifold. explore manifold, can interpolate two text encodings generate images interpolated points: ’ve interpolated encodings, can generate images point. Note order maintain stability resulting images keep diffusion noise constant images. Now ’ve generated interpolated images, let’s take look ! Throughout tutorial, ’re going export sequences images gifs can easily viewed temporal context. sequences images first last images don’t match conceptually, rubber-band gif. ’re running Colab, can view GIFs running: results may seem surprising. Generally, interpolating prompts produces coherent looking images, often demonstrates progressive concept shift contents two prompts. indicative high quality representation space, closely mirrors natural structure visual world. best visualize , much fine-grained interpolation, using hundreds steps. order keep batch size small (don’t OOM GPU), requires manually batching interpolated encodings. resulting gif shows much clearer coherent shift two prompts. Try prompts experiment! can even extend concept one image. example, can interpolate four prompts: can also interpolate allowing diffusion noise vary dropping diffusion_noise parameter: Next – let’s go walks!","code":"prompt_1 = \"A watercolor painting of a Golden Retriever at the beach\" prompt_2 = \"A still life DSLR photo of a bowl of fruit\" interpolation_steps = 5  encoding_1 = ops.squeeze(model.encode_text(prompt_1)) encoding_2 = ops.squeeze(model.encode_text(prompt_2))  interpolated_encodings = ops.linspace(encoding_1, encoding_2, interpolation_steps)  # Show the size of the latent manifold print(f\"Encoding shape: {encoding_1.shape}\") seed = 12345 noise = keras.random.normal((512 // 8, 512 // 8, 4), seed=seed)  images = model.generate_image(     interpolated_encodings,     batch_size=interpolation_steps,     diffusion_noise=noise, ) from IPython.display import Image as IImage IImage(\"doggo-and-fruit-5.gif\") def export_as_gif(filename, images, frames_per_second=10, rubber_band=False):     if rubber_band:         images += images[2:-1][::-1]     images[0].save(         filename,         save_all=True,         append_images=images[1:],         duration=1000 // frames_per_second,         loop=0,     )   export_as_gif(     \"doggo-and-fruit-5.gif\",     [Image.fromarray(img) for img in images],     frames_per_second=2,     rubber_band=True, ) interpolation_steps = 150 batch_size = 3 batches = interpolation_steps // batch_size  interpolated_encodings = ops.linspace(encoding_1, encoding_2, interpolation_steps) batched_encodings = ops.split(interpolated_encodings, batches)  images = [] for batch in range(batches):     images += [         Image.fromarray(img)         for img in model.generate_image(             batched_encodings[batch],             batch_size=batch_size,             num_steps=25,             diffusion_noise=noise,         )     ]  export_as_gif(\"doggo-and-fruit-150.gif\", images, rubber_band=True) prompt_1 = \"A watercolor painting of a Golden Retriever at the beach\" prompt_2 = \"A still life DSLR photo of a bowl of fruit\" prompt_3 = \"The eiffel tower in the style of starry night\" prompt_4 = \"An architectural sketch of a skyscraper\"  interpolation_steps = 6 batch_size = 3 batches = (interpolation_steps**2) // batch_size  encoding_1 = ops.squeeze(model.encode_text(prompt_1)) encoding_2 = ops.squeeze(model.encode_text(prompt_2)) encoding_3 = ops.squeeze(model.encode_text(prompt_3)) encoding_4 = ops.squeeze(model.encode_text(prompt_4))  interpolated_encodings = ops.linspace(     ops.linspace(encoding_1, encoding_2, interpolation_steps),     ops.linspace(encoding_3, encoding_4, interpolation_steps),     interpolation_steps, ) interpolated_encodings = ops.reshape(     interpolated_encodings, (interpolation_steps**2, 77, 768) ) batched_encodings = ops.split(interpolated_encodings, batches)  images = [] for batch in range(batches):     images.append(         model.generate_image(             batched_encodings[batch],             batch_size=batch_size,             diffusion_noise=noise,         )     )   def plot_grid(     images,     path,     grid_size,     scale=2, ):     fig = plt.figure(figsize=(grid_size * scale, grid_size * scale))     fig.tight_layout()     plt.subplots_adjust(wspace=0, hspace=0)     plt.margins(x=0, y=0)     plt.axis(\"off\")     images = images.astype(int)     for row in range(grid_size):         for col in range(grid_size):             index = row * grid_size + col             plt.subplot(grid_size, grid_size, index + 1)             plt.imshow(images[index].astype(\"uint8\"))             plt.axis(\"off\")             plt.margins(x=0, y=0)     plt.savefig(         fname=path,         pad_inches=0,         bbox_inches=\"tight\",         transparent=False,         dpi=60,     )   images = np.concatenate(images) plot_grid(images, \"4-way-interpolation.jpg\", interpolation_steps) images = [] for batch in range(batches):     images.append(model.generate_image(batched_encodings[batch], batch_size=batch_size))  images = np.concatenate(images) plot_grid(images, \"4-way-interpolation-varying-noise.jpg\", interpolation_steps)"},{"path":"https://keras.posit.co/articles/examples/latent_walk.html","id":"a-walk-around-a-text-prompt","dir":"Articles > Examples","previous_headings":"","what":"A walk around a text prompt","title":"A walk through latent space with Stable Diffusion","text":"next experiment go walk around latent manifold starting point produced particular prompt. Perhaps unsurprisingly, walking far encoder’s latent manifold produces images look incoherent. Try setting prompt, adjusting step_size increase decrease magnitude walk. Note magnitude walk gets large, walk often leads areas produce extremely noisy images.","code":"walk_steps = 150 batch_size = 3 batches = walk_steps // batch_size step_size = 0.005  encoding = ops.squeeze(     model.encode_text(\"The Eiffel Tower in the style of starry night\") ) # Note that (77, 768) is the shape of the text encoding. delta = ops.ones_like(encoding) * step_size  walked_encodings = [] for step_index in range(walk_steps):     walked_encodings.append(encoding)     encoding += delta walked_encodings = ops.stack(walked_encodings) batched_encodings = ops.split(walked_encodings, batches)  images = [] for batch in range(batches):     images += [         Image.fromarray(img)         for img in model.generate_image(             batched_encodings[batch],             batch_size=batch_size,             num_steps=25,             diffusion_noise=noise,         )     ]  export_as_gif(\"eiffel-tower-starry-night.gif\", images, rubber_band=True)"},{"path":"https://keras.posit.co/articles/examples/latent_walk.html","id":"a-circular-walk-through-the-diffusion-noise-space-for-a-single-prompt","dir":"Articles > Examples","previous_headings":"","what":"A circular walk through the diffusion noise space for a single prompt","title":"A walk through latent space with Stable Diffusion","text":"final experiment stick one prompt explore variety images diffusion model can produce prompt. controlling noise used seed diffusion process. create two noise components, x y, walk 0 2π, summing cosine x component sin y component produce noise. Using approach, end walk arrives noise inputs began walk, get “loopable” result! Experiment prompts different values unconditional_guidance_scale!","code":"prompt = \"An oil paintings of cows in a field next to a windmill in Holland\" encoding = ops.squeeze(model.encode_text(prompt)) walk_steps = 150 batch_size = 3 batches = walk_steps // batch_size  walk_noise_x = keras.random.normal(noise.shape, dtype=\"float64\") walk_noise_y = keras.random.normal(noise.shape, dtype=\"float64\")  walk_scale_x = ops.cos(ops.linspace(0, 2, walk_steps) * math.pi) walk_scale_y = ops.sin(ops.linspace(0, 2, walk_steps) * math.pi) noise_x = ops.tensordot(walk_scale_x, walk_noise_x, axes=0) noise_y = ops.tensordot(walk_scale_y, walk_noise_y, axes=0) noise = ops.add(noise_x, noise_y) batched_noise = ops.split(noise, batches)  images = [] for batch in range(batches):     images += [         Image.fromarray(img)         for img in model.generate_image(             encoding,             batch_size=batch_size,             num_steps=25,             diffusion_noise=batched_noise[batch],         )     ]  export_as_gif(\"cows.gif\", images)"},{"path":"https://keras.posit.co/articles/examples/latent_walk.html","id":"conclusion","dir":"Articles > Examples","previous_headings":"","what":"Conclusion","title":"A walk through latent space with Stable Diffusion","text":"Stable Diffusion offers lot just single text--image generation. Exploring latent manifold text encoder noise space diffusion model two fun ways experience power model, KerasCV makes easy!","code":""},{"path":"https://keras.posit.co/articles/examples/learnable_resizer.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Learning to Resize in Computer Vision","text":"","code":"from keras import layers import keras as keras from keras import ops  from tensorflow import data as tf_data from tensorflow import image as tf_image from tensorflow import one_hot as tf_one_hot  import tensorflow_datasets as tfds  tfds.disable_progress_bar()  import matplotlib.pyplot as plt import numpy as np"},{"path":"https://keras.posit.co/articles/examples/learnable_resizer.html","id":"define-hyperparameters","dir":"Articles > Examples","previous_headings":"","what":"Define hyperparameters","title":"Learning to Resize in Computer Vision","text":"order facilitate mini-batch learning, need fixed shape images inside given batch. initial resizing required. first resize images (300 x 300) shape learn optimal representation (150 x 150) resolution. example, use bilinear interpolation learnable image resizer module dependent specific interpolation method. can also use others, bicubic.","code":"INP_SIZE = (300, 300) TARGET_SIZE = (150, 150) INTERPOLATION = \"bilinear\"  AUTO = tf_data.AUTOTUNE BATCH_SIZE = 50 EPOCHS = 5"},{"path":"https://keras.posit.co/articles/examples/learnable_resizer.html","id":"load-and-prepare-the-dataset","dir":"Articles > Examples","previous_headings":"","what":"Load and prepare the dataset","title":"Learning to Resize in Computer Vision","text":"example, use 40% total training dataset.","code":"train_ds, validation_ds = tfds.load(     \"cats_vs_dogs\",     # Reserve 10% for validation     split=[\"train[:40%]\", \"train[40%:50%]\"],     as_supervised=True, )   def preprocess_dataset(image, label):     image = tf_image.resize(image, (INP_SIZE[0], INP_SIZE[1]))     label = tf_one_hot(label, depth=2)     return (image, label)   train_ds = (     train_ds.shuffle(BATCH_SIZE * 100)     .map(preprocess_dataset, num_parallel_calls=AUTO)     .batch(BATCH_SIZE)     .prefetch(AUTO) ) validation_ds = (     validation_ds.map(preprocess_dataset, num_parallel_calls=AUTO)     .batch(BATCH_SIZE)     .prefetch(AUTO) )"},{"path":"https://keras.posit.co/articles/examples/learnable_resizer.html","id":"define-the-learnable-resizer-utilities","dir":"Articles > Examples","previous_headings":"","what":"Define the learnable resizer utilities","title":"Learning to Resize in Computer Vision","text":"figure (courtesy: Learning Resize Images Computer Vision Tasks) presents structure learnable resizing module:","code":"def conv_block(     x, filters, kernel_size, strides, activation=layers.LeakyReLU(0.2) ):     x = layers.Conv2D(         filters, kernel_size, strides, padding=\"same\", use_bias=False     )(x)     x = layers.BatchNormalization()(x)     if activation:         x = activation(x)     return x   def res_block(x):     inputs = x     x = conv_block(x, 16, 3, 1)     x = conv_block(x, 16, 3, 1, activation=None)     return layers.Add()([inputs, x])   def get_learnable_resizer(     filters=16, num_res_blocks=1, interpolation=INTERPOLATION ):     inputs = layers.Input(shape=[None, None, 3])      # First, perform naive resizing.     naive_resize = layers.Resizing(*TARGET_SIZE, interpolation=interpolation)(         inputs     )      # First convolution block without batch normalization.     x = layers.Conv2D(         filters=filters, kernel_size=7, strides=1, padding=\"same\"     )(inputs)     x = layers.LeakyReLU(0.2)(x)      # Second convolution block with batch normalization.     x = layers.Conv2D(         filters=filters, kernel_size=1, strides=1, padding=\"same\"     )(x)     x = layers.LeakyReLU(0.2)(x)     x = layers.BatchNormalization()(x)      # Intermediate resizing as a bottleneck.     bottleneck = layers.Resizing(*TARGET_SIZE, interpolation=interpolation)(x)      # Residual passes.     for _ in range(num_res_blocks):         x = res_block(bottleneck)      # Projection.     x = layers.Conv2D(         filters=filters,         kernel_size=3,         strides=1,         padding=\"same\",         use_bias=False,     )(x)     x = layers.BatchNormalization()(x)      # Skip connection.     x = layers.Add()([bottleneck, x])      # Final resized image.     x = layers.Conv2D(filters=3, kernel_size=7, strides=1, padding=\"same\")(x)     final_resize = layers.Add()([naive_resize, x])      return keras.Model(inputs, final_resize, name=\"learnable_resizer\")   learnable_resizer = get_learnable_resizer()"},{"path":"https://keras.posit.co/articles/examples/learnable_resizer.html","id":"visualize-the-outputs-of-the-learnable-resizing-module","dir":"Articles > Examples","previous_headings":"","what":"Visualize the outputs of the learnable resizing module","title":"Learning to Resize in Computer Vision","text":", visualize resized images look like passed random weights resizer.","code":"sample_images, _ = next(iter(train_ds))  get_np = lambda image: ops.convert_to_numpy(     ops.squeeze(image) )  # Helper to convert image from any backend to numpy  plt.figure(figsize=(16, 10)) for i, image in enumerate(sample_images[:6]):     image = image / 255      ax = plt.subplot(3, 4, 2 * i + 1)     plt.title(\"Input Image\")     plt.imshow(image.numpy().squeeze())     plt.axis(\"off\")      ax = plt.subplot(3, 4, 2 * i + 2)     resized_image = learnable_resizer(image[None, ...])     plt.title(\"Resized Image\")     plt.imshow(get_np(resized_image))     plt.axis(\"off\")"},{"path":"https://keras.posit.co/articles/examples/learnable_resizer.html","id":"model-building-utility","dir":"Articles > Examples","previous_headings":"","what":"Model building utility","title":"Learning to Resize in Computer Vision","text":"structure learnable image resizer module allows flexible integrations different vision models.","code":"def get_model():     backbone = keras.applications.DenseNet121(         weights=None,         include_top=True,         classes=2,         input_shape=((TARGET_SIZE[0], TARGET_SIZE[1], 3)),     )     backbone.trainable = True      inputs = layers.Input((INP_SIZE[0], INP_SIZE[1], 3))     x = layers.Rescaling(scale=1.0 / 255)(inputs)     x = learnable_resizer(x)     outputs = backbone(x)      return keras.Model(inputs, outputs)"},{"path":"https://keras.posit.co/articles/examples/learnable_resizer.html","id":"compile-and-train-our-model-with-learnable-resizer","dir":"Articles > Examples","previous_headings":"","what":"Compile and train our model with learnable resizer","title":"Learning to Resize in Computer Vision","text":"","code":"model = get_model() model.compile(     loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),     optimizer=\"sgd\",     metrics=[\"accuracy\"], ) model.fit(train_ds, validation_data=validation_ds, epochs=EPOCHS)"},{"path":"https://keras.posit.co/articles/examples/learnable_resizer.html","id":"visualize-the-outputs-of-the-trained-visualizer","dir":"Articles > Examples","previous_headings":"","what":"Visualize the outputs of the trained visualizer","title":"Learning to Resize in Computer Vision","text":"plot shows visuals images improved training. following table shows benefits using resizing module comparison using bilinear interpolation: details, can check repository. Note -reported models trained 10 epochs 90% training set Cats Dogs unlike example. Also, note increase number parameters due resizing module negligible. ensure improvement performance due stochasticity, models trained using initial random weights. Now, question worth asking - isn’t improved accuracy simply consequence adding layers (resizer mini network ) model, compared baseline? show case, authors conduct following experiment: Take pre-trained model trained size, say (224 x 224). Now, first, use infer predictions images resized lower resolution. Record performance. second experiment, plug resizer module top pre-trained model warm-start training. Record performance. Now, authors argue using second option better helps model learn adjust representations better respect given resolution. Since results purely empirical, experiments analyzing cross-channel interaction even better. worth noting elements like Squeeze Excitation (SE) blocks, Global Context (GC) blocks also add parameters existing network known help network process information systematic ways improve overall performance.","code":"plt.figure(figsize=(16, 10)) for i, image in enumerate(sample_images[:6]):     image = image / 255      ax = plt.subplot(3, 4, 2 * i + 1)     plt.title(\"Input Image\")     plt.imshow(image.numpy().squeeze())     plt.axis(\"off\")      ax = plt.subplot(3, 4, 2 * i + 2)     resized_image = learnable_resizer(image[None, ...])     plt.title(\"Resized Image\")     plt.imshow(get_np(resized_image) / 10)     plt.axis(\"off\")"},{"path":"https://keras.posit.co/articles/examples/learnable_resizer.html","id":"notes","dir":"Articles > Examples","previous_headings":"","what":"Notes","title":"Learning to Resize in Computer Vision","text":"impose shape bias inside vision models, Geirhos et al. trained combination natural stylized images. might interesting investigate learnable resizing module achieve something similar outputs seem discard texture information. resizer module can handle arbitrary resolutions aspect ratios important tasks like object detection segmentation. another closely related topic adaptive image resizing attempts resize images/feature maps adaptively training. EfficientV2 uses idea.","code":""},{"path":"https://keras.posit.co/articles/examples/lstm_character_level_text_generation.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Character-level text generation with LSTM","text":"example demonstrates use LSTM model generate text character--character. least 20 epochs required generated text starts sounding locally coherent. recommended run script GPU, recurrent networks quite computationally intensive. try script new data, make sure corpus least ~100k characters. ~1M better.","code":""},{"path":"https://keras.posit.co/articles/examples/lstm_character_level_text_generation.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Character-level text generation with LSTM","text":"","code":"import keras as keras from keras import layers  import numpy as np import random import io"},{"path":"https://keras.posit.co/articles/examples/lstm_character_level_text_generation.html","id":"prepare-the-data","dir":"Articles > Examples","previous_headings":"","what":"Prepare the data","title":"Character-level text generation with LSTM","text":"","code":"path = keras.utils.get_file(     \"nietzsche.txt\",     origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\", ) with io.open(path, encoding=\"utf-8\") as f:     text = f.read().lower() text = text.replace(\"\\n\", \" \")  # We remove newlines chars for nicer display print(\"Corpus length:\", len(text))  chars = sorted(list(set(text))) print(\"Total chars:\", len(chars)) char_indices = dict((c, i) for i, c in enumerate(chars)) indices_char = dict((i, c) for i, c in enumerate(chars))  # cut the text in semi-redundant sequences of maxlen characters maxlen = 40 step = 3 sentences = [] next_chars = [] for i in range(0, len(text) - maxlen, step):     sentences.append(text[i : i + maxlen])     next_chars.append(text[i + maxlen]) print(\"Number of sequences:\", len(sentences))  x = np.zeros((len(sentences), maxlen, len(chars)), dtype=\"bool\") y = np.zeros((len(sentences), len(chars)), dtype=\"bool\") for i, sentence in enumerate(sentences):     for t, char in enumerate(sentence):         x[i, t, char_indices[char]] = 1     y[i, char_indices[next_chars[i]]] = 1"},{"path":"https://keras.posit.co/articles/examples/lstm_character_level_text_generation.html","id":"build-the-model-a-single-lstm-layer","dir":"Articles > Examples","previous_headings":"","what":"Build the model: a single LSTM layer","title":"Character-level text generation with LSTM","text":"","code":"model = keras.Sequential(     [         keras.Input(shape=(maxlen, len(chars))),         layers.LSTM(128),         layers.Dense(len(chars), activation=\"softmax\"),     ] ) optimizer = keras.optimizers.RMSprop(learning_rate=0.01) model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)"},{"path":"https://keras.posit.co/articles/examples/lstm_character_level_text_generation.html","id":"prepare-the-text-sampling-function","dir":"Articles > Examples","previous_headings":"","what":"Prepare the text sampling function","title":"Character-level text generation with LSTM","text":"","code":"def sample(preds, temperature=1.0):     # helper function to sample an index from a probability array     preds = np.asarray(preds).astype(\"float64\")     preds = np.log(preds) / temperature     exp_preds = np.exp(preds)     preds = exp_preds / np.sum(exp_preds)     probas = np.random.multinomial(1, preds, 1)     return np.argmax(probas)"},{"path":"https://keras.posit.co/articles/examples/lstm_character_level_text_generation.html","id":"train-the-model","dir":"Articles > Examples","previous_headings":"","what":"Train the model","title":"Character-level text generation with LSTM","text":"","code":"epochs = 40 batch_size = 128  for epoch in range(epochs):     model.fit(x, y, batch_size=batch_size, epochs=1)     print()     print(\"Generating text after epoch: %d\" % epoch)      start_index = random.randint(0, len(text) - maxlen - 1)     for diversity in [0.2, 0.5, 1.0, 1.2]:         print(\"...Diversity:\", diversity)          generated = \"\"         sentence = text[start_index : start_index + maxlen]         print('...Generating with seed: \"' + sentence + '\"')          for i in range(400):             x_pred = np.zeros((1, maxlen, len(chars)))             for t, char in enumerate(sentence):                 x_pred[0, t, char_indices[char]] = 1.0             preds = model.predict(x_pred, verbose=0)[0]             next_index = sample(preds, diversity)             next_char = indices_char[next_index]             sentence = sentence[1:] + next_char             generated += next_char          print(\"...Generated: \", generated)         print(\"-\")"},{"path":"https://keras.posit.co/articles/examples/lstm_seq2seq.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Character-level recurrent sequence-to-sequence model","text":"example demonstrates implement basic character-level recurrent sequence--sequence model. apply translating short English sentences short French sentences, character--character. Note fairly unusual character-level machine translation, word-level models common domain. Summary algorithm start input sequences domain (e.g. English sentences) corresponding target sequences another domain (e.g. French sentences). encoder LSTM turns input sequences 2 state vectors (keep last LSTM state discard outputs). decoder LSTM trained turn target sequences sequence offset one timestep future, training process called “teacher forcing” context. uses initial state state vectors encoder. Effectively, decoder learns generate targets[t+1...] given targets[...t], conditioned input sequence. Encode input sequence state vectors Start target sequence size 1 (just start--sequence character) Feed state vectors 1-char target sequence decoder produce predictions next character Sample next character using predictions (simply use argmax). Append sampled character target sequence Repeat generate end--sequence character hit character limit.","code":""},{"path":"https://keras.posit.co/articles/examples/lstm_seq2seq.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Character-level recurrent sequence-to-sequence model","text":"","code":"import numpy as np import keras as keras import os from pathlib import Path"},{"path":"https://keras.posit.co/articles/examples/lstm_seq2seq.html","id":"download-the-data","dir":"Articles > Examples","previous_headings":"","what":"Download the data","title":"Character-level recurrent sequence-to-sequence model","text":"","code":"fpath = keras.utils.get_file(     origin=\"http://www.manythings.org/anki/fra-eng.zip\" ) dirpath = Path(fpath).parent.absolute() os.system(f\"unzip -q {fpath} -d {dirpath}\")"},{"path":"https://keras.posit.co/articles/examples/lstm_seq2seq.html","id":"configuration","dir":"Articles > Examples","previous_headings":"","what":"Configuration","title":"Character-level recurrent sequence-to-sequence model","text":"","code":"batch_size = 64  # Batch size for training. epochs = 100  # Number of epochs to train for. latent_dim = 256  # Latent dimensionality of the encoding space. num_samples = 10000  # Number of samples to train on. # Path to the data txt file on disk. data_path = os.path.join(dirpath, \"fra.txt\")"},{"path":"https://keras.posit.co/articles/examples/lstm_seq2seq.html","id":"prepare-the-data","dir":"Articles > Examples","previous_headings":"","what":"Prepare the data","title":"Character-level recurrent sequence-to-sequence model","text":"","code":"# Vectorize the data. input_texts = [] target_texts = [] input_characters = set() target_characters = set() with open(data_path, \"r\", encoding=\"utf-8\") as f:     lines = f.read().split(\"\\n\") for line in lines[: min(num_samples, len(lines) - 1)]:     input_text, target_text, _ = line.split(\"\\t\")     # We use \"tab\" as the \"start sequence\" character     # for the targets, and \"\\n\" as \"end sequence\" character.     target_text = \"\\t\" + target_text + \"\\n\"     input_texts.append(input_text)     target_texts.append(target_text)     for char in input_text:         if char not in input_characters:             input_characters.add(char)     for char in target_text:         if char not in target_characters:             target_characters.add(char)  input_characters = sorted(list(input_characters)) target_characters = sorted(list(target_characters)) num_encoder_tokens = len(input_characters) num_decoder_tokens = len(target_characters) max_encoder_seq_length = max([len(txt) for txt in input_texts]) max_decoder_seq_length = max([len(txt) for txt in target_texts])  print(\"Number of samples:\", len(input_texts)) print(\"Number of unique input tokens:\", num_encoder_tokens) print(\"Number of unique output tokens:\", num_decoder_tokens) print(\"Max sequence length for inputs:\", max_encoder_seq_length) print(\"Max sequence length for outputs:\", max_decoder_seq_length)  input_token_index = dict([(char, i) for i, char in enumerate(input_characters)]) target_token_index = dict(     [(char, i) for i, char in enumerate(target_characters)] )  encoder_input_data = np.zeros(     (len(input_texts), max_encoder_seq_length, num_encoder_tokens),     dtype=\"float32\", ) decoder_input_data = np.zeros(     (len(input_texts), max_decoder_seq_length, num_decoder_tokens),     dtype=\"float32\", ) decoder_target_data = np.zeros(     (len(input_texts), max_decoder_seq_length, num_decoder_tokens),     dtype=\"float32\", )  for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):     for t, char in enumerate(input_text):         encoder_input_data[i, t, input_token_index[char]] = 1.0     encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0     for t, char in enumerate(target_text):         # decoder_target_data is ahead of decoder_input_data by one timestep         decoder_input_data[i, t, target_token_index[char]] = 1.0         if t > 0:             # decoder_target_data will be ahead by one timestep             # and will not include the start character.             decoder_target_data[i, t - 1, target_token_index[char]] = 1.0     decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0     decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"},{"path":"https://keras.posit.co/articles/examples/lstm_seq2seq.html","id":"build-the-model","dir":"Articles > Examples","previous_headings":"","what":"Build the model","title":"Character-level recurrent sequence-to-sequence model","text":"","code":"# Define an input sequence and process it. encoder_inputs = keras.Input(shape=(None, num_encoder_tokens)) encoder = keras.layers.LSTM(latent_dim, return_state=True) encoder_outputs, state_h, state_c = encoder(encoder_inputs)  # We discard `encoder_outputs` and only keep the states. encoder_states = [state_h, state_c]  # Set up the decoder, using `encoder_states` as initial state. decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))  # We set up our decoder to return full output sequences, # and to return internal states as well. We don't use the # return states in the training model, but we will use them in inference. decoder_lstm = keras.layers.LSTM(     latent_dim, return_sequences=True, return_state=True ) decoder_outputs, _, _ = decoder_lstm(     decoder_inputs, initial_state=encoder_states ) decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\") decoder_outputs = decoder_dense(decoder_outputs)  # Define the model that will turn # `encoder_input_data` & `decoder_input_data` into `decoder_target_data` model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"},{"path":"https://keras.posit.co/articles/examples/lstm_seq2seq.html","id":"train-the-model","dir":"Articles > Examples","previous_headings":"","what":"Train the model","title":"Character-level recurrent sequence-to-sequence model","text":"","code":"model.compile(     optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"] ) model.fit(     [encoder_input_data, decoder_input_data],     decoder_target_data,     batch_size=batch_size,     epochs=epochs,     validation_split=0.2, ) # Save model model.save(\"s2s_model.keras\")"},{"path":"https://keras.posit.co/articles/examples/lstm_seq2seq.html","id":"run-inference-sampling","dir":"Articles > Examples","previous_headings":"","what":"Run inference (sampling)","title":"Character-level recurrent sequence-to-sequence model","text":"encode input retrieve initial decoder state run one step decoder initial state “start sequence” token target. Output next target token. Repeat current target token current states can now generate decoded sentences :","code":"# Define sampling models # Restore the model and construct the encoder and decoder. model = keras.models.load_model(\"s2s_model.keras\")  encoder_inputs = model.input[0]  # input_1 encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1 encoder_states = [state_h_enc, state_c_enc] encoder_model = keras.Model(encoder_inputs, encoder_states)  decoder_inputs = model.input[1]  # input_2 decoder_state_input_h = keras.Input(shape=(latent_dim,)) decoder_state_input_c = keras.Input(shape=(latent_dim,)) decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c] decoder_lstm = model.layers[3] decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(     decoder_inputs, initial_state=decoder_states_inputs ) decoder_states = [state_h_dec, state_c_dec] decoder_dense = model.layers[4] decoder_outputs = decoder_dense(decoder_outputs) decoder_model = keras.Model(     [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states )  # Reverse-lookup token index to decode sequences back to # something readable. reverse_input_char_index = dict(     (i, char) for char, i in input_token_index.items() ) reverse_target_char_index = dict(     (i, char) for char, i in target_token_index.items() )   def decode_sequence(input_seq):     # Encode the input as state vectors.     states_value = encoder_model.predict(input_seq, verbose=0)      # Generate empty target sequence of length 1.     target_seq = np.zeros((1, 1, num_decoder_tokens))     # Populate the first character of target sequence with the start character.     target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0      # Sampling loop for a batch of sequences     # (to simplify, here we assume a batch of size 1).     stop_condition = False     decoded_sentence = \"\"     while not stop_condition:         output_tokens, h, c = decoder_model.predict(             [target_seq] + states_value, verbose=0         )          # Sample a token         sampled_token_index = np.argmax(output_tokens[0, -1, :])         sampled_char = reverse_target_char_index[sampled_token_index]         decoded_sentence += sampled_char          # Exit condition: either hit max length         # or find stop character.         if (             sampled_char == \"\\n\"             or len(decoded_sentence) > max_decoder_seq_length         ):             stop_condition = True          # Update the target sequence (of length 1).         target_seq = np.zeros((1, 1, num_decoder_tokens))         target_seq[0, 0, sampled_token_index] = 1.0          # Update states         states_value = [h, c]     return decoded_sentence for seq_index in range(20):     # Take one sequence (part of the training set)     # for trying out decoding.     input_seq = encoder_input_data[seq_index : seq_index + 1]     decoded_sentence = decode_sequence(input_seq)     print(\"-\")     print(\"Input sentence:\", input_texts[seq_index])     print(\"Decoded sentence:\", decoded_sentence)"},{"path":"https://keras.posit.co/articles/examples/metric_learning.html","id":"overview","dir":"Articles > Examples","previous_headings":"","what":"Overview","title":"Metric learning for image similarity search","text":"Metric learning aims train models can embed inputs high-dimensional space “similar” inputs, defined training scheme, located close . models trained can produce embeddings downstream systems similarity useful; examples include ranking signal search form pretrained embedding model another supervised problem. detailed overview metric learning see: metric learning? “Using crossentropy metric learning” tutorial","code":""},{"path":"https://keras.posit.co/articles/examples/metric_learning.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Metric learning for image similarity search","text":"","code":"import random import matplotlib.pyplot as plt import numpy as np import tensorflow as tf from collections import defaultdict from PIL import Image from sklearn.metrics import ConfusionMatrixDisplay import keras as keras from keras import layers from keras.datasets import cifar10"},{"path":"https://keras.posit.co/articles/examples/metric_learning.html","id":"dataset","dir":"Articles > Examples","previous_headings":"","what":"Dataset","title":"Metric learning for image similarity search","text":"example using CIFAR-10 dataset. get sense dataset can visualise grid 25 random examples. Metric learning provides training data explicit (X, y) pairs instead uses multiple instances related way want express similarity. example use instances class represent similarity; single training instance one image, pair images class. referring images pair ’ll use common metric learning names anchor (randomly chosen image) positive (another randomly chosen image class). facilitate need build form lookup maps classes instances class. generating data training sample lookup. example using simplest approach training; batch consist (anchor, positive) pairs spread across classes. goal learning move anchor positive pairs closer together away instances batch. case batch size dictated number classes; CIFAR-10 10. can visualise batch another collage. top row shows randomly chosen anchors 10 classes, bottom row shows corresponding 10 positives.","code":"(x_train, y_train), (x_test, y_test) = cifar10.load_data()  x_train = x_train.astype(\"float32\") / 255.0 y_train = np.squeeze(y_train) x_test = x_test.astype(\"float32\") / 255.0 y_test = np.squeeze(y_test) height_width = 32   def show_collage(examples):     box_size = height_width + 2     num_rows, num_cols = examples.shape[:2]      collage = Image.new(         mode=\"RGB\",         size=(num_cols * box_size, num_rows * box_size),         color=(250, 250, 250),     )     for row_idx in range(num_rows):         for col_idx in range(num_cols):             array = (np.array(examples[row_idx, col_idx]) * 255).astype(                 np.uint8             )             collage.paste(                 Image.fromarray(array), (col_idx * box_size, row_idx * box_size)             )      # Double size for visualisation.     collage = collage.resize((2 * num_cols * box_size, 2 * num_rows * box_size))     return collage   # Show a collage of 5x5 random images. sample_idxs = np.random.randint(0, 50000, size=(5, 5)) examples = x_train[sample_idxs] show_collage(examples) class_idx_to_train_idxs = defaultdict(list) for y_train_idx, y in enumerate(y_train):     class_idx_to_train_idxs[y].append(y_train_idx)  class_idx_to_test_idxs = defaultdict(list) for y_test_idx, y in enumerate(y_test):     class_idx_to_test_idxs[y].append(y_test_idx) num_classes = 10   class AnchorPositivePairs(keras.utils.PyDataset):     def __init__(self, num_batchs):         super().__init__()         self.num_batchs = num_batchs      def __len__(self):         return self.num_batchs      def __getitem__(self, _idx):         x = np.empty(             (2, num_classes, height_width, height_width, 3), dtype=np.float32         )         for class_idx in range(num_classes):             examples_for_class = class_idx_to_train_idxs[class_idx]             anchor_idx = random.choice(examples_for_class)             positive_idx = random.choice(examples_for_class)             while positive_idx == anchor_idx:                 positive_idx = random.choice(examples_for_class)             x[0, class_idx] = x_train[anchor_idx]             x[1, class_idx] = x_train[positive_idx]         return x examples = next(iter(AnchorPositivePairs(num_batchs=1)))  show_collage(examples)"},{"path":"https://keras.posit.co/articles/examples/metric_learning.html","id":"embedding-model","dir":"Articles > Examples","previous_headings":"","what":"Embedding model","title":"Metric learning for image similarity search","text":"define custom model train_step first embeds anchors positives uses pairwise dot products logits softmax. Next describe architecture maps image embedding. model simply consists sequence 2d convolutions followed global pooling final linear projection embedding space. common metric learning normalise embeddings can use simple dot products measure similarity. simplicity model intentionally small. Finally run training. Google Colab GPU instance takes minute.","code":"class EmbeddingModel(keras.Model):     def train_step(self, data):         # Note: Workaround for open issue, to be removed.         if isinstance(data, tuple):             data = data[0]         anchors, positives = data[0], data[1]          with tf.GradientTape() as tape:             # Run both anchors and positives through model.             anchor_embeddings = self(anchors, training=True)             positive_embeddings = self(positives, training=True)              # Calculate cosine similarity between anchors and positives. As they have             # been normalised this is just the pair wise dot products.             similarities = tf.einsum(                 \"ae,pe->ap\", anchor_embeddings, positive_embeddings             )              # Since we intend to use these as logits we scale them by a temperature.             # This value would normally be chosen as a hyper parameter.             temperature = 0.2             similarities /= temperature              # We use these similarities as logits for a softmax. The labels for             # this call are just the sequence [0, 1, 2, ..., num_classes] since we             # want the main diagonal values, which correspond to the anchor/positive             # pairs, to be high. This loss will move embeddings for the             # anchor/positive pairs together and move all other pairs apart.             sparse_labels = tf.range(num_classes)             loss = self.compute_loss(y=sparse_labels, y_pred=similarities)          # Calculate gradients and apply via optimizer.         gradients = tape.gradient(loss, self.trainable_variables)         self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))          # Update and return metrics (specifically the one for the loss value).         for metric in self.metrics:             metric.update_state(sparse_labels, similarities)         return {m.name: m.result() for m in self.metrics} inputs = layers.Input(shape=(height_width, height_width, 3)) x = layers.Conv2D(filters=32, kernel_size=3, strides=2, activation=\"relu\")(     inputs ) x = layers.Conv2D(filters=64, kernel_size=3, strides=2, activation=\"relu\")(x) x = layers.Conv2D(filters=128, kernel_size=3, strides=2, activation=\"relu\")(x) x = layers.GlobalAveragePooling2D()(x) embeddings = layers.Dense(units=8, activation=None)(x) embeddings = keras.layers.UnitNormalization()(embeddings)  model = EmbeddingModel(inputs, embeddings) model.compile(     optimizer=keras.optimizers.Adam(learning_rate=1e-3),     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), )  history = model.fit(AnchorPositivePairs(num_batchs=1000), epochs=20)  plt.plot(history.history[\"loss\"]) plt.show()"},{"path":"https://keras.posit.co/articles/examples/metric_learning.html","id":"testing","dir":"Articles > Examples","previous_headings":"","what":"Testing","title":"Metric learning for image similarity search","text":"can review quality model applying test set considering near neighbours embedding space. First embed test set calculate near neighbours. Recall since embeddings unit length can calculate cosine similarity via dot products. visual check embeddings can build collage near neighbours 5 random examples. first column image randomly selected image, following 10 columns show nearest neighbours order similarity. can also get quantified view performance considering correctness near neighbours terms confusion matrix. Let us sample 10 examples 10 classes consider near neighbours form prediction; , example near neighbours share class? observe animal class generally well, confused animal classes. vehicle classes follow pattern.","code":"near_neighbours_per_example = 10  embeddings = model.predict(x_test) gram_matrix = np.einsum(\"ae,be->ab\", embeddings, embeddings) near_neighbours = np.argsort(gram_matrix.T)[     :, -(near_neighbours_per_example + 1) : ] num_collage_examples = 5  examples = np.empty(     (         num_collage_examples,         near_neighbours_per_example + 1,         height_width,         height_width,         3,     ),     dtype=np.float32, ) for row_idx in range(num_collage_examples):     examples[row_idx, 0] = x_test[row_idx]     anchor_near_neighbours = reversed(near_neighbours[row_idx][:-1])     for col_idx, nn_idx in enumerate(anchor_near_neighbours):         examples[row_idx, col_idx + 1] = x_test[nn_idx]  show_collage(examples) confusion_matrix = np.zeros((num_classes, num_classes))  # For each class. for class_idx in range(num_classes):     # Consider 10 examples.     example_idxs = class_idx_to_test_idxs[class_idx][:10]     for y_test_idx in example_idxs:         # And count the classes of its near neighbours.         for nn_idx in near_neighbours[y_test_idx][:-1]:             nn_class_idx = y_test[nn_idx]             confusion_matrix[class_idx, nn_class_idx] += 1  # Display a confusion matrix. labels = [     \"Airplane\",     \"Automobile\",     \"Bird\",     \"Cat\",     \"Deer\",     \"Dog\",     \"Frog\",     \"Horse\",     \"Ship\",     \"Truck\", ] disp = ConfusionMatrixDisplay(     confusion_matrix=confusion_matrix, display_labels=labels ) disp.plot(     include_values=True, cmap=\"viridis\", ax=None, xticks_rotation=\"vertical\" ) plt.show()"},{"path":"https://keras.posit.co/articles/examples/mirnet.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Low-light image enhancement using MIRNet","text":"goal recovering high-quality image content degraded version, image restoration enjoys numerous applications, photography, security, medical imaging, remote sensing. example, implement MIRNet model low-light image enhancement, fully-convolutional architecture learns enriched set features combines contextual information multiple scales, simultaneously preserving high-resolution spatial details.","code":""},{"path":"https://keras.posit.co/articles/examples/mirnet.html","id":"references","dir":"Articles > Examples","previous_headings":"Introduction","what":"References:","title":"Low-light image enhancement using MIRNet","text":"Learning Enriched Features Real Image Restoration Enhancement Retinex Theory Color Vision Two deterministic half-quadratic regularization algorithms computed imaging","code":""},{"path":"https://keras.posit.co/articles/examples/mirnet.html","id":"downloading-loldataset","dir":"Articles > Examples","previous_headings":"","what":"Downloading LOLDataset","title":"Low-light image enhancement using MIRNet","text":"LoL Dataset created low-light image enhancement. provides 485 images training 15 testing. image pair dataset consists low-light input image corresponding well-exposed reference image. pip install -q git+https://github.com/keras-team/keras wget https://huggingface.co/datasets/geekyrakshit/LoL-Dataset/resolve/main/lol_dataset.zip unzip -q lol_dataset.zip && rm lol_dataset.zip","code":"import os  os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  import random import numpy as np from glob import glob from PIL import Image, ImageOps import matplotlib.pyplot as plt  import keras as keras from keras import layers  import tensorflow as tf"},{"path":"https://keras.posit.co/articles/examples/mirnet.html","id":"creating-a-tensorflow-dataset","dir":"Articles > Examples","previous_headings":"","what":"Creating a TensorFlow Dataset","title":"Low-light image enhancement using MIRNet","text":"use 300 image pairs LoL Dataset’s training set training, use remaining 185 image pairs validation. generate random crops size 128 x 128 image pairs used training validation.","code":"random.seed(10)  IMAGE_SIZE = 128 BATCH_SIZE = 4 MAX_TRAIN_IMAGES = 300   def read_image(image_path):     image = tf.io.read_file(image_path)     image = tf.image.decode_png(image, channels=3)     image.set_shape([None, None, 3])     image = tf.cast(image, dtype=tf.float32) / 255.0     return image   def random_crop(low_image, enhanced_image):     low_image_shape = tf.shape(low_image)[:2]     low_w = tf.random.uniform(         shape=(), maxval=low_image_shape[1] - IMAGE_SIZE + 1, dtype=tf.int32     )     low_h = tf.random.uniform(         shape=(), maxval=low_image_shape[0] - IMAGE_SIZE + 1, dtype=tf.int32     )     low_image_cropped = low_image[         low_h : low_h + IMAGE_SIZE, low_w : low_w + IMAGE_SIZE     ]     enhanced_image_cropped = enhanced_image[         low_h : low_h + IMAGE_SIZE, low_w : low_w + IMAGE_SIZE     ]     # in order to avoid `NONE` during shape inference     low_image_cropped.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])     enhanced_image_cropped.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])     return low_image_cropped, enhanced_image_cropped   def load_data(low_light_image_path, enhanced_image_path):     low_light_image = read_image(low_light_image_path)     enhanced_image = read_image(enhanced_image_path)     low_light_image, enhanced_image = random_crop(         low_light_image, enhanced_image     )     return low_light_image, enhanced_image   def get_dataset(low_light_images, enhanced_images):     dataset = tf.data.Dataset.from_tensor_slices(         (low_light_images, enhanced_images)     )     dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)     dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)     return dataset   train_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[     :MAX_TRAIN_IMAGES ] train_enhanced_images = sorted(glob(\"./lol_dataset/our485/high/*\"))[     :MAX_TRAIN_IMAGES ]  val_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[     MAX_TRAIN_IMAGES: ] val_enhanced_images = sorted(glob(\"./lol_dataset/our485/high/*\"))[     MAX_TRAIN_IMAGES: ]  test_low_light_images = sorted(glob(\"./lol_dataset/eval15/low/*\")) test_enhanced_images = sorted(glob(\"./lol_dataset/eval15/high/*\"))   train_dataset = get_dataset(train_low_light_images, train_enhanced_images) val_dataset = get_dataset(val_low_light_images, val_enhanced_images)   print(\"Train Dataset:\", train_dataset.element_spec) print(\"Val Dataset:\", val_dataset.element_spec)"},{"path":"https://keras.posit.co/articles/examples/mirnet.html","id":"mirnet-model","dir":"Articles > Examples","previous_headings":"","what":"MIRNet Model","title":"Low-light image enhancement using MIRNet","text":"main features MIRNet model: feature extraction model computes complementary set features across multiple spatial scales, maintaining original high-resolution features preserve precise spatial details. regularly repeated mechanism information exchange, features across multi-resolution branches progressively fused together improved representation learning. new approach fuse multi-scale features using selective kernel network dynamically combines variable receptive fields faithfully preserves original feature information spatial resolution. recursive residual design progressively breaks input signal order simplify overall learning process, allows construction deep networks.","code":""},{"path":"https://keras.posit.co/articles/examples/mirnet.html","id":"selective-kernel-feature-fusion","dir":"Articles > Examples","previous_headings":"MIRNet Model","what":"Selective Kernel Feature Fusion","title":"Low-light image enhancement using MIRNet","text":"Selective Kernel Feature Fusion SKFF module performs dynamic adjustment receptive fields via two operations: Fuse Select. Fuse operator generates global feature descriptors combining information multi-resolution streams. Select operator uses descriptors recalibrate feature maps (different streams) followed aggregation. Fuse: SKFF receives inputs three parallel convolution streams carrying different scales information. first combine multi-scale features using element-wise sum, apply Global Average Pooling (GAP) across spatial dimension. Next, apply channel- downscaling convolution layer generate compact feature representation passes three parallel channel-upscaling convolution layers (one resolution stream) provides us three feature descriptors. Select: operator applies softmax function feature descriptors obtain corresponding activations used adaptively recalibrate multi-scale feature maps. aggregated features defined sum product corresponding multi-scale feature feature descriptor.","code":"def selective_kernel_feature_fusion(     multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3 ):     channels = list(multi_scale_feature_1.shape)[-1]     combined_feature = layers.Add()(         [multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3]     )     gap = layers.GlobalAveragePooling2D()(combined_feature)     channel_wise_statistics = layers.Reshape((1, 1, channels))(gap)     compact_feature_representation = layers.Conv2D(         filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"     )(channel_wise_statistics)     feature_descriptor_1 = layers.Conv2D(         channels, kernel_size=(1, 1), activation=\"softmax\"     )(compact_feature_representation)     feature_descriptor_2 = layers.Conv2D(         channels, kernel_size=(1, 1), activation=\"softmax\"     )(compact_feature_representation)     feature_descriptor_3 = layers.Conv2D(         channels, kernel_size=(1, 1), activation=\"softmax\"     )(compact_feature_representation)     feature_1 = multi_scale_feature_1 * feature_descriptor_1     feature_2 = multi_scale_feature_2 * feature_descriptor_2     feature_3 = multi_scale_feature_3 * feature_descriptor_3     aggregated_feature = layers.Add()([feature_1, feature_2, feature_3])     return aggregated_feature"},{"path":"https://keras.posit.co/articles/examples/mirnet.html","id":"dual-attention-unit","dir":"Articles > Examples","previous_headings":"MIRNet Model","what":"Dual Attention Unit","title":"Low-light image enhancement using MIRNet","text":"Dual Attention Unit DAU used extract features convolutional streams. SKFF block fuses information across multi-resolution branches, also need mechanism share information within feature tensor, along spatial channel dimensions done DAU block. DAU suppresses less useful features allows informative ones pass . feature recalibration achieved using Channel Attention Spatial Attention mechanisms. Channel Attention branch exploits inter-channel relationships convolutional feature maps applying squeeze excitation operations. Given feature map, squeeze operation applies Global Average Pooling across spatial dimensions encode global context, thus yielding feature descriptor. excitation operator passes feature descriptor two convolutional layers followed sigmoid gating generates activations. Finally, output Channel Attention branch obtained rescaling input feature map output activations. Spatial Attention branch designed exploit inter-spatial dependencies convolutional features. goal Spatial Attention generate spatial attention map use recalibrate incoming features. generate spatial attention map, Spatial Attention branch first independently applies Global Average Pooling Max Pooling operations input features along channel dimensions concatenates outputs form resultant feature map passed convolution sigmoid activation obtain spatial attention map. spatial attention map used rescale input feature map.","code":"class ChannelPooling(layers.Layer):     def __init__(self, axis=-1, *args, **kwargs):         super().__init__(*args, **kwargs)         self.axis = axis         self.concat = layers.Concatenate(axis=self.axis)      def call(self, inputs):         average_pooling = tf.expand_dims(             tf.reduce_mean(inputs, axis=-1), axis=-1         )         max_pooling = tf.expand_dims(tf.reduce_max(inputs, axis=-1), axis=-1)         return self.concat([average_pooling, max_pooling])      def get_config(self):         config = super().get_config()         config.update({\"axis\": self.axis})   def spatial_attention_block(input_tensor):     compressed_feature_map = ChannelPooling(axis=-1)(input_tensor)     feature_map = layers.Conv2D(1, kernel_size=(1, 1))(compressed_feature_map)     feature_map = keras.activations.sigmoid(feature_map)     return input_tensor * feature_map   def channel_attention_block(input_tensor):     channels = list(input_tensor.shape)[-1]     average_pooling = layers.GlobalAveragePooling2D()(input_tensor)     feature_descriptor = layers.Reshape((1, 1, channels))(average_pooling)     feature_activations = layers.Conv2D(         filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"     )(feature_descriptor)     feature_activations = layers.Conv2D(         filters=channels, kernel_size=(1, 1), activation=\"sigmoid\"     )(feature_activations)     return input_tensor * feature_activations   def dual_attention_unit_block(input_tensor):     channels = list(input_tensor.shape)[-1]     feature_map = layers.Conv2D(         channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"     )(input_tensor)     feature_map = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(         feature_map     )     channel_attention = channel_attention_block(feature_map)     spatial_attention = spatial_attention_block(feature_map)     concatenation = layers.Concatenate(axis=-1)(         [channel_attention, spatial_attention]     )     concatenation = layers.Conv2D(channels, kernel_size=(1, 1))(concatenation)     return layers.Add()([input_tensor, concatenation])"},{"path":"https://keras.posit.co/articles/examples/mirnet.html","id":"multi-scale-residual-block","dir":"Articles > Examples","previous_headings":"MIRNet Model","what":"Multi-Scale Residual Block","title":"Low-light image enhancement using MIRNet","text":"Multi-Scale Residual Block capable generating spatially-precise output maintaining high-resolution representations, receiving rich contextual information low-resolutions. MRB consists multiple (three paper) fully-convolutional streams connected parallel. allows information exchange across parallel streams order consolidate high-resolution features help low-resolution features, vice versa. MIRNet employs recursive residual design (skip connections) ease flow information learning process. order maintain residual nature architecture, residual resizing modules used perform downsampling upsampling operations used Multi-scale Residual Block.","code":"# Recursive Residual Modules   def down_sampling_module(input_tensor):     channels = list(input_tensor.shape)[-1]     main_branch = layers.Conv2D(         channels, kernel_size=(1, 1), activation=\"relu\"     )(input_tensor)     main_branch = layers.Conv2D(         channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"     )(main_branch)     main_branch = layers.MaxPooling2D()(main_branch)     main_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(main_branch)     skip_branch = layers.MaxPooling2D()(input_tensor)     skip_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(skip_branch)     return layers.Add()([skip_branch, main_branch])   def up_sampling_module(input_tensor):     channels = list(input_tensor.shape)[-1]     main_branch = layers.Conv2D(         channels, kernel_size=(1, 1), activation=\"relu\"     )(input_tensor)     main_branch = layers.Conv2D(         channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"     )(main_branch)     main_branch = layers.UpSampling2D()(main_branch)     main_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(main_branch)     skip_branch = layers.UpSampling2D()(input_tensor)     skip_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(skip_branch)     return layers.Add()([skip_branch, main_branch])   # MRB Block def multi_scale_residual_block(input_tensor, channels):     # features     level1 = input_tensor     level2 = down_sampling_module(input_tensor)     level3 = down_sampling_module(level2)     # DAU     level1_dau = dual_attention_unit_block(level1)     level2_dau = dual_attention_unit_block(level2)     level3_dau = dual_attention_unit_block(level3)     # SKFF     level1_skff = selective_kernel_feature_fusion(         level1_dau,         up_sampling_module(level2_dau),         up_sampling_module(up_sampling_module(level3_dau)),     )     level2_skff = selective_kernel_feature_fusion(         down_sampling_module(level1_dau),         level2_dau,         up_sampling_module(level3_dau),     )     level3_skff = selective_kernel_feature_fusion(         down_sampling_module(down_sampling_module(level1_dau)),         down_sampling_module(level2_dau),         level3_dau,     )     # DAU 2     level1_dau_2 = dual_attention_unit_block(level1_skff)     level2_dau_2 = up_sampling_module((dual_attention_unit_block(level2_skff)))     level3_dau_2 = up_sampling_module(         up_sampling_module(dual_attention_unit_block(level3_skff))     )     # SKFF 2     skff_ = selective_kernel_feature_fusion(         level1_dau_2, level2_dau_2, level3_dau_2     )     conv = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(skff_)     return layers.Add()([input_tensor, conv])"},{"path":"https://keras.posit.co/articles/examples/mirnet.html","id":"mirnet-model-1","dir":"Articles > Examples","previous_headings":"MIRNet Model","what":"MIRNet Model","title":"Low-light image enhancement using MIRNet","text":"","code":"def recursive_residual_group(input_tensor, num_mrb, channels):     conv1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(         input_tensor     )     for _ in range(num_mrb):         conv1 = multi_scale_residual_block(conv1, channels)     conv2 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(conv1)     return layers.Add()([conv2, input_tensor])   def mirnet_model(num_rrg, num_mrb, channels):     input_tensor = keras.Input(shape=[None, None, 3])     x1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(         input_tensor     )     for _ in range(num_rrg):         x1 = recursive_residual_group(x1, num_mrb, channels)     conv = layers.Conv2D(3, kernel_size=(3, 3), padding=\"same\")(x1)     output_tensor = layers.Add()([input_tensor, conv])     return keras.Model(input_tensor, output_tensor)   model = mirnet_model(num_rrg=3, num_mrb=2, channels=64)"},{"path":"https://keras.posit.co/articles/examples/mirnet.html","id":"training","dir":"Articles > Examples","previous_headings":"","what":"Training","title":"Low-light image enhancement using MIRNet","text":"train MIRNet using Charbonnier Loss loss function Adam Optimizer learning rate 1e-4. use Peak Signal Noise Ratio PSNR metric expression ratio maximum possible value (power) signal power distorting noise affects quality representation.","code":"def charbonnier_loss(y_true, y_pred):     return tf.reduce_mean(tf.sqrt(tf.square(y_true - y_pred) + tf.square(1e-3)))   def peak_signal_noise_ratio(y_true, y_pred):     return tf.image.psnr(y_pred, y_true, max_val=255.0)   optimizer = keras.optimizers.Adam(learning_rate=1e-4) model.compile(     optimizer=optimizer,     loss=charbonnier_loss,     metrics=[peak_signal_noise_ratio], )  history = model.fit(     train_dataset,     validation_data=val_dataset,     epochs=50,     callbacks=[         keras.callbacks.ReduceLROnPlateau(             monitor=\"val_peak_signal_noise_ratio\",             factor=0.5,             patience=5,             verbose=1,             min_delta=1e-7,             mode=\"max\",         )     ], )   def plot_history(value):     plt.plot(history.history[value], label=f\"train_{value}\")     plt.plot(history.history[f\"val_{value}\"], label=f\"val_{value}\")     plt.xlabel(\"Epochs\")     plt.ylabel(value)     plt.title(f\"Train and Validation {value} Over Epochs\", fontsize=14)     plt.legend()     plt.grid()     plt.show()   plot_history(\"loss\") plot_history(\"peak_signal_noise_ratio\")"},{"path":"https://keras.posit.co/articles/examples/mirnet.html","id":"inference","dir":"Articles > Examples","previous_headings":"","what":"Inference","title":"Low-light image enhancement using MIRNet","text":"","code":"def plot_results(images, titles, figure_size=(12, 12)):     fig = plt.figure(figsize=figure_size)     for i in range(len(images)):         fig.add_subplot(1, len(images), i + 1).set_title(titles[i])         _ = plt.imshow(images[i])         plt.axis(\"off\")     plt.show()   def infer(original_image):     image = keras.utils.img_to_array(original_image)     image = image.astype(\"float32\") / 255.0     image = np.expand_dims(image, axis=0)     output = model.predict(image, verbose=0)     output_image = output[0] * 255.0     output_image = output_image.clip(0, 255)     output_image = output_image.reshape(         (np.shape(output_image)[0], np.shape(output_image)[1], 3)     )     output_image = Image.fromarray(np.uint8(output_image))     original_image = Image.fromarray(np.uint8(original_image))     return output_image"},{"path":"https://keras.posit.co/articles/examples/mirnet.html","id":"inference-on-test-images","dir":"Articles > Examples","previous_headings":"Inference","what":"Inference on Test Images","title":"Low-light image enhancement using MIRNet","text":"compare test images LOLDataset enhanced MIRNet images enhanced via PIL.ImageOps.autocontrast() function. can use trained model hosted Hugging Face Hub try demo Hugging Face Spaces.","code":"for low_light_image in random.sample(test_low_light_images, 6):     original_image = Image.open(low_light_image)     enhanced_image = infer(original_image)     plot_results(         [original_image, ImageOps.autocontrast(original_image), enhanced_image],         [\"Original\", \"PIL Autocontrast\", \"MIRNet Enhanced\"],         (20, 12),     )"},{"path":"https://keras.posit.co/articles/examples/mixup.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"MixUp augmentation for image classification","text":"mixup domain-agnostic data augmentation technique proposed mixup: Beyond Empirical Risk Minimization Zhang et al. ’s implemented following formulas:  (Note lambda values values [0, 1] range sampled Beta distribution.) technique quite systematically named. literally mixing features corresponding labels. Implementation-wise ’s simple. Neural networks prone memorizing corrupt labels. mixup relaxes combining different features one another (happens labels ) network get overconfident relationship features labels. mixup specifically useful sure selecting set augmentation transforms given dataset, medical imaging datasets, example. mixup can extended variety data modalities computer vision, naturallanguage processing, speech, .","code":""},{"path":"https://keras.posit.co/articles/examples/mixup.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"MixUp augmentation for image classification","text":"","code":"import numpy as np import keras as keras import matplotlib.pyplot as plt  from keras import layers  # TF imports related to tf.data preprocessing from tensorflow import data as tf_data from tensorflow import image as tf_image from tensorflow.random import gamma as tf_random_gamma"},{"path":"https://keras.posit.co/articles/examples/mixup.html","id":"prepare-the-dataset","dir":"Articles > Examples","previous_headings":"","what":"Prepare the dataset","title":"MixUp augmentation for image classification","text":"example, using FashionMNIST dataset. recipe can used classification datasets well.","code":"(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()  x_train = x_train.astype(\"float32\") / 255.0 x_train = np.reshape(x_train, (-1, 28, 28, 1)) y_train = keras.ops.one_hot(y_train, 10)  x_test = x_test.astype(\"float32\") / 255.0 x_test = np.reshape(x_test, (-1, 28, 28, 1)) y_test = keras.ops.one_hot(y_test, 10)"},{"path":"https://keras.posit.co/articles/examples/mixup.html","id":"define-hyperparameters","dir":"Articles > Examples","previous_headings":"","what":"Define hyperparameters","title":"MixUp augmentation for image classification","text":"","code":"AUTO = tf_data.AUTOTUNE BATCH_SIZE = 64 EPOCHS = 10"},{"path":"https://keras.posit.co/articles/examples/mixup.html","id":"convert-the-data-into-tensorflow-dataset-objects","dir":"Articles > Examples","previous_headings":"","what":"Convert the data into TensorFlow Dataset objects","title":"MixUp augmentation for image classification","text":"","code":"# Put aside a few samples to create our validation set val_samples = 2000 x_val, y_val = x_train[:val_samples], y_train[:val_samples] new_x_train, new_y_train = x_train[val_samples:], y_train[val_samples:]  train_ds_one = (     tf_data.Dataset.from_tensor_slices((new_x_train, new_y_train))     .shuffle(BATCH_SIZE * 100)     .batch(BATCH_SIZE) ) train_ds_two = (     tf_data.Dataset.from_tensor_slices((new_x_train, new_y_train))     .shuffle(BATCH_SIZE * 100)     .batch(BATCH_SIZE) ) # Because we will be mixing up the images and their corresponding labels, we will be # combining two shuffled datasets from the same training data. train_ds = tf_data.Dataset.zip((train_ds_one, train_ds_two))  val_ds = tf_data.Dataset.from_tensor_slices((x_val, y_val)).batch(BATCH_SIZE)  test_ds = tf_data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)"},{"path":"https://keras.posit.co/articles/examples/mixup.html","id":"define-the-mixup-technique-function","dir":"Articles > Examples","previous_headings":"","what":"Define the mixup technique function","title":"MixUp augmentation for image classification","text":"perform mixup routine, create new virtual datasets using training data dataset, apply lambda value within [0, 1] range sampled Beta distribution — , example, new_x = lambda * x1 + (1 - lambda) * x2 (x1 x2 images) equation applied labels well. Note , combining two images create single one. Theoretically, can combine many want comes increased computation cost. certain cases, may help improve performance well.","code":"def sample_beta_distribution(size, concentration_0=0.2, concentration_1=0.2):     gamma_1_sample = tf_random_gamma(shape=[size], alpha=concentration_1)     gamma_2_sample = tf_random_gamma(shape=[size], alpha=concentration_0)     return gamma_1_sample / (gamma_1_sample + gamma_2_sample)   def mix_up(ds_one, ds_two, alpha=0.2):     # Unpack two datasets     images_one, labels_one = ds_one     images_two, labels_two = ds_two     batch_size = keras.backend.shape(images_one)[0]      # Sample lambda and reshape it to do the mixup     l = sample_beta_distribution(batch_size, alpha, alpha)     x_l = keras.ops.reshape(l, (batch_size, 1, 1, 1))     y_l = keras.ops.reshape(l, (batch_size, 1))      # Perform mixup on both images and labels by combining a pair of images/labels     # (one from each dataset) into one image/label     images = images_one * x_l + images_two * (1 - x_l)     labels = labels_one * y_l + labels_two * (1 - y_l)     return (images, labels)"},{"path":"https://keras.posit.co/articles/examples/mixup.html","id":"visualize-the-new-augmented-dataset","dir":"Articles > Examples","previous_headings":"","what":"Visualize the new augmented dataset","title":"MixUp augmentation for image classification","text":"","code":"# First create the new dataset using our `mix_up` utility train_ds_mu = train_ds.map(     lambda ds_one, ds_two: mix_up(ds_one, ds_two, alpha=0.2),     num_parallel_calls=AUTO, )  # Let's preview 9 samples from the dataset sample_images, sample_labels = next(iter(train_ds_mu)) plt.figure(figsize=(10, 10)) for i, (image, label) in enumerate(zip(sample_images[:9], sample_labels[:9])):     ax = plt.subplot(3, 3, i + 1)     plt.imshow(image.numpy().squeeze())     print(label.numpy().tolist())     plt.axis(\"off\")"},{"path":"https://keras.posit.co/articles/examples/mixup.html","id":"model-building","dir":"Articles > Examples","previous_headings":"","what":"Model building","title":"MixUp augmentation for image classification","text":"sake reproducibility, serialize initial random weights shallow network.","code":"def get_training_model():     model = keras.Sequential(         [             layers.Conv2D(                 16, (5, 5), activation=\"relu\", input_shape=(28, 28, 1)             ),             layers.MaxPooling2D(pool_size=(2, 2)),             layers.Conv2D(32, (5, 5), activation=\"relu\"),             layers.MaxPooling2D(pool_size=(2, 2)),             layers.Dropout(0.2),             layers.GlobalAveragePooling2D(),             layers.Dense(128, activation=\"relu\"),             layers.Dense(10, activation=\"softmax\"),         ]     )     return model initial_model = get_training_model() initial_model.save_weights(\"initial_weights.weights.h5\")"},{"path":"https://keras.posit.co/articles/examples/mixup.html","id":"train-the-model-with-the-mixed-up-dataset","dir":"Articles > Examples","previous_headings":"","what":"1. Train the model with the mixed up dataset","title":"MixUp augmentation for image classification","text":"","code":"model = get_training_model() model.load_weights(\"initial_weights.weights.h5\") model.compile(     loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"] ) model.fit(train_ds_mu, validation_data=val_ds, epochs=EPOCHS) _, test_acc = model.evaluate(test_ds) print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"},{"path":"https://keras.posit.co/articles/examples/mixup.html","id":"train-the-model-without-the-mixed-up-dataset","dir":"Articles > Examples","previous_headings":"","what":"2. Train the model without the mixed up dataset","title":"MixUp augmentation for image classification","text":"Readers encouraged try mixup different datasets different domains experiment lambda parameter. strongly advised check original paper well - authors present several ablation studies mixup showing can improve generalization, well show results combining two images create single one.","code":"model = get_training_model() model.load_weights(\"initial_weights.weights.h5\") model.compile(     loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"] ) # Notice that we are NOT using the mixed up dataset here model.fit(train_ds_one, validation_data=val_ds, epochs=EPOCHS) _, test_acc = model.evaluate(test_ds) print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"},{"path":"https://keras.posit.co/articles/examples/mixup.html","id":"notes","dir":"Articles > Examples","previous_headings":"","what":"Notes","title":"MixUp augmentation for image classification","text":"mixup, can create synthetic examples — especially lack large dataset - without incurring high computational costs. Label smoothing mixup usually work well together label smoothing already modifies hard labels factor. mixup work well using Supervised Contrastive Learning (SCL) since SCL expects true labels pre-training phase. benefits mixup include (described paper) robustness adversarial examples stabilized GAN (Generative Adversarial Networks) training. number data augmentation techniques extend mixup CutMix AugMix.","code":""},{"path":"https://keras.posit.co/articles/examples/mlp_image_classification.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Image classification with modern MLP models","text":"example implements three modern attention-free, multi-layer perceptron (MLP) based models image classification, demonstrated CIFAR-100 dataset: MLP-Mixer model, Ilya Tolstikhin et al., based two types MLPs. FNet model, James Lee-Thorp et al., based unparameterized Fourier Transform. gMLP model, Hanxiao Liu et al., based MLP gating. purpose example compare models, might perform differently different datasets well-tuned hyperparameters. Rather, show simple implementations main building blocks.","code":""},{"path":"https://keras.posit.co/articles/examples/mlp_image_classification.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Image classification with modern MLP models","text":"","code":"import numpy as np import keras as keras from keras import layers"},{"path":"https://keras.posit.co/articles/examples/mlp_image_classification.html","id":"prepare-the-data","dir":"Articles > Examples","previous_headings":"","what":"Prepare the data","title":"Image classification with modern MLP models","text":"","code":"num_classes = 100 input_shape = (32, 32, 3)  (x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()  print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\") print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"},{"path":"https://keras.posit.co/articles/examples/mlp_image_classification.html","id":"configure-the-hyperparameters","dir":"Articles > Examples","previous_headings":"","what":"Configure the hyperparameters","title":"Image classification with modern MLP models","text":"","code":"weight_decay = 0.0001 batch_size = 128 num_epochs = 50 dropout_rate = 0.2 image_size = 64  # We'll resize input images to this size. patch_size = 8  # Size of the patches to be extracted from the input images. num_patches = (image_size // patch_size) ** 2  # Size of the data array. embedding_dim = 256  # Number of hidden units. num_blocks = 4  # Number of blocks.  print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\") print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \") print(f\"Patches per image: {num_patches}\") print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")"},{"path":"https://keras.posit.co/articles/examples/mlp_image_classification.html","id":"build-a-classification-model","dir":"Articles > Examples","previous_headings":"","what":"Build a classification model","title":"Image classification with modern MLP models","text":"implement method builds classifier given processing blocks.","code":"def build_classifier(blocks, positional_encoding=False):     inputs = layers.Input(shape=input_shape)     # Augment data.     augmented = data_augmentation(inputs)     # Create patches.     patches = Patches(patch_size)(augmented)     # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.     x = layers.Dense(units=embedding_dim)(patches)     if positional_encoding:         x = x + PositionEmbedding(sequence_length=num_patches)(x)     # Process x using the module blocks.     x = blocks(x)     # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor.     representation = layers.GlobalAveragePooling1D()(x)     # Apply dropout.     representation = layers.Dropout(rate=dropout_rate)(representation)     # Compute logits outputs.     logits = layers.Dense(num_classes)(representation)     # Create the Keras model.     return keras.Model(inputs=inputs, outputs=logits)"},{"path":"https://keras.posit.co/articles/examples/mlp_image_classification.html","id":"define-an-experiment","dir":"Articles > Examples","previous_headings":"","what":"Define an experiment","title":"Image classification with modern MLP models","text":"implement utility function compile, train, evaluate given model.","code":"def run_experiment(model):     # Create Adam optimizer with weight decay.     optimizer = keras.optimizers.AdamW(         learning_rate=learning_rate,         weight_decay=weight_decay,     )     # Compile the model.     model.compile(         optimizer=optimizer,         loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),         metrics=[             keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),             keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),         ],     )     # Create a learning rate scheduler callback.     reduce_lr = keras.callbacks.ReduceLROnPlateau(         monitor=\"val_loss\", factor=0.5, patience=5     )     # Create an early stopping callback.     early_stopping = keras.callbacks.EarlyStopping(         monitor=\"val_loss\", patience=10, restore_best_weights=True     )     # Fit the model.     history = model.fit(         x=x_train,         y=y_train,         batch_size=batch_size,         epochs=num_epochs,         validation_split=0.1,         callbacks=[early_stopping, reduce_lr],     )      _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)     print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")     print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")      # Return history to plot learning curves.     return history"},{"path":"https://keras.posit.co/articles/examples/mlp_image_classification.html","id":"use-data-augmentation","dir":"Articles > Examples","previous_headings":"","what":"Use data augmentation","title":"Image classification with modern MLP models","text":"","code":"data_augmentation = keras.Sequential(     [         layers.Normalization(),         layers.Resizing(image_size, image_size),         layers.RandomFlip(\"horizontal\"),         layers.RandomZoom(height_factor=0.2, width_factor=0.2),     ],     name=\"data_augmentation\", ) # Compute the mean and the variance of the training data for normalization. data_augmentation.layers[0].adapt(x_train)"},{"path":"https://keras.posit.co/articles/examples/mlp_image_classification.html","id":"implement-patch-extraction-as-a-layer","dir":"Articles > Examples","previous_headings":"","what":"Implement patch extraction as a layer","title":"Image classification with modern MLP models","text":"","code":"class Patches(layers.Layer):     def __init__(self, patch_size, **kwargs):         super().__init__(**kwargs)         self.patch_size = patch_size      def call(self, x):         patches = keras.ops.image.extract_patches(x, self.patch_size)         batch_size = keras.ops.shape(patches)[0]         num_patches = keras.ops.shape(patches)[1] * keras.ops.shape(patches)[2]         patch_dim = keras.ops.shape(patches)[3]         out = keras.ops.reshape(patches, (batch_size, num_patches, patch_dim))         return out"},{"path":"https://keras.posit.co/articles/examples/mlp_image_classification.html","id":"implement-position-embedding-as-a-layer","dir":"Articles > Examples","previous_headings":"","what":"Implement position embedding as a layer","title":"Image classification with modern MLP models","text":"","code":"class PositionEmbedding(keras.layers.Layer):     def __init__(         self,         sequence_length,         initializer=\"glorot_uniform\",         **kwargs,     ):         super().__init__(**kwargs)         if sequence_length is None:             raise ValueError(                 \"`sequence_length` must be an Integer, received `None`.\"             )         self.sequence_length = int(sequence_length)         self.initializer = keras.initializers.get(initializer)      def get_config(self):         config = super().get_config()         config.update(             {                 \"sequence_length\": self.sequence_length,                 \"initializer\": keras.initializers.serialize(self.initializer),             }         )         return config      def build(self, input_shape):         feature_size = input_shape[-1]         self.position_embeddings = self.add_weight(             name=\"embeddings\",             shape=[self.sequence_length, feature_size],             initializer=self.initializer,             trainable=True,         )          super().build(input_shape)      def call(self, inputs, start_index=0):         shape = keras.ops.shape(inputs)         feature_length = shape[-1]         sequence_length = shape[-2]         # trim to match the length of the input sequence, which might be less         # than the sequence_length of the layer.         position_embeddings = keras.ops.convert_to_tensor(self.position_embeddings)         position_embeddings = keras.ops.slice(             position_embeddings,             (start_index, 0),             (sequence_length, feature_length),         )         return keras.ops.broadcast_to(position_embeddings, shape)      def compute_output_shape(self, input_shape):         return input_shape"},{"path":"https://keras.posit.co/articles/examples/mlp_image_classification.html","id":"the-mlp-mixer-model","dir":"Articles > Examples","previous_headings":"","what":"The MLP-Mixer model","title":"Image classification with modern MLP models","text":"MLP-Mixer architecture based exclusively multi-layer perceptrons (MLPs), contains two types MLP layers: One applied independently image patches, mixes per-location features. applied across patches (along channels), mixes spatial information. similar depthwise separable convolution based model Xception model, two chained dense transforms, max pooling, layer normalization instead batch normalization.","code":""},{"path":"https://keras.posit.co/articles/examples/mlp_image_classification.html","id":"implement-the-mlp-mixer-module","dir":"Articles > Examples","previous_headings":"The MLP-Mixer model","what":"Implement the MLP-Mixer module","title":"Image classification with modern MLP models","text":"","code":"class MLPMixerLayer(layers.Layer):     def __init__(         self, num_patches, hidden_units, dropout_rate, *args, **kwargs     ):         super().__init__(*args, **kwargs)          self.mlp1 = keras.Sequential(             [                 layers.Dense(units=num_patches, activation=\"gelu\"),                 layers.Dense(units=num_patches),                 layers.Dropout(rate=dropout_rate),             ]         )         self.mlp2 = keras.Sequential(             [                 layers.Dense(units=num_patches, activation=\"gelu\"),                 layers.Dense(units=hidden_units),                 layers.Dropout(rate=dropout_rate),             ]         )         self.normalize = layers.LayerNormalization(epsilon=1e-6)      def build(self, input_shape):         return super().build(input_shape)      def call(self, inputs):         # Apply layer normalization.         x = self.normalize(inputs)         # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].         x_channels = keras.ops.transpose(x, axes=(0, 2, 1))         # Apply mlp1 on each channel independently.         mlp1_outputs = self.mlp1(x_channels)         # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].         mlp1_outputs = keras.ops.transpose(mlp1_outputs, axes=(0, 2, 1))         # Add skip connection.         x = mlp1_outputs + inputs         # Apply layer normalization.         x_patches = self.normalize(x)         # Apply mlp2 on each patch independtenly.         mlp2_outputs = self.mlp2(x_patches)         # Add skip connection.         x = x + mlp2_outputs         return x"},{"path":"https://keras.posit.co/articles/examples/mlp_image_classification.html","id":"build-train-and-evaluate-the-mlp-mixer-model","dir":"Articles > Examples","previous_headings":"The MLP-Mixer model","what":"Build, train, and evaluate the MLP-Mixer model","title":"Image classification with modern MLP models","text":"Note training model current settings V100 GPUs takes around 8 seconds per epoch. MLP-Mixer model tends much less number parameters compared convolutional transformer-based models, leads less training serving computational cost. mentioned MLP-Mixer paper, pre-trained large datasets, modern regularization schemes, MLP-Mixer attains competitive scores state---art models. can obtain better results increasing embedding dimensions, increasing number mixer blocks, training model longer. may also try increase size input images use different patch sizes.","code":"mlpmixer_blocks = keras.Sequential(     [         MLPMixerLayer(num_patches, embedding_dim, dropout_rate)         for _ in range(num_blocks)     ] ) learning_rate = 0.005 mlpmixer_classifier = build_classifier(mlpmixer_blocks) history = run_experiment(mlpmixer_classifier)"},{"path":"https://keras.posit.co/articles/examples/mlp_image_classification.html","id":"the-fnet-model","dir":"Articles > Examples","previous_headings":"","what":"The FNet model","title":"Image classification with modern MLP models","text":"FNet uses similar block Transformer block. However, FNet replaces self-attention layer Transformer block parameter-free 2D Fourier transformation layer: One 1D Fourier Transform applied along patches. One 1D Fourier Transform applied along channels.","code":""},{"path":"https://keras.posit.co/articles/examples/mlp_image_classification.html","id":"implement-the-fnet-module","dir":"Articles > Examples","previous_headings":"The FNet model","what":"Implement the FNet module","title":"Image classification with modern MLP models","text":"","code":"class FNetLayer(layers.Layer):     def __init__(         self, embedding_dim, dropout_rate, *args, **kwargs     ):         super().__init__(*args, **kwargs)          self.ffn = keras.Sequential(             [                 layers.Dense(units=embedding_dim, activation=\"gelu\"),                 layers.Dropout(rate=dropout_rate),                 layers.Dense(units=embedding_dim),             ]         )          self.normalize1 = layers.LayerNormalization(epsilon=1e-6)         self.normalize2 = layers.LayerNormalization(epsilon=1e-6)      def call(self, inputs):         # Apply fourier transformations.         real_part = inputs         im_part = keras.ops.zeros_like(inputs)         x = keras.ops.fft2((real_part, im_part))[0]         # Add skip connection.         x = x + inputs         # Apply layer normalization.         x = self.normalize1(x)         # Apply Feedfowrad network.         x_ffn = self.ffn(x)         # Add skip connection.         x = x + x_ffn         # Apply layer normalization.         return self.normalize2(x)"},{"path":"https://keras.posit.co/articles/examples/mlp_image_classification.html","id":"build-train-and-evaluate-the-fnet-model","dir":"Articles > Examples","previous_headings":"The FNet model","what":"Build, train, and evaluate the FNet model","title":"Image classification with modern MLP models","text":"Note training model current settings V100 GPUs takes around 8 seconds per epoch. shown FNet paper, better results can achieved increasing embedding dimensions, increasing number FNet blocks, training model longer. may also try increase size input images use different patch sizes. FNet scales efficiently long inputs, runs much faster attention-based Transformer models, produces competitive accuracy results.","code":"fnet_blocks = keras.Sequential(     [         FNetLayer(embedding_dim, dropout_rate)         for _ in range(num_blocks)     ] ) learning_rate = 0.001 fnet_classifier = build_classifier(fnet_blocks, positional_encoding=True) history = run_experiment(fnet_classifier)"},{"path":"https://keras.posit.co/articles/examples/mlp_image_classification.html","id":"the-gmlp-model","dir":"Articles > Examples","previous_headings":"","what":"The gMLP model","title":"Image classification with modern MLP models","text":"gMLP MLP architecture features Spatial Gating Unit (SGU). SGU enables cross-patch interactions across spatial (channel) dimension, : Transforming input spatially applying linear projection across patches (along channels). Applying element-wise multiplication input spatial transformation.","code":""},{"path":"https://keras.posit.co/articles/examples/mlp_image_classification.html","id":"implement-the-gmlp-module","dir":"Articles > Examples","previous_headings":"The gMLP model","what":"Implement the gMLP module","title":"Image classification with modern MLP models","text":"","code":"class gMLPLayer(layers.Layer):     def __init__(         self, num_patches, embedding_dim, dropout_rate, *args, **kwargs     ):         super().__init__(*args, **kwargs)          self.channel_projection1 = keras.Sequential(             [                 layers.Dense(units=embedding_dim * 2, activation=\"gelu\"),                 layers.Dropout(rate=dropout_rate),             ]         )          self.channel_projection2 = layers.Dense(units=embedding_dim)          self.spatial_projection = layers.Dense(             units=num_patches, bias_initializer=\"Ones\"         )          self.normalize1 = layers.LayerNormalization(epsilon=1e-6)         self.normalize2 = layers.LayerNormalization(epsilon=1e-6)      def spatial_gating_unit(self, x):         # Split x along the channel dimensions.         # Tensors u and v will in the shape of [batch_size, num_patchs, embedding_dim].         u, v = keras.ops.split(x, indices_or_sections=2, axis=2)         # Apply layer normalization.         v = self.normalize2(v)         # Apply spatial projection.         v_channels = keras.ops.transpose(v, axes=(0, 2, 1))         v_projected = self.spatial_projection(v_channels)         v_projected = keras.ops.transpose(v_projected, axes=(0, 2, 1))         # Apply element-wise multiplication.         return u * v_projected      def call(self, inputs):         # Apply layer normalization.         x = self.normalize1(inputs)         # Apply the first channel projection. x_projected shape: [batch_size, num_patches, embedding_dim * 2].         x_projected = self.channel_projection1(x)         # Apply the spatial gating unit. x_spatial shape: [batch_size, num_patches, embedding_dim].         x_spatial = self.spatial_gating_unit(x_projected)         # Apply the second channel projection. x_projected shape: [batch_size, num_patches, embedding_dim].         x_projected = self.channel_projection2(x_spatial)         # Add skip connection.         return x + x_projected"},{"path":"https://keras.posit.co/articles/examples/mlp_image_classification.html","id":"build-train-and-evaluate-the-gmlp-model","dir":"Articles > Examples","previous_headings":"The gMLP model","what":"Build, train, and evaluate the gMLP model","title":"Image classification with modern MLP models","text":"Note training model current settings V100 GPUs takes around 9 seconds per epoch. shown gMLP paper, better results can achieved increasing embedding dimensions, increasing number gMLP blocks, training model longer. may also try increase size input images use different patch sizes. Note , paper used advanced regularization strategies, MixUp CutMix, well AutoAugment.","code":"gmlp_blocks = keras.Sequential(     [         gMLPLayer(num_patches, embedding_dim, dropout_rate)         for _ in range(num_blocks)     ] ) learning_rate = 0.003 gmlp_classifier = build_classifier(gmlp_blocks) history = run_experiment(gmlp_classifier)"},{"path":"https://keras.posit.co/articles/examples/mnist_convnet.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Simple MNIST convnet","text":"","code":"library(keras3)"},{"path":"https://keras.posit.co/articles/examples/mnist_convnet.html","id":"prepare-the-data","dir":"Articles > Examples","previous_headings":"","what":"Prepare the data","title":"Simple MNIST convnet","text":"","code":"# Model / data parameters num_classes <- 10 input_shape <- c(28, 28, 1)  # Load the data and split it between train and test sets c(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_mnist()  # Scale images to the [0, 1] range x_train <- x_train / 255 x_test <- x_test / 255 # Make sure images have shape (28, 28, 1) x_train <- op_expand_dims(x_train, -1) x_test <- op_expand_dims(x_test, -1)   dim(x_train) ## [1] 60000    28    28     1 dim(x_test) ## [1] 10000    28    28     1 # convert class vectors to binary class matrices y_train <- to_categorical(y_train, num_classes) y_test <- to_categorical(y_test, num_classes)"},{"path":"https://keras.posit.co/articles/examples/mnist_convnet.html","id":"build-the-model","dir":"Articles > Examples","previous_headings":"","what":"Build the model","title":"Simple MNIST convnet","text":"","code":"model <- keras_model_sequential(input_shape = input_shape) model %>%   layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = \"relu\") %>%   layer_max_pooling_2d(pool_size = c(2, 2)) %>%   layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = \"relu\") %>%   layer_max_pooling_2d(pool_size = c(2, 2)) %>%   layer_flatten() %>%   layer_dropout(rate = 0.5) %>%   layer_dense(units = num_classes, activation = \"softmax\")  summary(model) ## Model: \"sequential\" ## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓ ## ┃ Layer (type)                    ┃ Output Shape              ┃    Param # ┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩ ## │ conv2d_1 (Conv2D)               │ (None, 26, 26, 32)        │        320 │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ max_pooling2d_1 (MaxPooling2D)  │ (None, 13, 13, 32)        │          0 │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ conv2d (Conv2D)                 │ (None, 11, 11, 64)        │     18,496 │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ max_pooling2d (MaxPooling2D)    │ (None, 5, 5, 64)          │          0 │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ flatten (Flatten)               │ (None, 1600)              │          0 │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ dropout (Dropout)               │ (None, 1600)              │          0 │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ dense (Dense)                   │ (None, 10)                │     16,010 │ ## └─────────────────────────────────┴───────────────────────────┴────────────┘ ##  Total params: 34,826 (136.04 KB) ##  Trainable params: 34,826 (136.04 KB) ##  Non-trainable params: 0 (0.00 B)"},{"path":"https://keras.posit.co/articles/examples/mnist_convnet.html","id":"train-the-model","dir":"Articles > Examples","previous_headings":"","what":"Train the model","title":"Simple MNIST convnet","text":"","code":"batch_size <- 128 epochs <- 15  model %>% compile(   loss = \"categorical_crossentropy\",   optimizer = \"adam\",   metrics = \"accuracy\" )  model %>% fit(   x_train, y_train,   batch_size = batch_size,   epochs = epochs,   validation_split = 0.1 ) ## Epoch 1/15 ## 422/422 - 7s - 17ms/step - accuracy: 0.8816 - loss: 0.3836 - val_accuracy: 0.9787 - val_loss: 0.0822 ## Epoch 2/15 ## 422/422 - 6s - 15ms/step - accuracy: 0.9652 - loss: 0.1147 - val_accuracy: 0.9848 - val_loss: 0.0557 ## Epoch 3/15 ## 422/422 - 6s - 15ms/step - accuracy: 0.9736 - loss: 0.0861 - val_accuracy: 0.9883 - val_loss: 0.0449 ## Epoch 4/15 ## 422/422 - 6s - 15ms/step - accuracy: 0.9774 - loss: 0.0708 - val_accuracy: 0.9893 - val_loss: 0.0399 ## Epoch 5/15 ## 422/422 - 6s - 15ms/step - accuracy: 0.9803 - loss: 0.0623 - val_accuracy: 0.9908 - val_loss: 0.0372 ## Epoch 6/15 ## 422/422 - 6s - 15ms/step - accuracy: 0.9828 - loss: 0.0556 - val_accuracy: 0.9918 - val_loss: 0.0351 ## Epoch 7/15 ## 422/422 - 6s - 15ms/step - accuracy: 0.9843 - loss: 0.0512 - val_accuracy: 0.9915 - val_loss: 0.0348 ## Epoch 8/15 ## 422/422 - 7s - 16ms/step - accuracy: 0.9852 - loss: 0.0478 - val_accuracy: 0.9920 - val_loss: 0.0328 ## Epoch 9/15 ## 422/422 - 7s - 16ms/step - accuracy: 0.9864 - loss: 0.0432 - val_accuracy: 0.9918 - val_loss: 0.0303 ## Epoch 10/15 ## 422/422 - 7s - 16ms/step - accuracy: 0.9870 - loss: 0.0402 - val_accuracy: 0.9925 - val_loss: 0.0309 ## Epoch 11/15 ## 422/422 - 7s - 16ms/step - accuracy: 0.9877 - loss: 0.0388 - val_accuracy: 0.9920 - val_loss: 0.0303 ## Epoch 12/15 ## 422/422 - 7s - 15ms/step - accuracy: 0.9879 - loss: 0.0362 - val_accuracy: 0.9923 - val_loss: 0.0306 ## Epoch 13/15 ## 422/422 - 8s - 19ms/step - accuracy: 0.9880 - loss: 0.0354 - val_accuracy: 0.9923 - val_loss: 0.0293 ## Epoch 14/15 ## 422/422 - 7s - 16ms/step - accuracy: 0.9885 - loss: 0.0346 - val_accuracy: 0.9922 - val_loss: 0.0296 ## Epoch 15/15 ## 422/422 - 6s - 15ms/step - accuracy: 0.9893 - loss: 0.0321 - val_accuracy: 0.9925 - val_loss: 0.0274"},{"path":"https://keras.posit.co/articles/examples/mnist_convnet.html","id":"evaluate-the-trained-model","dir":"Articles > Examples","previous_headings":"","what":"Evaluate the trained model","title":"Simple MNIST convnet","text":"","code":"score <- model %>% evaluate(x_test, y_test, verbose=0) score ##            loss compile_metrics ##      0.02563373      0.99190003"},{"path":"https://keras.posit.co/articles/examples/multiple_choice_task_with_transfer_learning.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"MultipleChoice Task with Transfer Learning","text":"example, demonstrate perform MultipleChoice task finetuning pre-trained DebertaV3 model. task, several candidate answers provided along context model trained select correct answer unlike question answering. use SWAG dataset demonstrate example.","code":""},{"path":"https://keras.posit.co/articles/examples/multiple_choice_task_with_transfer_learning.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"MultipleChoice Task with Transfer Learning","text":"pip install -q keras –upgrade pip install -q keras-nlp –upgrade","code":"import os os.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"  import keras_nlp import keras as keras import tensorflow as tf  import numpy as np import pandas as pd  import matplotlib.pyplot as plt"},{"path":"https://keras.posit.co/articles/examples/multiple_choice_task_with_transfer_learning.html","id":"dataset","dir":"Articles > Examples","previous_headings":"","what":"Dataset","title":"MultipleChoice Task with Transfer Learning","text":"example ’ll use SWAG dataset multiplechoice task. wget “https://github.com/rowanz/swagaf/archive/refs/heads/master.zip” -O swag.zip unzip -q /content/swag.zip ls /content/swagaf-master/data","code":""},{"path":"https://keras.posit.co/articles/examples/multiple_choice_task_with_transfer_learning.html","id":"configuration","dir":"Articles > Examples","previous_headings":"","what":"Configuration","title":"MultipleChoice Task with Transfer Learning","text":"","code":"class CFG:     preset = \"deberta_v3_extra_small_en\"  # Name of pretrained models     sequence_length = 200  # Input sequence length     seed = 42  # Random seed     epochs = 5  # Training epochs     batch_size = 8  # Batch size     augment = True  # Augmentation (Shuffle Options)"},{"path":"https://keras.posit.co/articles/examples/multiple_choice_task_with_transfer_learning.html","id":"reproducibility","dir":"Articles > Examples","previous_headings":"","what":"Reproducibility","title":"MultipleChoice Task with Transfer Learning","text":"Sets value random seed produce similar result run.","code":"keras.utils.set_random_seed(CFG.seed)"},{"path":"https://keras.posit.co/articles/examples/multiple_choice_task_with_transfer_learning.html","id":"meta-data","dir":"Articles > Examples","previous_headings":"","what":"Meta Data","title":"MultipleChoice Task with Transfer Learning","text":"train.csv - used training. sent1 sent2: fields show sentence starts, put two together, get startphrase field. label: identifies correct sentence ending. val.csv - similar train.csv used validation.","code":"# Train data train_df = pd.read_csv(     \"/content/swagaf-master/data/train.csv\", index_col=0 )  # Read CSV file into a DataFrame train_df = train_df.sample(frac=0.02) print(\"# Train Data: {:,}\".format(len(train_df)))  # Valid data valid_df = pd.read_csv(     \"/content/swagaf-master/data/val.csv\", index_col=0 )  # Read CSV file into a DataFrame valid_df = valid_df.sample(frac=0.02) print(\"# Valid Data: {:,}\".format(len(valid_df)))"},{"path":"https://keras.posit.co/articles/examples/multiple_choice_task_with_transfer_learning.html","id":"contextualize-options","dir":"Articles > Examples","previous_headings":"","what":"Contextualize Options","title":"MultipleChoice Task with Transfer Learning","text":"approach entails furnishing model question answer pairs, opposed employing single question five options. practice, signifies five options, supply model set five questions combined respective answer choice (e.g., (Q + ), (Q + B), ). analogy draws parallels practice revisiting question multiple times exam promote deeper understanding problem hand. Notably, context SWAG dataset, question start sentence options possible ending sentence. Apply make_options function row dataframe","code":"# Define a function to create options based on the prompt and choices def make_options(row):     row[\"options\"] = [         f\"{row.startphrase}\\n{row.ending0}\",  # Option 0         f\"{row.startphrase}\\n{row.ending1}\",  # Option 1         f\"{row.startphrase}\\n{row.ending2}\",  # Option 2         f\"{row.startphrase}\\n{row.ending3}\",     ]  # Option 3     return row train_df = train_df.apply(make_options, axis=1) valid_df = valid_df.apply(make_options, axis=1)"},{"path":"https://keras.posit.co/articles/examples/multiple_choice_task_with_transfer_learning.html","id":"preprocessing","dir":"Articles > Examples","previous_headings":"","what":"Preprocessing","title":"MultipleChoice Task with Transfer Learning","text":": preprocessor takes input strings transforms dictionary (token_ids, padding_mask) containing preprocessed tensors. process starts tokenization, input strings converted sequences token IDs. ’s important: Initially, raw text data complex challenging modeling due high dimensionality. converting text compact set tokens, transforming \"quick brown fox\" [\"\", \"qu\", \"##ick\", \"br\", \"##\", \"fox\"], simplify data. Many models rely special tokens additional tensors understand input. tokens help divide input identify padding, among tasks. Making sequences length padding boosts computational efficiency, making subsequent steps smoother. Explore following pages access available preprocessing tokenizer layers KerasNLP: - Preprocessing - Tokenizers Now, let’s examine output shape preprocessing layer looks like. output shape layer can represented \\((num\\_choices, sequence\\_length)\\). ’ll use preprocessing_fn function transform text option using dataset.map(preprocessing_fn) method.","code":"preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(     preset=CFG.preset,  # Name of the model     sequence_length=CFG.sequence_length,  # Max sequence length, will be padded if shorter ) outs = preprocessor(     train_df.options.iloc[0] )  # Process options for the first row  # Display the shape of each processed output for k, v in outs.items():     print(k, \":\", v.shape) def preprocess_fn(text, label=None):     text = preprocessor(text)  # Preprocess text     return (         (text, label) if label is not None else text     )  # Return processed text and label if available"},{"path":"https://keras.posit.co/articles/examples/multiple_choice_task_with_transfer_learning.html","id":"augmentation","dir":"Articles > Examples","previous_headings":"","what":"Augmentation","title":"MultipleChoice Task with Transfer Learning","text":"notebook, ’ll experiment interesting augmentation technique, option_shuffle. Since ’re providing model one option time, can introduce shuffle order options. instance, options [, C, E, D, B] rearranged [D, B, , E, C]. practice help model focus content options , rather influenced positions. Note: Even though option_shuffle function written pure tensorflow, can used backend (e.g. JAX, PyTorch) used tf.data.Dataset pipeline compatible Keras 3 routines. following function, ’ll merge augmentation functions apply text. augmentations applied data using dataset.map(augment_fn) approach.","code":"def option_shuffle(options, labels, prob=0.50, seed=None):     if tf.random.uniform([]) > prob:  # Shuffle probability check         return options, labels     # Shuffle indices of options and labels in the same order     indices = tf.random.shuffle(tf.range(tf.shape(options)[0]), seed=seed)     # Shuffle options and labels     options = tf.gather(options, indices)     labels = tf.gather(labels, indices)     return options, labels def augment_fn(text, label=None):     text, label = option_shuffle(text, label, prob=0.5  # Shuffle the options     return (text, label) if label is not None else text"},{"path":"https://keras.posit.co/articles/examples/multiple_choice_task_with_transfer_learning.html","id":"dataloader","dir":"Articles > Examples","previous_headings":"","what":"DataLoader","title":"MultipleChoice Task with Transfer Learning","text":"code sets robust data flow pipeline using tf.data.Dataset data processing. Notable aspects tf.data include ability simplify pipeline construction represent components sequences. learn tf.data, refer documentation. Now let’s create train valid dataloader using funciton.","code":"def build_dataset(     texts,     labels=None,     batch_size=32,     cache=False,     augment=False,     repeat=False,     shuffle=1024, ):     AUTO = tf.data.AUTOTUNE  # AUTOTUNE option     slices = (         (texts,)         if labels is None         else (texts, keras.utils.to_categorical(labels, num_classes=4))     )  # Create slices     ds = tf.data.Dataset.from_tensor_slices(         slices     )  # Create dataset from slices     ds = ds.cache() if cache else ds  # Cache dataset if enabled     if augment:  # Apply augmentation if enabled         ds = ds.map(augment_fn, num_parallel_calls=AUTO)     ds = ds.map(         preprocess_fn, num_parallel_calls=AUTO     )  # Map preprocessing function     ds = ds.repeat() if repeat else ds  # Repeat dataset if enabled     opt = tf.data.Options()  # Create dataset options     if shuffle:         ds = ds.shuffle(shuffle, seed=CFG.seed)  # Shuffle dataset if enabled         opt.experimental_deterministic = False     ds = ds.with_options(opt)  # Set dataset options     ds = ds.batch(batch_size, drop_remainder=True)  # Batch dataset     ds = ds.prefetch(AUTO)  # Prefetch next batch     return ds  # Return the built dataset # Build train dataloader train_texts = train_df.options.tolist()  # Extract training texts train_labels = train_df.label.tolist()  # Extract training labels train_ds = build_dataset(     train_texts,     train_labels,     batch_size=CFG.batch_size,     cache=True,     shuffle=True,     repeat=True,     augment=CFG.augment, )  # Build valid dataloader valid_texts = valid_df.options.tolist()  # Extract validation texts valid_labels = valid_df.label.tolist()  # Extract validation labels valid_ds = build_dataset(     valid_texts,     valid_labels,     batch_size=CFG.batch_size,     cache=True,     shuffle=False,     repeat=False,     augment=False, )"},{"path":"https://keras.posit.co/articles/examples/multiple_choice_task_with_transfer_learning.html","id":"lr-schedule","dir":"Articles > Examples","previous_headings":"","what":"LR Schedule","title":"MultipleChoice Task with Transfer Learning","text":"Implementing learning rate scheduler crucial transfer learning. learning rate initiates lr_start gradually tapers lr_min using cosine curve. Importance: well-structured learning rate schedule essential efficient model training, ensuring optimal convergence avoiding issues overshooting stagnation.","code":"import math   def get_lr_callback(batch_size=8, mode=\"cos\", epochs=10, plot=False):     lr_start, lr_max, lr_min = 1.0e-6, 0.6e-6 * batch_size, 1e-6     lr_ramp_ep, lr_sus_ep = 2, 0      def lrfn(epoch):  # Learning rate update function         if epoch < lr_ramp_ep:             lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start         elif epoch < lr_ramp_ep + lr_sus_ep:             lr = lr_max         else:             decay_total_epochs, decay_epoch_index = (                 epochs - lr_ramp_ep - lr_sus_ep + 3,                 epoch - lr_ramp_ep - lr_sus_ep,             )             phase = math.pi * decay_epoch_index / decay_total_epochs             lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min         return lr      if plot:  # Plot lr curve if plot is True         plt.figure(figsize=(10, 5))         plt.plot(             np.arange(epochs),             [lrfn(epoch) for epoch in np.arange(epochs)],             marker=\"o\",         )         plt.xlabel(\"epoch\")         plt.ylabel(\"lr\")         plt.title(\"LR Scheduler\")         plt.show()      return keras.callbacks.LearningRateScheduler(         lrfn, verbose=False     )  # Create lr callback   _ = get_lr_callback(CFG.batch_size, plot=True)"},{"path":"https://keras.posit.co/articles/examples/multiple_choice_task_with_transfer_learning.html","id":"callbacks","dir":"Articles > Examples","previous_headings":"","what":"Callbacks","title":"MultipleChoice Task with Transfer Learning","text":"function gather training callbacks, lr_scheduler, model_checkpoint.","code":"def get_callbacks():     callbacks = []     lr_cb = get_lr_callback(CFG.batch_size)  # Get lr callback     ckpt_cb = keras.callbacks.ModelCheckpoint(         f\"best.keras\",         monitor=\"val_accuracy\",         save_best_only=True,         save_weights_only=False,         mode=\"max\",     )  # Get Model checkpoint callback     callbacks.extend([lr_cb, ckpt_cb])  # Add lr and checkpoint callbacks     return callbacks  # Return the list of callbacks   callbacks = get_callbacks()"},{"path":"https://keras.posit.co/articles/examples/multiple_choice_task_with_transfer_learning.html","id":"multiplechoice-model","dir":"Articles > Examples","previous_headings":"","what":"MultipleChoice Model","title":"MultipleChoice Task with Transfer Learning","text":"coding perspective, remember use model five options, shared weights. Despite figure suggesting five separate models, , fact, one model shared weights. Another point consider input shapes Classifier MultipleChoice. Input shape Multiple Choice: \\((batch\\_size, num\\_choices, seq\\_length)\\) Input shape Classifier: \\((batch\\_size, seq\\_length)\\) Certainly, ’s clear can’t directly give data multiple-choice task model input shapes don’t match. handle , ’ll use slicing. means ’ll separate features option, like \\(feature_{(Q + )}\\) \\(feature_{(Q + B)}\\), give one one NLP classifier. get prediction scores \\(logits_{(Q + )}\\) \\(logits_{(Q + B)}\\) options, ’ll use Softmax function, like \\(\\operatorname{Softmax}([logits_{(Q + )}, logits_{(Q + B)}])\\), combine . final step helps us make ultimate decision choice. Note classifier, set num_classes=1 instead 5. classifier produces single output option. dealing five options, individual outputs joined together processed softmax function generate final result, dimension 5. Let’s checkout model summary better insight model. Finally, let’s check model structure visually everything place.","code":"# Selects one option from five class SelectOption(keras.layers.Layer):     def __init__(self, index, **kwargs):         super().__init__(**kwargs)         self.index = index      def call(self, inputs):         # Selects a specific slice from the inputs tensor         return inputs[:, self.index, :]      def get_config(self):         # For serialize the model         base_config = super().get_config()         config = {             \"index\": self.index,         }         return {**base_config, **config}   def build_model():     # Define input layers     inputs = {         \"token_ids\": keras.Input(             shape=(4, None), dtype=t\"int32\", name=\"token_ids\"         ),         \"padding_mask\": keras.Input(             shape=(4, None), dtype=\"int32\", name=\"padding_mask\"         ),     }     # Create a DebertaV3Classifier model     classifier = keras_nlp.models.DebertaV3Classifier.from_preset(         CFG.preset,         preprocessor=None,         num_classes=1,  # one output per one option, for five options total 5 outputs     )     logits = []     # Loop through each option (Q+A), (Q+B) etc and compute associted logits     for option_idx in range(4):         option = {             k: SelectOption(option_idx, name=f\"{k}_{option_idx}\")(v)             for k, v in inputs.items()         }         logit = classifier(option)         logits.append(logit)      # Compute final output     logits = keras.layers.Concatenate(axis=-1)(logits)     outputs = keras.layers.Softmax(axis=-1)(logits)     model = keras.Model(inputs, outputs)      # Compile the model with optimizer, loss, and metrics     model.compile(         optimizer=keras.optimizers.AdamW(5e-6),         loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.02),         metrics=[             keras.metrics.CategoricalAccuracy(name=\"accuracy\"),         ],         jit_compile=True,     )     return model   # Build the Build model = build_model() model.summary() keras.utils.plot_model(model, show_shapes=True)"},{"path":"https://keras.posit.co/articles/examples/multiple_choice_task_with_transfer_learning.html","id":"pre-trained-models","dir":"Articles > Examples","previous_headings":"MultipleChoice Model","what":"Pre-trained Models","title":"MultipleChoice Task with Transfer Learning","text":"KerasNLP library provides comprehensive, ready--use implementations popular NLP model architectures. features variety pre-trained models including Bert, Roberta, DebertaV3, . notebook, ’ll showcase usage DistillBert. However, feel free explore available models KerasNLP documentation. Also deeper understanding KerasNLP, refer informative getting started guide. approach involves using keras_nlp.models.XXClassifier process question option pari (e.g. (Q+), (Q+B), etc.), generating logits. logits combined passed softmax function produce final output.","code":""},{"path":"https://keras.posit.co/articles/examples/multiple_choice_task_with_transfer_learning.html","id":"classifier-for-multiple-choice-tasks","dir":"Articles > Examples","previous_headings":"MultipleChoice Model","what":"Classifier for Multiple-Choice Tasks","title":"MultipleChoice Task with Transfer Learning","text":"dealing multiple-choice questions, instead giving model question options together (Q + + B + C ...), provide model one option time along question. instance, (Q + ), (Q + B), . prediction scores (logits) options, combine using Softmax function get ultimate result. given options model, text’s length increase, making harder model handle. picture illustrates idea:","code":""},{"path":"https://keras.posit.co/articles/examples/ner_transformers.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Named Entity Recognition using Transformers","text":"Named Entity Recognition (NER) process identifying named entities text. Example named entities : “Person”, “Location”, “Organization”, “Dates” etc. NER essentially token classification task every token classified one predetermined categories. exercise, train simple Transformer based model perform NER. using data CoNLL 2003 shared task. information dataset, please visit dataset website. However, since obtaining data requires additional step getting free license, using HuggingFace’s datasets library contains processed version dataset.","code":""},{"path":"https://keras.posit.co/articles/examples/ner_transformers.html","id":"install-the-open-source-datasets-library-from-huggingface","dir":"Articles > Examples","previous_headings":"","what":"Install the open source datasets library from HuggingFace","title":"Named Entity Recognition using Transformers","text":"also download script used evaluate NER models. pip3 install datasets wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py Next, let’s define TokenAndPositionEmbedding layer:","code":"import os import numpy as np import keras as keras from keras import layers from datasets import load_dataset from collections import Counter from conlleval import evaluate  # imports for data preprocessing from tensorflow import data as tf_data from tensorflow import strings as tf_strings   class TransformerBlock(layers.Layer):     def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):         super().__init__()         self.att = keras.layers.MultiHeadAttention(             num_heads=num_heads, key_dim=embed_dim         )         self.ffn = keras.Sequential(             [                 keras.layers.Dense(ff_dim, activation=\"relu\"),                 keras.layers.Dense(embed_dim),             ]         )         self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)         self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)         self.dropout1 = keras.layers.Dropout(rate)         self.dropout2 = keras.layers.Dropout(rate)      def call(self, inputs, training=False):         attn_output = self.att(inputs, inputs)         attn_output = self.dropout1(attn_output, training=training)         out1 = self.layernorm1(inputs + attn_output)         ffn_output = self.ffn(out1)         ffn_output = self.dropout2(ffn_output, training=training)         return self.layernorm2(out1 + ffn_output) class TokenAndPositionEmbedding(layers.Layer):     def __init__(self, maxlen, vocab_size, embed_dim):         super().__init__()         self.token_emb = keras.layers.Embedding(             input_dim=vocab_size, output_dim=embed_dim         )         self.pos_emb = keras.layers.Embedding(             input_dim=maxlen, output_dim=embed_dim         )      def call(self, inputs):         maxlen = keras.ops.backend.shape(inputs)[-1]         positions = keras.ops.arange(start=0, stop=maxlen, step=1)         position_embeddings = self.pos_emb(positions)         token_embeddings = self.token_emb(inputs)         return token_embeddings + position_embeddings"},{"path":"https://keras.posit.co/articles/examples/ner_transformers.html","id":"build-the-ner-model-class-as-a-keras-model-subclass","dir":"Articles > Examples","previous_headings":"","what":"Build the NER model class as a keras.Model subclass","title":"Named Entity Recognition using Transformers","text":"","code":"class NERModel(keras.Model):     def __init__(         self,         num_tags,         vocab_size,         maxlen=128,         embed_dim=32,         num_heads=2,         ff_dim=32,     ):         super().__init__()         self.embedding_layer = TokenAndPositionEmbedding(             maxlen, vocab_size, embed_dim         )         self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)         self.dropout1 = layers.Dropout(0.1)         self.ff = layers.Dense(ff_dim, activation=\"relu\")         self.dropout2 = layers.Dropout(0.1)         self.ff_final = layers.Dense(num_tags, activation=\"softmax\")      def call(self, inputs, training=False):         x = self.embedding_layer(inputs)         x = self.transformer_block(x)         x = self.dropout1(x, training=training)         x = self.ff(x)         x = self.dropout2(x, training=training)         x = self.ff_final(x)         return x"},{"path":"https://keras.posit.co/articles/examples/ner_transformers.html","id":"load-the-conll-2003-dataset-from-the-datasets-library-and-process-it","dir":"Articles > Examples","previous_headings":"","what":"Load the CoNLL 2003 dataset from the datasets library and process it","title":"Named Entity Recognition using Transformers","text":"export data tab-separated file format easy read tf.data.Dataset object.","code":"conll_data = load_dataset(\"conll2003\") def export_to_file(export_file_path, data):     with open(export_file_path, \"w\") as f:         for record in data:             ner_tags = record[\"ner_tags\"]             tokens = record[\"tokens\"]             if len(tokens) > 0:                 f.write(                     str(len(tokens))                     + \"\\t\"                     + \"\\t\".join(tokens)                     + \"\\t\"                     + \"\\t\".join(map(str, ner_tags))                     + \"\\n\"                 )   os.mkdir(\"data\") export_to_file(\"./data/conll_train.txt\", conll_data[\"train\"]) export_to_file(\"./data/conll_val.txt\", conll_data[\"validation\"])"},{"path":"https://keras.posit.co/articles/examples/ner_transformers.html","id":"make-the-ner-label-lookup-table","dir":"Articles > Examples","previous_headings":"","what":"Make the NER label lookup table","title":"Named Entity Recognition using Transformers","text":"NER labels usually provided IOB, IOB2 IOBES formats. Checkout link information: Wikipedia Note start label numbering 1 since 0 reserved padding. total 10 labels: 9 NER dataset one padding. Get list tokens training dataset. used create vocabulary. Create 2 new Dataset objects training validation data Print one line make sure looks good. first record line number tokens. tokens followed ner tags. using following map function transform data dataset: using custom loss function ignore loss padded tokens.","code":"def make_tag_lookup_table():     iob_labels = [\"B\", \"I\"]     ner_labels = [\"PER\", \"ORG\", \"LOC\", \"MISC\"]     all_labels = [         (label1, label2) for label2 in ner_labels for label1 in iob_labels     ]     all_labels = [\"-\".join([a, b]) for a, b in all_labels]     all_labels = [\"[PAD]\", \"O\"] + all_labels     return dict(zip(range(0, len(all_labels) + 1), all_labels))   mapping = make_tag_lookup_table() print(mapping) all_tokens = sum(conll_data[\"train\"][\"tokens\"], []) all_tokens_array = np.array(list(map(str.lower, all_tokens)))  counter = Counter(all_tokens_array) print(len(counter))  num_tags = len(mapping) vocab_size = 20000  # We only take (vocab_size - 2) most commons words from the training data since # the `StringLookup` class uses 2 additional tokens - one denoting an unknown # token and another one denoting a masking token vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]  # The StringLook class will convert tokens to token IDs lookup_layer = keras.layers.StringLookup(vocabulary=vocabulary) train_data = tf_data.TextLineDataset(\"./data/conll_train.txt\") val_data = tf_data.TextLineDataset(\"./data/conll_val.txt\") print(list(train_data.take(1).as_numpy_iterator())) # Data preprocessing using Tensorflow def map_record_to_training_data(record):     record = tf_strings.split(record, sep=\"\\t\")     length = tf_strings.to_number(record[0], out_type=\"int32\")     tokens = record[1 : length + 1]     tags = record[length + 1 :]     tags = tf_strings.to_number(tags, out_type=\"int64\")     tags += 1     return tokens, tags   def lowercase_and_convert_to_ids(tokens):     tokens = tf_strings.lower(tokens)     return lookup_layer(tokens)   # We use `padded_batch` here because each record in the dataset has a # different length. batch_size = 32 train_dataset = (     train_data.map(map_record_to_training_data)     .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))     .padded_batch(batch_size) ) val_dataset = (     val_data.map(map_record_to_training_data)     .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))     .padded_batch(batch_size) )  ner_model = NERModel(num_tags, vocab_size, embed_dim=32, num_heads=4, ff_dim=64) class CustomNonPaddingTokenLoss(keras.losses.Loss):     def __init__(self, name=\"custom_ner_loss\"):         super().__init__(name=name)      def call(self, y_true, y_pred):         loss_fn = keras.losses.SparseCategoricalCrossentropy(             from_logits=True, reduction=None         )         loss = loss_fn(y_true, y_pred)         mask = keras.backend.cast((y_true > 0), dtype=\"float32\")         loss = loss * mask         return keras.ops.sum(loss) / keras.ops.sum(mask)   loss = CustomNonPaddingTokenLoss()"},{"path":"https://keras.posit.co/articles/examples/ner_transformers.html","id":"compile-and-fit-the-model","dir":"Articles > Examples","previous_headings":"","what":"Compile and fit the model","title":"Named Entity Recognition using Transformers","text":"","code":"ner_model.compile(optimizer=\"adam\", loss=loss) ner_model.fit(train_dataset, epochs=10)   def tokenize_and_convert_to_ids(text):     tokens = text.split()     return lowercase_and_convert_to_ids(tokens)   # Sample inference using the trained model sample_input = tokenize_and_convert_to_ids(     \"eu rejects german call to boycott british lamb\" ) sample_input = keras.ops.reshape(sample_input, new_shape=[1, -1]) print(sample_input)  output = ner_model.predict(sample_input) prediction = np.argmax(output, axis=-1)[0] prediction = [mapping[i] for i in prediction]  # eu -> B-ORG, german -> B-MISC, british -> B-MISC print(prediction)"},{"path":"https://keras.posit.co/articles/examples/ner_transformers.html","id":"metrics-calculation","dir":"Articles > Examples","previous_headings":"","what":"Metrics calculation","title":"Named Entity Recognition using Transformers","text":"function calculate metrics. function calculates F1 score overall NER dataset well individual scores NER tag.","code":"def calculate_metrics(dataset):     all_true_tag_ids, all_predicted_tag_ids = [], []      for x, y in dataset:         output = ner_model.predict(x)         predictions = np.argmax(output, axis=-1)         predictions = np.reshape(predictions, [-1])          true_tag_ids = np.reshape(y, [-1])          mask = (true_tag_ids > 0) & (predictions > 0)         true_tag_ids = true_tag_ids[mask]         predicted_tag_ids = predictions[mask]          all_true_tag_ids.append(true_tag_ids)         all_predicted_tag_ids.append(predicted_tag_ids)      all_true_tag_ids = np.concatenate(all_true_tag_ids)     all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)      predicted_tags = [mapping[tag] for tag in all_predicted_tag_ids]     real_tags = [mapping[tag] for tag in all_true_tag_ids]      evaluate(real_tags, predicted_tags)   calculate_metrics(val_dataset)"},{"path":"https://keras.posit.co/articles/examples/ner_transformers.html","id":"conclusions","dir":"Articles > Examples","previous_headings":"","what":"Conclusions","title":"Named Entity Recognition using Transformers","text":"exercise, created simple transformer based named entity recognition model. trained CoNLL 2003 shared task data got overall F1 score around 70%. State art NER models fine-tuned pretrained models BERT ELECTRA can easily get much higher F1 score -90-95% dataset owing inherent knowledge words part pretraining process usage subword tokenization.","code":""},{"path":"https://keras.posit.co/articles/examples/neural_machine_translation_with_keras_nlp.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"English-to-Spanish translation with KerasNLP","text":"KerasNLP provides building blocks NLP (model layers, tokenizers, metrics, etc.) makes convenient construct NLP pipelines. example, ’ll use KerasNLP layers build encoder-decoder Transformer model, train English--Spanish machine translation task. example based English--Spanish NMT example fchollet. original example low-level implements layers scratch, whereas example uses KerasNLP show advanced approaches, subword tokenization using metrics compute quality generated translations. ’ll learn : Tokenize text using keras_nlp.tokenizers.WordPieceTokenizer. Implement sequence--sequence Transformer model using KerasNLP’s keras_nlp.layers.TransformerEncoder, keras_nlp.layers.TransformerDecoder keras_nlp.layers.TokenAndPositionEmbedding layers, train . Use keras_nlp.samplers generate translations unseen input sentences using top-p decoding strategy! Don’t worry aren’t familiar KerasNLP. tutorial start basics. Let’s dive right !","code":""},{"path":"https://keras.posit.co/articles/examples/neural_machine_translation_with_keras_nlp.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"English-to-Spanish translation with KerasNLP","text":"start implementing pipeline, let’s import libraries need. !pip install -q rouge-score !pip install -q git+https://github.com/keras-team/keras-nlp.git –upgrade Let’s also define parameters/hyperparameters.","code":"import keras_nlp import pathlib import random  import keras as keras from keras import ops  import tensorflow.data as tf_data from tensorflow_text.tools.wordpiece_vocab import (     bert_vocab_from_dataset as bert_vocab, ) BATCH_SIZE = 64 EPOCHS = 1  # This should be at least 10 for convergence MAX_SEQUENCE_LENGTH = 40 ENG_VOCAB_SIZE = 15000 SPA_VOCAB_SIZE = 15000  EMBED_DIM = 256 INTERMEDIATE_DIM = 2048 NUM_HEADS = 8"},{"path":"https://keras.posit.co/articles/examples/neural_machine_translation_with_keras_nlp.html","id":"downloading-the-data","dir":"Articles > Examples","previous_headings":"","what":"Downloading the data","title":"English-to-Spanish translation with KerasNLP","text":"’ll working English--Spanish translation dataset provided Anki. Let’s download :","code":"text_file = keras.utils.get_file(     fname=\"spa-eng.zip\",     origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",     extract=True, ) text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""},{"path":"https://keras.posit.co/articles/examples/neural_machine_translation_with_keras_nlp.html","id":"parsing-the-data","dir":"Articles > Examples","previous_headings":"","what":"Parsing the data","title":"English-to-Spanish translation with KerasNLP","text":"line contains English sentence corresponding Spanish sentence. English sentence source sequence Spanish one target sequence. adding text list, convert lowercase. ’s sentence pairs look like: Now, let’s split sentence pairs training set, validation set, test set.","code":"with open(text_file) as f:     lines = f.read().split(\"\\n\")[:-1] text_pairs = [] for line in lines:     eng, spa = line.split(\"\\t\")     eng = eng.lower()     spa = spa.lower()     text_pairs.append((eng, spa)) for _ in range(5):     print(random.choice(text_pairs)) random.shuffle(text_pairs) num_val_samples = int(0.15 * len(text_pairs)) num_train_samples = len(text_pairs) - 2 * num_val_samples train_pairs = text_pairs[:num_train_samples] val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples] test_pairs = text_pairs[num_train_samples + num_val_samples :]  print(f\"{len(text_pairs)} total pairs\") print(f\"{len(train_pairs)} training pairs\") print(f\"{len(val_pairs)} validation pairs\") print(f\"{len(test_pairs)} test pairs\")"},{"path":"https://keras.posit.co/articles/examples/neural_machine_translation_with_keras_nlp.html","id":"tokenizing-the-data","dir":"Articles > Examples","previous_headings":"","what":"Tokenizing the data","title":"English-to-Spanish translation with KerasNLP","text":"’ll define two tokenizers - one source language (English), target language (Spanish). ’ll using keras_nlp.tokenizers.WordPieceTokenizer tokenize text. keras_nlp.tokenizers.WordPieceTokenizer takes WordPiece vocabulary functions tokenizing text, detokenizing sequences tokens. define two tokenizers, first need train dataset . WordPiece tokenization algorithm subword tokenization algorithm; training corpus gives us vocabulary subwords. subword tokenizer compromise word tokenizers (word tokenizers need large vocabularies good coverage input words), character tokenizers (characters don’t really encode meaning like words ). Luckily, KerasNLP makes simple train WordPiece corpus keras_nlp.tokenizers.compute_word_piece_vocabulary utility. Every vocabulary special, reserved tokens. four tokens: \"[PAD]\" - Padding token. Padding tokens appended input sequence length input sequence length shorter maximum sequence length. \"[UNK]\" - Unknown token. \"[START]\" - Token marks start input sequence. \"[END]\" - Token marks end input sequence. Let’s see tokens! Now, let’s define tokenizers. configure tokenizers vocabularies trained . Let’s try tokenize sample dataset! verify whether text tokenized correctly, can also detokenize list tokens back original text.","code":"def train_word_piece(text_samples, vocab_size, reserved_tokens):     word_piece_ds = tf_data.Dataset.from_tensor_slices(text_samples)     vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(         word_piece_ds.batch(1000).prefetch(2),         vocabulary_size=vocab_size,         reserved_tokens=reserved_tokens,     )     return vocab reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]  eng_samples = [text_pair[0] for text_pair in train_pairs] eng_vocab = train_word_piece(eng_samples, ENG_VOCAB_SIZE, reserved_tokens)  spa_samples = [text_pair[1] for text_pair in train_pairs] spa_vocab = train_word_piece(spa_samples, SPA_VOCAB_SIZE, reserved_tokens) print(\"English Tokens: \", eng_vocab[100:110]) print(\"Spanish Tokens: \", spa_vocab[100:110]) eng_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(     vocabulary=eng_vocab, lowercase=False ) spa_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(     vocabulary=spa_vocab, lowercase=False ) eng_input_ex = text_pairs[0][0] eng_tokens_ex = eng_tokenizer.tokenize(eng_input_ex) print(\"English sentence: \", eng_input_ex) print(\"Tokens: \", eng_tokens_ex) print(     \"Recovered text after detokenizing: \",     eng_tokenizer.detokenize(eng_tokens_ex), )  print()  spa_input_ex = text_pairs[0][1] spa_tokens_ex = spa_tokenizer.tokenize(spa_input_ex) print(\"Spanish sentence: \", spa_input_ex) print(\"Tokens: \", spa_tokens_ex) print(     \"Recovered text after detokenizing: \",     spa_tokenizer.detokenize(spa_tokens_ex), )"},{"path":"https://keras.posit.co/articles/examples/neural_machine_translation_with_keras_nlp.html","id":"format-datasets","dir":"Articles > Examples","previous_headings":"","what":"Format datasets","title":"English-to-Spanish translation with KerasNLP","text":"Next, ’ll format datasets. training step, model seek predict target words N+1 (beyond) using source sentence target words 0 N. , training dataset yield tuple (inputs, targets), : inputs dictionary keys encoder_inputs decoder_inputs. encoder_inputs tokenized source sentence decoder_inputs target sentence “far”, say, words 0 N used predict word N+1 (beyond) target sentence. target target sentence offset one step: provides next words target sentence – model try predict. add special tokens, \"[START]\" \"[END]\", input Spanish sentence tokenizing text. also pad input fixed length. can easily done using keras_nlp.layers.StartEndPacker. Let’s take quick look sequence shapes (batches 64 pairs, sequences 40 steps long):","code":"def preprocess_batch(eng, spa):     batch_size = ops.shape(spa)[0]      eng = eng_tokenizer(eng)     spa = spa_tokenizer(spa)      # Pad `eng` to `MAX_SEQUENCE_LENGTH`.     eng_start_end_packer = keras_nlp.layers.StartEndPacker(         sequence_length=MAX_SEQUENCE_LENGTH,         pad_value=eng_tokenizer.token_to_id(\"[PAD]\"),     )     eng = eng_start_end_packer(eng)      # Add special tokens (`\"[START]\"` and `\"[END]\"`) to `spa` and pad it as well.     spa_start_end_packer = keras_nlp.layers.StartEndPacker(         sequence_length=MAX_SEQUENCE_LENGTH + 1,         start_value=spa_tokenizer.token_to_id(\"[START]\"),         end_value=spa_tokenizer.token_to_id(\"[END]\"),         pad_value=spa_tokenizer.token_to_id(\"[PAD]\"),     )     spa = spa_start_end_packer(spa)      return (         {             \"encoder_inputs\": eng,             \"decoder_inputs\": spa[:, :-1],         },         spa[:, 1:],     )   def make_dataset(pairs):     eng_texts, spa_texts = zip(*pairs)     eng_texts = list(eng_texts)     spa_texts = list(spa_texts)     dataset = tf_data.Dataset.from_tensor_slices((eng_texts, spa_texts))     dataset = dataset.batch(BATCH_SIZE)     dataset = dataset.map(preprocess_batch, num_parallel_calls=tf_data.AUTOTUNE)     return dataset.shuffle(2048).prefetch(16).cache()   train_ds = make_dataset(train_pairs) val_ds = make_dataset(val_pairs) for inputs, targets in train_ds.take(1):     print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')     print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')     print(f\"targets.shape: {targets.shape}\")"},{"path":"https://keras.posit.co/articles/examples/neural_machine_translation_with_keras_nlp.html","id":"building-the-model","dir":"Articles > Examples","previous_headings":"","what":"Building the model","title":"English-to-Spanish translation with KerasNLP","text":"Now, let’s move exciting part - defining model! first need embedding layer, .e., vector every token input sequence. embedding layer can initialised randomly. also need positional embedding layer encodes word order sequence. convention add two embeddings. KerasNLP keras_nlp.layers.TokenAndPositionEmbedding layer steps us. sequence--sequence Transformer consists keras_nlp.layers.TransformerEncoder layer keras_nlp.layers.TransformerDecoder layer chained together. source sequence passed keras_nlp.layers.TransformerEncoder, produce new representation . new representation passed keras_nlp.layers.TransformerDecoder, together target sequence far (target words 0 N). keras_nlp.layers.TransformerDecoder seek predict next words target sequence (N+1 beyond). key detail makes possible causal masking. keras_nlp.layers.TransformerDecoder sees entire sequence , thus must make sure uses information target tokens 0 N predicting token N+1 (otherwise, use information future, result model used inference time). Causal masking enabled default keras_nlp.layers.TransformerDecoder. also need mask padding tokens (\"[PAD]\"). , can set mask_zero argument keras_nlp.layers.TokenAndPositionEmbedding layer True. propagated subsequent layers.","code":"# Encoder encoder_inputs = keras.Input(     shape=(None,), dtype=\"int64\", name=\"encoder_inputs\" )  x = keras_nlp.layers.TokenAndPositionEmbedding(     vocabulary_size=ENG_VOCAB_SIZE,     sequence_length=MAX_SEQUENCE_LENGTH,     embedding_dim=EMBED_DIM, )(encoder_inputs)  encoder_outputs = keras_nlp.layers.TransformerEncoder(     intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS )(inputs=x) encoder = keras.Model(encoder_inputs, encoder_outputs)   # Decoder decoder_inputs = keras.Input(     shape=(None,), dtype=\"int64\", name=\"decoder_inputs\" ) encoded_seq_inputs = keras.Input(     shape=(None, EMBED_DIM), name=\"decoder_state_inputs\" )  x = keras_nlp.layers.TokenAndPositionEmbedding(     vocabulary_size=SPA_VOCAB_SIZE,     sequence_length=MAX_SEQUENCE_LENGTH,     embedding_dim=EMBED_DIM, )(decoder_inputs)  x = keras_nlp.layers.TransformerDecoder(     intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS )(decoder_sequence=x, encoder_sequence=encoded_seq_inputs) x = keras.layers.Dropout(0.5)(x) decoder_outputs = keras.layers.Dense(SPA_VOCAB_SIZE, activation=\"softmax\")(x) decoder = keras.Model(     [         decoder_inputs,         encoded_seq_inputs,     ],     decoder_outputs, ) decoder_outputs = decoder([decoder_inputs, encoder_outputs])  transformer = keras.Model(     [encoder_inputs, decoder_inputs],     decoder_outputs,     name=\"transformer\", )"},{"path":"https://keras.posit.co/articles/examples/neural_machine_translation_with_keras_nlp.html","id":"training-our-model","dir":"Articles > Examples","previous_headings":"","what":"Training our model","title":"English-to-Spanish translation with KerasNLP","text":"’ll use accuracy quick way monitor training progress validation data. Note machine translation typically uses BLEU scores well metrics, rather accuracy. However, order use metrics like ROUGE, BLEU, etc. decode probabilities generate text. Text generation computationally expensive, performing training recommended. train 1 epoch, get model actually converge train least 10 epochs.","code":"transformer.summary() transformer.compile(     \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"] ) transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)"},{"path":"https://keras.posit.co/articles/examples/neural_machine_translation_with_keras_nlp.html","id":"decoding-test-sentences-qualitative-analysis","dir":"Articles > Examples","previous_headings":"","what":"Decoding test sentences (qualitative analysis)","title":"English-to-Spanish translation with KerasNLP","text":"Finally, let’s demonstrate translate brand new English sentences. simply feed model tokenized English sentence well target token \"[START]\". model outputs probabilities next token. repeatedly generated next token conditioned tokens generated far, hit token \"[END]\". decoding, use keras_nlp.samplers module KerasNLP. Greedy Decoding text decoding method outputs likely next token time step, .e., token highest probability.","code":"def decode_sequences(input_sentences):     batch_size = 1      # Tokenize the encoder input.     encoder_input_tokens = ops.convert_to_tensor(eng_tokenizer(input_sentences))     if len(encoder_input_tokens[0]) < MAX_SEQUENCE_LENGTH:         pads = ops.full(             (1, MAX_SEQUENCE_LENGTH - len(encoder_input_tokens[0])), 0         )         encoder_input_tokens = ops.concatenate([encoder_input_tokens, pads], 1)      # Define a function that outputs the next token's probability given the     # input sequence.     def next(prompt, cache, index):         logits = transformer([encoder_input_tokens, prompt])[:, index - 1, :]         # Ignore hidden states for now; only needed for contrastive search.         hidden_states = None         return logits, hidden_states, cache      # Build a prompt of length 40 with a start token and padding tokens.     length = 40     start = ops.full((batch_size, 1), spa_tokenizer.token_to_id(\"[START]\"))     pad = ops.full((batch_size, length - 1), spa_tokenizer.token_to_id(\"[PAD]\"))     prompt = ops.concatenate((start, pad), axis=-1)      generated_tokens = keras_nlp.samplers.GreedySampler()(         next,         prompt,         end_token_id=spa_tokenizer.token_to_id(\"[END]\"),         index=1,  # Start sampling after start token.     )     generated_sentences = spa_tokenizer.detokenize(generated_tokens)     return generated_sentences   test_eng_texts = [pair[0] for pair in test_pairs] for i in range(2):     input_sentence = random.choice(test_eng_texts)     translated = decode_sequences([input_sentence])     translated = translated.numpy()[0].decode(\"utf-8\")     translated = (         translated.replace(\"[PAD]\", \"\")         .replace(\"[START]\", \"\")         .replace(\"[END]\", \"\")         .strip()     )     print(f\"** Example {i} **\")     print(input_sentence)     print(translated)     print()"},{"path":"https://keras.posit.co/articles/examples/neural_machine_translation_with_keras_nlp.html","id":"evaluating-our-model-quantitative-analysis","dir":"Articles > Examples","previous_headings":"","what":"Evaluating our model (quantitative analysis)","title":"English-to-Spanish translation with KerasNLP","text":"many metrics used text generation tasks. , evaluate translations generated model, let’s compute ROUGE-1 ROUGE-2 scores. Essentially, ROUGE-N score based number common n-grams reference text generated text. ROUGE-1 ROUGE-2 use number common unigrams bigrams, respectively. calculate score 30 test samples (since decoding expensive process). 10 epochs, scores follows:","code":"rouge_1 = keras_nlp.metrics.RougeN(order=1) rouge_2 = keras_nlp.metrics.RougeN(order=2)  for test_pair in test_pairs[:30]:     input_sentence = test_pair[0]     reference_sentence = test_pair[1]      translated_sentence = decode_sequences([input_sentence])     translated_sentence = translated_sentence.numpy()[0].decode(\"utf-8\")     translated_sentence = (         translated_sentence.replace(\"[PAD]\", \"\")         .replace(\"[START]\", \"\")         .replace(\"[END]\", \"\")         .strip()     )      rouge_1(reference_sentence, translated_sentence)     rouge_2(reference_sentence, translated_sentence)  print(\"ROUGE-1 Score: \", rouge_1.result()) print(\"ROUGE-2 Score: \", rouge_2.result())"},{"path":"https://keras.posit.co/articles/examples/neural_machine_translation_with_transformer.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"English-to-Spanish translation with a sequence-to-sequence Transformer","text":"example, ’ll build sequence--sequence Transformer model, ’ll train English--Spanish machine translation task. ’ll learn : Vectorize text using Keras TextVectorization layer. Implement TransformerEncoder layer, TransformerDecoder layer, PositionalEmbedding layer. Prepare data training sequence--sequence model. Use trained model generate translations never-seen-input sentences (sequence--sequence inference). code featured adapted book Deep Learning Python, Second Edition (chapter 11: Deep learning text). present example fairly barebones, detailed explanations building block works, well theory behind Transformers, recommend reading book.","code":""},{"path":"https://keras.posit.co/articles/examples/neural_machine_translation_with_transformer.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"English-to-Spanish translation with a sequence-to-sequence Transformer","text":"","code":"# We set the backend to TensorFlow. The code works with # both `tensorflow` and `torch`. It does not work with JAX # due to the behavior of `jax.numpy.tile` in a jit scope # (used in `TransformerDecoder.get_causal_attention_mask()`: # `tile` in JAX does not support a dynamic `reps` argument. # You can make the code work in JAX by wrapping the # inside of the `get_causal_attention_mask` method in # a decorator to prevent jit compilation: # `with jax.ensure_compile_time_eval():`. import os os[\"KERAS_BACKEND\"] = \"tensorflow\"  import pathlib import random import string import re import numpy as np  import tensorflow.data as tf_data import tensorflow.strings as tf_strings  import keras as keras from keras import layers from keras import ops from keras.layers import TextVectorization"},{"path":"https://keras.posit.co/articles/examples/neural_machine_translation_with_transformer.html","id":"downloading-the-data","dir":"Articles > Examples","previous_headings":"","what":"Downloading the data","title":"English-to-Spanish translation with a sequence-to-sequence Transformer","text":"’ll working English--Spanish translation dataset provided Anki. Let’s download :","code":"text_file = keras.utils.get_file(     fname=\"spa-eng.zip\",     origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",     extract=True, ) text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""},{"path":"https://keras.posit.co/articles/examples/neural_machine_translation_with_transformer.html","id":"parsing-the-data","dir":"Articles > Examples","previous_headings":"","what":"Parsing the data","title":"English-to-Spanish translation with a sequence-to-sequence Transformer","text":"line contains English sentence corresponding Spanish sentence. English sentence source sequence Spanish one target sequence. prepend token \"[start]\" append token \"[end]\" Spanish sentence. ’s sentence pairs look like: Now, let’s split sentence pairs training set, validation set, test set.","code":"with open(text_file) as f:     lines = f.read().split(\"\\n\")[:-1] text_pairs = [] for line in lines:     eng, spa = line.split(\"\\t\")     spa = \"[start] \" + spa + \" [end]\"     text_pairs.append((eng, spa)) for _ in range(5):     print(random.choice(text_pairs)) random.shuffle(text_pairs) num_val_samples = int(0.15 * len(text_pairs)) num_train_samples = len(text_pairs) - 2 * num_val_samples train_pairs = text_pairs[:num_train_samples] val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples] test_pairs = text_pairs[num_train_samples + num_val_samples :]  print(f\"{len(text_pairs)} total pairs\") print(f\"{len(train_pairs)} training pairs\") print(f\"{len(val_pairs)} validation pairs\") print(f\"{len(test_pairs)} test pairs\")"},{"path":"https://keras.posit.co/articles/examples/neural_machine_translation_with_transformer.html","id":"vectorizing-the-text-data","dir":"Articles > Examples","previous_headings":"","what":"Vectorizing the text data","title":"English-to-Spanish translation with a sequence-to-sequence Transformer","text":"’ll use two instances TextVectorization layer vectorize text data (one English one Spanish), say, turn original strings integer sequences integer represents index word vocabulary. English layer use default string standardization (strip punctuation characters) splitting scheme (split whitespace), Spanish layer use custom standardization, add character \"¿\" set punctuation characters stripped. Note: production-grade machine translation model, recommend stripping punctuation characters either language. Instead, recommend turning punctuation character token, achieve providing custom split function TextVectorization layer. Next, ’ll format datasets. training step, model seek predict target words N+1 (beyond) using source sentence target words 0 N. , training dataset yield tuple (inputs, targets), : inputs dictionary keys encoder_inputs decoder_inputs. encoder_inputs vectorized source sentence encoder_inputs target sentence “far”, say, words 0 N used predict word N+1 (beyond) target sentence. target target sentence offset one step: provides next words target sentence – model try predict. Let’s take quick look sequence shapes (batches 64 pairs, sequences 20 steps long):","code":"strip_chars = string.punctuation + \"¿\" strip_chars = strip_chars.replace(\"[\", \"\") strip_chars = strip_chars.replace(\"]\", \"\")  vocab_size = 15000 sequence_length = 20 batch_size = 64   def custom_standardization(input_string):     lowercase = tf_strings.lower(input_string)     return tf_strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")   eng_vectorization = TextVectorization(     max_tokens=vocab_size,     output_mode=\"int\",     output_sequence_length=sequence_length, ) spa_vectorization = TextVectorization(     max_tokens=vocab_size,     output_mode=\"int\",     output_sequence_length=sequence_length + 1,     standardize=custom_standardization, ) train_eng_texts = [pair[0] for pair in train_pairs] train_spa_texts = [pair[1] for pair in train_pairs] eng_vectorization.adapt(train_eng_texts) spa_vectorization.adapt(train_spa_texts) def format_dataset(eng, spa):     eng = eng_vectorization(eng)     spa = spa_vectorization(spa)     return (         {             \"encoder_inputs\": eng,             \"decoder_inputs\": spa[:, :-1],         },         spa[:, 1:],     )   def make_dataset(pairs):     eng_texts, spa_texts = zip(*pairs)     eng_texts = list(eng_texts)     spa_texts = list(spa_texts)     dataset = tf_data.Dataset.from_tensor_slices((eng_texts, spa_texts))     dataset = dataset.batch(batch_size)     dataset = dataset.map(format_dataset)     return dataset.shuffle(2048).prefetch(16).cache()   train_ds = make_dataset(train_pairs) val_ds = make_dataset(val_pairs) for inputs, targets in train_ds.take(1):     print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')     print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')     print(f\"targets.shape: {targets.shape}\")"},{"path":"https://keras.posit.co/articles/examples/neural_machine_translation_with_transformer.html","id":"building-the-model","dir":"Articles > Examples","previous_headings":"","what":"Building the model","title":"English-to-Spanish translation with a sequence-to-sequence Transformer","text":"sequence--sequence Transformer consists TransformerEncoder TransformerDecoder chained together. make model aware word order, also use PositionalEmbedding layer. source sequence pass TransformerEncoder, produce new representation . new representation passed TransformerDecoder, together target sequence far (target words 0 N). TransformerDecoder seek predict next words target sequence (N+1 beyond). key detail makes possible causal masking (see method get_causal_attention_mask() TransformerDecoder). TransformerDecoder sees entire sequences , thus must make sure uses information target tokens 0 N predicting token N+1 (otherwise, use information future, result model used inference time). Next, assemble end--end model.","code":"import keras.ops as ops  class TransformerEncoder(layers.Layer):     def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):         super().__init__(**kwargs)         self.embed_dim = embed_dim         self.dense_dim = dense_dim         self.num_heads = num_heads         self.attention = layers.MultiHeadAttention(             num_heads=num_heads, key_dim=embed_dim         )         self.dense_proj = keras.Sequential(             [                 layers.Dense(dense_dim, activation=\"relu\"),                 layers.Dense(embed_dim),             ]         )         self.layernorm_1 = layers.LayerNormalization()         self.layernorm_2 = layers.LayerNormalization()         self.supports_masking = True      def call(self, inputs, mask=None):         if mask is not None:             padding_mask = ops.cast(mask[:, None, :], dtype=\"int32\")         else:             padding_mask = None          attention_output = self.attention(             query=inputs, value=inputs, key=inputs, attention_mask=padding_mask         )         proj_input = self.layernorm_1(inputs + attention_output)         proj_output = self.dense_proj(proj_input)         return self.layernorm_2(proj_input + proj_output)      def get_config(self):         config = super().get_config()         config.update(             {                 \"embed_dim\": self.embed_dim,                 \"dense_dim\": self.dense_dim,                 \"num_heads\": self.num_heads,             }         )         return config   class PositionalEmbedding(layers.Layer):     def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):         super().__init__(**kwargs)         self.token_embeddings = layers.Embedding(             input_dim=vocab_size, output_dim=embed_dim         )         self.position_embeddings = layers.Embedding(             input_dim=sequence_length, output_dim=embed_dim         )         self.sequence_length = sequence_length         self.vocab_size = vocab_size         self.embed_dim = embed_dim      def call(self, inputs):         length = ops.shape(inputs)[-1]         positions = ops.arange(0, length, 1)         embedded_tokens = self.token_embeddings(inputs)         embedded_positions = self.position_embeddings(positions)         return embedded_tokens + embedded_positions      def compute_mask(self, inputs, mask=None):         if mask is None:             return None         else:             return ops.not_equal(inputs, 0)      def get_config(self):         config = super().get_config()         config.update(             {                 \"sequence_length\": self.sequence_length,                 \"vocab_size\": self.vocab_size,                 \"embed_dim\": self.embed_dim,             }         )         return config   class TransformerDecoder(layers.Layer):     def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):         super().__init__(**kwargs)         self.embed_dim = embed_dim         self.latent_dim = latent_dim         self.num_heads = num_heads         self.attention_1 = layers.MultiHeadAttention(             num_heads=num_heads, key_dim=embed_dim         )         self.attention_2 = layers.MultiHeadAttention(             num_heads=num_heads, key_dim=embed_dim         )         self.dense_proj = keras.Sequential(             [                 layers.Dense(latent_dim, activation=\"relu\"),                 layers.Dense(embed_dim),             ]         )         self.layernorm_1 = layers.LayerNormalization()         self.layernorm_2 = layers.LayerNormalization()         self.layernorm_3 = layers.LayerNormalization()         self.supports_masking = True      def call(self, inputs, encoder_outputs, mask=None):         causal_mask = self.get_causal_attention_mask(inputs)         if mask is not None:             padding_mask = ops.cast(mask[:, None, :], dtype=\"int32\")             padding_mask = ops.minimum(padding_mask, causal_mask)         else:             padding_mask = None          attention_output_1 = self.attention_1(             query=inputs, value=inputs, key=inputs, attention_mask=causal_mask         )         out_1 = self.layernorm_1(inputs + attention_output_1)          attention_output_2 = self.attention_2(             query=out_1,             value=encoder_outputs,             key=encoder_outputs,             attention_mask=padding_mask,         )         out_2 = self.layernorm_2(out_1 + attention_output_2)          proj_output = self.dense_proj(out_2)         return self.layernorm_3(out_2 + proj_output)      def get_causal_attention_mask(self, inputs):         input_shape = ops.shape(inputs)         batch_size, sequence_length = input_shape[0], input_shape[1]         i = ops.arange(sequence_length)[:, None]         j = ops.arange(sequence_length)         mask = ops.cast(i >= j, dtype=\"int32\")         mask = ops.reshape(mask, (1, input_shape[1], input_shape[1]))         mult = ops.concatenate(             [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])],             axis=0,         )         return ops.tile(mask, mult)      def get_config(self):         config = super().get_config()         config.update(             {                 \"embed_dim\": self.embed_dim,                 \"latent_dim\": self.latent_dim,                 \"num_heads\": self.num_heads,             }         )         return config embed_dim = 256 latent_dim = 2048 num_heads = 8  encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\") x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs) encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x) encoder = keras.Model(encoder_inputs, encoder_outputs)  decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\") encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\") x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs) x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs) x = layers.Dropout(0.5)(x) decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x) decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)  decoder_outputs = decoder([decoder_inputs, encoder_outputs]) transformer = keras.Model(     [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\" )"},{"path":"https://keras.posit.co/articles/examples/neural_machine_translation_with_transformer.html","id":"training-our-model","dir":"Articles > Examples","previous_headings":"","what":"Training our model","title":"English-to-Spanish translation with a sequence-to-sequence Transformer","text":"’ll use accuracy quick way monitor training progress validation data. Note machine translation typically uses BLEU scores well metrics, rather accuracy. train 1 epoch, get model actually converge train least 30 epochs.","code":"epochs = 1  # This should be at least 30 for convergence  transformer.summary() transformer.compile(     \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"] ) transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"},{"path":"https://keras.posit.co/articles/examples/neural_machine_translation_with_transformer.html","id":"decoding-test-sentences","dir":"Articles > Examples","previous_headings":"","what":"Decoding test sentences","title":"English-to-Spanish translation with a sequence-to-sequence Transformer","text":"Finally, let’s demonstrate translate brand new English sentences. simply feed model vectorized English sentence well target token \"[start]\", repeatedly generated next token, hit token \"[end]\". 30 epochs, get results : handed money. [start] ella le pasó el dinero [end] Tom never heard Mary sing. [start] tom nunca ha oído cantar mary [end] Perhaps come tomorrow. [start] tal vez ella vendrá mañana [end] love write. [start] encanta escribir [end] French improving little little. [start] su francés va [UNK] sólo un poco [end] hotel told call . [start] mi hotel dijo que te [UNK] [end]","code":"spa_vocab = spa_vectorization.get_vocabulary() spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab)) max_decoded_sentence_length = 20   def decode_sequence(input_sentence):     tokenized_input_sentence = eng_vectorization([input_sentence])     decoded_sentence = \"[start]\"     for i in range(max_decoded_sentence_length):         tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]         predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])          # ops.argmax(predictions[0, i, :]) is not a concrete value for jax here         sampled_token_index = ops.convert_to_numpy(ops.argmax(predictions[0, i, :])).item(0)         sampled_token = spa_index_lookup[sampled_token_index]         decoded_sentence += \" \" + sampled_token          if sampled_token == \"[end]\":             break     return decoded_sentence   test_eng_texts = [pair[0] for pair in test_pairs] for _ in range(30):     input_sentence = random.choice(test_eng_texts)     translated = decode_sequence(input_sentence)"},{"path":"https://keras.posit.co/articles/examples/neural_style_transfer.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Neural style transfer","text":"Style transfer consists generating image “content” base image, “style” different picture (typically artistic). achieved optimization loss function 3 components: “style loss”, “content loss”, “total variation loss”: total variation loss imposes local spatial continuity pixels combination image, giving visual coherence. style loss deep learning keeps –one defined using deep convolutional neural network. Precisely, consists sum L2 distances Gram matrices representations base image style reference image, extracted different layers convnet (trained ImageNet). general idea capture color/texture information different spatial scales (fairly large scales –defined depth layer considered). content loss L2 distance features base image (extracted deep layer) features combination image, keeping generated image close enough original one. Reference: Neural Algorithm Artistic Style","code":""},{"path":"https://keras.posit.co/articles/examples/neural_style_transfer.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Neural style transfer","text":"","code":"import numpy as np import tensorflow as tf import keras as keras from keras.applications import vgg19  base_image_path = keras.utils.get_file(     \"paris.jpg\", \"https://i.imgur.com/F28w3Ac.jpg\" ) style_reference_image_path = keras.utils.get_file(     \"starry_night.jpg\", \"https://i.imgur.com/9ooB60I.jpg\" ) result_prefix = \"paris_generated\"  # Weights of the different loss components total_variation_weight = 1e-6 style_weight = 1e-6 content_weight = 2.5e-8  # Dimensions of the generated picture. width, height = keras.utils.load_img(base_image_path).size img_nrows = 400 img_ncols = int(width * img_nrows / height)"},{"path":"https://keras.posit.co/articles/examples/neural_style_transfer.html","id":"lets-take-a-look-at-our-base-content-image-and-our-style-reference-image","dir":"Articles > Examples","previous_headings":"","what":"Let’s take a look at our base (content) image and our style reference image","title":"Neural style transfer","text":"","code":"from IPython.display import Image, display  display(Image(base_image_path)) display(Image(style_reference_image_path))"},{"path":"https://keras.posit.co/articles/examples/neural_style_transfer.html","id":"image-preprocessing-deprocessing-utilities","dir":"Articles > Examples","previous_headings":"","what":"Image preprocessing / deprocessing utilities","title":"Neural style transfer","text":"","code":"def preprocess_image(image_path):     # Util function to open, resize and format pictures into appropriate tensors     img = keras.utils.load_img(image_path, target_size=(img_nrows, img_ncols))     img = keras.utils.img_to_array(img)     img = np.expand_dims(img, axis=0)     img = vgg19.preprocess_input(img)     return tf.convert_to_tensor(img)   def deprocess_image(x):     # Util function to convert a tensor into a valid image     x = x.reshape((img_nrows, img_ncols, 3))     # Remove zero-center by mean pixel     x[:, :, 0] += 103.939     x[:, :, 1] += 116.779     x[:, :, 2] += 123.68     # 'BGR'->'RGB'     x = x[:, :, ::-1]     x = np.clip(x, 0, 255).astype(\"uint8\")     return x"},{"path":"https://keras.posit.co/articles/examples/neural_style_transfer.html","id":"compute-the-style-transfer-loss","dir":"Articles > Examples","previous_headings":"","what":"Compute the style transfer loss","title":"Neural style transfer","text":"First, need define 4 utility functions: gram_matrix (used compute style loss) style_loss function, keeps generated image close local textures style reference image content_loss function, keeps high-level representation generated image close base image total_variation_loss function, regularization loss keeps generated image locally-coherent Next, let’s create feature extraction model retrieves intermediate activations VGG19 (dict, name). Finally, ’s code computes style transfer loss.","code":"# The gram matrix of an image tensor (feature-wise outer product)   def gram_matrix(x):     x = tf.transpose(x, (2, 0, 1))     features = tf.reshape(x, (tf.shape(x)[0], -1))     gram = tf.matmul(features, tf.transpose(features))     return gram   # The \"style loss\" is designed to maintain # the style of the reference image in the generated image. # It is based on the gram matrices (which capture style) of # feature maps from the style reference image # and from the generated image   def style_loss(style, combination):     S = gram_matrix(style)     C = gram_matrix(combination)     channels = 3     size = img_nrows * img_ncols     return tf.reduce_sum(tf.square(S - C)) / (         4.0 * (channels**2) * (size**2)     )   # An auxiliary loss function # designed to maintain the \"content\" of the # base image in the generated image   def content_loss(base, combination):     return tf.reduce_sum(tf.square(combination - base))   # The 3rd loss function, total variation loss, # designed to keep the generated image locally coherent   def total_variation_loss(x):     a = tf.square(         x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, 1:, : img_ncols - 1, :]     )     b = tf.square(         x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, : img_nrows - 1, 1:, :]     )     return tf.reduce_sum(tf.pow(a + b, 1.25)) # Build a VGG19 model loaded with pre-trained ImageNet weights model = vgg19.VGG19(weights=\"imagenet\", include_top=False)  # Get the symbolic outputs of each \"key\" layer (we gave them unique names). outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])  # Set up a model that returns the activation values for every layer in # VGG19 (as a dict). feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict) # List of layers to use for the style loss. style_layer_names = [     \"block1_conv1\",     \"block2_conv1\",     \"block3_conv1\",     \"block4_conv1\",     \"block5_conv1\", ] # The layer to use for the content loss. content_layer_name = \"block5_conv2\"   def compute_loss(combination_image, base_image, style_reference_image):     input_tensor = tf.concat(         [base_image, style_reference_image, combination_image], axis=0     )     features = feature_extractor(input_tensor)      # Initialize the loss     loss = tf.zeros(shape=())      # Add content loss     layer_features = features[content_layer_name]     base_image_features = layer_features[0, :, :, :]     combination_features = layer_features[2, :, :, :]     loss = loss + content_weight * content_loss(         base_image_features, combination_features     )     # Add style loss     for layer_name in style_layer_names:         layer_features = features[layer_name]         style_reference_features = layer_features[1, :, :, :]         combination_features = layer_features[2, :, :, :]         sl = style_loss(style_reference_features, combination_features)         loss += (style_weight / len(style_layer_names)) * sl      # Add total variation loss     loss += total_variation_weight * total_variation_loss(combination_image)     return loss"},{"path":"https://keras.posit.co/articles/examples/neural_style_transfer.html","id":"add-a-tf-function-decorator-to-loss-gradient-computation","dir":"Articles > Examples","previous_headings":"","what":"Add a tf.function decorator to loss & gradient computation","title":"Neural style transfer","text":"compile , thus make fast.","code":"@tf.function def compute_loss_and_grads(     combination_image, base_image, style_reference_image ):     with tf.GradientTape() as tape:         loss = compute_loss(             combination_image, base_image, style_reference_image         )     grads = tape.gradient(loss, combination_image)     return loss, grads"},{"path":"https://keras.posit.co/articles/examples/neural_style_transfer.html","id":"the-training-loop","dir":"Articles > Examples","previous_headings":"","what":"The training loop","title":"Neural style transfer","text":"Repeatedly run vanilla gradient descent steps minimize loss, save resulting image every 100 iterations. decay learning rate 0.96 every 100 steps. 4000 iterations, get following result: Example available HuggingFace Trained Model | Demo — | —  |","code":"optimizer = keras.optimizers.SGD(     keras.optimizers.schedules.ExponentialDecay(         initial_learning_rate=100.0, decay_steps=100, decay_rate=0.96     ) )  base_image = preprocess_image(base_image_path) style_reference_image = preprocess_image(style_reference_image_path) combination_image = tf.Variable(preprocess_image(base_image_path))  iterations = 4000 for i in range(1, iterations + 1):     loss, grads = compute_loss_and_grads(         combination_image, base_image, style_reference_image     )     optimizer.apply_gradients([(grads, combination_image)])     if i % 100 == 0:         print(\"Iteration %d: loss=%.2f\" % (i, loss))         img = deprocess_image(combination_image.numpy())         fname = result_prefix + \"_at_iteration_%d.png\" % i         keras.utils.save_img(fname, img) display(Image(result_prefix + \"_at_iteration_4000.png\"))"},{"path":"https://keras.posit.co/articles/examples/object_detection_using_vision_transformer.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Object detection with Vision Transformers","text":"article Vision Transformer (ViT) architecture Alexey Dosovitskiy et al. demonstrates pure transformer applied directly sequences image patches can perform well object detection tasks. Keras example, implement object detection ViT train Caltech 101 dataset detect airplane given image. example requires TensorFlow 2.4 higher, TensorFlow Addons, import AdamW optimizer. TensorFlow Addons can installed via following command:","code":"pip install -U git+https://github.com/keras-team/keras"},{"path":"https://keras.posit.co/articles/examples/object_detection_using_vision_transformer.html","id":"imports-and-setup","dir":"Articles > Examples","previous_headings":"","what":"Imports and setup","title":"Object detection with Vision Transformers","text":"","code":"import os  os.environ[\"KERAS_BACKEND\"] = \"jax\"  # @param [\"tensorflow\", \"jax\", \"torch\"]   import numpy as np import keras as keras from keras import layers from keras import ops import matplotlib.pyplot as plt import numpy as np import cv2 import os import scipy.io import shutil"},{"path":"https://keras.posit.co/articles/examples/object_detection_using_vision_transformer.html","id":"prepare-dataset","dir":"Articles > Examples","previous_headings":"","what":"Prepare dataset","title":"Object detection with Vision Transformers","text":"use Caltech 101 Dataset.","code":"# Path to images and annotations path_images = \"/101_ObjectCategories/airplanes/\" path_annot = \"/Annotations/Airplanes_Side_2/\"  path_to_downloaded_file = keras.utils.get_file(     fname=\"caltech_101_zipped\",     origin=\"https://data.caltech.edu/records/mzrjq-6wc02/files/caltech-101.zip\",     extract=True,     archive_format=\"zip\",  # downloaded file format     cache_dir=\"/\",  # cache and extract in current directory )  # Extracting tar files found inside main zip file shutil.unpack_archive(\"/datasets/caltech-101/101_ObjectCategories.tar.gz\", \"/\") shutil.unpack_archive(\"/datasets/caltech-101/Annotations.tar\", \"/\")  # list of paths to images and annotations image_paths = [     f for f in os.listdir(path_images) if os.path.isfile(os.path.join(path_images, f)) ] annot_paths = [     f for f in os.listdir(path_annot) if os.path.isfile(os.path.join(path_annot, f)) ]  image_paths.sort() annot_paths.sort()  image_size = 224  # resize input images to this size  images, targets = [], []  # loop over the annotations and images, preprocess them and store in lists for i in range(0, len(annot_paths)):     # Access bounding box coordinates     annot = scipy.io.loadmat(path_annot + annot_paths[i])[\"box_coord\"][0]      top_left_x, top_left_y = annot[2], annot[0]     bottom_right_x, bottom_right_y = annot[3], annot[1]      image = keras.utils.load_img(         path_images + image_paths[i],     )     (w, h) = image.size[:2]      # resize train set images     if i < int(len(annot_paths) * 0.8):         # resize image if it is for training dataset         image = image.resize((image_size, image_size))      # convert image to array and append to list     images.append(keras.utils.img_to_array(image))      # apply relative scaling to bounding boxes as per given image and append to list     targets.append(         (             float(top_left_x) / w,             float(top_left_y) / h,             float(bottom_right_x) / w,             float(bottom_right_y) / h,         )     )  # Convert the list to numpy array, split to train and test dataset (x_train), (y_train) = (     np.asarray(images[: int(len(images) * 0.8)]),     np.asarray(targets[: int(len(targets) * 0.8)]), ) (x_test), (y_test) = (     np.asarray(images[int(len(images) * 0.8) :]),     np.asarray(targets[int(len(targets) * 0.8) :]), )"},{"path":"https://keras.posit.co/articles/examples/object_detection_using_vision_transformer.html","id":"implement-multilayer-perceptron-mlp","dir":"Articles > Examples","previous_headings":"","what":"Implement multilayer-perceptron (MLP)","title":"Object detection with Vision Transformers","text":"use code Keras example Image classification Vision Transformer reference.","code":"def mlp(x, hidden_units, dropout_rate):     for units in hidden_units:         x = layers.Dense(units, activation=keras.activations.gelu)(x)         x = layers.Dropout(dropout_rate)(x)     return x"},{"path":"https://keras.posit.co/articles/examples/object_detection_using_vision_transformer.html","id":"implement-the-patch-creation-layer","dir":"Articles > Examples","previous_headings":"","what":"Implement the patch creation layer","title":"Object detection with Vision Transformers","text":"","code":"class Patches(layers.Layer):     def __init__(self, patch_size):         super().__init__()         self.patch_size = patch_size      def call(self, images):         input_shape = ops.shape(images)         batch_size = input_shape[0]         height = input_shape[1]         width = input_shape[2]         channels = input_shape[3]         num_patches_h = height // self.patch_size         num_patches_w = width // self.patch_size         patches = keras.ops.image.extract_patches(images, size=self.patch_size)         patches = ops.reshape(             patches,             (                 batch_size,                 num_patches_h * num_patches_w,                 self.patch_size * self.patch_size * channels,             ),         )         return patches      def get_config(self):         config = super().get_config()         config.update({\"patch_size\": self.patch_size})         return config"},{"path":"https://keras.posit.co/articles/examples/object_detection_using_vision_transformer.html","id":"display-patches-for-an-input-image","dir":"Articles > Examples","previous_headings":"","what":"Display patches for an input image","title":"Object detection with Vision Transformers","text":"","code":"patch_size = 32  # Size of the patches to be extracted from the input images  plt.figure(figsize=(4, 4)) plt.imshow(x_train[0].astype(\"uint8\")) plt.axis(\"off\")  patches = Patches(patch_size)(np.expand_dims(x_train[0], axis=0)) print(f\"Image size: {image_size} X {image_size}\") print(f\"Patch size: {patch_size} X {patch_size}\") print(f\"{patches.shape[1]} patches per image \\n{patches.shape[-1]} elements per patch\")   n = int(np.sqrt(patches.shape[1])) plt.figure(figsize=(4, 4)) for i, patch in enumerate(patches[0]):     ax = plt.subplot(n, n, i + 1)     patch_img = ops.reshape(patch, (patch_size, patch_size, 3))     plt.imshow(ops.convert_to_numpy(patch_img).astype(\"uint8\"))     plt.axis(\"off\")"},{"path":"https://keras.posit.co/articles/examples/object_detection_using_vision_transformer.html","id":"implement-the-patch-encoding-layer","dir":"Articles > Examples","previous_headings":"","what":"Implement the patch encoding layer","title":"Object detection with Vision Transformers","text":"PatchEncoder layer linearly transforms patch projecting vector size projection_dim. also adds learnable position embedding projected vector.","code":"class PatchEncoder(layers.Layer):     def __init__(self, num_patches, projection_dim):         super().__init__()         self.num_patches = num_patches         self.projection = layers.Dense(units=projection_dim)         self.position_embedding = layers.Embedding(             input_dim=num_patches, output_dim=projection_dim         )      # Override function to avoid error while saving model     def get_config(self):         config = super().get_config().copy()         config.update(             {                 \"input_shape\": input_shape,                 \"patch_size\": patch_size,                 \"num_patches\": num_patches,                 \"projection_dim\": projection_dim,                 \"num_heads\": num_heads,                 \"transformer_units\": transformer_units,                 \"transformer_layers\": transformer_layers,                 \"mlp_head_units\": mlp_head_units,             }         )         return config      def call(self, patch):         positions = ops.expand_dims(             ops.arange(start=0, stop=self.num_patches, step=1), axis=0         )         projected_patches = self.projection(patch)         encoded = projected_patches + self.position_embedding(positions)         return encoded"},{"path":"https://keras.posit.co/articles/examples/object_detection_using_vision_transformer.html","id":"build-the-vit-model","dir":"Articles > Examples","previous_headings":"","what":"Build the ViT model","title":"Object detection with Vision Transformers","text":"ViT model multiple Transformer blocks. MultiHeadAttention layer used self-attention, applied sequence image patches. encoded patches (skip connection) self-attention layer outputs normalized fed multilayer perceptron (MLP). model outputs four dimensions representing bounding box coordinates object.","code":"def create_vit_object_detector(     input_shape,     patch_size,     num_patches,     projection_dim,     num_heads,     transformer_units,     transformer_layers,     mlp_head_units, ):     inputs = keras.Input(shape=input_shape)     # Create patches     patches = Patches(patch_size)(inputs)     # Encode patches     encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)      # Create multiple layers of the Transformer block.     for _ in range(transformer_layers):         # Layer normalization 1.         x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)         # Create a multi-head attention layer.         attention_output = layers.MultiHeadAttention(             num_heads=num_heads, key_dim=projection_dim, dropout=0.1         )(x1, x1)         # Skip connection 1.         x2 = layers.Add()([attention_output, encoded_patches])         # Layer normalization 2.         x3 = layers.LayerNormalization(epsilon=1e-6)(x2)         # MLP         x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)         # Skip connection 2.         encoded_patches = layers.Add()([x3, x2])      # Create a [batch_size, projection_dim] tensor.     representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)     representation = layers.Flatten()(representation)     representation = layers.Dropout(0.3)(representation)     # Add MLP.     features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.3)      bounding_box = layers.Dense(4)(         features     )  # Final four neurons that output bounding box      # return Keras model.     return keras.Model(inputs=inputs, outputs=bounding_box)"},{"path":"https://keras.posit.co/articles/examples/object_detection_using_vision_transformer.html","id":"run-the-experiment","dir":"Articles > Examples","previous_headings":"","what":"Run the experiment","title":"Object detection with Vision Transformers","text":"","code":"def run_experiment(model, learning_rate, weight_decay, batch_size, num_epochs):      optimizer = keras.optimizers.AdamW(         learning_rate=learning_rate, weight_decay=weight_decay     )      # Compile model.     model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError())      checkpoint_filepath = \"vit_object_detector.weights.h5\"     checkpoint_callback = keras.callbacks.ModelCheckpoint(         checkpoint_filepath,         monitor=\"val_loss\",         save_best_only=True,         save_weights_only=True,     )      history = model.fit(         x=x_train,         y=y_train,         batch_size=batch_size,         epochs=num_epochs,         validation_split=0.1,         callbacks=[             checkpoint_callback,             keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10),         ],     )      return history   input_shape = (image_size, image_size, 3)  # input image shape learning_rate = 0.001 weight_decay = 0.0001 batch_size = 32 num_epochs = 15 num_patches = (image_size // patch_size) ** 2 projection_dim = 64 num_heads = 4 # Size of the transformer layers transformer_units = [     projection_dim * 2,     projection_dim, ] transformer_layers = 4 mlp_head_units = [2048, 1024, 512, 64, 32]  # Size of the dense layers   history = [] num_patches = (image_size // patch_size) ** 2  vit_object_detector = create_vit_object_detector(     input_shape,     patch_size,     num_patches,     projection_dim,     num_heads,     transformer_units,     transformer_layers,     mlp_head_units, )  # Train model history = run_experiment(     vit_object_detector, learning_rate, weight_decay, batch_size, num_epochs )   def plot_history(item):     plt.plot(history.history[item], label=item)     plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)     plt.xlabel(\"Epochs\")     plt.ylabel(item)     plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)     plt.legend()     plt.grid()     plt.show()   plot_history(\"loss\")"},{"path":"https://keras.posit.co/articles/examples/object_detection_using_vision_transformer.html","id":"evaluate-the-model","dir":"Articles > Examples","previous_headings":"","what":"Evaluate the model","title":"Object detection with Vision Transformers","text":"example demonstrates pure Transformer can trained predict bounding boxes object given image, thus extending use Transformers object detection tasks. model can improved tuning hyper-parameters pre-training.","code":"import matplotlib.patches as patches  # Saves the model in current path vit_object_detector.save(\"vit_object_detector.keras\")  # To calculate IoU (intersection over union, given two bounding boxes) def bounding_box_intersection_over_union(box_predicted, box_truth):     # get (x, y) coordinates of intersection of bounding boxes     top_x_intersect = max(box_predicted[0], box_truth[0])     top_y_intersect = max(box_predicted[1], box_truth[1])     bottom_x_intersect = min(box_predicted[2], box_truth[2])     bottom_y_intersect = min(box_predicted[3], box_truth[3])      # calculate area of the intersection bb (bounding box)     intersection_area = max(0, bottom_x_intersect - top_x_intersect + 1) * max(         0, bottom_y_intersect - top_y_intersect + 1     )      # calculate area of the prediction bb and ground-truth bb     box_predicted_area = (box_predicted[2] - box_predicted[0] + 1) * (         box_predicted[3] - box_predicted[1] + 1     )     box_truth_area = (box_truth[2] - box_truth[0] + 1) * (         box_truth[3] - box_truth[1] + 1     )      # calculate intersection over union by taking intersection     # area and dividing it by the sum of predicted bb and ground truth     # bb areas subtracted by  the interesection area      # return ioU     return intersection_area / float(         box_predicted_area + box_truth_area - intersection_area     )   i, mean_iou = 0, 0  # Compare results for 10 images in the test set for input_image in x_test[:10]:     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 15))     im = input_image      # Display the image     ax1.imshow(im.astype(\"uint8\"))     ax2.imshow(im.astype(\"uint8\"))      input_image = cv2.resize(         input_image, (image_size, image_size), interpolation=cv2.INTER_AREA     )     input_image = np.expand_dims(input_image, axis=0)     preds = vit_object_detector.predict(input_image)[0]      (h, w) = (im).shape[0:2]      top_left_x, top_left_y = int(preds[0] * w), int(preds[1] * h)      bottom_right_x, bottom_right_y = int(preds[2] * w), int(preds[3] * h)      box_predicted = [top_left_x, top_left_y, bottom_right_x, bottom_right_y]     # Create the bounding box     rect = patches.Rectangle(         (top_left_x, top_left_y),         bottom_right_x - top_left_x,         bottom_right_y - top_left_y,         facecolor=\"none\",         edgecolor=\"red\",         linewidth=1,     )     # Add the bounding box to the image     ax1.add_patch(rect)     ax1.set_xlabel(         \"Predicted: \"         + str(top_left_x)         + \", \"         + str(top_left_y)         + \", \"         + str(bottom_right_x)         + \", \"         + str(bottom_right_y)     )      top_left_x, top_left_y = int(y_test[i][0] * w), int(y_test[i][1] * h)      bottom_right_x, bottom_right_y = int(y_test[i][2] * w), int(y_test[i][3] * h)      box_truth = top_left_x, top_left_y, bottom_right_x, bottom_right_y      mean_iou += bounding_box_intersection_over_union(box_predicted, box_truth)     # Create the bounding box     rect = patches.Rectangle(         (top_left_x, top_left_y),         bottom_right_x - top_left_x,         bottom_right_y - top_left_y,         facecolor=\"none\",         edgecolor=\"red\",         linewidth=1,     )     # Add the bounding box to the image     ax2.add_patch(rect)     ax2.set_xlabel(         \"Target: \"         + str(top_left_x)         + \", \"         + str(top_left_y)         + \", \"         + str(bottom_right_x)         + \", \"         + str(bottom_right_y)         + \"\\n\"         + \"IoU\"         + str(bounding_box_intersection_over_union(box_predicted, box_truth))     )     i = i + 1  print(\"mean_iou: \" + str(mean_iou / len(x_test[:10]))) plt.show()"},{"path":"https://keras.posit.co/articles/examples/oxford_pets_image_segmentation.html","id":"download-the-data","dir":"Articles > Examples","previous_headings":"","what":"Download the data","title":"Image segmentation with a U-Net-like architecture","text":"","code":"options(timeout = 5000) download.file(   \"https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\",   \"datasets/images.tar.gz\" ) download.file(   \"https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\",   \"datasets/annotations.tar.gz\" )  untar(\"datasets/images.tar.gz\", exdir = \"datasets\") untar(\"datasets/annotations.tar.gz\", exdir = \"datasets\")"},{"path":"https://keras.posit.co/articles/examples/oxford_pets_image_segmentation.html","id":"prepare-paths-of-input-images-and-target-segmentation-masks","dir":"Articles > Examples","previous_headings":"","what":"Prepare paths of input images and target segmentation masks","title":"Image segmentation with a U-Net-like architecture","text":"","code":"library(keras3) input_dir <- \"datasets/images/\" target_dir <- \"datasets/annotations/trimaps/\" img_size <- c(160, 160) num_classes <- 3 batch_size <- 32  input_img_paths <- fs::dir_ls(input_dir, glob = \"*.jpg\") %>% sort() target_img_paths <- fs::dir_ls(target_dir, glob = \"*.png\") %>% sort()  cat(\"Number of samples:\", length(input_img_paths), \"\\n\") ## Number of samples: 7390 for (i in 1:10) {   cat(input_img_paths[i], \"|\", target_img_paths[i], \"\\n\") } ## datasets/images/Abyssinian_1.jpg | datasets/annotations/trimaps/Abyssinian_1.png ## datasets/images/Abyssinian_10.jpg | datasets/annotations/trimaps/Abyssinian_10.png ## datasets/images/Abyssinian_100.jpg | datasets/annotations/trimaps/Abyssinian_100.png ## datasets/images/Abyssinian_101.jpg | datasets/annotations/trimaps/Abyssinian_101.png ## datasets/images/Abyssinian_102.jpg | datasets/annotations/trimaps/Abyssinian_102.png ## datasets/images/Abyssinian_103.jpg | datasets/annotations/trimaps/Abyssinian_103.png ## datasets/images/Abyssinian_104.jpg | datasets/annotations/trimaps/Abyssinian_104.png ## datasets/images/Abyssinian_105.jpg | datasets/annotations/trimaps/Abyssinian_105.png ## datasets/images/Abyssinian_106.jpg | datasets/annotations/trimaps/Abyssinian_106.png ## datasets/images/Abyssinian_107.jpg | datasets/annotations/trimaps/Abyssinian_107.png"},{"path":"https://keras.posit.co/articles/examples/oxford_pets_image_segmentation.html","id":"what-does-one-input-image-and-corresponding-segmentation-mask-look-like","dir":"Articles > Examples","previous_headings":"","what":"What does one input image and corresponding segmentation mask look like?","title":"Image segmentation with a U-Net-like architecture","text":"","code":"# Display input image #10 input_img_paths[10] %>%   jpeg::readJPEG() %>%   as.raster() %>%   plot() target_img_paths[10] %>%   png::readPNG() %>%   magrittr::multiply_by(255) %>%   as.raster(max = 3) %>%   plot()"},{"path":"https://keras.posit.co/articles/examples/oxford_pets_image_segmentation.html","id":"prepare-dataset-to-load-vectorize-batches-of-data","dir":"Articles > Examples","previous_headings":"","what":"Prepare dataset to load & vectorize batches of data","title":"Image segmentation with a U-Net-like architecture","text":"","code":"library(tensorflow) ## ## Attaching package: 'tensorflow' ## The following objects are masked from 'package:keras3': ## ##     set_random_seed, shape library(tfdatasets) ## ## Attaching package: 'tfdatasets' ## The following object is masked from 'package:keras3': ## ##     shape # Returns a tf_dataset get_dataset <- function(batch_size, img_size, input_img_paths, target_img_paths,                         max_dataset_len = NULL) {    img_size <- as.integer(img_size)    load_img_masks <- function(input_img_path, target_img_path) {     input_img <- input_img_path %>%       tf$io$read_file() %>%       tf$io$decode_jpeg(channels = 3) %>%       tf$image$resize(img_size) %>%       tf$image$convert_image_dtype(\"float32\")      target_img <- target_img_path %>%       tf$io$read_file() %>%       tf$io$decode_png(channels = 1) %>%       tf$image$resize(img_size, method = \"nearest\") %>%       tf$image$convert_image_dtype(\"uint8\")      # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:     target_img <- target_img - 1L      list(input_img, target_img)   }    if (!is.null(max_dataset_len)) {     input_img_paths <- input_img_paths[1:max_dataset_len]     target_img_paths <- target_img_paths[1:max_dataset_len]   }    list(input_img_paths, target_img_paths) %>%     tensor_slices_dataset() %>%     dataset_map(load_img_masks, num_parallel_calls = tf$data$AUTOTUNE) %>%     dataset_batch(batch_size) }"},{"path":"https://keras.posit.co/articles/examples/oxford_pets_image_segmentation.html","id":"prepare-u-net-xception-style-model","dir":"Articles > Examples","previous_headings":"","what":"Prepare U-Net Xception-style model","title":"Image segmentation with a U-Net-like architecture","text":"","code":"get_model <- function(img_size, num_classes) {    inputs <- layer_input(shape = c(img_size, 3))    ### [First half of the network: downsampling inputs] ###    # Entry block   x <- inputs %>%     layer_conv_2d(filters = 32, kernel_size = 3, strides = 2, padding = \"same\") %>%     layer_batch_normalization() %>%     layer_activation(\"relu\")    previous_block_activation <- x  # Set aside residual    for (filters in c(64, 128, 256)) {     x <- x %>%       layer_activation(\"relu\") %>%       layer_separable_conv_2d(filters = filters, kernel_size = 3, padding = \"same\") %>%       layer_batch_normalization() %>%        layer_activation(\"relu\") %>%       layer_separable_conv_2d(filters = filters, kernel_size = 3, padding = \"same\") %>%       layer_batch_normalization() %>%        layer_max_pooling_2d(pool_size = 3, strides = 2, padding = \"same\")      residual <- previous_block_activation %>%       layer_conv_2d(filters = filters, kernel_size = 1, strides = 2, padding = \"same\")      x <- layer_add(list(x, residual))  # Add back residual     previous_block_activation <- x  # Set aside next residual   }    ### [Second half of the network: upsampling inputs] ###    for (filters in c(256, 128, 64, 32)) {     x <- x %>%       layer_activation(\"relu\") %>%       layer_conv_2d_transpose(filters = filters, kernel_size = 3, padding = \"same\") %>%       layer_batch_normalization() %>%        layer_activation(\"relu\") %>%       layer_conv_2d_transpose(filters = filters, kernel_size = 3, padding = \"same\") %>%       layer_batch_normalization() %>%        layer_upsampling_2d(size = 2)      # Project residual     residual <- previous_block_activation %>%       layer_upsampling_2d(size = 2) %>%       layer_conv_2d(filters = filters, kernel_size = 1, padding = \"same\")      x <- layer_add(list(x, residual))  # Add back residual     previous_block_activation <- x  # Set aside next residual   }    # Add a per-pixel classification layer   outputs <- x %>%     layer_conv_2d(num_classes, 3, activation=\"softmax\", padding=\"same\")    # Define the model   keras_model(inputs, outputs) }  # Build model model <- get_model(img_size, num_classes) summary(model) ## Model: \"functional_1\" ## ┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓ ## ┃ Layer (type)        ┃ Output Shape      ┃ Param # ┃ Connected to         ┃ ## ┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩ ## │ input_layer         │ (None, 160, 160,  │       0 │ -                    │ ## │ (InputLayer)        │ 3)                │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ conv2d (Conv2D)     │ (None, 80, 80,    │     896 │ input_layer[0][0]    │ ## │                     │ 32)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ batch_normalization │ (None, 80, 80,    │     128 │ conv2d[0][0]         │ ## │ (BatchNormalizatio… │ 32)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ activation          │ (None, 80, 80,    │       0 │ batch_normalization… │ ## │ (Activation)        │ 32)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ activation_2        │ (None, 80, 80,    │       0 │ activation[0][0]     │ ## │ (Activation)        │ 32)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ separable_conv2d_1  │ (None, 80, 80,    │   2,400 │ activation_2[0][0]   │ ## │ (SeparableConv2D)   │ 64)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ batch_normalizatio… │ (None, 80, 80,    │     256 │ separable_conv2d_1[… │ ## │ (BatchNormalizatio… │ 64)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ activation_1        │ (None, 80, 80,    │       0 │ batch_normalization… │ ## │ (Activation)        │ 64)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ separable_conv2d    │ (None, 80, 80,    │   4,736 │ activation_1[0][0]   │ ## │ (SeparableConv2D)   │ 64)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ batch_normalizatio… │ (None, 80, 80,    │     256 │ separable_conv2d[0]… │ ## │ (BatchNormalizatio… │ 64)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ max_pooling2d       │ (None, 40, 40,    │       0 │ batch_normalization… │ ## │ (MaxPooling2D)      │ 64)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ conv2d_1 (Conv2D)   │ (None, 40, 40,    │   2,112 │ activation[0][0]     │ ## │                     │ 64)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ add (Add)           │ (None, 40, 40,    │       0 │ max_pooling2d[0][0], │ ## │                     │ 64)               │         │ conv2d_1[0][0]       │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ activation_4        │ (None, 40, 40,    │       0 │ add[0][0]            │ ## │ (Activation)        │ 64)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ separable_conv2d_3  │ (None, 40, 40,    │   8,896 │ activation_4[0][0]   │ ## │ (SeparableConv2D)   │ 128)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ batch_normalizatio… │ (None, 40, 40,    │     512 │ separable_conv2d_3[… │ ## │ (BatchNormalizatio… │ 128)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ activation_3        │ (None, 40, 40,    │       0 │ batch_normalization… │ ## │ (Activation)        │ 128)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ separable_conv2d_2  │ (None, 40, 40,    │  17,664 │ activation_3[0][0]   │ ## │ (SeparableConv2D)   │ 128)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ batch_normalizatio… │ (None, 40, 40,    │     512 │ separable_conv2d_2[… │ ## │ (BatchNormalizatio… │ 128)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ max_pooling2d_1     │ (None, 20, 20,    │       0 │ batch_normalization… │ ## │ (MaxPooling2D)      │ 128)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ conv2d_2 (Conv2D)   │ (None, 20, 20,    │   8,320 │ add[0][0]            │ ## │                     │ 128)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ add_1 (Add)         │ (None, 20, 20,    │       0 │ max_pooling2d_1[0][… │ ## │                     │ 128)              │         │ conv2d_2[0][0]       │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ activation_6        │ (None, 20, 20,    │       0 │ add_1[0][0]          │ ## │ (Activation)        │ 128)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ separable_conv2d_5  │ (None, 20, 20,    │  34,176 │ activation_6[0][0]   │ ## │ (SeparableConv2D)   │ 256)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ batch_normalizatio… │ (None, 20, 20,    │   1,024 │ separable_conv2d_5[… │ ## │ (BatchNormalizatio… │ 256)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ activation_5        │ (None, 20, 20,    │       0 │ batch_normalization… │ ## │ (Activation)        │ 256)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ separable_conv2d_4  │ (None, 20, 20,    │  68,096 │ activation_5[0][0]   │ ## │ (SeparableConv2D)   │ 256)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ batch_normalizatio… │ (None, 20, 20,    │   1,024 │ separable_conv2d_4[… │ ## │ (BatchNormalizatio… │ 256)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ max_pooling2d_2     │ (None, 10, 10,    │       0 │ batch_normalization… │ ## │ (MaxPooling2D)      │ 256)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ conv2d_3 (Conv2D)   │ (None, 10, 10,    │  33,024 │ add_1[0][0]          │ ## │                     │ 256)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ add_2 (Add)         │ (None, 10, 10,    │       0 │ max_pooling2d_2[0][… │ ## │                     │ 256)              │         │ conv2d_3[0][0]       │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ activation_8        │ (None, 10, 10,    │       0 │ add_2[0][0]          │ ## │ (Activation)        │ 256)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ conv2d_transpose_1  │ (None, 10, 10,    │ 590,080 │ activation_8[0][0]   │ ## │ (Conv2DTranspose)   │ 256)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ batch_normalizatio… │ (None, 10, 10,    │   1,024 │ conv2d_transpose_1[… │ ## │ (BatchNormalizatio… │ 256)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ activation_7        │ (None, 10, 10,    │       0 │ batch_normalization… │ ## │ (Activation)        │ 256)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ conv2d_transpose    │ (None, 10, 10,    │ 590,080 │ activation_7[0][0]   │ ## │ (Conv2DTranspose)   │ 256)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ batch_normalizatio… │ (None, 10, 10,    │   1,024 │ conv2d_transpose[0]… │ ## │ (BatchNormalizatio… │ 256)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ up_sampling2d_1     │ (None, 20, 20,    │       0 │ add_2[0][0]          │ ## │ (UpSampling2D)      │ 256)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ up_sampling2d       │ (None, 20, 20,    │       0 │ batch_normalization… │ ## │ (UpSampling2D)      │ 256)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ conv2d_4 (Conv2D)   │ (None, 20, 20,    │  65,792 │ up_sampling2d_1[0][… │ ## │                     │ 256)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ add_3 (Add)         │ (None, 20, 20,    │       0 │ up_sampling2d[0][0], │ ## │                     │ 256)              │         │ conv2d_4[0][0]       │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ activation_10       │ (None, 20, 20,    │       0 │ add_3[0][0]          │ ## │ (Activation)        │ 256)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ conv2d_transpose_3  │ (None, 20, 20,    │ 295,040 │ activation_10[0][0]  │ ## │ (Conv2DTranspose)   │ 128)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ batch_normalizatio… │ (None, 20, 20,    │     512 │ conv2d_transpose_3[… │ ## │ (BatchNormalizatio… │ 128)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ activation_9        │ (None, 20, 20,    │       0 │ batch_normalization… │ ## │ (Activation)        │ 128)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ conv2d_transpose_2  │ (None, 20, 20,    │ 147,584 │ activation_9[0][0]   │ ## │ (Conv2DTranspose)   │ 128)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ batch_normalizatio… │ (None, 20, 20,    │     512 │ conv2d_transpose_2[… │ ## │ (BatchNormalizatio… │ 128)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ up_sampling2d_3     │ (None, 40, 40,    │       0 │ add_3[0][0]          │ ## │ (UpSampling2D)      │ 256)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ up_sampling2d_2     │ (None, 40, 40,    │       0 │ batch_normalization… │ ## │ (UpSampling2D)      │ 128)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ conv2d_5 (Conv2D)   │ (None, 40, 40,    │  32,896 │ up_sampling2d_3[0][… │ ## │                     │ 128)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ add_4 (Add)         │ (None, 40, 40,    │       0 │ up_sampling2d_2[0][… │ ## │                     │ 128)              │         │ conv2d_5[0][0]       │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ activation_12       │ (None, 40, 40,    │       0 │ add_4[0][0]          │ ## │ (Activation)        │ 128)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ conv2d_transpose_5  │ (None, 40, 40,    │  73,792 │ activation_12[0][0]  │ ## │ (Conv2DTranspose)   │ 64)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ batch_normalizatio… │ (None, 40, 40,    │     256 │ conv2d_transpose_5[… │ ## │ (BatchNormalizatio… │ 64)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ activation_11       │ (None, 40, 40,    │       0 │ batch_normalization… │ ## │ (Activation)        │ 64)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ conv2d_transpose_4  │ (None, 40, 40,    │  36,928 │ activation_11[0][0]  │ ## │ (Conv2DTranspose)   │ 64)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ batch_normalizatio… │ (None, 40, 40,    │     256 │ conv2d_transpose_4[… │ ## │ (BatchNormalizatio… │ 64)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ up_sampling2d_5     │ (None, 80, 80,    │       0 │ add_4[0][0]          │ ## │ (UpSampling2D)      │ 128)              │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ up_sampling2d_4     │ (None, 80, 80,    │       0 │ batch_normalization… │ ## │ (UpSampling2D)      │ 64)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ conv2d_6 (Conv2D)   │ (None, 80, 80,    │   8,256 │ up_sampling2d_5[0][… │ ## │                     │ 64)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ add_5 (Add)         │ (None, 80, 80,    │       0 │ up_sampling2d_4[0][… │ ## │                     │ 64)               │         │ conv2d_6[0][0]       │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ activation_14       │ (None, 80, 80,    │       0 │ add_5[0][0]          │ ## │ (Activation)        │ 64)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ conv2d_transpose_7  │ (None, 80, 80,    │  18,464 │ activation_14[0][0]  │ ## │ (Conv2DTranspose)   │ 32)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ batch_normalizatio… │ (None, 80, 80,    │     128 │ conv2d_transpose_7[… │ ## │ (BatchNormalizatio… │ 32)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ activation_13       │ (None, 80, 80,    │       0 │ batch_normalization… │ ## │ (Activation)        │ 32)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ conv2d_transpose_6  │ (None, 80, 80,    │   9,248 │ activation_13[0][0]  │ ## │ (Conv2DTranspose)   │ 32)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ batch_normalizatio… │ (None, 80, 80,    │     128 │ conv2d_transpose_6[… │ ## │ (BatchNormalizatio… │ 32)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ up_sampling2d_7     │ (None, 160, 160,  │       0 │ add_5[0][0]          │ ## │ (UpSampling2D)      │ 64)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ up_sampling2d_6     │ (None, 160, 160,  │       0 │ batch_normalization… │ ## │ (UpSampling2D)      │ 32)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ conv2d_7 (Conv2D)   │ (None, 160, 160,  │   2,080 │ up_sampling2d_7[0][… │ ## │                     │ 32)               │         │                      │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ add_6 (Add)         │ (None, 160, 160,  │       0 │ up_sampling2d_6[0][… │ ## │                     │ 32)               │         │ conv2d_7[0][0]       │ ## ├─────────────────────┼───────────────────┼─────────┼──────────────────────┤ ## │ conv2d_8 (Conv2D)   │ (None, 160, 160,  │     867 │ add_6[0][0]          │ ## │                     │ 3)                │         │                      │ ## └─────────────────────┴───────────────────┴─────────┴──────────────────────┘ ##  Total params: 2,058,979 (7.85 MB) ##  Trainable params: 2,055,203 (7.84 MB) ##  Non-trainable params: 3,776 (14.75 KB)"},{"path":"https://keras.posit.co/articles/examples/oxford_pets_image_segmentation.html","id":"set-aside-a-validation-split","dir":"Articles > Examples","previous_headings":"","what":"Set aside a validation split","title":"Image segmentation with a U-Net-like architecture","text":"","code":"# Split our img paths into a training and a validation set val_samples <- 1000 val_samples <- sample.int(length(input_img_paths), val_samples)  train_input_img_paths <- input_img_paths[-val_samples] train_target_img_paths <- target_img_paths[-val_samples]  val_input_img_paths <- input_img_paths[val_samples] val_target_img_paths <- target_img_paths[val_samples]  # Instantiate dataset for each split # Limit input files in `max_dataset_len` for faster epoch training time. # Remove the `max_dataset_len` arg when running with full dataset. train_dataset <- get_dataset(     batch_size,     img_size,     train_input_img_paths,     train_target_img_paths,     max_dataset_len=1000 ) valid_dataset <- get_dataset(     batch_size, img_size, val_input_img_paths, val_target_img_paths )"},{"path":"https://keras.posit.co/articles/examples/oxford_pets_image_segmentation.html","id":"train-the-model","dir":"Articles > Examples","previous_headings":"","what":"Train the model","title":"Image segmentation with a U-Net-like architecture","text":"","code":"# Configure the model for training. # We use the \"sparse\" version of categorical_crossentropy # because our target data is integers. model %>% compile(   optimizer = optimizer_adam(1e-4),   loss=\"sparse_categorical_crossentropy\" )  callbacks <- list(     callback_model_checkpoint(\"models/oxford_segmentation.keras\", save_best_only=TRUE) )  # Train the model, doing validation at the end of each epoch. epochs <- 50 model %>% fit(     train_dataset,     epochs=epochs,     validation_data=valid_dataset,     callbacks=callbacks,     verbose=2 ) ## Epoch 1/50 ## 32/32 - 48s - 2s/step - loss: 1.3455 - val_loss: 1.7105 ## Epoch 2/50 ## 32/32 - 42s - 1s/step - loss: 0.9089 - val_loss: 1.9919 ## Epoch 3/50 ## 32/32 - 42s - 1s/step - loss: 0.7865 - val_loss: 2.1189 ## Epoch 4/50 ## 32/32 - 42s - 1s/step - loss: 0.7201 - val_loss: 2.3125 ## Epoch 5/50 ## 32/32 - 43s - 1s/step - loss: 0.6854 - val_loss: 2.4161 ## Epoch 6/50 ## 32/32 - 41s - 1s/step - loss: 0.6598 - val_loss: 2.5891 ## Epoch 7/50 ## 32/32 - 41s - 1s/step - loss: 0.6390 - val_loss: 2.7069 ## Epoch 8/50 ## 32/32 - 41s - 1s/step - loss: 0.6174 - val_loss: 2.8991 ## Epoch 9/50 ## 32/32 - 41s - 1s/step - loss: 0.5979 - val_loss: 3.0073 ## Epoch 10/50 ## 32/32 - 41s - 1s/step - loss: 0.5797 - val_loss: 3.1945 ## Epoch 11/50 ## 32/32 - 40s - 1s/step - loss: 0.5626 - val_loss: 3.2092 ## Epoch 12/50 ## 32/32 - 41s - 1s/step - loss: 0.5456 - val_loss: 3.3397 ## Epoch 13/50 ## 32/32 - 42s - 1s/step - loss: 0.5286 - val_loss: 3.1881 ## Epoch 14/50 ## 32/32 - 42s - 1s/step - loss: 0.5109 - val_loss: 3.1979 ## Epoch 15/50 ## 32/32 - 42s - 1s/step - loss: 0.4918 - val_loss: 2.8694 ## Epoch 16/50 ## 32/32 - 42s - 1s/step - loss: 0.4714 - val_loss: 2.6890 ## Epoch 17/50 ## 32/32 - 42s - 1s/step - loss: 0.4494 - val_loss: 2.3180 ## Epoch 18/50 ## 32/32 - 42s - 1s/step - loss: 0.4258 - val_loss: 1.9738 ## Epoch 19/50 ## 32/32 - 43s - 1s/step - loss: 0.4018 - val_loss: 1.5957 ## Epoch 20/50 ## 32/32 - 43s - 1s/step - loss: 0.3778 - val_loss: 1.3536 ## Epoch 21/50 ## 32/32 - 42s - 1s/step - loss: 0.3540 - val_loss: 1.1855 ## Epoch 22/50 ## 32/32 - 41s - 1s/step - loss: 0.3314 - val_loss: 1.1029 ## Epoch 23/50 ## 32/32 - 42s - 1s/step - loss: 0.3106 - val_loss: 1.0832 ## Epoch 24/50 ## 32/32 - 41s - 1s/step - loss: 0.2917 - val_loss: 1.0801 ## Epoch 25/50 ## 32/32 - 43s - 1s/step - loss: 0.2757 - val_loss: 1.1009 ## Epoch 26/50 ## 32/32 - 42s - 1s/step - loss: 0.2624 - val_loss: 1.1736 ## Epoch 27/50 ## 32/32 - 43s - 1s/step - loss: 0.2551 - val_loss: 1.1818 ## Epoch 28/50 ## 32/32 - 43s - 1s/step - loss: 0.2840 - val_loss: 1.2634 ## Epoch 29/50 ## 32/32 - 42s - 1s/step - loss: 0.3318 - val_loss: 1.4185 ## Epoch 30/50 ## 32/32 - 43s - 1s/step - loss: 0.3176 - val_loss: 1.2149 ## Epoch 31/50 ## 32/32 - 42s - 1s/step - loss: 0.2980 - val_loss: 1.5469 ## Epoch 32/50 ## 32/32 - 42s - 1s/step - loss: 0.2940 - val_loss: 1.6380 ## Epoch 33/50 ## 32/32 - 45s - 1s/step - loss: 0.2892 - val_loss: 2.1326 ## Epoch 34/50 ## 32/32 - 43s - 1s/step - loss: 0.3018 - val_loss: 2.2094 ## Epoch 35/50 ## 32/32 - 43s - 1s/step - loss: 0.3028 - val_loss: 2.7867 ## Epoch 36/50 ## 32/32 - 44s - 1s/step - loss: 0.3162 - val_loss: 1.8726 ## Epoch 37/50 ## 32/32 - 43s - 1s/step - loss: 0.3106 - val_loss: 1.9691 ## Epoch 38/50 ## 32/32 - 43s - 1s/step - loss: 0.3025 - val_loss: 3.3606 ## Epoch 39/50 ## 32/32 - 44s - 1s/step - loss: 0.2930 - val_loss: 3.9138 ## Epoch 40/50 ## 32/32 - 43s - 1s/step - loss: 0.2809 - val_loss: 3.0231 ## Epoch 41/50 ## 32/32 - 42s - 1s/step - loss: 0.2726 - val_loss: 2.3985 ## Epoch 42/50 ## 32/32 - 42s - 1s/step - loss: 0.2627 - val_loss: 2.7177 ## Epoch 43/50 ## 32/32 - 42s - 1s/step - loss: 0.2572 - val_loss: 2.0692 ## Epoch 44/50 ## 32/32 - 42s - 1s/step - loss: 0.2507 - val_loss: 1.7416 ## Epoch 45/50 ## 32/32 - 43s - 1s/step - loss: 0.2526 - val_loss: 6.5888 ## Epoch 46/50 ## 32/32 - 42s - 1s/step - loss: 0.2545 - val_loss: 4.9365 ## Epoch 47/50 ## 32/32 - 42s - 1s/step - loss: 0.2538 - val_loss: 3.5521 ## Epoch 48/50 ## 32/32 - 41s - 1s/step - loss: 0.2459 - val_loss: 8.8581 ## Epoch 49/50 ## 32/32 - 42s - 1s/step - loss: 0.2424 - val_loss: 10.6086 ## Epoch 50/50 ## 32/32 - 42s - 1s/step - loss: 0.2353 - val_loss: 12.4796"},{"path":"https://keras.posit.co/articles/examples/oxford_pets_image_segmentation.html","id":"visualize-predictions","dir":"Articles > Examples","previous_headings":"","what":"Visualize predictions","title":"Image segmentation with a U-Net-like architecture","text":"","code":"model <- load_model(\"models/oxford_segmentation.keras\") # Generate predictions for all images in the validation set val_dataset <- get_dataset(     batch_size, img_size, val_input_img_paths, val_target_img_paths ) val_preds <- predict(model, val_dataset) ## 32/32 - 12s - 367ms/step display_mask <- function(i) {   # Quick utility to display a model's prediction.   mask <- val_preds[i,,,] %>%     apply(c(1,2), which.max) %>%     array_reshape(dim = c(img_size, 1))   mask <- abind::abind(mask, mask, mask, along = 3)   plot(as.raster(mask, max = 3)) }  # Display results for validation image #10 i <- 10  par(mfrow = c(1, 3)) # Display input image input_img_paths[i] %>%   jpeg::readJPEG() %>%   as.raster() %>%   plot()  # Display ground-truth target mask target_img_paths[i] %>%   png::readPNG() %>%   magrittr::multiply_by(255) %>%   as.raster(max = 3) %>%   plot()  # Display mask predicted by our model display_mask(i)  # Note that the model only sees inputs at 150x150."},{"path":"https://keras.posit.co/articles/examples/perceiver_image_classification.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Image classification with Perceiver","text":"example implements Perceiver: General Perception Iterative Attention model Andrew Jaegle et al. image classification, demonstrates CIFAR-100 dataset. Perceiver model leverages asymmetric attention mechanism iteratively distill inputs tight latent bottleneck, allowing scale handle large inputs. words: let’s assume input data array (e.g. image) M elements (.e. patches), M large. standard Transformer model, self-attention operation performed M elements. complexity operation O(M^2). However, Perceiver model creates latent array size N elements, N << M, performs two operations iteratively: Cross-attention Transformer latent array data array - complexity operation O(M.N). Self-attention Transformer latent array - complexity operation O(N^2). example requires TensorFlow 2.4 higher, well TensorFlow Addons, can installed using following command:","code":"pip install -U tensorflow-addons"},{"path":"https://keras.posit.co/articles/examples/perceiver_image_classification.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Image classification with Perceiver","text":"","code":"import numpy as np import tensorflow as tf import keras as keras from keras import layers"},{"path":"https://keras.posit.co/articles/examples/perceiver_image_classification.html","id":"prepare-the-data","dir":"Articles > Examples","previous_headings":"","what":"Prepare the data","title":"Image classification with Perceiver","text":"","code":"num_classes = 100 input_shape = (32, 32, 3)  (x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()  print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\") print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"},{"path":"https://keras.posit.co/articles/examples/perceiver_image_classification.html","id":"configure-the-hyperparameters","dir":"Articles > Examples","previous_headings":"","what":"Configure the hyperparameters","title":"Image classification with Perceiver","text":"Note , order use pixel individual input data array, set patch_size 1.","code":"learning_rate = 0.001 weight_decay = 0.0001 batch_size = 64 num_epochs = 50 dropout_rate = 0.2 image_size = 64  # We'll resize input images to this size. patch_size = 2  # Size of the patches to be extract from the input images. num_patches = (image_size // patch_size) ** 2  # Size of the data array. latent_dim = 256  # Size of the latent array. projection_dim = (     256  # Embedding size of each element in the data and latent arrays. ) num_heads = 8  # Number of Transformer heads. ffn_units = [     projection_dim,     projection_dim, ]  # Size of the Transformer Feedforward network. num_transformer_blocks = 4 num_iterations = (     2  # Repetitions of the cross-attention and Transformer modules. ) classifier_units = [     projection_dim,     num_classes, ]  # Size of the Feedforward network of the final classifier.  print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\") print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \") print(f\"Patches per image: {num_patches}\") print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\") print(f\"Latent array shape: {latent_dim} X {projection_dim}\") print(f\"Data array shape: {num_patches} X {projection_dim}\")"},{"path":"https://keras.posit.co/articles/examples/perceiver_image_classification.html","id":"use-data-augmentation","dir":"Articles > Examples","previous_headings":"","what":"Use data augmentation","title":"Image classification with Perceiver","text":"","code":"data_augmentation = keras.Sequential(     [         layers.Normalization(),         layers.Resizing(image_size, image_size),         layers.RandomFlip(\"horizontal\"),         layers.RandomZoom(height_factor=0.2, width_factor=0.2),     ],     name=\"data_augmentation\", ) # Compute the mean and the variance of the training data for normalization. data_augmentation.layers[0].adapt(x_train)"},{"path":"https://keras.posit.co/articles/examples/perceiver_image_classification.html","id":"implement-feedforward-network-ffn","dir":"Articles > Examples","previous_headings":"","what":"Implement Feedforward network (FFN)","title":"Image classification with Perceiver","text":"","code":"def create_ffn(hidden_units, dropout_rate):     ffn_layers = []     for units in hidden_units[:-1]:         ffn_layers.append(layers.Dense(units, activation=tf.nn.gelu))      ffn_layers.append(layers.Dense(units=hidden_units[-1]))     ffn_layers.append(layers.Dropout(dropout_rate))      ffn = keras.Sequential(ffn_layers)     return ffn"},{"path":"https://keras.posit.co/articles/examples/perceiver_image_classification.html","id":"implement-patch-creation-as-a-layer","dir":"Articles > Examples","previous_headings":"","what":"Implement patch creation as a layer","title":"Image classification with Perceiver","text":"","code":"class Patches(layers.Layer):     def __init__(self, patch_size):         super().__init__()         self.patch_size = patch_size      def call(self, images):         batch_size = tf.shape(images)[0]         patches = tf.image.extract_patches(             images=images,             sizes=[1, self.patch_size, self.patch_size, 1],             strides=[1, self.patch_size, self.patch_size, 1],             rates=[1, 1, 1, 1],             padding=\"VALID\",         )         patch_dims = patches.shape[-1]         patches = tf.reshape(patches, [batch_size, -1, patch_dims])         return patches"},{"path":"https://keras.posit.co/articles/examples/perceiver_image_classification.html","id":"implement-the-patch-encoding-layer","dir":"Articles > Examples","previous_headings":"","what":"Implement the patch encoding layer","title":"Image classification with Perceiver","text":"PatchEncoder layer linearly transform patch projecting vector size latent_dim. addition, adds learnable position embedding projected vector. Note orginal Perceiver paper uses Fourier feature positional encodings.","code":"class PatchEncoder(layers.Layer):     def __init__(self, num_patches, projection_dim):         super().__init__()         self.num_patches = num_patches         self.projection = layers.Dense(units=projection_dim)         self.position_embedding = layers.Embedding(             input_dim=num_patches, output_dim=projection_dim         )      def call(self, patches):         positions = tf.range(start=0, limit=self.num_patches, delta=1)         encoded = self.projection(patches) + self.position_embedding(positions)         return encoded"},{"path":"https://keras.posit.co/articles/examples/perceiver_image_classification.html","id":"build-the-perceiver-model","dir":"Articles > Examples","previous_headings":"","what":"Build the Perceiver model","title":"Image classification with Perceiver","text":"Perceiver consists two modules: cross-attention module standard Transformer self-attention.","code":""},{"path":"https://keras.posit.co/articles/examples/perceiver_image_classification.html","id":"cross-attention-module","dir":"Articles > Examples","previous_headings":"Build the Perceiver model","what":"Cross-attention module","title":"Image classification with Perceiver","text":"cross-attention expects (latent_dim, projection_dim) latent array, (data_dim,  projection_dim) data array inputs, produce (latent_dim, projection_dim) latent array output. apply cross-attention, query vectors generated latent array, key value vectors generated encoded image. Note data array example image, data_dim set num_patches.","code":"def create_cross_attention_module(     latent_dim, data_dim, projection_dim, ffn_units, dropout_rate ):     inputs = {         # Recieve the latent array as an input of shape [1, latent_dim, projection_dim].         \"latent_array\": layers.Input(             shape=(latent_dim, projection_dim), name=\"latent_array\"         ),         # Recieve the data_array (encoded image) as an input of shape [batch_size, data_dim, projection_dim].         \"data_array\": layers.Input(             shape=(data_dim, projection_dim), name=\"data_array\"         ),     }      # Apply layer norm to the inputs     latent_array = layers.LayerNormalization(epsilon=1e-6)(         inputs[\"latent_array\"]     )     data_array = layers.LayerNormalization(epsilon=1e-6)(inputs[\"data_array\"])      # Create query tensor: [1, latent_dim, projection_dim].     query = layers.Dense(units=projection_dim)(latent_array)     # Create key tensor: [batch_size, data_dim, projection_dim].     key = layers.Dense(units=projection_dim)(data_array)     # Create value tensor: [batch_size, data_dim, projection_dim].     value = layers.Dense(units=projection_dim)(data_array)      # Generate cross-attention outputs: [batch_size, latent_dim, projection_dim].     attention_output = layers.Attention(use_scale=True, dropout=0.1)(         [query, key, value], return_attention_scores=False     )     # Skip connection 1.     attention_output = layers.Add()([attention_output, latent_array])      # Apply layer norm.     attention_output = layers.LayerNormalization(epsilon=1e-6)(attention_output)     # Apply Feedforward network.     ffn = create_ffn(hidden_units=ffn_units, dropout_rate=dropout_rate)     outputs = ffn(attention_output)     # Skip connection 2.     outputs = layers.Add()([outputs, attention_output])      # Create the Keras model.     model = keras.Model(inputs=inputs, outputs=outputs)     return model"},{"path":"https://keras.posit.co/articles/examples/perceiver_image_classification.html","id":"transformer-module","dir":"Articles > Examples","previous_headings":"Build the Perceiver model","what":"Transformer module","title":"Image classification with Perceiver","text":"Transformer expects output latent vector cross-attention module input, applies multi-head self-attention latent_dim elements, followed feedforward network, produce another (latent_dim, projection_dim) latent array.","code":"def create_transformer_module(     latent_dim,     projection_dim,     num_heads,     num_transformer_blocks,     ffn_units,     dropout_rate, ):     # input_shape: [1, latent_dim, projection_dim]     inputs = layers.Input(shape=(latent_dim, projection_dim))      x0 = inputs     # Create multiple layers of the Transformer block.     for _ in range(num_transformer_blocks):         # Apply layer normalization 1.         x1 = layers.LayerNormalization(epsilon=1e-6)(x0)         # Create a multi-head self-attention layer.         attention_output = layers.MultiHeadAttention(             num_heads=num_heads, key_dim=projection_dim, dropout=0.1         )(x1, x1)         # Skip connection 1.         x2 = layers.Add()([attention_output, x0])         # Apply layer normalization 2.         x3 = layers.LayerNormalization(epsilon=1e-6)(x2)         # Apply Feedforward network.         ffn = create_ffn(hidden_units=ffn_units, dropout_rate=dropout_rate)         x3 = ffn(x3)         # Skip connection 2.         x0 = layers.Add()([x3, x2])      # Create the Keras model.     model = keras.Model(inputs=inputs, outputs=x0)     return model"},{"path":"https://keras.posit.co/articles/examples/perceiver_image_classification.html","id":"perceiver-model","dir":"Articles > Examples","previous_headings":"Build the Perceiver model","what":"Perceiver model","title":"Image classification with Perceiver","text":"Perceiver model repeats cross-attention Transformer modules num_iterations times—shared weights skip connections—allow latent array iteratively extract information input image needed.","code":"class Perceiver(keras.Model):     def __init__(         self,         patch_size,         data_dim,         latent_dim,         projection_dim,         num_heads,         num_transformer_blocks,         ffn_units,         dropout_rate,         num_iterations,         classifier_units,     ):         super().__init__()          self.latent_dim = latent_dim         self.data_dim = data_dim         self.patch_size = patch_size         self.projection_dim = projection_dim         self.num_heads = num_heads         self.num_transformer_blocks = num_transformer_blocks         self.ffn_units = ffn_units         self.dropout_rate = dropout_rate         self.num_iterations = num_iterations         self.classifier_units = classifier_units      def build(self, input_shape):         # Create latent array.         self.latent_array = self.add_weight(             shape=(self.latent_dim, self.projection_dim),             initializer=\"random_normal\",             trainable=True,         )          # Create patching module.         self.patcher = Patches(self.patch_size)          # Create patch encoder.         self.patch_encoder = PatchEncoder(self.data_dim, self.projection_dim)          # Create cross-attenion module.         self.cross_attention = create_cross_attention_module(             self.latent_dim,             self.data_dim,             self.projection_dim,             self.ffn_units,             self.dropout_rate,         )          # Create Transformer module.         self.transformer = create_transformer_module(             self.latent_dim,             self.projection_dim,             self.num_heads,             self.num_transformer_blocks,             self.ffn_units,             self.dropout_rate,         )          # Create global average pooling layer.         self.global_average_pooling = layers.GlobalAveragePooling1D()          # Create a classification head.         self.classification_head = create_ffn(             hidden_units=self.classifier_units, dropout_rate=self.dropout_rate         )          super().build(input_shape)      def call(self, inputs):         # Augment data.         augmented = data_augmentation(inputs)         # Create patches.         patches = self.patcher(augmented)         # Encode patches.         encoded_patches = self.patch_encoder(patches)         # Prepare cross-attention inputs.         cross_attention_inputs = {             \"latent_array\": tf.expand_dims(self.latent_array, 0),             \"data_array\": encoded_patches,         }         # Apply the cross-attention and the Transformer modules iteratively.         for _ in range(self.num_iterations):             # Apply cross-attention from the latent array to the data array.             latent_array = self.cross_attention(cross_attention_inputs)             # Apply self-attention Transformer to the latent array.             latent_array = self.transformer(latent_array)             # Set the latent array of the next iteration.             cross_attention_inputs[\"latent_array\"] = latent_array          # Apply global average pooling to generate a [batch_size, projection_dim] repesentation tensor.         representation = self.global_average_pooling(latent_array)         # Generate logits.         logits = self.classification_head(representation)         return logits"},{"path":"https://keras.posit.co/articles/examples/perceiver_image_classification.html","id":"compile-train-and-evaluate-the-mode","dir":"Articles > Examples","previous_headings":"","what":"Compile, train, and evaluate the mode","title":"Image classification with Perceiver","text":"Note training perceiver model current settings V100 GPUs takes around 200 seconds. 40 epochs, Perceiver model achieves around 53% accuracy 81% top-5 accuracy test data. mentioned ablations Perceiver paper, can obtain better results increasing latent array size, increasing (projection) dimensions latent array data array elements, increasing number blocks Transformer module, increasing number iterations applying cross-attention latent Transformer modules. may also try increase size input images use different patch sizes. Perceiver benefits inceasing model size. However, larger models needs bigger accelerators fit train efficiently. Perceiver paper used 32 TPU cores run experiments.","code":"def run_experiment(model):     # Create Adam optimizer with weight decay.     optimizer = keras.optimizers.Adam(         learning_rate=learning_rate,         weight_decay=weight_decay,     )      # Compile the model.     model.compile(         optimizer=optimizer,         loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),         metrics=[             keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),             keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),         ],     )      # Create a learning rate scheduler callback.     reduce_lr = keras.callbacks.ReduceLROnPlateau(         monitor=\"val_loss\", factor=0.2, patience=3     )      # Create an early stopping callback.     early_stopping = keras.callbacks.EarlyStopping(         monitor=\"val_loss\", patience=15, restore_best_weights=True     )      # Fit the model.     history = model.fit(         x=x_train,         y=y_train,         batch_size=batch_size,         epochs=num_epochs,         validation_split=0.1,         callbacks=[early_stopping, reduce_lr],     )      _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)     print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")     print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")      # Return history to plot learning curves.     return history perceiver_classifier = Perceiver(     patch_size,     num_patches,     latent_dim,     projection_dim,     num_heads,     num_transformer_blocks,     ffn_units,     dropout_rate,     num_iterations,     classifier_units, )   history = run_experiment(perceiver_classifier)"},{"path":"https://keras.posit.co/articles/examples/pixelcnn.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"PixelCNN","text":"PixelCNN generative model proposed 2016 van den Oord et al. (reference: Conditional Image Generation PixelCNN Decoders). designed generate images (data types) iteratively input vector probability distribution prior elements dictates probability distribution later elements. following example, images generated fashion, pixel--pixel, via masked convolution kernel looks data previously generated pixels (origin top left) generate later pixels. inference, output network used probability ditribution new pixel values sampled generate new image (, MNIST, pixels values either black white).","code":"import numpy as np import keras from keras import layers from keras import ops from tqdm import tqdm"},{"path":"https://keras.posit.co/articles/examples/pixelcnn.html","id":"getting-the-data","dir":"Articles > Examples","previous_headings":"","what":"Getting the Data","title":"PixelCNN","text":"","code":"# Model / data parameters num_classes = 10 input_shape = (28, 28, 1) n_residual_blocks = 5 # The data, split between train and test sets (x, _), (y, _) = keras.datasets.mnist.load_data() # Concatenate all the images together data = np.concatenate((x, y), axis=0) # Round all pixel values less than 33% of the max 256 value to 0 # anything above this value gets rounded up to 1 so that all values are either # 0 or 1 data = np.where(data < (0.33 * 256), 0, 1) data = data.astype(np.float32)"},{"path":"https://keras.posit.co/articles/examples/pixelcnn.html","id":"create-two-classes-for-the-requisite-layers-for-the-model","dir":"Articles > Examples","previous_headings":"","what":"Create two classes for the requisite Layers for the model","title":"PixelCNN","text":"","code":"# The first layer is the PixelCNN layer. This layer simply # builds on the 2D convolutional layer, but includes masking. class PixelConvLayer(layers.Layer):     def __init__(self, mask_type, **kwargs):         super().__init__()         self.mask_type = mask_type         self.conv = layers.Conv2D(**kwargs)      def build(self, input_shape):         # Build the conv2d layer to initialize kernel variables         self.conv.build(input_shape)         # Use the initialized kernel to create the mask         kernel_shape = ops.shape(self.conv.kernel)         self.mask = np.zeros(shape=kernel_shape)         self.mask[: kernel_shape[0] // 2, ...] = 1.0         self.mask[kernel_shape[0] // 2, : kernel_shape[1] // 2, ...] = 1.0         if self.mask_type == \"B\":             self.mask[kernel_shape[0] // 2, kernel_shape[1] // 2, ...] = 1.0      def call(self, inputs):         self.conv.kernel.assign(self.conv.kernel * self.mask)         return self.conv(inputs)   # Next, we build our residual block layer. # This is just a normal residual block, but based on the PixelConvLayer. class ResidualBlock(keras.layers.Layer):     def __init__(self, filters, **kwargs):         super().__init__(**kwargs)         self.conv1 = keras.layers.Conv2D(             filters=filters, kernel_size=1, activation=\"relu\"         )         self.pixel_conv = PixelConvLayer(             mask_type=\"B\",             filters=filters // 2,             kernel_size=3,             activation=\"relu\",             padding=\"same\",         )         self.conv2 = keras.layers.Conv2D(             filters=filters, kernel_size=1, activation=\"relu\"         )      def call(self, inputs):         x = self.conv1(inputs)         x = self.pixel_conv(x)         x = self.conv2(x)         return keras.layers.add([inputs, x])"},{"path":"https://keras.posit.co/articles/examples/pixelcnn.html","id":"build-the-model-based-on-the-original-paper","dir":"Articles > Examples","previous_headings":"","what":"Build the model based on the original paper","title":"PixelCNN","text":"","code":"inputs = keras.Input(shape=input_shape) x = PixelConvLayer(     mask_type=\"A\", filters=128, kernel_size=7, activation=\"relu\", padding=\"same\" )(inputs)  for _ in range(n_residual_blocks):     x = ResidualBlock(filters=128)(x)  for _ in range(2):     x = PixelConvLayer(         mask_type=\"B\",         filters=128,         kernel_size=1,         strides=1,         activation=\"relu\",         padding=\"valid\",     )(x)  out = keras.layers.Conv2D(     filters=1, kernel_size=1, strides=1, activation=\"sigmoid\", padding=\"valid\" )(x)  pixel_cnn = keras.Model(inputs, out) adam = keras.optimizers.Adam(learning_rate=0.0005) pixel_cnn.compile(optimizer=adam, loss=\"binary_crossentropy\")  pixel_cnn.summary() pixel_cnn.fit(     x=data, y=data, batch_size=128, epochs=50, validation_split=0.1, verbose=2 )"},{"path":"https://keras.posit.co/articles/examples/pixelcnn.html","id":"demonstration","dir":"Articles > Examples","previous_headings":"","what":"Demonstration","title":"PixelCNN","text":"PixelCNN generate full image . Instead, must generate pixel order, append last generated pixel current image, feed image back model repeat process.","code":"from IPython.display import Image, display  # Create an empty array of pixels. batch = 4 pixels = np.zeros(shape=(batch,) + (pixel_cnn.input_shape)[1:]) batch, rows, cols, channels = pixels.shape  # Iterate over the pixels because generation has to be done sequentially pixel by pixel. for row in tqdm(range(rows)):     for col in range(cols):         for channel in range(channels):             # Feed the whole array and retrieving the pixel value probabilities for the next             # pixel.             probs = pixel_cnn.predict(pixels)[:, row, col, channel]             # Use the probabilities to pick pixel values and append the values to the image             # frame.             pixels[:, row, col, channel] = ops.ceil(                 probs - keras.random.uniform(probs.shape)             )   def deprocess_image(x):     # Stack the single channeled black and white image to rgb values.     x = np.stack((x, x, x), 2)     # Undo preprocessing     x *= 255.0     # Convert to uint8 and clip to the valid range [0, 255]     x = np.clip(x, 0, 255).astype(\"uint8\")     return x   # Iterate over the generated images and plot them with matplotlib. for i, pic in enumerate(pixels):     keras.utils.save_img(         \"generated_image_{}.png\".format(i), deprocess_image(np.squeeze(pic, -1))     )  display(Image(\"generated_image_0.png\")) display(Image(\"generated_image_1.png\")) display(Image(\"generated_image_2.png\")) display(Image(\"generated_image_3.png\"))"},{"path":"https://keras.posit.co/articles/examples/pretrained_word_embeddings.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Using pre-trained word embeddings","text":"","code":"import numpy as np import tensorflow.data as tf_data import keras as keras"},{"path":"https://keras.posit.co/articles/examples/pretrained_word_embeddings.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Using pre-trained word embeddings","text":"example, show train text classification model uses pre-trained word embeddings. ’ll work Newsgroup20 dataset, set 20,000 message board messages belonging 20 different topic categories. pre-trained word embeddings, ’ll use GloVe embeddings.","code":""},{"path":"https://keras.posit.co/articles/examples/pretrained_word_embeddings.html","id":"download-the-newsgroup20-data","dir":"Articles > Examples","previous_headings":"","what":"Download the Newsgroup20 data","title":"Using pre-trained word embeddings","text":"","code":"data_path = keras.utils.get_file(     \"news20.tar.gz\",     \"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\",     untar=True, )"},{"path":"https://keras.posit.co/articles/examples/pretrained_word_embeddings.html","id":"lets-take-a-look-at-the-data","dir":"Articles > Examples","previous_headings":"","what":"Let’s take a look at the data","title":"Using pre-trained word embeddings","text":"’s example one file contains: can see, header lines leaking file’s category, either explicitly (first line literally category name), implicitly, e.g. via Organization filed. Let’s get rid headers: ’s actually one category doesn’t expected number files, difference small enough problem remains balanced classification problem.","code":"import os import pathlib  data_dir = pathlib.Path(data_path).parent / \"20_newsgroup\" dirnames = os.listdir(data_dir) print(\"Number of directories:\", len(dirnames)) print(\"Directory names:\", dirnames)  fnames = os.listdir(data_dir / \"comp.graphics\") print(\"Number of files in comp.graphics:\", len(fnames)) print(\"Some example filenames:\", fnames[:5]) print(open(data_dir / \"comp.graphics\" / \"38987\").read()) samples = [] labels = [] class_names = [] class_index = 0 for dirname in sorted(os.listdir(data_dir)):     class_names.append(dirname)     dirpath = data_dir / dirname     fnames = os.listdir(dirpath)     print(\"Processing %s, %d files found\" % (dirname, len(fnames)))     for fname in fnames:         fpath = dirpath / fname         f = open(fpath, encoding=\"latin-1\")         content = f.read()         lines = content.split(\"\\n\")         lines = lines[10:]         content = \"\\n\".join(lines)         samples.append(content)         labels.append(class_index)     class_index += 1  print(\"Classes:\", class_names) print(\"Number of samples:\", len(samples))"},{"path":"https://keras.posit.co/articles/examples/pretrained_word_embeddings.html","id":"shuffle-and-split-the-data-into-training-validation-sets","dir":"Articles > Examples","previous_headings":"","what":"Shuffle and split the data into training & validation sets","title":"Using pre-trained word embeddings","text":"","code":"# Shuffle the data seed = 1337 rng = np.random.RandomState(seed) rng.shuffle(samples) rng = np.random.RandomState(seed) rng.shuffle(labels)  # Extract a training & validation split validation_split = 0.2 num_validation_samples = int(validation_split * len(samples)) train_samples = samples[:-num_validation_samples] val_samples = samples[-num_validation_samples:] train_labels = labels[:-num_validation_samples] val_labels = labels[-num_validation_samples:]"},{"path":"https://keras.posit.co/articles/examples/pretrained_word_embeddings.html","id":"create-a-vocabulary-index","dir":"Articles > Examples","previous_headings":"","what":"Create a vocabulary index","title":"Using pre-trained word embeddings","text":"Let’s use TextVectorization index vocabulary found dataset. Later, ’ll use layer instance vectorize samples. layer consider top 20,000 words, truncate pad sequences actually 200 tokens long. can retrieve computed vocabulary used via vectorizer.get_vocabulary(). Let’s print top 5 words: Let’s vectorize test sentence: can see, “” gets represented “2”. 0, given “” first word vocabulary? ’s index 0 reserved padding index 1 reserved “vocabulary” tokens. ’s dict mapping words indices: can see, obtain encoding test sentence:","code":"from keras.layers import TextVectorization  vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200) text_ds = tf_data.Dataset.from_tensor_slices(train_samples).batch(128) vectorizer.adapt(text_ds) vectorizer.get_vocabulary()[:5] output = vectorizer([[\"the cat sat on the mat\"]]) output.numpy()[0, :6] voc = vectorizer.get_vocabulary() word_index = dict(zip(voc, range(len(voc)))) test = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"] [word_index[w] for w in test]"},{"path":"https://keras.posit.co/articles/examples/pretrained_word_embeddings.html","id":"load-pre-trained-word-embeddings","dir":"Articles > Examples","previous_headings":"","what":"Load pre-trained word embeddings","title":"Using pre-trained word embeddings","text":"Let’s download pre-trained GloVe embeddings (822M zip file). ’ll need run following commands: archive contains text-encoded vectors various sizes: 50-dimensional, 100-dimensional, 200-dimensional, 300-dimensional. ’ll use 100D ones. Let’s make dict mapping words (strings) NumPy vector representation: Now, let’s prepare corresponding embedding matrix can use Keras Embedding layer. ’s simple NumPy matrix entry index pre-trained vector word index vectorizer’s vocabulary. Next, load pre-trained word embeddings matrix Embedding layer. Note set trainable=False keep embeddings fixed (don’t want update training).","code":"!wget http://nlp.stanford.edu/data/glove.6B.zip !unzip -q glove.6B.zip path_to_glove_file = os.path.join(     os.path.expanduser(\"~\"), \".keras/datasets/glove.6B.100d.txt\" )  embeddings_index = {} with open(path_to_glove_file) as f:     for line in f:         word, coefs = line.split(maxsplit=1)         coefs = np.fromstring(coefs, \"f\", sep=\" \")         embeddings_index[word] = coefs  print(\"Found %s word vectors.\" % len(embeddings_index)) num_tokens = len(voc) + 2 embedding_dim = 100 hits = 0 misses = 0  # Prepare embedding matrix embedding_matrix = np.zeros((num_tokens, embedding_dim)) for word, i in word_index.items():     embedding_vector = embeddings_index.get(word)     if embedding_vector is not None:         # Words not found in embedding index will be all-zeros.         # This includes the representation for \"padding\" and \"OOV\"         embedding_matrix[i] = embedding_vector         hits += 1     else:         misses += 1 print(\"Converted %d words (%d misses)\" % (hits, misses)) from keras.layers import Embedding  embedding_layer = Embedding(     num_tokens,     embedding_dim,     trainable=False, ) embedding_layer.build((1,)) embedding_layer.set_weights([embedding_matrix])"},{"path":"https://keras.posit.co/articles/examples/pretrained_word_embeddings.html","id":"build-the-model","dir":"Articles > Examples","previous_headings":"","what":"Build the model","title":"Using pre-trained word embeddings","text":"simple 1D convnet global max pooling classifier end.","code":"from keras import layers  int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\") embedded_sequences = embedding_layer(int_sequences_input) x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences) x = layers.MaxPooling1D(5)(x) x = layers.Conv1D(128, 5, activation=\"relu\")(x) x = layers.MaxPooling1D(5)(x) x = layers.Conv1D(128, 5, activation=\"relu\")(x) x = layers.GlobalMaxPooling1D()(x) x = layers.Dense(128, activation=\"relu\")(x) x = layers.Dropout(0.5)(x) preds = layers.Dense(len(class_names), activation=\"softmax\")(x) model = keras.Model(int_sequences_input, preds) model.summary()"},{"path":"https://keras.posit.co/articles/examples/pretrained_word_embeddings.html","id":"train-the-model","dir":"Articles > Examples","previous_headings":"","what":"Train the model","title":"Using pre-trained word embeddings","text":"First, convert list--strings data NumPy arrays integer indices. arrays right-padded. use categorical crossentropy loss since ’re softmax classification. Moreover, use sparse_categorical_crossentropy since labels integers.","code":"x_train = vectorizer(np.array([[s] for s in train_samples])).numpy() x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()  y_train = np.array(train_labels) y_val = np.array(val_labels) model.compile(     loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"] ) model.fit(     x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val) )"},{"path":"https://keras.posit.co/articles/examples/pretrained_word_embeddings.html","id":"export-an-end-to-end-model","dir":"Articles > Examples","previous_headings":"","what":"Export an end-to-end model","title":"Using pre-trained word embeddings","text":"Now, may want export Model object takes input string arbitrary length, rather sequence indices. make model much portable, since wouldn’t worry input preprocessing pipeline. vectorizer actually Keras layer, ’s simple:","code":"string_input = keras.Input(shape=(1,), dtype=\"string\") x = vectorizer(string_input) preds = model(x) end_to_end_model = keras.Model(string_input, preds)  probabilities = end_to_end_model(     keras.ops.convert_to_tensor(         [[\"this message is about computer graphics and 3D modeling\"]]     ) )  print(class_names[np.argmax(probabilities[0])])"},{"path":[]},{"path":"https://keras.posit.co/articles/examples/pretraining_BERT.html","id":"bert-bidirectional-encoder-representations-from-transformers","dir":"Articles > Examples","previous_headings":"Introduction","what":"BERT (Bidirectional Encoder Representations from Transformers)","title":"Pretraining BERT with Hugging Face Transformers","text":"field computer vision, researchers repeatedly shown value transfer learning — pretraining neural network model known task/dataset, instance ImageNet classification, performing fine-tuning — using trained neural network basis new specific-purpose model. recent years, researchers shown similar technique can useful many natural language tasks. BERT makes use Transformer, attention mechanism learns contextual relations words (subwords) text. vanilla form, Transformer includes two separate mechanisms — encoder reads text input decoder produces prediction task. Since BERT’s goal generate language model, encoder mechanism necessary. detailed workings Transformer described paper Google. opposed directional models, read text input sequentially (left--right right--left), Transformer encoder reads entire sequence words . Therefore considered bidirectional, though accurate say ’s non-directional. characteristic allows model learn context word based surroundings (left right word). training language models, challenge defining prediction goal. Many models predict next word sequence (e.g. \"child came home _\"), directional approach inherently limits context learning. overcome challenge, BERT uses two training strategies:","code":""},{"path":"https://keras.posit.co/articles/examples/pretraining_BERT.html","id":"masked-language-modeling-mlm","dir":"Articles > Examples","previous_headings":"Introduction","what":"Masked Language Modeling (MLM)","title":"Pretraining BERT with Hugging Face Transformers","text":"feeding word sequences BERT, 15% words sequence replaced [MASK] token. model attempts predict original value masked words, based context provided , non-masked, words sequence.","code":""},{"path":"https://keras.posit.co/articles/examples/pretraining_BERT.html","id":"next-sentence-prediction-nsp","dir":"Articles > Examples","previous_headings":"Introduction","what":"Next Sentence Prediction (NSP)","title":"Pretraining BERT with Hugging Face Transformers","text":"BERT training process, model receives pairs sentences input learns predict second sentence pair subsequent sentence original document. training, 50% inputs pair second sentence subsequent sentence original document, 50% random sentence corpus chosen second sentence. assumption random sentence represent disconnect first sentence. Though Google provides pretrained BERT checkpoint English, may often need either pretrain model scratch different language, continued-pretraining fit model new domain. notebook, pretrain BERT scratch optimizing MLM NSP objectves using 🤗 Transformers WikiText English dataset loaded 🤗 Datasets.","code":""},{"path":[]},{"path":"https://keras.posit.co/articles/examples/pretraining_BERT.html","id":"installing-the-requirements","dir":"Articles > Examples","previous_headings":"Setup","what":"Installing the requirements","title":"Pretraining BERT with Hugging Face Transformers","text":"pip install git+https://github.com/huggingface/transformers.git pip install datasets pip install huggingface-hub pip install nltk","code":""},{"path":"https://keras.posit.co/articles/examples/pretraining_BERT.html","id":"importing-the-necessary-libraries","dir":"Articles > Examples","previous_headings":"Setup","what":"Importing the necessary libraries","title":"Pretraining BERT with Hugging Face Transformers","text":"","code":"import nltk import random import logging  import keras as keras  nltk.download(\"punkt\") # Set random seed keras.utils.set_random_seed(42)"},{"path":"https://keras.posit.co/articles/examples/pretraining_BERT.html","id":"define-certain-variables","dir":"Articles > Examples","previous_headings":"Setup","what":"Define certain variables","title":"Pretraining BERT with Hugging Face Transformers","text":"","code":"TOKENIZER_BATCH_SIZE = 256  # Batch-size to train the tokenizer on TOKENIZER_VOCABULARY = 25000  # Total number of unique subwords the tokenizer can have  BLOCK_SIZE = 128  # Maximum number of tokens in an input sample NSP_PROB = 0.50  # Probability that the next sentence is the actual next sentence in NSP SHORT_SEQ_PROB = 0.1  # Probability of generating shorter sequences to minimize the mismatch between pretraining and fine-tuning. MAX_LENGTH = 512  # Maximum number of tokens in an input sample after padding  MLM_PROB = 0.2  # Probability with which tokens are masked in MLM  TRAIN_BATCH_SIZE = 2  # Batch-size for pretraining the model on MAX_EPOCHS = 1  # Maximum number of epochs to train the model for LEARNING_RATE = 1e-4  # Learning rate for training the model  MODEL_CHECKPOINT = \"bert-base-cased\"  # Name of pretrained model from 🤗 Model Hub"},{"path":"https://keras.posit.co/articles/examples/pretraining_BERT.html","id":"load-the-wikitext-dataset","dir":"Articles > Examples","previous_headings":"","what":"Load the WikiText dataset","title":"Pretraining BERT with Hugging Face Transformers","text":"now download WikiText language modeling dataset. collection 100 million tokens extracted set verified “Good” “Featured” articles Wikipedia. load dataset 🤗 Datasets. purpose demonstration notebook, work train split dataset. can easily done load_dataset function. dataset just one column raw text, need pretraining BERT!","code":"from datasets import load_dataset  dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\") print(dataset)"},{"path":"https://keras.posit.co/articles/examples/pretraining_BERT.html","id":"training-a-new-tokenizer","dir":"Articles > Examples","previous_headings":"","what":"Training a new Tokenizer","title":"Pretraining BERT with Hugging Face Transformers","text":"First train tokenizer scratch corpus, can can use train language model scratch. need train tokenizer? ’s Transformer models often use subword tokenization algorithms, need trained identify parts words often present corpus using. 🤗 Transformers Tokenizer (name indicates) tokenize inputs (including converting tokens corresponding IDs pretrained vocabulary) put format model expects, well generate inputs model requires. First make list raw documents WikiText corpus: Next make batch_iterator function aid us train tokenizer. notebook, train tokenizer exact algorithms parameters existing one. instance, train new version BERT-CASED tokenzier Wikitext-2 using tokenization algorithm. First need load tokenizer want use model: Now train tokenizer using entire train split Wikitext-2 dataset. now done training new tokenizer! Next move data pre-processing steps.","code":"all_texts = [     doc for doc in dataset[\"train\"][\"text\"] if len(doc) > 0 and not doc.startswith(\" =\") ] def batch_iterator():     for i in range(0, len(all_texts), TOKENIZER_BATCH_SIZE):         yield all_texts[i : i + TOKENIZER_BATCH_SIZE] from transformers import AutoTokenizer  tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT) tokenizer = tokenizer.train_new_from_iterator(     batch_iterator(), vocab_size=TOKENIZER_VOCABULARY )"},{"path":"https://keras.posit.co/articles/examples/pretraining_BERT.html","id":"data-pre-processing","dir":"Articles > Examples","previous_headings":"","what":"Data Pre-processing","title":"Pretraining BERT with Hugging Face Transformers","text":"sake demonstrating workflow, notebook take small subsets entire WikiText train test splits. can feed texts model, need pre-process get ready task. mentioned earlier, BERT pretraining task includes two tasks total, NSP task MLM task. 🤗 Transformers easy implement collator called DataCollatorForLanguageModeling. However, need get data ready NSP manually. Next write simple function called prepare_train_features helps us pre-processing compatible 🤗 Datasets. summarize, pre-processing function : Get dataset ready NSP task creating pairs sentences (,B), B either actually follows , B randomly sampled somewhere else corpus. also generate corresponding label pair, 1 B actually follows 0 . Tokenize text dataset ’s corresponding token ids used embedding look-BERT Create additional inputs model like token_type_ids, attention_mask, etc. MLM going use preprocessing dataset one additional step: randomly mask tokens (replacing [MASK]) labels adjusted include masked tokens (don’t predict non-masked tokens). use tokenizer trained , make sure [MASK] token among special tokens passed training! get data ready MLM, simply use collator called DataCollatorForLanguageModeling provided 🤗 Transformers library dataset already ready NSP task. collator expects certain parameters. use default ones original BERT paper notebook. return_tensors='tf' ensures get tf.Tensor objects back. Next define training set train model. , 🤗 Datasets provides us to_tf_dataset method help us integrate dataset collator defined . method expects certain parameters: columns: columns serve independant variables label_cols: columns serve labels dependant variables batch_size: batch size training shuffle: whether want shuffle training dataset collate_fn: collator function","code":"dataset[\"train\"] = dataset[\"train\"].select([i for i in range(1000)]) dataset[\"validation\"] = dataset[\"validation\"].select([i for i in range(1000)]) # We define the maximum number of tokens after tokenization that each training sample # will have max_num_tokens = BLOCK_SIZE - tokenizer.num_special_tokens_to_add(pair=True)   def prepare_train_features(examples):     \"\"\"Function to prepare features for NSP task      Arguments:       examples: A dictionary with 1 key (\"text\")         text: List of raw documents (str)     Returns:       examples:  A dictionary with 4 keys         input_ids: List of tokenized, concatnated, and batched           sentences from the individual raw documents (int)         token_type_ids: List of integers (0 or 1) corresponding           to: 0 for senetence no. 1 and padding, 1 for sentence           no. 2         attention_mask: List of integers (0 or 1) corresponding           to: 1 for non-padded tokens, 0 for padded         next_sentence_label: List of integers (0 or 1) corresponding           to: 1 if the second sentence actually follows the first,           0 if the senetence is sampled from somewhere else in the corpus     \"\"\"      # Remove un-wanted samples from the training set     examples[\"document\"] = [         d.strip() for d in examples[\"text\"] if len(d) > 0 and not d.startswith(\" =\")     ]     # Split the documents from the dataset into it's individual sentences     examples[\"sentences\"] = [         nltk.tokenize.sent_tokenize(document) for document in examples[\"document\"]     ]     # Convert the tokens into ids using the trained tokenizer     examples[\"tokenized_sentences\"] = [         [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent)) for sent in doc]         for doc in examples[\"sentences\"]     ]      # Define the outputs     examples[\"input_ids\"] = []     examples[\"token_type_ids\"] = []     examples[\"attention_mask\"] = []     examples[\"next_sentence_label\"] = []      for doc_index, document in enumerate(examples[\"tokenized_sentences\"]):         current_chunk = []  # a buffer stored current working segments         current_length = 0         i = 0          # We *usually* want to fill up the entire sequence since we are padding         # to `block_size` anyways, so short sequences are generally wasted         # computation. However, we *sometimes*         # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter         # sequences to minimize the mismatch between pretraining and fine-tuning.         # The `target_seq_length` is just a rough target however, whereas         # `block_size` is a hard limit.         target_seq_length = max_num_tokens          if random.random() < SHORT_SEQ_PROB:             target_seq_length = random.randint(2, max_num_tokens)          while i < len(document):             segment = document[i]             current_chunk.append(segment)             current_length += len(segment)             if i == len(document) - 1 or current_length >= target_seq_length:                 if current_chunk:                     # `a_end` is how many segments from `current_chunk` go into the `A`                     # (first) sentence.                     a_end = 1                     if len(current_chunk) >= 2:                         a_end = random.randint(1, len(current_chunk) - 1)                      tokens_a = []                     for j in range(a_end):                         tokens_a.extend(current_chunk[j])                      tokens_b = []                      if len(current_chunk) == 1 or random.random() < NSP_PROB:                         is_random_next = True                         target_b_length = target_seq_length - len(tokens_a)                          # This should rarely go for more than one iteration for large                         # corpora. However, just to be careful, we try to make sure that                         # the random document is not the same as the document                         # we're processing.                         for _ in range(10):                             random_document_index = random.randint(                                 0, len(examples[\"tokenized_sentences\"]) - 1                             )                             if random_document_index != doc_index:                                 break                          random_document = examples[\"tokenized_sentences\"][                             random_document_index                         ]                         random_start = random.randint(0, len(random_document) - 1)                         for j in range(random_start, len(random_document)):                             tokens_b.extend(random_document[j])                             if len(tokens_b) >= target_b_length:                                 break                         # We didn't actually use these segments so we \"put them back\" so                         # they don't go to waste.                         num_unused_segments = len(current_chunk) - a_end                         i -= num_unused_segments                     else:                         is_random_next = False                         for j in range(a_end, len(current_chunk)):                             tokens_b.extend(current_chunk[j])                      input_ids = tokenizer.build_inputs_with_special_tokens(                         tokens_a, tokens_b                     )                     # add token type ids, 0 for sentence a, 1 for sentence b                     token_type_ids = tokenizer.create_token_type_ids_from_sequences(                         tokens_a, tokens_b                     )                      padded = tokenizer.pad(                         {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids},                         padding=\"max_length\",                         max_length=MAX_LENGTH,                     )                      examples[\"input_ids\"].append(padded[\"input_ids\"])                     examples[\"token_type_ids\"].append(padded[\"token_type_ids\"])                     examples[\"attention_mask\"].append(padded[\"attention_mask\"])                     examples[\"next_sentence_label\"].append(1 if is_random_next else 0)                     current_chunk = []                     current_length = 0             i += 1      # We delete all the un-necessary columns from our dataset     del examples[\"document\"]     del examples[\"sentences\"]     del examples[\"text\"]     del examples[\"tokenized_sentences\"]      return examples   tokenized_dataset = dataset.map(     prepare_train_features,     batched=True,     remove_columns=[\"text\"],     num_proc=1, ) from transformers import DataCollatorForLanguageModeling  collater = DataCollatorForLanguageModeling(     tokenizer=tokenizer, mlm=True, mlm_probability=MLM_PROB, return_tensors=\"tf\" ) train = tokenized_dataset[\"train\"].to_tf_dataset(     columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"],     label_cols=[\"labels\", \"next_sentence_label\"],     batch_size=TRAIN_BATCH_SIZE,     shuffle=True,     collate_fn=collater, )  validation = tokenized_dataset[\"validation\"].to_tf_dataset(     columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"],     label_cols=[\"labels\", \"next_sentence_label\"],     batch_size=TRAIN_BATCH_SIZE,     shuffle=True,     collate_fn=collater, )"},{"path":"https://keras.posit.co/articles/examples/pretraining_BERT.html","id":"defining-the-model","dir":"Articles > Examples","previous_headings":"","what":"Defining the model","title":"Pretraining BERT with Hugging Face Transformers","text":"define model, first need define config help us define certain parameters model architecture. includes parameters like number transformer layers, number attention heads, hidden dimension, etc. notebook, try define exact config defined original BERT paper. can easily achieve using BertConfig class 🤗 Transformers library. from_pretrained() method expects name model. define simplest model also trained model, .e., bert-base-cased. defining model use TFBertForPreTraining class 🤗 Transformers library. class internally handles everything starting defining model, unpacking inputs calculating loss. need anything except defining model correct config want! Now define optimizer compile model. loss calculation handled internally need worry ! Finally steps done now can start training model! model now trained! suggest please train model complete dataset atleast 50 epochs decent performance. pretrained model now acts language model meant fine-tuned downstream task. Thus can now fine-tuned downstream task like Question Answering, Text Classification etc.! Now can push model 🤗 Model Hub also share friends, family, favorite pets: can load identifier \"-username/-name--picked\" instance: push model can load future! , since ’s pretrained model generally use fine-tuning downstream task, can also load task like: case, pretraining head dropped model just initialized transformer layers. new task-specific head added random weights.","code":"from transformers import BertConfig  config = BertConfig.from_pretrained(MODEL_CHECKPOINT) from transformers import TFBertForPreTraining  model = TFBertForPreTraining(config) from keras.optimizers import Adam  model.compile(optimizer=Adam(learning_rate=LEARNING_RATE)) model.fit(train, validation_data=validation, epochs=MAX_EPOCHS) model.push_to_hub(\"pretrained-bert\", organization=\"keras-io\") tokenizer.push_to_hub(\"pretrained-bert\", organization=\"keras-io\") from transformers import TFBertForPreTraining  model = TFBertForPreTraining.from_pretrained(\"your-username/my-awesome-model\") from transformers import TFBertForSequenceClassification  model = TFBertForSequenceClassification.from_pretrained(\"your-username/my-awesome-model\")"},{"path":"https://keras.posit.co/articles/examples/reptile.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Few-Shot learning with Reptile","text":"Reptile algorithm developed OpenAI perform model-agnostic meta-learning. Specifically, algorithm designed quickly learn perform new tasks minimal training (-shot learning). algorithm works performing Stochastic Gradient Descent using difference weights trained mini-batch never-seen-data model weights prior training fixed number meta-iterations.","code":"import os  os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  import keras as keras from keras import layers  import matplotlib.pyplot as plt import numpy as np import random import tensorflow as tf import tensorflow_datasets as tfds"},{"path":"https://keras.posit.co/articles/examples/reptile.html","id":"define-the-hyperparameters","dir":"Articles > Examples","previous_headings":"","what":"Define the Hyperparameters","title":"Few-Shot learning with Reptile","text":"","code":"learning_rate = 0.003 meta_step_size = 0.25  inner_batch_size = 25 eval_batch_size = 25  meta_iters = 2000 eval_iters = 5 inner_iters = 4  eval_interval = 1 train_shots = 20 shots = 5 classes = 5"},{"path":"https://keras.posit.co/articles/examples/reptile.html","id":"prepare-the-data","dir":"Articles > Examples","previous_headings":"","what":"Prepare the data","title":"Few-Shot learning with Reptile","text":"Omniglot dataset dataset 1,623 characters taken 50 different alphabets, 20 examples character. 20 samples character drawn online via Amazon’s Mechanical Turk. -shot learning task, k samples (“shots”) drawn randomly n randomly-chosen classes. n numerical values used create new set temporary labels use test model’s ability learn new task given examples. words, training 5 classes, new class labels either 0, 1, 2, 3, 4. Omniglot great dataset task since many different classes draw , reasonable number samples class.","code":"class Dataset:     # This class will facilitate the creation of a few-shot dataset     # from the Omniglot dataset that can be sampled from quickly while also     # allowing to create new labels at the same time.     def __init__(self, training):         # Download the tfrecord files containing the omniglot data and convert to a         # dataset.         split = \"train\" if training else \"test\"         ds = tfds.load(             \"omniglot\", split=split, as_supervised=True, shuffle_files=False         )         # Iterate over the dataset to get each individual image and its class,         # and put that data into a dictionary.         self.data = {}          def extraction(image, label):             # This function will shrink the Omniglot images to the desired size,             # scale pixel values and convert the RGB image to grayscale             image = tf.image.convert_image_dtype(image, tf.float32)             image = tf.image.rgb_to_grayscale(image)             image = tf.image.resize(image, [28, 28])             return image, label          for image, label in ds.map(extraction):             image = image.numpy()             label = str(label.numpy())             if label not in self.data:                 self.data[label] = []             self.data[label].append(image)         self.labels = list(self.data.keys())      def get_mini_dataset(         self, batch_size, repetitions, shots, num_classes, split=False     ):         temp_labels = np.zeros(shape=(num_classes * shots))         temp_images = np.zeros(shape=(num_classes * shots, 28, 28, 1))         if split:             test_labels = np.zeros(shape=(num_classes))             test_images = np.zeros(shape=(num_classes, 28, 28, 1))          # Get a random subset of labels from the entire label set.         label_subset = random.choices(self.labels, k=num_classes)         for class_idx, class_obj in enumerate(label_subset):             # Use enumerated index value as a temporary label for mini-batch in             # few shot learning.             temp_labels[class_idx * shots : (class_idx + 1) * shots] = class_idx             # If creating a split dataset for testing, select an extra sample from each             # label to create the test dataset.             if split:                 test_labels[class_idx] = class_idx                 images_to_split = random.choices(                     self.data[label_subset[class_idx]], k=shots + 1                 )                 test_images[class_idx] = images_to_split[-1]                 temp_images[                     class_idx * shots : (class_idx + 1) * shots                 ] = images_to_split[:-1]             else:                 # For each index in the randomly selected label_subset, sample the                 # necessary number of images.                 temp_images[                     class_idx * shots : (class_idx + 1) * shots                 ] = random.choices(self.data[label_subset[class_idx]], k=shots)          dataset = tf.data.Dataset.from_tensor_slices(             (temp_images.astype(np.float32), temp_labels.astype(np.int32))         )         dataset = dataset.shuffle(100).batch(batch_size).repeat(repetitions)         if split:             return dataset, test_images, test_labels         return dataset   import urllib3  urllib3.disable_warnings()  # Disable SSL warnings that may happen during download. train_dataset = Dataset(training=True) test_dataset = Dataset(training=False)"},{"path":"https://keras.posit.co/articles/examples/reptile.html","id":"visualize-some-examples-from-the-dataset","dir":"Articles > Examples","previous_headings":"","what":"Visualize some examples from the dataset","title":"Few-Shot learning with Reptile","text":"","code":"_, axarr = plt.subplots(nrows=5, ncols=5, figsize=(20, 20))  sample_keys = list(train_dataset.data.keys())  for a in range(5):     for b in range(5):         temp_image = train_dataset.data[sample_keys[a]][b]         temp_image = np.stack((temp_image[:, :, 0],) * 3, axis=2)         temp_image *= 255         temp_image = np.clip(temp_image, 0, 255).astype(\"uint8\")         if b == 2:             axarr[a, b].set_title(\"Class : \" + sample_keys[a])         axarr[a, b].imshow(temp_image, cmap=\"gray\")         axarr[a, b].xaxis.set_visible(False)         axarr[a, b].yaxis.set_visible(False) plt.show()"},{"path":"https://keras.posit.co/articles/examples/reptile.html","id":"build-the-model","dir":"Articles > Examples","previous_headings":"","what":"Build the model","title":"Few-Shot learning with Reptile","text":"","code":"def conv_bn(x):     x = layers.Conv2D(filters=64, kernel_size=3, strides=2, padding=\"same\")(x)     x = layers.BatchNormalization()(x)     return layers.ReLU()(x)   inputs = layers.Input(shape=(28, 28, 1)) x = conv_bn(inputs) x = conv_bn(x) x = conv_bn(x) x = conv_bn(x) x = layers.Flatten()(x) outputs = layers.Dense(classes, activation=\"softmax\")(x) model = keras.Model(inputs=inputs, outputs=outputs) model.compile() optimizer = keras.optimizers.SGD(learning_rate=learning_rate)"},{"path":"https://keras.posit.co/articles/examples/reptile.html","id":"train-the-model","dir":"Articles > Examples","previous_headings":"","what":"Train the model","title":"Few-Shot learning with Reptile","text":"","code":"training = [] testing = [] for meta_iter in range(meta_iters):     frac_done = meta_iter / meta_iters     cur_meta_step_size = (1 - frac_done) * meta_step_size     # Temporarily save the weights from the model.     old_vars = model.get_weights()     # Get a sample from the full dataset.     mini_dataset = train_dataset.get_mini_dataset(         inner_batch_size, inner_iters, train_shots, classes     )     for images, labels in mini_dataset:         with tf.GradientTape() as tape:             preds = model(images)             loss = keras.losses.sparse_categorical_crossentropy(labels, preds)         grads = tape.gradient(loss, model.trainable_weights)         optimizer.apply_gradients(zip(grads, model.trainable_weights))     new_vars = model.get_weights()     # Perform SGD for the meta step.     for var in range(len(new_vars)):         new_vars[var] = old_vars[var] + (             (new_vars[var] - old_vars[var]) * cur_meta_step_size         )     # After the meta-learning step, reload the newly-trained weights into the model.     model.set_weights(new_vars)     # Evaluation loop     if meta_iter % eval_interval == 0:         accuracies = []         for dataset in (train_dataset, test_dataset):             # Sample a mini dataset from the full dataset.             train_set, test_images, test_labels = dataset.get_mini_dataset(                 eval_batch_size, eval_iters, shots, classes, split=True             )             old_vars = model.get_weights()             # Train on the samples and get the resulting accuracies.             for images, labels in train_set:                 with tf.GradientTape() as tape:                     preds = model(images)                     loss = keras.losses.sparse_categorical_crossentropy(                         labels, preds                     )                 grads = tape.gradient(loss, model.trainable_weights)                 optimizer.apply_gradients(zip(grads, model.trainable_weights))             test_preds = model.predict(test_images)             test_preds = tf.argmax(test_preds).numpy()             num_correct = (test_preds == test_labels).sum()             # Reset the weights after getting the evaluation accuracies.             model.set_weights(old_vars)             accuracies.append(num_correct / classes)         training.append(accuracies[0])         testing.append(accuracies[1])         if meta_iter % 100 == 0:             print(                 \"batch %d: train=%f test=%f\"                 % (meta_iter, accuracies[0], accuracies[1])             )"},{"path":"https://keras.posit.co/articles/examples/reptile.html","id":"visualize-results","dir":"Articles > Examples","previous_headings":"","what":"Visualize Results","title":"Few-Shot learning with Reptile","text":"","code":"# First, some preprocessing to smooth the training and testing arrays for display. window_length = 100 train_s = np.r_[     training[window_length - 1 : 0 : -1],     training,     training[-1:-window_length:-1], ] test_s = np.r_[     testing[window_length - 1 : 0 : -1], testing, testing[-1:-window_length:-1] ] w = np.hamming(window_length) train_y = np.convolve(w / w.sum(), train_s, mode=\"valid\") test_y = np.convolve(w / w.sum(), test_s, mode=\"valid\")  # Display the training accuracies. x = np.arange(0, len(test_y), 1) plt.plot(x, test_y, x, train_y) plt.legend([\"test\", \"train\"]) plt.grid()  train_set, test_images, test_labels = dataset.get_mini_dataset(     eval_batch_size, eval_iters, shots, classes, split=True ) for images, labels in train_set:     with tf.GradientTape() as tape:         preds = model(images)         loss = keras.losses.sparse_categorical_crossentropy(labels, preds)     grads = tape.gradient(loss, model.trainable_weights)     optimizer.apply_gradients(zip(grads, model.trainable_weights)) test_preds = model.predict(test_images) test_preds = tf.argmax(test_preds).numpy()  _, axarr = plt.subplots(nrows=1, ncols=5, figsize=(20, 20))  sample_keys = list(train_dataset.data.keys())  for i, ax in zip(range(5), axarr):     temp_image = np.stack((test_images[i, :, :, 0],) * 3, axis=2)     temp_image *= 255     temp_image = np.clip(temp_image, 0, 255).astype(\"uint8\")     ax.set_title(         \"Label : {}, Prediction : {}\".format(int(test_labels[i]), test_preds[i])     )     ax.imshow(temp_image, cmap=\"gray\")     ax.xaxis.set_visible(False)     ax.yaxis.set_visible(False) plt.show()"},{"path":[]},{"path":"https://keras.posit.co/articles/examples/semisupervised_simclr.html","id":"semi-supervised-learning","dir":"Articles > Examples","previous_headings":"Introduction","what":"Semi-supervised learning","title":"Semi-supervised image classification using contrastive pretraining with SimCLR","text":"Semi-supervised learning machine learning paradigm deals partially labeled datasets. applying deep learning real world, one usually gather large dataset make work well. However, cost labeling scales linearly dataset size (labeling example takes constant time), model performance scales sublinearly . means labeling samples becomes less less cost-efficient, gathering unlabeled data generally cheap, usually readily available large quantities. Semi-supervised learning offers solve problem requiring partially labeled dataset, label-efficient utilizing unlabeled examples learning well. example, pretrain encoder contrastive learning STL-10 semi-supervised dataset using labels , fine-tune using labeled subset.","code":""},{"path":"https://keras.posit.co/articles/examples/semisupervised_simclr.html","id":"contrastive-learning","dir":"Articles > Examples","previous_headings":"Introduction","what":"Contrastive learning","title":"Semi-supervised image classification using contrastive pretraining with SimCLR","text":"highest level, main idea behind contrastive learning learn representations invariant image augmentations self-supervised manner. One problem objective trivial degenerate solution: case representations constant, depend input images. Contrastive learning avoids trap modifying objective following way: pulls representations augmented versions/views image closer (contracting positives), simultaneously pushing different images away (contrasting negatives) representation space. One contrastive approach SimCLR, essentially identifies core components needed optimize objective, can achieve high performance scaling simple approach. Another approach SimSiam (Keras example), whose main difference SimCLR former use negatives loss. Therefore, explicitly prevent trivial solution, , instead, avoids implicitly architecture design (asymmetric encoding paths using predictor network batch normalization (BatchNorm) applied final layers). reading SimCLR, check official Google AI blog post, overview self-supervised learning across vision language check blog post.","code":""},{"path":"https://keras.posit.co/articles/examples/semisupervised_simclr.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Semi-supervised image classification using contrastive pretraining with SimCLR","text":"","code":"# Make sure we are able to handle large datasets import resource  low, high = resource.getrlimit(resource.RLIMIT_NOFILE) resource.setrlimit(resource.RLIMIT_NOFILE, (high, high))  import math import matplotlib.pyplot as plt import tensorflow as tf import tensorflow_datasets as tfds  import keras as keras from keras import layers"},{"path":"https://keras.posit.co/articles/examples/semisupervised_simclr.html","id":"hyperparameter-setup","dir":"Articles > Examples","previous_headings":"","what":"Hyperparameter setup","title":"Semi-supervised image classification using contrastive pretraining with SimCLR","text":"","code":"# Dataset hyperparameters unlabeled_dataset_size = 100000 labeled_dataset_size = 5000 image_size = 96 image_channels = 3  # Algorithm hyperparameters num_epochs = 1 batch_size = 525  # Corresponds to 200 steps per epoch width = 128 temperature = 0.1 # Stronger augmentations for contrastive, weaker ones for supervised training contrastive_augmentation = {\"min_area\": 0.25, \"brightness\": 0.6, \"jitter\": 0.2} classification_augmentation = {     \"min_area\": 0.75,     \"brightness\": 0.3,     \"jitter\": 0.1, }"},{"path":"https://keras.posit.co/articles/examples/semisupervised_simclr.html","id":"dataset","dir":"Articles > Examples","previous_headings":"","what":"Dataset","title":"Semi-supervised image classification using contrastive pretraining with SimCLR","text":"training simultaneously load large batch unlabeled images along smaller batch labeled images.","code":"def prepare_dataset():     # Labeled and unlabeled samples are loaded synchronously     # with batch sizes selected accordingly     steps_per_epoch = (         unlabeled_dataset_size + labeled_dataset_size     ) // batch_size     unlabeled_batch_size = unlabeled_dataset_size // steps_per_epoch     labeled_batch_size = labeled_dataset_size // steps_per_epoch     print(         f\"batch size is {unlabeled_batch_size} (unlabeled) + {labeled_batch_size} (labeled)\"     )      # Turning off shuffle to lower resource usage     unlabeled_train_dataset = (         tfds.load(             \"stl10\", split=\"unlabelled\", as_supervised=True, shuffle_files=False         )         .shuffle(buffer_size=10 * unlabeled_batch_size)         .batch(unlabeled_batch_size)     )     labeled_train_dataset = (         tfds.load(             \"stl10\", split=\"train\", as_supervised=True, shuffle_files=False         )         .shuffle(buffer_size=10 * labeled_batch_size)         .batch(labeled_batch_size)     )     test_dataset = (         tfds.load(\"stl10\", split=\"test\", as_supervised=True)         .batch(batch_size)         .prefetch(buffer_size=tf.data.AUTOTUNE)     )      # Labeled and unlabeled datasets are zipped together     train_dataset = tf.data.Dataset.zip(         (unlabeled_train_dataset, labeled_train_dataset)     ).prefetch(buffer_size=tf.data.AUTOTUNE)      return train_dataset, labeled_train_dataset, test_dataset   # Load STL10 dataset train_dataset, labeled_train_dataset, test_dataset = prepare_dataset()"},{"path":"https://keras.posit.co/articles/examples/semisupervised_simclr.html","id":"image-augmentations","dir":"Articles > Examples","previous_headings":"","what":"Image augmentations","title":"Semi-supervised image classification using contrastive pretraining with SimCLR","text":"two important image augmentations contrastive learning following: Cropping: forces model encode different parts image similarly, implement RandomTranslation RandomZoom layers Color jitter: prevents trivial color histogram-based solution task distorting color histograms. principled way implement affine transformations color space. example use random horizontal flips well. Stronger augmentations applied contrastive learning, along weaker ones supervised classification avoid overfitting labeled examples. implement random color jitter custom preprocessing layer. Using preprocessing layers data augmentation following two advantages: data augmentation run GPU batches, training bottlenecked data pipeline environments constrained CPU resources (Colab Notebook, personal machine) Deployment easier data preprocessing pipeline encapsulated model, reimplemented deploying ","code":"# Distorts the color distibutions of images class RandomColorAffine(layers.Layer):     def __init__(self, brightness=0, jitter=0, **kwargs):         super().__init__(**kwargs)          self.brightness = brightness         self.jitter = jitter      def get_config(self):         config = super().get_config()         config.update({\"brightness\": self.brightness, \"jitter\": self.jitter})         return config      def call(self, images, training=True):         if training:             batch_size = tf.shape(images)[0]              # Same for all colors             brightness_scales = 1 + tf.random.uniform(                 (batch_size, 1, 1, 1),                 minval=-self.brightness,                 maxval=self.brightness,             )             # Different for all colors             jitter_matrices = tf.random.uniform(                 (batch_size, 1, 3, 3), minval=-self.jitter, maxval=self.jitter             )              color_transforms = (                 tf.eye(3, batch_shape=[batch_size, 1]) * brightness_scales                 + jitter_matrices             )             images = tf.clip_by_value(tf.matmul(images, color_transforms), 0, 1)         return images   # Image augmentation module def get_augmenter(min_area, brightness, jitter):     zoom_factor = 1.0 - math.sqrt(min_area)     return keras.Sequential(         [             keras.Input(shape=(image_size, image_size, image_channels)),             layers.Rescaling(1 / 255, dtype=\"uint8\"),             layers.RandomFlip(\"horizontal\"),             layers.RandomTranslation(zoom_factor / 2, zoom_factor / 2),             layers.RandomZoom((-zoom_factor, 0.0), (-zoom_factor, 0.0)),             RandomColorAffine(brightness, jitter),         ]     )   def visualize_augmentations(num_images):     # Sample a batch from a dataset     images = next(iter(train_dataset))[0][0][:num_images]      # Apply augmentations     augmented_images = zip(         images,         get_augmenter(**classification_augmentation)(images),         get_augmenter(**contrastive_augmentation)(images),         get_augmenter(**contrastive_augmentation)(images),     )     row_titles = [         \"Original:\",         \"Weakly augmented:\",         \"Strongly augmented:\",         \"Strongly augmented:\",     ]     plt.figure(figsize=(num_images * 2.2, 4 * 2.2), dpi=100)     for column, image_row in enumerate(augmented_images):         for row, image in enumerate(image_row):             plt.subplot(4, num_images, row * num_images + column + 1)             plt.imshow(image)             if column == 0:                 plt.title(row_titles[row], loc=\"left\")             plt.axis(\"off\")     plt.tight_layout()   visualize_augmentations(num_images=8)"},{"path":"https://keras.posit.co/articles/examples/semisupervised_simclr.html","id":"encoder-architecture","dir":"Articles > Examples","previous_headings":"","what":"Encoder architecture","title":"Semi-supervised image classification using contrastive pretraining with SimCLR","text":"","code":"# Define the encoder architecture def get_encoder():     return keras.Sequential(         [             keras.Input(shape=(image_size, image_size, image_channels)),             layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),             layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),             layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),             layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),             layers.Flatten(),             layers.Dense(width, activation=\"relu\"),         ],         name=\"encoder\",     )"},{"path":"https://keras.posit.co/articles/examples/semisupervised_simclr.html","id":"supervised-baseline-model","dir":"Articles > Examples","previous_headings":"","what":"Supervised baseline model","title":"Semi-supervised image classification using contrastive pretraining with SimCLR","text":"baseline supervised model trained using random initialization.","code":"# Baseline supervised training with random initialization baseline_model = keras.Sequential(     [         keras.Input(shape=(image_size, image_size, image_channels)),         get_augmenter(**classification_augmentation),         get_encoder(),         layers.Dense(10),     ],     name=\"baseline_model\", ) baseline_model.compile(     optimizer=keras.optimizers.Adam(),     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),     metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")], )  baseline_history = baseline_model.fit(     labeled_train_dataset, epochs=num_epochs, validation_data=test_dataset )  print(     \"Maximal validation accuracy: {:.2f}%\".format(         max(baseline_history.history[\"val_acc\"]) * 100     ) )"},{"path":"https://keras.posit.co/articles/examples/semisupervised_simclr.html","id":"self-supervised-model-for-contrastive-pretraining","dir":"Articles > Examples","previous_headings":"","what":"Self-supervised model for contrastive pretraining","title":"Semi-supervised image classification using contrastive pretraining with SimCLR","text":"pretrain encoder unlabeled images contrastive loss. nonlinear projection head attached top encoder, improves quality representations encoder. use InfoNCE/NT-Xent/N-pairs loss, can interpreted following way: treat image batch class. , two examples (pair augmented views) “class”. view’s representation compared every possible pair’s one (augmented versions). use temperature-scaled cosine similarity compared representations logits. Finally, use categorical cross-entropy “classification” loss following two metrics used monitoring pretraining performance: Contrastive accuracy (SimCLR Table 5): Self-supervised metric, ratio cases representation image similar differently augmented version’s one, representation image current batch. Self-supervised metrics can used hyperparameter tuning even case labeled examples. Linear probing accuracy: Linear probing popular metric evaluate self-supervised classifiers. computed accuracy logistic regression classifier trained top encoder’s features. case, done training single dense layer top frozen encoder. Note contrary traditional approach classifier trained pretraining phase, example train pretraining. might slightly decrease accuracy, way can monitor value training, helps experimentation debugging. Another widely used supervised metric KNN accuracy, accuracy KNN classifier trained top encoder’s features, implemented example.","code":"# Define the contrastive model with model-subclassing class ContrastiveModel(keras.Model):     def __init__(self):         super().__init__()          self.temperature = temperature         self.contrastive_augmenter = get_augmenter(**contrastive_augmentation)         self.classification_augmenter = get_augmenter(             **classification_augmentation         )         self.encoder = get_encoder()         # Non-linear MLP as projection head         self.projection_head = keras.Sequential(             [                 keras.Input(shape=(width,)),                 layers.Dense(width, activation=\"relu\"),                 layers.Dense(width),             ],             name=\"projection_head\",         )         # Single dense layer for linear probing         self.linear_probe = keras.Sequential(             [layers.Input(shape=(width,)), layers.Dense(10)],             name=\"linear_probe\",         )          self.encoder.summary()         self.projection_head.summary()         self.linear_probe.summary()      def compile(self, contrastive_optimizer, probe_optimizer, **kwargs):         super().compile(**kwargs)          self.contrastive_optimizer = contrastive_optimizer         self.probe_optimizer = probe_optimizer          # self.contrastive_loss will be defined as a method         self.probe_loss = keras.losses.SparseCategoricalCrossentropy(             from_logits=True         )          self.contrastive_loss_tracker = keras.metrics.Mean(name=\"c_loss\")         self.contrastive_accuracy = keras.metrics.SparseCategoricalAccuracy(             name=\"c_acc\"         )         self.probe_loss_tracker = keras.metrics.Mean(name=\"p_loss\")         self.probe_accuracy = keras.metrics.SparseCategoricalAccuracy(             name=\"p_acc\"         )      @property     def metrics(self):         return [             self.contrastive_loss_tracker,             self.contrastive_accuracy,             self.probe_loss_tracker,             self.probe_accuracy,         ]      def contrastive_loss(self, projections_1, projections_2):         # InfoNCE loss (information noise-contrastive estimation)         # NT-Xent loss (normalized temperature-scaled cross entropy)          # Cosine similarity: the dot product of the l2-normalized feature vectors         projections_1 = tf.math.l2_normalize(projections_1, axis=1)         projections_2 = tf.math.l2_normalize(projections_2, axis=1)         similarities = (             tf.matmul(projections_1, projections_2, transpose_b=True)             / self.temperature         )          # The similarity between the representations of two augmented views of the         # same image should be higher than their similarity with other views         batch_size = tf.shape(projections_1)[0]         contrastive_labels = tf.range(batch_size)         self.contrastive_accuracy.update_state(contrastive_labels, similarities)         self.contrastive_accuracy.update_state(             contrastive_labels, tf.transpose(similarities)         )          # The temperature-scaled similarities are used as logits for cross-entropy         # a symmetrized version of the loss is used here         loss_1_2 = keras.losses.sparse_categorical_crossentropy(             contrastive_labels, similarities, from_logits=True         )         loss_2_1 = keras.losses.sparse_categorical_crossentropy(             contrastive_labels, tf.transpose(similarities), from_logits=True         )         return (loss_1_2 + loss_2_1) / 2      def train_step(self, data):         (unlabeled_images, _), (labeled_images, labels) = data          # Both labeled and unlabeled images are used, without labels         images = tf.concat((unlabeled_images, labeled_images), axis=0)         # Each image is augmented twice, differently         augmented_images_1 = self.contrastive_augmenter(images, training=True)         augmented_images_2 = self.contrastive_augmenter(images, training=True)         with tf.GradientTape() as tape:             features_1 = self.encoder(augmented_images_1, training=True)             features_2 = self.encoder(augmented_images_2, training=True)             # The representations are passed through a projection mlp             projections_1 = self.projection_head(features_1, training=True)             projections_2 = self.projection_head(features_2, training=True)             contrastive_loss = self.contrastive_loss(                 projections_1, projections_2             )         gradients = tape.gradient(             contrastive_loss,             self.encoder.trainable_weights             + self.projection_head.trainable_weights,         )         self.contrastive_optimizer.apply_gradients(             zip(                 gradients,                 self.encoder.trainable_weights                 + self.projection_head.trainable_weights,             )         )         self.contrastive_loss_tracker.update_state(contrastive_loss)          # Labels are only used in evalutation for an on-the-fly logistic regression         preprocessed_images = self.classification_augmenter(             labeled_images, training=True         )         with tf.GradientTape() as tape:             # the encoder is used in inference mode here to avoid regularization             # and updating the batch normalization paramers if they are used             features = self.encoder(preprocessed_images, training=False)             class_logits = self.linear_probe(features, training=True)             probe_loss = self.probe_loss(labels, class_logits)         gradients = tape.gradient(             probe_loss, self.linear_probe.trainable_weights         )         self.probe_optimizer.apply_gradients(             zip(gradients, self.linear_probe.trainable_weights)         )         self.probe_loss_tracker.update_state(probe_loss)         self.probe_accuracy.update_state(labels, class_logits)          return {m.name: m.result() for m in self.metrics}      def test_step(self, data):         labeled_images, labels = data          # For testing the components are used with a training=False flag         preprocessed_images = self.classification_augmenter(             labeled_images, training=False         )         features = self.encoder(preprocessed_images, training=False)         class_logits = self.linear_probe(features, training=False)         probe_loss = self.probe_loss(labels, class_logits)         self.probe_loss_tracker.update_state(probe_loss)         self.probe_accuracy.update_state(labels, class_logits)          # Only the probe metrics are logged at test time         return {m.name: m.result() for m in self.metrics[2:]}   # Contrastive pretraining pretraining_model = ContrastiveModel() pretraining_model.compile(     contrastive_optimizer=keras.optimizers.Adam(),     probe_optimizer=keras.optimizers.Adam(), )  pretraining_history = pretraining_model.fit(     train_dataset, epochs=num_epochs, validation_data=test_dataset ) print(     \"Maximal validation accuracy: {:.2f}%\".format(         max(pretraining_history.history[\"val_p_acc\"]) * 100     ) )"},{"path":"https://keras.posit.co/articles/examples/semisupervised_simclr.html","id":"supervised-finetuning-of-the-pretrained-encoder","dir":"Articles > Examples","previous_headings":"","what":"Supervised finetuning of the pretrained encoder","title":"Semi-supervised image classification using contrastive pretraining with SimCLR","text":"finetune encoder labeled examples, attaching single randomly initalized fully connected classification layer top.","code":"# Supervised finetuning of the pretrained encoder finetuning_model = keras.Sequential(     [         layers.Input(shape=(image_size, image_size, image_channels)),         get_augmenter(**classification_augmentation),         pretraining_model.encoder,         layers.Dense(10),     ],     name=\"finetuning_model\", ) finetuning_model.compile(     optimizer=keras.optimizers.Adam(),     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),     metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")], )  finetuning_history = finetuning_model.fit(     labeled_train_dataset, epochs=num_epochs, validation_data=test_dataset ) print(     \"Maximal validation accuracy: {:.2f}%\".format(         max(finetuning_history.history[\"val_acc\"]) * 100     ) )"},{"path":"https://keras.posit.co/articles/examples/semisupervised_simclr.html","id":"comparison-against-the-baseline","dir":"Articles > Examples","previous_headings":"","what":"Comparison against the baseline","title":"Semi-supervised image classification using contrastive pretraining with SimCLR","text":"comparing training curves, can see using contrastive pretraining, higher validation accuracy can reached, paired lower validation loss, means pretrained network able generalize better seeing small amount labeled examples.","code":"# The classification accuracies of the baseline and the pretraining + finetuning process: def plot_training_curves(     pretraining_history, finetuning_history, baseline_history ):     for metric_key, metric_name in zip([\"acc\", \"loss\"], [\"accuracy\", \"loss\"]):         plt.figure(figsize=(8, 5), dpi=100)         plt.plot(             baseline_history.history[f\"val_{metric_key}\"],             label=\"supervised baseline\",         )         plt.plot(             pretraining_history.history[f\"val_p_{metric_key}\"],             label=\"self-supervised pretraining\",         )         plt.plot(             finetuning_history.history[f\"val_{metric_key}\"],             label=\"supervised finetuning\",         )         plt.legend()         plt.title(f\"Classification {metric_name} during training\")         plt.xlabel(\"epochs\")         plt.ylabel(f\"validation {metric_name}\")   plot_training_curves(pretraining_history, finetuning_history, baseline_history)"},{"path":[]},{"path":"https://keras.posit.co/articles/examples/semisupervised_simclr.html","id":"architecture","dir":"Articles > Examples","previous_headings":"Improving further","what":"Architecture","title":"Semi-supervised image classification using contrastive pretraining with SimCLR","text":"experiment original paper demonstrated increasing width depth models improves performance higher rate supervised learning. Also, using ResNet-50 encoder quite standard literature. However keep mind, powerful models increase training time also require memory limit maximal batch size can use. reported usage BatchNorm layers sometimes degrade performance, introduces intra-batch dependency samples, used example. experiments however, using BatchNorm, especially projection head, improves performance.","code":""},{"path":"https://keras.posit.co/articles/examples/semisupervised_simclr.html","id":"hyperparameters","dir":"Articles > Examples","previous_headings":"Improving further","what":"Hyperparameters","title":"Semi-supervised image classification using contrastive pretraining with SimCLR","text":"hyperparameters used example tuned manually task architecture. Therefore, without changing , marginal gains can expected hyperparameter tuning. However different task model architecture need tuning, notes important ones: Batch size: since objective can interpreted classification batch images (loosely speaking), batch size actually important hyperparameter usual. higher, better. Temperature: temperature defines “softness” softmax distribution used cross-entropy loss, important hyperparameter. Lower values generally lead higher contrastive accuracy. recent trick (ALIGN) learn temperature’s value well (can done defining tf.Variable, applying gradients ). Even though provides good baseline value, experiments learned temperature somewhat lower optimal, optimized respect contrastive loss, perfect proxy representation quality. Image augmentation strength: pretraining stronger augmentations increase difficulty task, however point strong augmentations degrade performance. finetuning stronger augmentations reduce overfitting experience strong augmentations decrease performance gains pretraining. whole data augmentation pipeline can seen important hyperparameter algorithm, implementations custom image augmentation layers Keras can found repository. Learning rate schedule: constant schedule used , quite common literature use cosine decay schedule, can improve performance. Optimizer: Adam used example, provides good performance default parameters. SGD momentum requires tuning, however slightly increase performance.","code":""},{"path":"https://keras.posit.co/articles/examples/semisupervised_simclr.html","id":"related-works","dir":"Articles > Examples","previous_headings":"","what":"Related works","title":"Semi-supervised image classification using contrastive pretraining with SimCLR","text":"instance-level (image-level) contrastive learning methods: MoCo (v2, v3): uses momentum-encoder well, whose weights exponential moving average target encoder SwAV: uses clustering instead pairwise comparison BarlowTwins: uses cross correlation-based objective instead pairwise comparison Keras implementations MoCo BarlowTwins can found repository, includes Colab notebook. also new line works, optimize similar objective, without use negatives: BYOL: momentum-encoder + negatives SimSiam (Keras example): momentum-encoder + negatives experience, methods brittle (can collapse constant representation, get work using encoder architecture). Even though generally dependent model architecture, can improve performance smaller batch sizes. can use trained model hosted Hugging Face Hub try demo Hugging Face Spaces.","code":""},{"path":"https://keras.posit.co/articles/examples/shiftvit.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"A Vision Transformer without Attention","text":"Vision Transformers (ViTs) sparked wave research intersection Transformers Computer Vision (CV). ViTs can simultaneously model long- short-range dependencies, thanks Multi-Head Self-Attention mechanism Transformer block. Many researchers believe success ViTs purely due attention layer, seldom think parts ViT model. academic paper Shift Operation Meets Vision Transformer: Extremely Simple Alternative Attention Mechanism authors propose demystify success ViTs introduction PARAMETER operation place attention operation. swap attention operation shifting operation. example, minimally implement paper close alignement author’s official implementation.","code":""},{"path":"https://keras.posit.co/articles/examples/shiftvit.html","id":"setup-and-imports","dir":"Articles > Examples","previous_headings":"","what":"Setup and imports","title":"A Vision Transformer without Attention","text":"","code":"import os  os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  import numpy as np import matplotlib.pyplot as plt  import tensorflow as tf import keras as keras from keras import layers   # Setting seed for reproducibiltiy SEED = 42 keras.utils.set_random_seed(SEED)"},{"path":"https://keras.posit.co/articles/examples/shiftvit.html","id":"hyperparameters","dir":"Articles > Examples","previous_headings":"","what":"Hyperparameters","title":"A Vision Transformer without Attention","text":"hyperparameters chosen experiment. Please feel free tune .","code":"class Config(object):     # DATA     batch_size = 256     buffer_size = batch_size * 2     input_shape = (32, 32, 3)     num_classes = 10      # AUGMENTATION     image_size = 48      # ARCHITECTURE     patch_size = 4     projected_dim = 96     num_shift_blocks_per_stages = [2, 4, 8, 2]     epsilon = 1e-5     stochastic_depth_rate = 0.2     mlp_dropout_rate = 0.2     num_div = 12     shift_pixel = 1     mlp_expand_ratio = 2      # OPTIMIZER     lr_start = 1e-5     lr_max = 1e-3     weight_decay = 1e-4      # TRAINING     epochs = 100   config = Config()"},{"path":"https://keras.posit.co/articles/examples/shiftvit.html","id":"load-the-cifar-10-dataset","dir":"Articles > Examples","previous_headings":"","what":"Load the CIFAR-10 dataset","title":"A Vision Transformer without Attention","text":"use CIFAR-10 dataset experiments.","code":"(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data() (x_train, y_train), (x_val, y_val) = (     (x_train[:40000], y_train[:40000]),     (x_train[40000:], y_train[40000:]), ) print(f\"Training samples: {len(x_train)}\") print(f\"Validation samples: {len(x_val)}\") print(f\"Testing samples: {len(x_test)}\")  AUTO = tf.data.AUTOTUNE train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)) train_ds = (     train_ds.shuffle(config.buffer_size).batch(config.batch_size).prefetch(AUTO) )  val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)) val_ds = val_ds.batch(config.batch_size).prefetch(AUTO)  test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)) test_ds = test_ds.batch(config.batch_size).prefetch(AUTO)"},{"path":"https://keras.posit.co/articles/examples/shiftvit.html","id":"data-augmentation","dir":"Articles > Examples","previous_headings":"","what":"Data Augmentation","title":"A Vision Transformer without Attention","text":"augmentation pipeline consists : Rescaling Resizing Random cropping Random horizontal flipping Note: image data augmentation layers apply data transformations inference time. means layers called training=False behave differently. Refer documentation details.","code":"def get_augmentation_model():     \"\"\"Build the data augmentation model.\"\"\"     data_augmentation = keras.Sequential(         [             layers.Resizing(                 config.input_shape[0] + 20, config.input_shape[0] + 20             ),             layers.RandomCrop(config.image_size, config.image_size),             layers.RandomFlip(\"horizontal\"),             layers.Rescaling(1 / 255.0),         ]     )     return data_augmentation"},{"path":"https://keras.posit.co/articles/examples/shiftvit.html","id":"the-shiftvit-architecture","dir":"Articles > Examples","previous_headings":"","what":"The ShiftViT architecture","title":"A Vision Transformer without Attention","text":"section, build architecture proposed ShiftViT paper. architecture shown Fig. 1, inspired Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. authors propose modular architecture 4 stages. stage works spatial size, creating hierarchical architecture. input image size HxWx3 split non-overlapping patches size 4x4. done via patchify layer results individual tokens feature size 48 (4x4x3). stage comprises two parts: Embedding Generation Stacked Shift Blocks discuss stages modules detail follows. Note: Compared official implementation restructure key components better fit Keras API.","code":""},{"path":"https://keras.posit.co/articles/examples/shiftvit.html","id":"the-shiftvit-block","dir":"Articles > Examples","previous_headings":"The ShiftViT architecture","what":"The ShiftViT Block","title":"A Vision Transformer without Attention","text":"stage ShiftViT architecture comprises Shift Block shown Fig 2. Shift Block shown Fig. 3, comprises following: Shift Operation Linear Normalization MLP Layer","code":""},{"path":"https://keras.posit.co/articles/examples/shiftvit.html","id":"the-mlp-block","dir":"Articles > Examples","previous_headings":"The ShiftViT architecture > The ShiftViT Block","what":"The MLP block","title":"A Vision Transformer without Attention","text":"MLP block intended stack densely-connected layers.s","code":"class MLP(layers.Layer):     \"\"\"Get the MLP layer for each shift block.      Args:         mlp_expand_ratio (int): The ratio with which the first feature map is expanded.         mlp_dropout_rate (float): The rate for dropout.     \"\"\"      def __init__(self, mlp_expand_ratio, mlp_dropout_rate, **kwargs):         super().__init__(**kwargs)         self.mlp_expand_ratio = mlp_expand_ratio         self.mlp_dropout_rate = mlp_dropout_rate      def build(self, input_shape):         input_channels = input_shape[-1]         initial_filters = int(self.mlp_expand_ratio * input_channels)          self.mlp = keras.Sequential(             [                 layers.Dense(                     units=initial_filters,                     activation=tf.nn.gelu,                 ),                 layers.Dropout(rate=self.mlp_dropout_rate),                 layers.Dense(units=input_channels),                 layers.Dropout(rate=self.mlp_dropout_rate),             ]         )      def call(self, x):         x = self.mlp(x)         return x"},{"path":"https://keras.posit.co/articles/examples/shiftvit.html","id":"the-droppath-layer","dir":"Articles > Examples","previous_headings":"The ShiftViT architecture > The ShiftViT Block","what":"The DropPath layer","title":"A Vision Transformer without Attention","text":"Stochastic depth regularization technique randomly drops set layers. inference, layers kept . similar Dropout, operates block layers rather individual nodes present inside layer.","code":"class DropPath(layers.Layer):     \"\"\"Drop Path also known as the Stochastic Depth layer.      Refernece:         - https://keras.io/examples/vision/cct/#stochastic-depth-for-regularization         - github.com:rwightman/pytorch-image-models     \"\"\"      def __init__(self, drop_path_prob, **kwargs):         super().__init__(**kwargs)         self.drop_path_prob = drop_path_prob      def call(self, x, training=False):         if training:             keep_prob = 1 - self.drop_path_prob             shape = (tf.shape(x)[0],) + (1,) * (len(x.shape) - 1)             random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)             random_tensor = tf.floor(random_tensor)             return (x / keep_prob) * random_tensor         return x"},{"path":"https://keras.posit.co/articles/examples/shiftvit.html","id":"block","dir":"Articles > Examples","previous_headings":"The ShiftViT architecture > The ShiftViT Block","what":"Block","title":"A Vision Transformer without Attention","text":"important operation paper shift opperation. section, describe shift operation compare original implementation provided authors. generic feature map assumed shape [N, H, W, C]. choose num_div parameter decides division size channels. first 4 divisions shifted (1 pixel) left, right, , direction. remaining splits kept . partial shifting shifted channels padded overflown pixels chopped . completes partial shifting operation. original implementation, code approximately: TensorFlow infeasible us assign shifted channels tensor middle training process. resorted following procedure: Split channels num_div parameter. Select first four spilts shift pad respective directions. shifting padding, concatenate channel back. entire procedure explained Fig. 4.","code":"out[:, g * 0:g * 1, :, :-1] = x[:, g * 0:g * 1, :, 1:]  # shift left out[:, g * 1:g * 2, :, 1:] = x[:, g * 1:g * 2, :, :-1]  # shift right out[:, g * 2:g * 3, :-1, :] = x[:, g * 2:g * 3, 1:, :]  # shift up out[:, g * 3:g * 4, 1:, :] = x[:, g * 3:g * 4, :-1, :]  # shift down  out[:, g * 4:, :, :] = x[:, g * 4:, :, :]  # no shift class ShiftViTBlock(layers.Layer):     \"\"\"A unit ShiftViT Block      Args:         shift_pixel (int): The number of pixels to shift. Defaults to `1`.         mlp_expand_ratio (int): The ratio with which MLP features are             expanded. Defaults to `2`.         mlp_dropout_rate (float): The dropout rate used in MLP.         num_div (int): The number of divisions of the feature map's channel.             Totally, 4/num_div of channels will be shifted. Defaults to 12.         epsilon (float): Epsilon constant.         drop_path_prob (float): The drop probability for drop path.     \"\"\"      def __init__(         self,         epsilon,         drop_path_prob,         mlp_dropout_rate,         num_div=12,         shift_pixel=1,         mlp_expand_ratio=2,         **kwargs,     ):         super().__init__(**kwargs)         self.shift_pixel = shift_pixel         self.mlp_expand_ratio = mlp_expand_ratio         self.mlp_dropout_rate = mlp_dropout_rate         self.num_div = num_div         self.epsilon = epsilon         self.drop_path_prob = drop_path_prob      def build(self, input_shape):         self.H = input_shape[1]         self.W = input_shape[2]         self.C = input_shape[3]         self.layer_norm = layers.LayerNormalization(epsilon=self.epsilon)         self.drop_path = (             DropPath(drop_path_prob=self.drop_path_prob)             if self.drop_path_prob > 0.0             else layers.Activation(\"linear\")         )         self.mlp = MLP(             mlp_expand_ratio=self.mlp_expand_ratio,             mlp_dropout_rate=self.mlp_dropout_rate,         )      def get_shift_pad(self, x, mode):         \"\"\"Shifts the channels according to the mode chosen.\"\"\"         if mode == \"left\":             offset_height = 0             offset_width = 0             target_height = 0             target_width = self.shift_pixel         elif mode == \"right\":             offset_height = 0             offset_width = self.shift_pixel             target_height = 0             target_width = self.shift_pixel         elif mode == \"up\":             offset_height = 0             offset_width = 0             target_height = self.shift_pixel             target_width = 0         else:             offset_height = self.shift_pixel             offset_width = 0             target_height = self.shift_pixel             target_width = 0         crop = tf.image.crop_to_bounding_box(             x,             offset_height=offset_height,             offset_width=offset_width,             target_height=self.H - target_height,             target_width=self.W - target_width,         )         shift_pad = tf.image.pad_to_bounding_box(             crop,             offset_height=offset_height,             offset_width=offset_width,             target_height=self.H,             target_width=self.W,         )         return shift_pad      def call(self, x, training=False):         # Split the feature maps         x_splits = tf.split(             x, num_or_size_splits=self.C // self.num_div, axis=-1         )          # Shift the feature maps         x_splits[0] = self.get_shift_pad(x_splits[0], mode=\"left\")         x_splits[1] = self.get_shift_pad(x_splits[1], mode=\"right\")         x_splits[2] = self.get_shift_pad(x_splits[2], mode=\"up\")         x_splits[3] = self.get_shift_pad(x_splits[3], mode=\"down\")          # Concatenate the shifted and unshifted feature maps         x = tf.concat(x_splits, axis=-1)          # Add the residual connection         shortcut = x         x = shortcut + self.drop_path(             self.mlp(self.layer_norm(x)), training=training         )         return x"},{"path":"https://keras.posit.co/articles/examples/shiftvit.html","id":"the-shiftvit-blocks","dir":"Articles > Examples","previous_headings":"The ShiftViT architecture","what":"The ShiftViT blocks","title":"A Vision Transformer without Attention","text":"stage architecture shift blocks shown Fig.5. blocks contain variable number stacked ShiftViT block (built earlier section). Shift blocks followed PatchMerging layer scales feature inputs. PatchMerging layer helps pyramidal structure model.","code":""},{"path":"https://keras.posit.co/articles/examples/shiftvit.html","id":"the-patchmerging-layer","dir":"Articles > Examples","previous_headings":"The ShiftViT architecture > The ShiftViT blocks","what":"The PatchMerging layer","title":"A Vision Transformer without Attention","text":"layer merges two adjacent tokens. layer helps scaling features spatially increasing features channel wise. use Conv2D layer merge patches.","code":"class PatchMerging(layers.Layer):     \"\"\"The Patch Merging layer.      Args:         epsilon (float): The epsilon constant.     \"\"\"      def __init__(self, epsilon, **kwargs):         super().__init__(**kwargs)         self.epsilon = epsilon      def build(self, input_shape):         filters = 2 * input_shape[-1]         self.reduction = layers.Conv2D(             filters=filters,             kernel_size=2,             strides=2,             padding=\"same\",             use_bias=False,         )         self.layer_norm = layers.LayerNormalization(epsilon=self.epsilon)      def call(self, x):         # Apply the patch merging algorithm on the feature maps         x = self.layer_norm(x)         x = self.reduction(x)         return x"},{"path":"https://keras.posit.co/articles/examples/shiftvit.html","id":"stacked-shift-blocks","dir":"Articles > Examples","previous_headings":"The ShiftViT architecture > The ShiftViT blocks","what":"Stacked Shift Blocks","title":"A Vision Transformer without Attention","text":"stage variable number stacked ShiftViT Blocks, suggested paper. generic layer contain stacked shift vit blocks patch merging layer well. Combining two operations (shift ViT block patch merging) design choice picked better code reusability.","code":"# Note: This layer will have a different depth of stacking # for different stages on the model. class StackedShiftBlocks(layers.Layer):     \"\"\"The layer containing stacked ShiftViTBlocks.      Args:         epsilon (float): The epsilon constant.         mlp_dropout_rate (float): The dropout rate used in the MLP block.         num_shift_blocks (int): The number of shift vit blocks for this stage.         stochastic_depth_rate (float): The maximum drop path rate chosen.         is_merge (boolean): A flag that determines the use of the Patch Merge             layer after the shift vit blocks.         num_div (int): The division of channels of the feature map. Defaults to `12`.         shift_pixel (int): The number of pixels to shift. Defaults to `1`.         mlp_expand_ratio (int): The ratio with which the initial dense layer of             the MLP is expanded Defaults to `2`.     \"\"\"      def __init__(         self,         epsilon,         mlp_dropout_rate,         num_shift_blocks,         stochastic_depth_rate,         is_merge,         num_div=12,         shift_pixel=1,         mlp_expand_ratio=2,         **kwargs,     ):         super().__init__(**kwargs)         self.epsilon = epsilon         self.mlp_dropout_rate = mlp_dropout_rate         self.num_shift_blocks = num_shift_blocks         self.stochastic_depth_rate = stochastic_depth_rate         self.is_merge = is_merge         self.num_div = num_div         self.shift_pixel = shift_pixel         self.mlp_expand_ratio = mlp_expand_ratio      def build(self, input_shapes):         # Calculate stochastic depth probabilities.         # Reference: https://keras.io/examples/vision/cct/#the-final-cct-model         dpr = [             x             for x in np.linspace(                 start=0,                 stop=self.stochastic_depth_rate,                 num=self.num_shift_blocks,             )         ]          # Build the shift blocks as a list of ShiftViT Blocks         self.shift_blocks = list()         for num in range(self.num_shift_blocks):             self.shift_blocks.append(                 ShiftViTBlock(                     num_div=self.num_div,                     epsilon=self.epsilon,                     drop_path_prob=dpr[num],                     mlp_dropout_rate=self.mlp_dropout_rate,                     shift_pixel=self.shift_pixel,                     mlp_expand_ratio=self.mlp_expand_ratio,                 )             )         if self.is_merge:             self.patch_merge = PatchMerging(epsilon=self.epsilon)      def call(self, x, training=False):         for shift_block in self.shift_blocks:             x = shift_block(x, training=training)         if self.is_merge:             x = self.patch_merge(x)         return x"},{"path":"https://keras.posit.co/articles/examples/shiftvit.html","id":"the-shiftvit-model","dir":"Articles > Examples","previous_headings":"","what":"The ShiftViT model","title":"A Vision Transformer without Attention","text":"Build ShiftViT custom model.","code":"class ShiftViTModel(keras.Model):     \"\"\"The ShiftViT Model.      Args:         data_augmentation (keras.Model): A data augmentation model.         projected_dim (int): The dimension to which the patches of the image are             projected.         patch_size (int): The patch size of the images.         num_shift_blocks_per_stages (list[int]): A list of all the number of shit             blocks per stage.         epsilon (float): The epsilon constant.         mlp_dropout_rate (float): The dropout rate used in the MLP block.         stochastic_depth_rate (float): The maximum drop rate probability.         num_div (int): The number of divisions of the channesl of the feature             map. Defaults to `12`.         shift_pixel (int): The number of pixel to shift. Defaults to `1`.         mlp_expand_ratio (int): The ratio with which the initial mlp dense layer             is expanded to. Defaults to `2`.     \"\"\"      def __init__(         self,         data_augmentation,         projected_dim,         patch_size,         num_shift_blocks_per_stages,         epsilon,         mlp_dropout_rate,         stochastic_depth_rate,         num_div=12,         shift_pixel=1,         mlp_expand_ratio=2,         **kwargs,     ):         super().__init__(**kwargs)         self.data_augmentation = data_augmentation         self.patch_projection = layers.Conv2D(             filters=projected_dim,             kernel_size=patch_size,             strides=patch_size,             padding=\"same\",         )         self.stages = list()         for index, num_shift_blocks in enumerate(num_shift_blocks_per_stages):             if index == len(num_shift_blocks_per_stages) - 1:                 # This is the last stage, do not use the patch merge here.                 is_merge = False             else:                 is_merge = True             # Build the stages.             self.stages.append(                 StackedShiftBlocks(                     epsilon=epsilon,                     mlp_dropout_rate=mlp_dropout_rate,                     num_shift_blocks=num_shift_blocks,                     stochastic_depth_rate=stochastic_depth_rate,                     is_merge=is_merge,                     num_div=num_div,                     shift_pixel=shift_pixel,                     mlp_expand_ratio=mlp_expand_ratio,                 )             )         self.global_avg_pool = layers.GlobalAveragePooling2D()      def get_config(self):         config = super().get_config()         config.update(             {                 \"data_augmentation\": self.data_augmentation,                 \"patch_projection\": self.patch_projection,                 \"stages\": self.stages,                 \"global_avg_pool\": self.global_avg_pool,             }         )         return config      def _calculate_loss(self, data, training=False):         (images, labels) = data          # Augment the images         augmented_images = self.data_augmentation(images, training=training)          # Create patches and project the pathces.         projected_patches = self.patch_projection(augmented_images)          # Pass through the stages         x = projected_patches         for stage in self.stages:             x = stage(x, training=training)          # Get the logits.         logits = self.global_avg_pool(x)          # Calculate the loss and return it.         total_loss = self.compute_loss(data, labels, logits)         return total_loss, labels, logits      def train_step(self, inputs):         with tf.GradientTape() as tape:             total_loss, labels, logits = self._calculate_loss(                 data=inputs, training=True             )          # Apply gradients.         train_vars = [             self.data_augmentation.trainable_variables,             self.patch_projection.trainable_variables,             self.global_avg_pool.trainable_variables,         ]         train_vars = train_vars + [             stage.trainable_variables for stage in self.stages         ]          # Optimize the gradients.         grads = tape.gradient(total_loss, train_vars)         trainable_variable_list = []         for grad, var in zip(grads, train_vars):             for g, v in zip(grad, var):                 trainable_variable_list.append((g, v))         self.optimizer.apply_gradients(trainable_variable_list)          # Update the metrics         for metric in self.metrics:             if metric.name == \"loss\":                 metric.update_state(total_loss)             else:                 metric.update_state(labels, logits)          # Return a dict mapping metric names to current value         return {m.name: m.result() for m in self.metrics}      def test_step(self, data):         loss, labels, logits = self._calculate_loss(data=data, training=False)          # Update the metrics         for metric in self.metrics:             if metric.name == \"loss\":                 metric.update_state(loss)             else:                 metric.update_state(labels, logits)          # Return a dict mapping metric names to current value         return {m.name: m.result() for m in self.metrics}"},{"path":"https://keras.posit.co/articles/examples/shiftvit.html","id":"instantiate-the-model","dir":"Articles > Examples","previous_headings":"","what":"Instantiate the model","title":"A Vision Transformer without Attention","text":"","code":"model = ShiftViTModel(     data_augmentation=get_augmentation_model(),     projected_dim=config.projected_dim,     patch_size=config.patch_size,     num_shift_blocks_per_stages=config.num_shift_blocks_per_stages,     epsilon=config.epsilon,     mlp_dropout_rate=config.mlp_dropout_rate,     stochastic_depth_rate=config.stochastic_depth_rate,     num_div=config.num_div,     shift_pixel=config.shift_pixel,     mlp_expand_ratio=config.mlp_expand_ratio, )"},{"path":"https://keras.posit.co/articles/examples/shiftvit.html","id":"learning-rate-schedule","dir":"Articles > Examples","previous_headings":"","what":"Learning rate schedule","title":"A Vision Transformer without Attention","text":"many experiments, want warm model slowly increasing learning rate cool model slowly decaying learning rate. warmup cosine decay, learning rate linearly increases warmup steps decays cosine decay.","code":"# Some code is taken from: # https://www.kaggle.com/ashusma/training-rfcx-tensorflow-tpu-effnet-b2. class WarmUpCosine(keras.optimizers.schedules.LearningRateSchedule):     \"\"\"A LearningRateSchedule that uses a warmup cosine decay schedule.\"\"\"      def __init__(self, lr_start, lr_max, warmup_steps, total_steps):         \"\"\"         Args:             lr_start: The initial learning rate             lr_max: The maximum learning rate to which lr should increase to in                 the warmup steps             warmup_steps: The number of steps for which the model warms up             total_steps: The total number of steps for the model training         \"\"\"         super().__init__()         self.lr_start = lr_start         self.lr_max = lr_max         self.warmup_steps = warmup_steps         self.total_steps = total_steps         self.pi = tf.constant(np.pi)      def __call__(self, step):         # Check whether the total number of steps is larger than the warmup         # steps. If not, then throw a value error.         if self.total_steps < self.warmup_steps:             raise ValueError(                 f\"Total number of steps {self.total_steps} must be\"                 + f\"larger or equal to warmup steps {self.warmup_steps}.\"             )          # `cos_annealed_lr` is a graph that increases to 1 from the initial         # step to the warmup step. After that this graph decays to -1 at the         # final step mark.         cos_annealed_lr = tf.cos(             self.pi             * (tf.cast(step, tf.float32) - self.warmup_steps)             / tf.cast(self.total_steps - self.warmup_steps, tf.float32)         )          # Shift the mean of the `cos_annealed_lr` graph to 1. Now the grpah goes         # from 0 to 2. Normalize the graph with 0.5 so that now it goes from 0         # to 1. With the normalized graph we scale it with `lr_max` such that         # it goes from 0 to `lr_max`         learning_rate = 0.5 * self.lr_max * (1 + cos_annealed_lr)          # Check whether warmup_steps is more than 0.         if self.warmup_steps > 0:             # Check whether lr_max is larger that lr_start. If not, throw a value             # error.             if self.lr_max < self.lr_start:                 raise ValueError(                     f\"lr_start {self.lr_start} must be smaller or\"                     + f\"equal to lr_max {self.lr_max}.\"                 )              # Calculate the slope with which the learning rate should increase             # in the warumup schedule. The formula for slope is m = ((b-a)/steps)             slope = (self.lr_max - self.lr_start) / self.warmup_steps              # With the formula for a straight line (y = mx+c) build the warmup             # schedule             warmup_rate = slope * tf.cast(step, tf.float32) + self.lr_start              # When the current step is lesser that warmup steps, get the line             # graph. When the current step is greater than the warmup steps, get             # the scaled cos graph.             learning_rate = tf.where(                 step < self.warmup_steps, warmup_rate, learning_rate             )          # When the current step is more that the total steps, return 0 else return         # the calculated graph.         return tf.where(             step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"         )"},{"path":"https://keras.posit.co/articles/examples/shiftvit.html","id":"compile-and-train-the-model","dir":"Articles > Examples","previous_headings":"","what":"Compile and train the model","title":"A Vision Transformer without Attention","text":"","code":"# Get the total number of steps for training. total_steps = int((len(x_train) / config.batch_size) * config.epochs)  # Calculate the number of steps for warmup. warmup_epoch_percentage = 0.15 warmup_steps = int(total_steps * warmup_epoch_percentage)  # Initialize the warmupcosine schedule. scheduled_lrs = WarmUpCosine(     lr_start=1e-5,     lr_max=1e-3,     warmup_steps=warmup_steps,     total_steps=total_steps, )  # Get the optimizer. optimizer = keras.optimizers.AdamW(     learning_rate=scheduled_lrs, weight_decay=config.weight_decay )  # Compile and pretrain the model. model.compile(     optimizer=optimizer,     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),     metrics=[         keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),         keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),     ], )  # Train the model history = model.fit(     train_ds,     epochs=config.epochs,     validation_data=val_ds,     callbacks=[         keras.callbacks.EarlyStopping(             monitor=\"val_accuracy\",             patience=5,             mode=\"auto\",         )     ], )  # Evaluate the model with the test dataset. print(\"TESTING\") loss, acc_top1, acc_top5 = model.evaluate(test_ds) print(f\"Loss: {loss:0.2f}\") print(f\"Top 1 test accuracy: {acc_top1*100:0.2f}%\") print(f\"Top 5 test accuracy: {acc_top5*100:0.2f}%\")"},{"path":"https://keras.posit.co/articles/examples/shiftvit.html","id":"conclusion","dir":"Articles > Examples","previous_headings":"","what":"Conclusion","title":"A Vision Transformer without Attention","text":"impactful contribution paper novel architecture, idea hierarchical ViTs trained attention can perform quite well. opens question essential attention performance ViTs. curious minds, suggest reading ConvNexT paper attends training paradigms architectural details ViTs rather providing novel architecture based attention. Acknowledgements: like thank PyImageSearch providing us resources helped completion project. like thank JarvisLabs.ai providing GPU credits. like thank Manim Community manim library. personal note thanks Puja Roychowdhury helping us Learning Rate Schedule.","code":""},{"path":"https://keras.posit.co/articles/examples/siamese_contrastive.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Image similarity estimation using a Siamese Network with a contrastive loss","text":"Siamese Networks neural networks share weights two sister networks, producing embedding vectors respective inputs. supervised similarity learning, networks trained maximize contrast (distance) embeddings inputs different classes, minimizing distance embeddings similar classes, resulting embedding spaces reflect class segmentation training inputs.","code":""},{"path":"https://keras.posit.co/articles/examples/siamese_contrastive.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Image similarity estimation using a Siamese Network with a contrastive loss","text":"","code":"import random import numpy as np import keras as keras from keras import ops import matplotlib.pyplot as plt"},{"path":"https://keras.posit.co/articles/examples/siamese_contrastive.html","id":"hyperparameters","dir":"Articles > Examples","previous_headings":"","what":"Hyperparameters","title":"Image similarity estimation using a Siamese Network with a contrastive loss","text":"","code":"epochs = 10 batch_size = 16 margin = 1  # Margin for contrastive loss."},{"path":"https://keras.posit.co/articles/examples/siamese_contrastive.html","id":"load-the-mnist-dataset","dir":"Articles > Examples","previous_headings":"","what":"Load the MNIST dataset","title":"Image similarity estimation using a Siamese Network with a contrastive loss","text":"","code":"(x_train_val, y_train_val), (x_test, y_test) = keras.datasets.mnist.load_data()  # Change the data type to a floating point format x_train_val = x_train_val.astype(\"float32\") x_test = x_test.astype(\"float32\")"},{"path":"https://keras.posit.co/articles/examples/siamese_contrastive.html","id":"define-training-and-validation-sets","dir":"Articles > Examples","previous_headings":"","what":"Define training and validation sets","title":"Image similarity estimation using a Siamese Network with a contrastive loss","text":"","code":"# Keep 50% of train_val  in validation set x_train, x_val = x_train_val[:30000], x_train_val[30000:] y_train, y_val = y_train_val[:30000], y_train_val[30000:] del x_train_val, y_train_val"},{"path":"https://keras.posit.co/articles/examples/siamese_contrastive.html","id":"create-pairs-of-images","dir":"Articles > Examples","previous_headings":"","what":"Create pairs of images","title":"Image similarity estimation using a Siamese Network with a contrastive loss","text":"train model differentiate digits different classes. example, digit 0 needs differentiated rest digits (1 9), digit 1 - 0 2 9, . carry , select N random images class (example, digit 0) pair N random images another class B (example, digit 1). , can repeat process classes digits (digit 9). paired digit 0 digits, can repeat process remaining classes rest digits (1 9). get: pairs_train.shape = (60000, 2, 28, 28) 60,000 pairs pair contains 2 images image shape (28, 28) Split training pairs Split validation pairs Split test pairs","code":"def make_pairs(x, y):     \"\"\"Creates a tuple containing image pairs with corresponding label.      Arguments:         x: List containing images, each index in this list corresponds to one image.         y: List containing labels, each label with datatype of `int`.      Returns:         Tuple containing two numpy arrays as (pairs_of_samples, labels),         where pairs_of_samples' shape is (2len(x), 2,n_features_dims) and         labels are a binary array of shape (2len(x)).     \"\"\"      num_classes = max(y) + 1     digit_indices = [np.where(y == i)[0] for i in range(num_classes)]      pairs = []     labels = []      for idx1 in range(len(x)):         # add a matching example         x1 = x[idx1]         label1 = y[idx1]         idx2 = random.choice(digit_indices[label1])         x2 = x[idx2]          pairs += [[x1, x2]]         labels += [0]          # add a non-matching example         label2 = random.randint(0, num_classes - 1)         while label2 == label1:             label2 = random.randint(0, num_classes - 1)          idx2 = random.choice(digit_indices[label2])         x2 = x[idx2]          pairs += [[x1, x2]]         labels += [1]      return np.array(pairs), np.array(labels).astype(\"float32\")   # make train pairs pairs_train, labels_train = make_pairs(x_train, y_train)  # make validation pairs pairs_val, labels_val = make_pairs(x_val, y_val)  # make test pairs pairs_test, labels_test = make_pairs(x_test, y_test) x_train_1 = pairs_train[:, 0]  # x_train_1.shape is (60000, 28, 28) x_train_2 = pairs_train[:, 1] x_val_1 = pairs_val[:, 0]  # x_val_1.shape = (60000, 28, 28) x_val_2 = pairs_val[:, 1] x_test_1 = pairs_test[:, 0]  # x_test_1.shape = (20000, 28, 28) x_test_2 = pairs_test[:, 1]"},{"path":"https://keras.posit.co/articles/examples/siamese_contrastive.html","id":"visualize-pairs-and-their-labels","dir":"Articles > Examples","previous_headings":"","what":"Visualize pairs and their labels","title":"Image similarity estimation using a Siamese Network with a contrastive loss","text":"Inspect training pairs Inspect validation pairs Inspect test pairs","code":"def visualize(     pairs, labels, to_show=6, num_col=3, predictions=None, test=False ):     \"\"\"Creates a plot of pairs and labels, and prediction if it's test dataset.      Arguments:         pairs: Numpy Array, of pairs to visualize, having shape                (Number of pairs, 2, 28, 28).         to_show: Int, number of examples to visualize (default is 6)                 `to_show` must be an integral multiple of `num_col`.                  Otherwise it will be trimmed if it is greater than num_col,                  and incremented if if it is less then num_col.         num_col: Int, number of images in one row - (default is 3)                  For test and train respectively, it should not exceed 3 and 7.         predictions: Numpy Array of predictions with shape (to_show, 1) -                      (default is None)                      Must be passed when test=True.         test: Boolean telling whether the dataset being visualized is               train dataset or test dataset - (default False).      Returns:         None.     \"\"\"      # Define num_row     # If to_show % num_col != 0     #    trim to_show,     #       to trim to_show limit num_row to the point where     #       to_show % num_col == 0     #     # If to_show//num_col == 0     #    then it means num_col is greater then to_show     #    increment to_show     #       to increment to_show set num_row to 1     num_row = to_show // num_col if to_show // num_col != 0 else 1      # `to_show` must be an integral multiple of `num_col`     #  we found num_row and we have num_col     #  to increment or decrement to_show     #  to make it integral multiple of `num_col`     #  simply set it equal to num_row * num_col     to_show = num_row * num_col      # Plot the images     fig, axes = plt.subplots(num_row, num_col, figsize=(5, 5))     for i in range(to_show):         # If the number of rows is 1, the axes array is one-dimensional         if num_row == 1:             ax = axes[i % num_col]         else:             ax = axes[i // num_col, i % num_col]          ax.imshow(             ops.concatenate([pairs[i][0], pairs[i][1]], axis=1), cmap=\"gray\"         )         ax.set_axis_off()         if test:             ax.set_title(                 \"True: {} | Pred: {:.5f}\".format(labels[i], predictions[i][0])             )         else:             ax.set_title(\"Label: {}\".format(labels[i]))     if test:         plt.tight_layout(rect=(0, 0, 1.9, 1.9), w_pad=0.0)     else:         plt.tight_layout(rect=(0, 0, 1.5, 1.5))     plt.show() visualize(pairs_train[:-1], labels_train[:-1], to_show=4, num_col=4) visualize(pairs_val[:-1], labels_val[:-1], to_show=4, num_col=4) visualize(pairs_test[:-1], labels_test[:-1], to_show=4, num_col=4)"},{"path":"https://keras.posit.co/articles/examples/siamese_contrastive.html","id":"define-the-model","dir":"Articles > Examples","previous_headings":"","what":"Define the model","title":"Image similarity estimation using a Siamese Network with a contrastive loss","text":"two input layers, leading network, produces embeddings. Lambda layer merges using Euclidean distance merged output fed final network.","code":"# Provided two tensors t1 and t2 # Euclidean distance = sqrt(sum(square(t1-t2))) def euclidean_distance(vects):     \"\"\"Find the Euclidean distance between two vectors.      Arguments:         vects: List containing two tensors of same length.      Returns:         Tensor containing euclidean distance         (as floating point value) between vectors.     \"\"\"      x, y = vects     sum_square = ops.sum(ops.square(x - y), axis=1, keepdims=True)     return ops.sqrt(ops.maximum(sum_square, keras.backend.epsilon()))   input = keras.layers.Input((28, 28, 1)) x = keras.layers.BatchNormalization()(input) x = keras.layers.Conv2D(4, (5, 5), activation=\"tanh\")(x) x = keras.layers.AveragePooling2D(pool_size=(2, 2))(x) x = keras.layers.Conv2D(16, (5, 5), activation=\"tanh\")(x) x = keras.layers.AveragePooling2D(pool_size=(2, 2))(x) x = keras.layers.Flatten()(x)  x = keras.layers.BatchNormalization()(x) x = keras.layers.Dense(10, activation=\"tanh\")(x) embedding_network = keras.Model(input, x)   input_1 = keras.layers.Input((28, 28, 1)) input_2 = keras.layers.Input((28, 28, 1))  # As mentioned above, Siamese Network share weights between # tower networks (sister networks). To allow this, we will use # same embedding network for both tower networks. tower_1 = embedding_network(input_1) tower_2 = embedding_network(input_2)  merge_layer = keras.layers.Lambda(euclidean_distance, output_shape=(1,))(     [tower_1, tower_2] ) normal_layer = keras.layers.BatchNormalization()(merge_layer) output_layer = keras.layers.Dense(1, activation=\"sigmoid\")(normal_layer) siamese = keras.Model(inputs=[input_1, input_2], outputs=output_layer)"},{"path":"https://keras.posit.co/articles/examples/siamese_contrastive.html","id":"define-the-contrastive-loss","dir":"Articles > Examples","previous_headings":"","what":"Define the contrastive Loss","title":"Image similarity estimation using a Siamese Network with a contrastive loss","text":"","code":"def loss(margin=1):     \"\"\"Provides 'contrastive_loss' an enclosing scope with variable 'margin'.      Arguments:         margin: Integer, defines the baseline for distance for which pairs                 should be classified as dissimilar. - (default is 1).      Returns:         'contrastive_loss' function with data ('margin') attached.     \"\"\"      # Contrastive loss = mean( (1-true_value) * square(prediction) +     #                         true_value * square( max(margin-prediction, 0) ))     def contrastive_loss(y_true, y_pred):         \"\"\"Calculates the contrastive loss.          Arguments:             y_true: List of labels, each label is of type float32.             y_pred: List of predictions of same length as of y_true,                     each label is of type float32.          Returns:             A tensor containing contrastive loss as floating point value.         \"\"\"          square_pred = ops.square(y_pred)         margin_square = ops.square(ops.maximum(margin - (y_pred), 0))         return ops.mean((1 - y_true) * square_pred + (y_true) * margin_square)      return contrastive_loss"},{"path":"https://keras.posit.co/articles/examples/siamese_contrastive.html","id":"compile-the-model-with-the-contrastive-loss","dir":"Articles > Examples","previous_headings":"","what":"Compile the model with the contrastive loss","title":"Image similarity estimation using a Siamese Network with a contrastive loss","text":"","code":"siamese.compile(     loss=loss(margin=margin), optimizer=\"RMSprop\", metrics=[\"accuracy\"] ) siamese.summary()"},{"path":"https://keras.posit.co/articles/examples/siamese_contrastive.html","id":"train-the-model","dir":"Articles > Examples","previous_headings":"","what":"Train the model","title":"Image similarity estimation using a Siamese Network with a contrastive loss","text":"","code":"history = siamese.fit(     [x_train_1, x_train_2],     labels_train,     validation_data=([x_val_1, x_val_2], labels_val),     batch_size=batch_size,     epochs=epochs, )"},{"path":"https://keras.posit.co/articles/examples/siamese_contrastive.html","id":"visualize-results","dir":"Articles > Examples","previous_headings":"","what":"Visualize results","title":"Image similarity estimation using a Siamese Network with a contrastive loss","text":"","code":"def plt_metric(history, metric, title, has_valid=True):     \"\"\"Plots the given 'metric' from 'history'.      Arguments:         history: history attribute of History object returned from Model.fit.         metric: Metric to plot, a string value present as key in 'history'.         title: A string to be used as title of plot.         has_valid: Boolean, true if valid data was passed to Model.fit else false.      Returns:         None.     \"\"\"     plt.plot(history[metric])     if has_valid:         plt.plot(history[\"val_\" + metric])         plt.legend([\"train\", \"validation\"], loc=\"upper left\")     plt.title(title)     plt.ylabel(metric)     plt.xlabel(\"epoch\")     plt.show()   # Plot the accuracy plt_metric(history=history.history, metric=\"accuracy\", title=\"Model accuracy\")  # Plot the contrastive loss plt_metric(history=history.history, metric=\"loss\", title=\"Contrastive Loss\")"},{"path":"https://keras.posit.co/articles/examples/siamese_contrastive.html","id":"evaluate-the-model","dir":"Articles > Examples","previous_headings":"","what":"Evaluate the model","title":"Image similarity estimation using a Siamese Network with a contrastive loss","text":"","code":"results = siamese.evaluate([x_test_1, x_test_2], labels_test) print(\"test loss, test acc:\", results)"},{"path":"https://keras.posit.co/articles/examples/siamese_contrastive.html","id":"visualize-the-predictions","dir":"Articles > Examples","previous_headings":"","what":"Visualize the predictions","title":"Image similarity estimation using a Siamese Network with a contrastive loss","text":"Example available HuggingFace | Trained Model | Demo | | :–: | :–: | |  |  |","code":"predictions = siamese.predict([x_test_1, x_test_2]) visualize(     pairs_test, labels_test, to_show=3, predictions=predictions, test=True )"},{"path":"https://keras.posit.co/articles/examples/siamese_network.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Image similarity estimation using a Siamese Network with a triplet loss","text":"Siamese Network type network architecture contains two identical subnetworks used generate feature vectors input compare . Siamese Networks can applied different use cases, like detecting duplicates, finding anomalies, face recognition. example uses Siamese Network three identical subnetworks. provide three images model, two similar (anchor positive samples), third unrelated (negative example.) goal model learn estimate similarity images. network learn, use triplet loss function. can find introduction triplet loss FaceNet paper Schroff et al,. 2015. example, define triplet loss function follows: L(, P, N) = max(‖f() - f(P)‖² - ‖f() - f(N)‖² + margin, 0) example uses Totally Looks Like dataset Rosenfeld et al., 2018.","code":""},{"path":"https://keras.posit.co/articles/examples/siamese_network.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Image similarity estimation using a Siamese Network with a triplet loss","text":"","code":"import matplotlib.pyplot as plt import numpy as np import os import tensorflow as tf from pathlib import Path from keras import layers from keras import optimizers from keras import metrics from keras import Model from keras.applications import resnet   target_shape = (200, 200)"},{"path":"https://keras.posit.co/articles/examples/siamese_network.html","id":"load-the-dataset","dir":"Articles > Examples","previous_headings":"","what":"Load the dataset","title":"Image similarity estimation using a Siamese Network with a triplet loss","text":"going load Totally Looks Like dataset unzip inside ~/.keras directory local environment. dataset consists two separate files: left.zip contains images use anchor. right.zip contains images use positive sample (image looks like anchor). gdown –id 1jvkbTr_giSP3Ru8OwGNCg6B4PvVbcO34 gdown –id 1EzBZUb_mh_Dp_FKD0P4XiYYSd0QBH5zW unzip -oq left.zip -d $cache_dir unzip -oq right.zip -d $cache_dir","code":"cache_dir = Path(Path.home()) / \".keras\" anchor_images_path = cache_dir / \"left\" positive_images_path = cache_dir / \"right\""},{"path":"https://keras.posit.co/articles/examples/siamese_network.html","id":"preparing-the-data","dir":"Articles > Examples","previous_headings":"","what":"Preparing the data","title":"Image similarity estimation using a Siamese Network with a triplet loss","text":"going use tf.data pipeline load data generate triplets need train Siamese network. ’ll set pipeline using zipped list anchor, positive, negative filenames source. pipeline load preprocess corresponding images. Let’s setup data pipeline using zipped list anchor, positive, negative image filename source. output pipeline contains triplet every image loaded preprocessed. Let’s take look examples triplets. Notice first two images look alike third one always different.","code":"def preprocess_image(filename):     \"\"\"     Load the specified file as a JPEG image, preprocess it and     resize it to the target shape.     \"\"\"      image_string = tf.io.read_file(filename)     image = tf.image.decode_jpeg(image_string, channels=3)     image = tf.image.convert_image_dtype(image, tf.float32)     image = tf.image.resize(image, target_shape)     return image   def preprocess_triplets(anchor, positive, negative):     \"\"\"     Given the filenames corresponding to the three images, load and     preprocess them.     \"\"\"      return (         preprocess_image(anchor),         preprocess_image(positive),         preprocess_image(negative),     ) # We need to make sure both the anchor and positive images are loaded in # sorted order so we can match them together. anchor_images = sorted(     [str(anchor_images_path / f) for f in os.listdir(anchor_images_path)] )  positive_images = sorted(     [str(positive_images_path / f) for f in os.listdir(positive_images_path)] )  image_count = len(anchor_images)  anchor_dataset = tf.data.Dataset.from_tensor_slices(anchor_images) positive_dataset = tf.data.Dataset.from_tensor_slices(positive_images)  # To generate the list of negative images, let's randomize the list of # available images and concatenate them together. rng = np.random.RandomState(seed=42) rng.shuffle(anchor_images) rng.shuffle(positive_images)  negative_images = anchor_images + positive_images np.random.RandomState(seed=32).shuffle(negative_images)  negative_dataset = tf.data.Dataset.from_tensor_slices(negative_images) negative_dataset = negative_dataset.shuffle(buffer_size=4096)  dataset = tf.data.Dataset.zip(     (anchor_dataset, positive_dataset, negative_dataset) ) dataset = dataset.shuffle(buffer_size=1024) dataset = dataset.map(preprocess_triplets)  # Let's now split our dataset in train and validation. train_dataset = dataset.take(round(image_count * 0.8)) val_dataset = dataset.skip(round(image_count * 0.8))  train_dataset = train_dataset.batch(32, drop_remainder=False) train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)  val_dataset = val_dataset.batch(32, drop_remainder=False) val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE) def visualize(anchor, positive, negative):     \"\"\"Visualize a few triplets from the supplied batches.\"\"\"      def show(ax, image):         ax.imshow(image)         ax.get_xaxis().set_visible(False)         ax.get_yaxis().set_visible(False)      fig = plt.figure(figsize=(9, 9))      axs = fig.subplots(3, 3)     for i in range(3):         show(axs[i, 0], anchor[i])         show(axs[i, 1], positive[i])         show(axs[i, 2], negative[i])   visualize(*list(train_dataset.take(1).as_numpy_iterator())[0])"},{"path":"https://keras.posit.co/articles/examples/siamese_network.html","id":"setting-up-the-embedding-generator-model","dir":"Articles > Examples","previous_headings":"","what":"Setting up the embedding generator model","title":"Image similarity estimation using a Siamese Network with a triplet loss","text":"Siamese Network generate embeddings images triplet. , use ResNet50 model pretrained ImageNet connect Dense layers can learn separate embeddings. freeze weights layers model layer conv5_block1_out. important avoid affecting weights model already learned. going leave bottom layers trainable, can fine-tune weights training.","code":"base_cnn = resnet.ResNet50(     weights=\"imagenet\", input_shape=target_shape + (3,), include_top=False )  flatten = layers.Flatten()(base_cnn.output) dense1 = layers.Dense(512, activation=\"relu\")(flatten) dense1 = layers.BatchNormalization()(dense1) dense2 = layers.Dense(256, activation=\"relu\")(dense1) dense2 = layers.BatchNormalization()(dense2) output = layers.Dense(256)(dense2)  embedding = Model(base_cnn.input, output, name=\"Embedding\")  trainable = False for layer in base_cnn.layers:     if layer.name == \"conv5_block1_out\":         trainable = True     layer.trainable = trainable"},{"path":"https://keras.posit.co/articles/examples/siamese_network.html","id":"setting-up-the-siamese-network-model","dir":"Articles > Examples","previous_headings":"","what":"Setting up the Siamese Network model","title":"Image similarity estimation using a Siamese Network with a triplet loss","text":"Siamese network receive triplet images input, generate embeddings, output distance anchor positive embedding, well distance anchor negative embedding. compute distance, can use custom layer DistanceLayer returns values tuple.","code":"class DistanceLayer(layers.Layer):     \"\"\"     This layer is responsible for computing the distance between the anchor     embedding and the positive embedding, and the anchor embedding and the     negative embedding.     \"\"\"      def __init__(self, **kwargs):         super().__init__(**kwargs)      def call(self, anchor, positive, negative):         ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)         an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)         return (ap_distance, an_distance)   anchor_input = layers.Input(name=\"anchor\", shape=target_shape + (3,)) positive_input = layers.Input(name=\"positive\", shape=target_shape + (3,)) negative_input = layers.Input(name=\"negative\", shape=target_shape + (3,))  distances = DistanceLayer()(     embedding(resnet.preprocess_input(anchor_input)),     embedding(resnet.preprocess_input(positive_input)),     embedding(resnet.preprocess_input(negative_input)), )  siamese_network = Model(     inputs=[anchor_input, positive_input, negative_input], outputs=distances )"},{"path":"https://keras.posit.co/articles/examples/siamese_network.html","id":"putting-everything-together","dir":"Articles > Examples","previous_headings":"","what":"Putting everything together","title":"Image similarity estimation using a Siamese Network with a triplet loss","text":"now need implement model custom training loop can compute triplet loss using three embeddings produced Siamese network. Let’s create Mean metric instance track loss training process.","code":"class SiameseModel(Model):     \"\"\"The Siamese Network model with a custom training and testing loops.      Computes the triplet loss using the three embeddings produced by the     Siamese Network.      The triplet loss is defined as:        L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)     \"\"\"      def __init__(self, siamese_network, margin=0.5):         super().__init__()         self.siamese_network = siamese_network         self.margin = margin         self.loss_tracker = metrics.Mean(name=\"loss\")      def call(self, inputs):         return self.siamese_network(inputs)      def train_step(self, data):         # GradientTape is a context manager that records every operation that         # you do inside. We are using it here to compute the loss so we can get         # the gradients and apply them using the optimizer specified in         # `compile()`.         with tf.GradientTape() as tape:             loss = self._compute_loss(data)          # Storing the gradients of the loss function with respect to the         # weights/parameters.         gradients = tape.gradient(loss, self.siamese_network.trainable_weights)          # Applying the gradients on the model using the specified optimizer         self.optimizer.apply_gradients(             zip(gradients, self.siamese_network.trainable_weights)         )          # Let's update and return the training loss metric.         self.loss_tracker.update_state(loss)         return {\"loss\": self.loss_tracker.result()}      def test_step(self, data):         loss = self._compute_loss(data)          # Let's update and return the loss metric.         self.loss_tracker.update_state(loss)         return {\"loss\": self.loss_tracker.result()}      def _compute_loss(self, data):         # The output of the network is a tuple containing the distances         # between the anchor and the positive example, and the anchor and         # the negative example.         ap_distance, an_distance = self.siamese_network(data)          # Computing the Triplet Loss by subtracting both distances and         # making sure we don't get a negative value.         loss = ap_distance - an_distance         loss = tf.maximum(loss + self.margin, 0.0)         return loss      @property     def metrics(self):         # We need to list our metrics here so the `reset_states()` can be         # called automatically.         return [self.loss_tracker]"},{"path":"https://keras.posit.co/articles/examples/siamese_network.html","id":"training","dir":"Articles > Examples","previous_headings":"","what":"Training","title":"Image similarity estimation using a Siamese Network with a triplet loss","text":"now ready train model.","code":"siamese_model = SiameseModel(siamese_network) siamese_model.compile(optimizer=optimizers.Adam(0.0001)) siamese_model.fit(train_dataset, epochs=10, validation_data=val_dataset)"},{"path":"https://keras.posit.co/articles/examples/siamese_network.html","id":"inspecting-what-the-network-has-learned","dir":"Articles > Examples","previous_headings":"","what":"Inspecting what the network has learned","title":"Image similarity estimation using a Siamese Network with a triplet loss","text":"point, can check network learned separate embeddings depending whether belong similar images. can use cosine similarity measure similarity embeddings. Let’s pick sample dataset check similarity embeddings generated image. Finally, can compute cosine similarity anchor positive images compare similarity anchor negative images. expect similarity anchor positive images larger similarity anchor negative images.","code":"sample = next(iter(train_dataset)) visualize(*sample)  anchor, positive, negative = sample anchor_embedding, positive_embedding, negative_embedding = (     embedding(resnet.preprocess_input(anchor)),     embedding(resnet.preprocess_input(positive)),     embedding(resnet.preprocess_input(negative)), ) cosine_similarity = metrics.CosineSimilarity()  positive_similarity = cosine_similarity(anchor_embedding, positive_embedding) print(\"Positive similarity:\", positive_similarity.numpy())  negative_similarity = cosine_similarity(anchor_embedding, negative_embedding) print(\"Negative similarity\", negative_similarity.numpy())"},{"path":"https://keras.posit.co/articles/examples/siamese_network.html","id":"summary","dir":"Articles > Examples","previous_headings":"","what":"Summary","title":"Image similarity estimation using a Siamese Network with a triplet loss","text":"tf.data API enables build efficient input pipelines model. particularly useful large dataset. can learn tf.data pipelines tf.data: Build TensorFlow input pipelines. example, use pre-trained ResNet50 part subnetwork generates feature embeddings. using transfer learning, can significantly reduce training time size dataset. Notice fine-tuning weights final layers ResNet50 network keeping rest layers untouched. Using name assigned layer, can freeze weights certain point keep last layers open. can create custom layers creating class inherits tf.keras.layers.Layer, DistanceLayer class. used cosine similarity metric measure 2 output embeddings similar . can implement custom training loop overriding train_step() method. train_step() uses tf.GradientTape, records every operation perform inside . example, use access gradients passed optimizer update model weights every step. details, check Intro Keras researchers Writing training loop scratch.","code":""},{"path":"https://keras.posit.co/articles/examples/simsiam.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Self-supervised contrastive learning with SimSiam","text":"","code":"import os os.environ['KERAS_BACKEND'] = 'tensorflow'  from keras import layers from keras import regularizers import keras as keras import tensorflow as tf  import matplotlib.pyplot as plt import numpy as np"},{"path":"https://keras.posit.co/articles/examples/simsiam.html","id":"define-hyperparameters","dir":"Articles > Examples","previous_headings":"","what":"Define hyperparameters","title":"Self-supervised contrastive learning with SimSiam","text":"","code":"AUTO = tf.data.AUTOTUNE BATCH_SIZE = 128 EPOCHS = 5 CROP_TO = 32 SEED = 26  PROJECT_DIM = 2048 LATENT_DIM = 512 WEIGHT_DECAY = 0.0005"},{"path":"https://keras.posit.co/articles/examples/simsiam.html","id":"load-the-cifar-10-dataset","dir":"Articles > Examples","previous_headings":"","what":"Load the CIFAR-10 dataset","title":"Self-supervised contrastive learning with SimSiam","text":"","code":"(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data() print(f\"Total training examples: {len(x_train)}\") print(f\"Total test examples: {len(x_test)}\")"},{"path":"https://keras.posit.co/articles/examples/simsiam.html","id":"defining-our-data-augmentation-pipeline","dir":"Articles > Examples","previous_headings":"","what":"Defining our data augmentation pipeline","title":"Self-supervised contrastive learning with SimSiam","text":"studied SimCLR right data augmentation pipeline critical SSL systems work effectively computer vision. Two particular augmentation transforms seem matter : 1.) Random resized crops 2.) Color distortions. SSL systems computer vision (BYOL, MoCoV2, SwAV, etc.) include training pipelines. noted augmentation pipeline generally dependent various properties dataset dealing . example, images dataset heavily object-centric taking random crops high probability may hurt training performance. Let’s now apply augmentation pipeline dataset visualize outputs.","code":"def flip_random_crop(image):     # With random crops we also apply horizontal flipping.     image = tf.image.random_flip_left_right(image)     image = tf.image.random_crop(image, (CROP_TO, CROP_TO, 3))     return image   def color_jitter(x, strength=[0.4, 0.4, 0.4, 0.1]):     x = tf.image.random_brightness(x, max_delta=0.8 * strength[0])     x = tf.image.random_contrast(         x, lower=1 - 0.8 * strength[1], upper=1 + 0.8 * strength[1]     )     x = tf.image.random_saturation(         x, lower=1 - 0.8 * strength[2], upper=1 + 0.8 * strength[2]     )     x = tf.image.random_hue(x, max_delta=0.2 * strength[3])     # Affine transformations can disturb the natural range of     # RGB images, hence this is needed.     x = tf.clip_by_value(x, 0, 255)     return x   def color_drop(x):     x = tf.image.rgb_to_grayscale(x)     x = tf.tile(x, [1, 1, 3])     return x   def random_apply(func, x, p):     if tf.random.uniform([], minval=0, maxval=1) < p:         return func(x)     else:         return x   def custom_augment(image):     # As discussed in the SimCLR paper, the series of augmentation     # transformations (except for random crops) need to be applied     # randomly to impose translational invariance.     image = flip_random_crop(image)     image = random_apply(color_jitter, image, p=0.8)     image = random_apply(color_drop, image, p=0.2)     return image"},{"path":"https://keras.posit.co/articles/examples/simsiam.html","id":"convert-the-data-into-tensorflow-dataset-objects","dir":"Articles > Examples","previous_headings":"","what":"Convert the data into TensorFlow Dataset objects","title":"Self-supervised contrastive learning with SimSiam","text":"create two different versions dataset without ground-truth labels. Notice images samples_images_one sample_images_two essentially augmented differently.","code":"ssl_ds_one = tf.data.Dataset.from_tensor_slices(x_train) ssl_ds_one = (     ssl_ds_one.shuffle(1024, seed=SEED)     .map(custom_augment, num_parallel_calls=AUTO)     .batch(BATCH_SIZE)     .prefetch(AUTO) )  ssl_ds_two = tf.data.Dataset.from_tensor_slices(x_train) ssl_ds_two = (     ssl_ds_two.shuffle(1024, seed=SEED)     .map(custom_augment, num_parallel_calls=AUTO)     .batch(BATCH_SIZE)     .prefetch(AUTO) )  # We then zip both of these datasets. ssl_ds = tf.data.Dataset.zip((ssl_ds_one, ssl_ds_two))  # Visualize a few augmented images. sample_images_one = next(iter(ssl_ds_one)) plt.figure(figsize=(10, 10)) for n in range(25):     ax = plt.subplot(5, 5, n + 1)     plt.imshow(sample_images_one[n].numpy().astype(\"int\"))     plt.axis(\"off\") plt.show()  # Ensure that the different versions of the dataset actually contain # identical images. sample_images_two = next(iter(ssl_ds_two)) plt.figure(figsize=(10, 10)) for n in range(25):     ax = plt.subplot(5, 5, n + 1)     plt.imshow(sample_images_two[n].numpy().astype(\"int\"))     plt.axis(\"off\") plt.show()"},{"path":"https://keras.posit.co/articles/examples/simsiam.html","id":"defining-the-encoder-and-the-predictor","dir":"Articles > Examples","previous_headings":"","what":"Defining the encoder and the predictor","title":"Self-supervised contrastive learning with SimSiam","text":"use implementation ResNet20 specifically configured CIFAR10 dataset. code taken keras-idiomatic-programmer repository. hyperparameters architectures referred Section 3 Appendix original paper. wget -q https://shorturl./QS369 -O resnet_cifar10_v2.py","code":"import resnet_cifar10_v2  N = 2 DEPTH = N * 9 + 2 NUM_BLOCKS = ((DEPTH - 2) // 9) - 1   def get_encoder():     # Input and backbone.     inputs = layers.Input((CROP_TO, CROP_TO, 3))     x = layers.Rescaling(scale=1.0 / 127.5, offset=-1)(inputs)     x = resnet_cifar10_v2.stem(x)     x = resnet_cifar10_v2.learner(x, NUM_BLOCKS)     x = layers.GlobalAveragePooling2D(name=\"backbone_pool\")(x)      # Projection head.     x = layers.Dense(         PROJECT_DIM,         use_bias=False,         kernel_regularizer=regularizers.l2(WEIGHT_DECAY),     )(x)     x = layers.BatchNormalization()(x)     x = layers.ReLU()(x)     x = layers.Dense(         PROJECT_DIM,         use_bias=False,         kernel_regularizer=regularizers.l2(WEIGHT_DECAY),     )(x)     outputs = layers.BatchNormalization()(x)     return keras.Model(inputs, outputs, name=\"encoder\")   def get_predictor():     model = keras.Sequential(         [             # Note the AutoEncoder-like structure.             layers.Input((PROJECT_DIM,)),             layers.Dense(                 LATENT_DIM,                 use_bias=False,                 kernel_regularizer=regularizers.l2(WEIGHT_DECAY),             ),             layers.ReLU(),             layers.BatchNormalization(),             layers.Dense(PROJECT_DIM),         ],         name=\"predictor\",     )     return model"},{"path":"https://keras.posit.co/articles/examples/simsiam.html","id":"defining-the-pre-training-loop","dir":"Articles > Examples","previous_headings":"","what":"Defining the (pre-)training loop","title":"Self-supervised contrastive learning with SimSiam","text":"One main reasons behind training networks kinds approaches utilize learned representations downstream tasks like classification. particular training phase also referred pre-training. start defining loss function. define training loop overriding train_step() function keras.Model class.","code":"def compute_loss(p, z):     # The authors of SimSiam emphasize the impact of     # the `stop_gradient` operator in the paper as it     # has an important role in the overall optimization.     z = tf.stop_gradient(z)     p = tf.math.l2_normalize(p, axis=1)     z = tf.math.l2_normalize(z, axis=1)     # Negative cosine similarity (minimizing this is     # equivalent to maximizing the similarity).     return -tf.reduce_mean(tf.reduce_sum((p * z), axis=1)) class SimSiam(keras.Model):     def __init__(self, encoder, predictor):         super().__init__()         self.encoder = encoder         self.predictor = predictor         self.loss_tracker = keras.metrics.Mean(name=\"loss\")      @property     def metrics(self):         return [self.loss_tracker]      def train_step(self, data):         # Unpack the data.         ds_one, ds_two = data          # Forward pass through the encoder and predictor.         with tf.GradientTape() as tape:             z1, z2 = self.encoder(ds_one), self.encoder(ds_two)             p1, p2 = self.predictor(z1), self.predictor(z2)             # Note that here we are enforcing the network to match             # the representations of two differently augmented batches             # of data.             loss = compute_loss(p1, z2) / 2 + compute_loss(p2, z1) / 2          # Compute gradients and update the parameters.         learnable_params = (             self.encoder.trainable_variables             + self.predictor.trainable_variables         )         gradients = tape.gradient(loss, learnable_params)         self.optimizer.apply_gradients(zip(gradients, learnable_params))          # Monitor loss.         self.loss_tracker.update_state(loss)         return {\"loss\": self.loss_tracker.result()}"},{"path":"https://keras.posit.co/articles/examples/simsiam.html","id":"pre-training-our-networks","dir":"Articles > Examples","previous_headings":"","what":"Pre-training our networks","title":"Self-supervised contrastive learning with SimSiam","text":"interest example, train model 5 epochs. reality, least 100 epochs. solution gets close -1 (minimum value loss) quickly different dataset different backbone architecture likely representation collapse. phenomenon encoder yields similar output images. case additional hyperparameter tuning required especially following areas: Strength color distortions probabilities. Learning rate schedule. Architecture backbone projection head.","code":"# Create a cosine decay learning scheduler. num_training_samples = len(x_train) steps = EPOCHS * (num_training_samples // BATCH_SIZE) lr_decayed_fn = keras.optimizers.schedules.CosineDecay(     initial_learning_rate=0.03, decay_steps=steps )  # Create an early stopping callback. early_stopping = keras.callbacks.EarlyStopping(     monitor=\"loss\", patience=5, restore_best_weights=True )  # Compile model and start training. simsiam = SimSiam(get_encoder(), get_predictor()) simsiam.compile(optimizer=keras.optimizers.SGD(lr_decayed_fn, momentum=0.6)) history = simsiam.fit(ssl_ds, epochs=EPOCHS, callbacks=[early_stopping])  # Visualize the training progress of the model. plt.plot(history.history[\"loss\"]) plt.grid() plt.title(\"Negative Cosine Similairty\") plt.show()"},{"path":"https://keras.posit.co/articles/examples/simsiam.html","id":"evaluating-our-ssl-method","dir":"Articles > Examples","previous_headings":"","what":"Evaluating our SSL method","title":"Self-supervised contrastive learning with SimSiam","text":"popularly used method evaluate SSL method computer vision (pre-training method ) learn linear classifier frozen features trained backbone model (case ResNet20) evaluate classifier unseen images. methods include fine-tuning source dataset even target dataset 5% 10% labels present. Practically, can use backbone model downstream task semantic segmentation, object detection, backbone models usually pre-trained pure supervised learning.","code":"# We first create labeled `Dataset` objects. train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)) test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))  # Then we shuffle, batch, and prefetch this dataset for performance. We # also apply random resized crops as an augmentation but only to the # training set. train_ds = (     train_ds.shuffle(1024)     .map(lambda x, y: (flip_random_crop(x), y), num_parallel_calls=AUTO)     .batch(BATCH_SIZE)     .prefetch(AUTO) ) test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTO)  # Extract the backbone ResNet20. backbone = keras.Model(     simsiam.encoder.input, simsiam.encoder.get_layer(\"backbone_pool\").output )  # We then create our linear classifier and train it. backbone.trainable = False inputs = layers.Input((CROP_TO, CROP_TO, 3)) x = backbone(inputs, training=False) outputs = layers.Dense(10, activation=\"softmax\")(x) linear_model = keras.Model(inputs, outputs, name=\"linear_model\")  # Compile model and start training. linear_model.compile(     loss=\"sparse_categorical_crossentropy\",     metrics=[\"accuracy\"],     optimizer=keras.optimizers.SGD(lr_decayed_fn, momentum=0.9), ) history = linear_model.fit(     train_ds, validation_data=test_ds, epochs=EPOCHS, callbacks=[early_stopping] ) _, test_acc = linear_model.evaluate(test_ds) print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"},{"path":"https://keras.posit.co/articles/examples/simsiam.html","id":"notes","dir":"Articles > Examples","previous_headings":"","what":"Notes","title":"Self-supervised contrastive learning with SimSiam","text":"data longer pre-training schedule benefit SSL general. SSL particularly helpful access limited labeled training data can manage build large corpus unlabeled data. Recently, using SSL method called SwAV, group researchers Facebook trained RegNet 2 Billion images. able achieve downstream performance close achieved pure supervised pre-training. downstream tasks, method even outperformed supervised counterparts. can check paper know details. Self-supervised learning: dark matter intelligence Understanding self-supervised learning using controlled datasets known structure","code":""},{"path":"https://keras.posit.co/articles/examples/speaker_recognition_using_cnn.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Speaker Recognition","text":"example demonstrates create model classify speakers frequency domain representation speech recordings, obtained via Fast Fourier Transform (FFT). shows following: use tf.data load, preprocess feed audio streams model create 1D convolutional network residual connections audio classification. process: prepare dataset speech samples different speakers, speaker label. add background noise samples augment data. take FFT samples. train 1D convnet predict correct speaker given noisy FFT speech sample. Note: example run TensorFlow 2.3 higher, tf-nightly. noise samples dataset need resampled sampling rate 16000 Hz using code example. order , need installed ffmpg.","code":""},{"path":"https://keras.posit.co/articles/examples/speaker_recognition_using_cnn.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Speaker Recognition","text":"","code":"import os  os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  import shutil import numpy as np  import tensorflow as tf import keras as keras  from pathlib import Path from IPython.display import display, Audio  # Get the data from https://www.kaggle.com/kongaevans/speaker-recognition-dataset/download # and save it to the 'Downloads' folder in your HOME directory DATASET_ROOT = os.path.join(     os.path.expanduser(\"~\"), \"Downloads/16000_pcm_speeches\" )  # The folders in which we will put the audio samples and the noise samples AUDIO_SUBFOLDER = \"audio\" NOISE_SUBFOLDER = \"noise\"  DATASET_AUDIO_PATH = os.path.join(DATASET_ROOT, AUDIO_SUBFOLDER) DATASET_NOISE_PATH = os.path.join(DATASET_ROOT, NOISE_SUBFOLDER)  # Percentage of samples to use for validation VALID_SPLIT = 0.1  # Seed to use when shuffling the dataset and the noise SHUFFLE_SEED = 43  # The sampling rate to use. # This is the one used in all the audio samples. # We will resample all the noise to this sampling rate. # This will also be the output size of the audio wave samples # (since all samples are of 1 second long) SAMPLING_RATE = 16000  # The factor to multiply the noise with according to: #   noisy_sample = sample + noise * prop * scale #      where prop = sample_amplitude / noise_amplitude SCALE = 0.5  BATCH_SIZE = 128 EPOCHS = 1"},{"path":"https://keras.posit.co/articles/examples/speaker_recognition_using_cnn.html","id":"data-preparation","dir":"Articles > Examples","previous_headings":"","what":"Data preparation","title":"Speaker Recognition","text":"dataset composed 7 folders, divided 2 groups: Speech samples, 5 folders 5 different speakers. folder contains 1500 audio files, 1 second long sampled 16000 Hz. Background noise samples, 2 folders total 6 files. files longer 1 second (originally sampled 16000 Hz, resample 16000 Hz). use 6 files create 354 1-second-long noise samples used training. Let’s sort 2 categories 2 folders: audio folder contain per-speaker speech sample folders noise folder contain noise samples sorting audio noise categories 2 folders, following directory structure: sorting, end following structure:","code":"main_directory/ ...speaker_a/ ...speaker_b/ ...speaker_c/ ...speaker_d/ ...speaker_e/ ...other/ ..._background_noise_/ main_directory/ ...audio/ ......speaker_a/ ......speaker_b/ ......speaker_c/ ......speaker_d/ ......speaker_e/ ...noise/ ......other/ ......_background_noise_/ # If folder `audio`, does not exist, create it, otherwise do nothing if os.path.exists(DATASET_AUDIO_PATH) is False:     os.makedirs(DATASET_AUDIO_PATH)  # If folder `noise`, does not exist, create it, otherwise do nothing if os.path.exists(DATASET_NOISE_PATH) is False:     os.makedirs(DATASET_NOISE_PATH)  for folder in os.listdir(DATASET_ROOT):     if os.path.isdir(os.path.join(DATASET_ROOT, folder)):         if folder in [AUDIO_SUBFOLDER, NOISE_SUBFOLDER]:             # If folder is `audio` or `noise`, do nothing             continue         elif folder in [\"other\", \"_background_noise_\"]:             # If folder is one of the folders that contains noise samples,             # move it to the `noise` folder             shutil.move(                 os.path.join(DATASET_ROOT, folder),                 os.path.join(DATASET_NOISE_PATH, folder),             )         else:             # Otherwise, it should be a speaker folder, then move it to             # `audio` folder             shutil.move(                 os.path.join(DATASET_ROOT, folder),                 os.path.join(DATASET_AUDIO_PATH, folder),             )"},{"path":"https://keras.posit.co/articles/examples/speaker_recognition_using_cnn.html","id":"noise-preparation","dir":"Articles > Examples","previous_headings":"","what":"Noise preparation","title":"Speaker Recognition","text":"section: load noise samples (resampled 16000) split noise samples chunks 16000 samples correspond 1 second duration Resample noise samples 16000 Hz","code":"# Get the list of all noise files noise_paths = [] for subdir in os.listdir(DATASET_NOISE_PATH):     subdir_path = Path(DATASET_NOISE_PATH) / subdir     if os.path.isdir(subdir_path):         noise_paths += [             os.path.join(subdir_path, filepath)             for filepath in os.listdir(subdir_path)             if filepath.endswith(\".wav\")         ]  print(     \"Found {} files belonging to {} directories\".format(         len(noise_paths), len(os.listdir(DATASET_NOISE_PATH))     ) ) command = (     \"for dir in `ls -1 \" + DATASET_NOISE_PATH + \"`; do \"     \"for file in `ls -1 \" + DATASET_NOISE_PATH + \"/$dir/*.wav`; do \"     \"sample_rate=`ffprobe -hide_banner -loglevel panic -show_streams \"     \"$file | grep sample_rate | cut -f2 -d=`; \"     \"if [ $sample_rate -ne 16000 ]; then \"     \"ffmpeg -hide_banner -loglevel panic -y \"     \"-i $file -ar 16000 temp.wav; \"     \"mv temp.wav $file; \"     \"fi; done; done\" ) os.system(command)   # Split noise into chunks of 16,000 steps each def load_noise_sample(path):     sample, sampling_rate = tf.audio.decode_wav(         tf.io.read_file(path), desired_channels=1     )     if sampling_rate == SAMPLING_RATE:         # Number of slices of 16000 each that can be generated from the noise sample         slices = int(sample.shape[0] / SAMPLING_RATE)         sample = tf.split(sample[: slices * SAMPLING_RATE], slices)         return sample     else:         print(\"Sampling rate for {} is incorrect. Ignoring it\".format(path))         return None   noises = [] for path in noise_paths:     sample = load_noise_sample(path)     if sample:         noises.extend(sample) noises = tf.stack(noises)  print(     \"{} noise files were split into {} noise samples where each is {} sec. long\".format(         len(noise_paths), noises.shape[0], noises.shape[1] // SAMPLING_RATE     ) )"},{"path":"https://keras.posit.co/articles/examples/speaker_recognition_using_cnn.html","id":"dataset-generation","dir":"Articles > Examples","previous_headings":"","what":"Dataset generation","title":"Speaker Recognition","text":"","code":"def paths_and_labels_to_dataset(audio_paths, labels):     \"\"\"Constructs a dataset of audios and labels.\"\"\"     path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)     audio_ds = path_ds.map(         lambda x: path_to_audio(x), num_parallel_calls=tf.data.AUTOTUNE     )     label_ds = tf.data.Dataset.from_tensor_slices(labels)     return tf.data.Dataset.zip((audio_ds, label_ds))   def path_to_audio(path):     \"\"\"Reads and decodes an audio file.\"\"\"     audio = tf.io.read_file(path)     audio, _ = tf.audio.decode_wav(audio, 1, SAMPLING_RATE)     return audio   def add_noise(audio, noises=None, scale=0.5):     if noises is not None:         # Create a random tensor of the same size as audio ranging from         # 0 to the number of noise stream samples that we have.         tf_rnd = tf.random.uniform(             (tf.shape(audio)[0],), 0, noises.shape[0], dtype=tf.int32         )         noise = tf.gather(noises, tf_rnd, axis=0)          # Get the amplitude proportion between the audio and the noise         prop = tf.math.reduce_max(audio, axis=1) / tf.math.reduce_max(             noise, axis=1         )         prop = tf.repeat(             tf.expand_dims(prop, axis=1), tf.shape(audio)[1], axis=1         )          # Adding the rescaled noise to audio         audio = audio + noise * prop * scale      return audio   def audio_to_fft(audio):     # Since tf.signal.fft applies FFT on the innermost dimension,     # we need to squeeze the dimensions and then expand them again     # after FFT     audio = tf.squeeze(audio, axis=-1)     fft = tf.signal.fft(         tf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64)     )     fft = tf.expand_dims(fft, axis=-1)      # Return the absolute value of the first half of the FFT     # which represents the positive frequencies     return tf.math.abs(fft[:, : (audio.shape[1] // 2), :])   # Get the list of audio file paths along with their corresponding labels  class_names = os.listdir(DATASET_AUDIO_PATH) print(     \"Our class names: {}\".format(         class_names,     ) )  audio_paths = [] labels = [] for label, name in enumerate(class_names):     print(         \"Processing speaker {}\".format(             name,         )     )     dir_path = Path(DATASET_AUDIO_PATH) / name     speaker_sample_paths = [         os.path.join(dir_path, filepath)         for filepath in os.listdir(dir_path)         if filepath.endswith(\".wav\")     ]     audio_paths += speaker_sample_paths     labels += [label] * len(speaker_sample_paths)  print(     \"Found {} files belonging to {} classes.\".format(         len(audio_paths), len(class_names)     ) )  # Shuffle rng = np.random.RandomState(SHUFFLE_SEED) rng.shuffle(audio_paths) rng = np.random.RandomState(SHUFFLE_SEED) rng.shuffle(labels)  # Split into training and validation num_val_samples = int(VALID_SPLIT * len(audio_paths)) print(\"Using {} files for training.\".format(len(audio_paths) - num_val_samples)) train_audio_paths = audio_paths[:-num_val_samples] train_labels = labels[:-num_val_samples]  print(\"Using {} files for validation.\".format(num_val_samples)) valid_audio_paths = audio_paths[-num_val_samples:] valid_labels = labels[-num_val_samples:]  # Create 2 datasets, one for training and the other for validation train_ds = paths_and_labels_to_dataset(train_audio_paths, train_labels) train_ds = train_ds.shuffle(     buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED ).batch(BATCH_SIZE)  valid_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels) valid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=SHUFFLE_SEED).batch(32)   # Add noise to the training set train_ds = train_ds.map(     lambda x, y: (add_noise(x, noises, scale=SCALE), y),     num_parallel_calls=tf.data.AUTOTUNE, )  # Transform audio wave to the frequency domain using `audio_to_fft` train_ds = train_ds.map(     lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.AUTOTUNE ) train_ds = train_ds.prefetch(tf.data.AUTOTUNE)  valid_ds = valid_ds.map(     lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.AUTOTUNE ) valid_ds = valid_ds.prefetch(tf.data.AUTOTUNE)"},{"path":"https://keras.posit.co/articles/examples/speaker_recognition_using_cnn.html","id":"model-definition","dir":"Articles > Examples","previous_headings":"","what":"Model Definition","title":"Speaker Recognition","text":"","code":"def residual_block(x, filters, conv_num=3, activation=\"relu\"):     # Shortcut     s = keras.layers.Conv1D(filters, 1, padding=\"same\")(x)     for i in range(conv_num - 1):         x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)         x = keras.layers.Activation(activation)(x)     x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)     x = keras.layers.Add()([x, s])     x = keras.layers.Activation(activation)(x)     return keras.layers.MaxPool1D(pool_size=2, strides=2)(x)   def build_model(input_shape, num_classes):     inputs = keras.layers.Input(shape=input_shape, name=\"input\")      x = residual_block(inputs, 16, 2)     x = residual_block(x, 32, 2)     x = residual_block(x, 64, 3)     x = residual_block(x, 128, 3)     x = residual_block(x, 128, 3)      x = keras.layers.AveragePooling1D(pool_size=3, strides=3)(x)     x = keras.layers.Flatten()(x)     x = keras.layers.Dense(256, activation=\"relu\")(x)     x = keras.layers.Dense(128, activation=\"relu\")(x)      outputs = keras.layers.Dense(         num_classes, activation=\"softmax\", name=\"output\"     )(x)      return keras.models.Model(inputs=inputs, outputs=outputs)   model = build_model((SAMPLING_RATE // 2, 1), len(class_names))  model.summary()  # Compile the model using Adam's default learning rate model.compile(     optimizer=\"Adam\",     loss=\"sparse_categorical_crossentropy\",     metrics=[\"accuracy\"], )  # Add callbacks: # 'EarlyStopping' to stop training when the model is not enhancing anymore # 'ModelCheckPoint' to always keep the model that has the best val_accuracy model_save_filename = \"model.keras\"  earlystopping_cb = keras.callbacks.EarlyStopping(     patience=10, restore_best_weights=True ) mdlcheckpoint_cb = keras.callbacks.ModelCheckpoint(     model_save_filename, monitor=\"val_accuracy\", save_best_only=True )"},{"path":"https://keras.posit.co/articles/examples/speaker_recognition_using_cnn.html","id":"training","dir":"Articles > Examples","previous_headings":"","what":"Training","title":"Speaker Recognition","text":"","code":"history = model.fit(     train_ds,     epochs=EPOCHS,     validation_data=valid_ds,     callbacks=[earlystopping_cb, mdlcheckpoint_cb], )"},{"path":"https://keras.posit.co/articles/examples/speaker_recognition_using_cnn.html","id":"evaluation","dir":"Articles > Examples","previous_headings":"","what":"Evaluation","title":"Speaker Recognition","text":"get ~ 98% validation accuracy.","code":"print(model.evaluate(valid_ds))"},{"path":"https://keras.posit.co/articles/examples/speaker_recognition_using_cnn.html","id":"demonstration","dir":"Articles > Examples","previous_headings":"","what":"Demonstration","title":"Speaker Recognition","text":"Let’s take samples : Predict speaker Compare prediction real speaker Listen audio see despite samples noisy, model still pretty accurate","code":"SAMPLES_TO_DISPLAY = 10  test_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels) test_ds = test_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(     BATCH_SIZE )  test_ds = test_ds.map(     lambda x, y: (add_noise(x, noises, scale=SCALE), y),     num_parallel_calls=tf.data.AUTOTUNE, )  for audios, labels in test_ds.take(1):     # Get the signal FFT     ffts = audio_to_fft(audios)     # Predict     y_pred = model.predict(ffts)     # Take random samples     rnd = np.random.randint(0, BATCH_SIZE, SAMPLES_TO_DISPLAY)     audios = audios.numpy()[rnd, :, :]     labels = labels.numpy()[rnd]     y_pred = np.argmax(y_pred, axis=-1)[rnd]      for index in range(SAMPLES_TO_DISPLAY):         # For every sample, print the true and predicted label         # as well as run the voice with the noise         print(             \"Speaker:\\33{} {}\\33[0m\\tPredicted:\\33{} {}\\33[0m\".format(                 \"[92m\" if labels[index] == y_pred[index] else \"[91m\",                 class_names[labels[index]],                 \"[92m\" if labels[index] == y_pred[index] else \"[91m\",                 class_names[y_pred[index]],             )         )         display(Audio(audios[index, :, :].squeeze(), rate=SAMPLING_RATE))"},{"path":"https://keras.posit.co/articles/examples/structured_data_classification_from_scratch.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Structured data classification from scratch","text":"example demonstrates structured data classification, starting raw CSV file. data includes numerical categorical features. use Keras preprocessing layers normalize numerical features vectorize categorical ones. Note example run TensorFlow 2.5 higher.","code":""},{"path":"https://keras.posit.co/articles/examples/structured_data_classification_from_scratch.html","id":"the-dataset","dir":"Articles > Examples","previous_headings":"Introduction","what":"The dataset","title":"Structured data classification from scratch","text":"dataset provided Cleveland Clinic Foundation Heart Disease. ’s CSV file 303 rows. row contains information patient (sample), column describes attribute patient (feature). use features predict whether patient heart disease (binary classification). ’s description feature:","code":""},{"path":"https://keras.posit.co/articles/examples/structured_data_classification_from_scratch.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Structured data classification from scratch","text":"","code":"import tensorflow as tf import pandas as pd import keras as keras from keras import layers"},{"path":"https://keras.posit.co/articles/examples/structured_data_classification_from_scratch.html","id":"preparing-the-data","dir":"Articles > Examples","previous_headings":"","what":"Preparing the data","title":"Structured data classification from scratch","text":"Let’s download data load Pandas dataframe: dataset includes 303 samples 14 columns per sample (13 features, plus target label): ’s preview samples: last column, “target”, indicates whether patient heart disease (1) (0). Let’s split data training validation set: Let’s generate tf.data.Dataset objects dataframe: Dataset yields tuple (input, target) input dictionary features target value 0 1: Let’s batch datasets:","code":"file_url = (     \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\" ) dataframe = pd.read_csv(file_url) dataframe.shape dataframe.head() val_dataframe = dataframe.sample(frac=0.2, random_state=1337) train_dataframe = dataframe.drop(val_dataframe.index)  print(     f\"Using {len(train_dataframe)} samples for training \"     f\"and {len(val_dataframe)} for validation\" ) def dataframe_to_dataset(dataframe):     dataframe = dataframe.copy()     labels = dataframe.pop(\"target\")     ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))     ds = ds.shuffle(buffer_size=len(dataframe))     return ds   train_ds = dataframe_to_dataset(train_dataframe) val_ds = dataframe_to_dataset(val_dataframe) for x, y in train_ds.take(1):     print(\"Input:\", x)     print(\"Target:\", y) train_ds = train_ds.batch(32) val_ds = val_ds.batch(32)"},{"path":"https://keras.posit.co/articles/examples/structured_data_classification_from_scratch.html","id":"feature-preprocessing-with-keras-layers","dir":"Articles > Examples","previous_headings":"","what":"Feature preprocessing with Keras layers","title":"Structured data classification from scratch","text":"following features categorical features encoded integers: sex cp fbs restecg exang ca encode features using one-hot encoding. two options : Use CategoryEncoding(), requires knowing range input values error input outside range. Use IntegerLookup() build lookup table inputs reserve output index unkown input values. example, want simple solution handle range inputs inference, use IntegerLookup(). also categorical feature encoded string: thal. create index possible features encode output using StringLookup() layer. Finally, following feature continuous numerical features: age trestbps chol thalach oldpeak slope features, use Normalization() layer make sure mean feature 0 standard deviation 1. , define 3 utility functions operations: encode_numerical_feature apply featurewise normalization numerical features. encode_string_categorical_feature first turn string inputs integer indices, one-hot encode integer indices. encode_integer_categorical_feature one-hot encode integer categorical features.","code":"from keras.layers import IntegerLookup from keras.layers import Normalization from keras.layers import StringLookup   def encode_numerical_feature(feature, name, dataset):     # Create a Normalization layer for our feature     normalizer = Normalization()      # Prepare a Dataset that only yields our feature     feature_ds = dataset.map(lambda x, y: x[name])     feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))      # Learn the statistics of the data     normalizer.adapt(feature_ds)      # Normalize the input feature     encoded_feature = normalizer(feature)     return encoded_feature   def encode_categorical_feature(feature, name, dataset, is_string):     lookup_class = StringLookup if is_string else IntegerLookup     # Create a lookup layer which will turn strings into integer indices     lookup = lookup_class(output_mode=\"binary\")      # Prepare a Dataset that only yields our feature     feature_ds = dataset.map(lambda x, y: x[name])     feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))      # Learn the set of possible string values and assign them a fixed integer index     lookup.adapt(feature_ds)      # Turn the string input into integer indices     encoded_feature = lookup(feature)     return encoded_feature"},{"path":"https://keras.posit.co/articles/examples/structured_data_classification_from_scratch.html","id":"build-a-model","dir":"Articles > Examples","previous_headings":"","what":"Build a model","title":"Structured data classification from scratch","text":"done, can create end--end model: Let’s visualize connectivity graph:","code":"# Categorical features encoded as integers sex = keras.Input(shape=(1,), name=\"sex\", dtype=\"int64\") cp = keras.Input(shape=(1,), name=\"cp\", dtype=\"int64\") fbs = keras.Input(shape=(1,), name=\"fbs\", dtype=\"int64\") restecg = keras.Input(shape=(1,), name=\"restecg\", dtype=\"int64\") exang = keras.Input(shape=(1,), name=\"exang\", dtype=\"int64\") ca = keras.Input(shape=(1,), name=\"ca\", dtype=\"int64\")  # Categorical feature encoded as string thal = keras.Input(shape=(1,), name=\"thal\", dtype=\"string\")  # Numerical features age = keras.Input(shape=(1,), name=\"age\") trestbps = keras.Input(shape=(1,), name=\"trestbps\") chol = keras.Input(shape=(1,), name=\"chol\") thalach = keras.Input(shape=(1,), name=\"thalach\") oldpeak = keras.Input(shape=(1,), name=\"oldpeak\") slope = keras.Input(shape=(1,), name=\"slope\")  all_inputs = [     sex,     cp,     fbs,     restecg,     exang,     ca,     thal,     age,     trestbps,     chol,     thalach,     oldpeak,     slope, ]  # Integer categorical features sex_encoded = encode_categorical_feature(sex, \"sex\", train_ds, False) cp_encoded = encode_categorical_feature(cp, \"cp\", train_ds, False) fbs_encoded = encode_categorical_feature(fbs, \"fbs\", train_ds, False) restecg_encoded = encode_categorical_feature(     restecg, \"restecg\", train_ds, False ) exang_encoded = encode_categorical_feature(exang, \"exang\", train_ds, False) ca_encoded = encode_categorical_feature(ca, \"ca\", train_ds, False)  # String categorical features thal_encoded = encode_categorical_feature(thal, \"thal\", train_ds, True)  # Numerical features age_encoded = encode_numerical_feature(age, \"age\", train_ds) trestbps_encoded = encode_numerical_feature(trestbps, \"trestbps\", train_ds) chol_encoded = encode_numerical_feature(chol, \"chol\", train_ds) thalach_encoded = encode_numerical_feature(thalach, \"thalach\", train_ds) oldpeak_encoded = encode_numerical_feature(oldpeak, \"oldpeak\", train_ds) slope_encoded = encode_numerical_feature(slope, \"slope\", train_ds)  all_features = layers.concatenate(     [         sex_encoded,         cp_encoded,         fbs_encoded,         restecg_encoded,         exang_encoded,         slope_encoded,         ca_encoded,         thal_encoded,         age_encoded,         trestbps_encoded,         chol_encoded,         thalach_encoded,         oldpeak_encoded,     ] ) x = layers.Dense(32, activation=\"relu\")(all_features) x = layers.Dropout(0.5)(x) output = layers.Dense(1, activation=\"sigmoid\")(x) model = keras.Model(all_inputs, output) model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"]) # `rankdir='LR'` is to make the graph horizontal. keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"},{"path":"https://keras.posit.co/articles/examples/structured_data_classification_from_scratch.html","id":"train-the-model","dir":"Articles > Examples","previous_headings":"","what":"Train the model","title":"Structured data classification from scratch","text":"quickly get 80% validation accuracy.","code":"model.fit(train_ds, epochs=50, validation_data=val_ds)"},{"path":"https://keras.posit.co/articles/examples/structured_data_classification_from_scratch.html","id":"inference-on-new-data","dir":"Articles > Examples","previous_headings":"","what":"Inference on new data","title":"Structured data classification from scratch","text":"get prediction new sample, can simply call model.predict(). just two things need : wrap scalars list batch dimension (models process batches data, single samples) Call convert_to_tensor feature","code":"sample = {     \"age\": 60,     \"sex\": 1,     \"cp\": 1,     \"trestbps\": 145,     \"chol\": 233,     \"fbs\": 1,     \"restecg\": 2,     \"thalach\": 150,     \"exang\": 0,     \"oldpeak\": 2.3,     \"slope\": 3,     \"ca\": 0,     \"thal\": \"fixed\", }  input_dict = {     name: tf.convert_to_tensor([value]) for name, value in sample.items() } predictions = model.predict(input_dict)  print(     f\"This particular patient had a {100 * predictions[0][0]:.1f} \"     \"percent probability of having a heart disease, \"     \"as evaluated by our model.\" )"},{"path":"https://keras.posit.co/articles/examples/structured_data_classification_with_feature_space.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Structured data classification with FeatureSpace","text":"example demonstrates structured data classification (also known tabular data classification), starting raw CSV file. data includes numerical features, integer categorical features, string categorical features. use utility keras.utils.FeatureSpace index, preprocess, encode features. code adapted example Structured data classification scratch. previous example managed low-level feature preprocessing encoding Keras preprocessing layers, example delegate everything FeatureSpace, making workflow extremely quick easy. Note example run TensorFlow 2.12 higher. release TensorFlow 2.12, can use tf-nightly.","code":""},{"path":"https://keras.posit.co/articles/examples/structured_data_classification_with_feature_space.html","id":"the-dataset","dir":"Articles > Examples","previous_headings":"Introduction","what":"The dataset","title":"Structured data classification with FeatureSpace","text":"dataset provided Cleveland Clinic Foundation Heart Disease. ’s CSV file 303 rows. row contains information patient (sample), column describes attribute patient (feature). use features predict whether patient heart disease (binary classification). ’s description feature:","code":""},{"path":"https://keras.posit.co/articles/examples/structured_data_classification_with_feature_space.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Structured data classification with FeatureSpace","text":"","code":"import tensorflow as tf import pandas as pd import keras as keras from keras.utils import FeatureSpace  keras.config.disable_traceback_filtering()"},{"path":"https://keras.posit.co/articles/examples/structured_data_classification_with_feature_space.html","id":"preparing-the-data","dir":"Articles > Examples","previous_headings":"","what":"Preparing the data","title":"Structured data classification with FeatureSpace","text":"Let’s download data load Pandas dataframe: dataset includes 303 samples 14 columns per sample (13 features, plus target label): ’s preview samples: last column, “target”, indicates whether patient heart disease (1) (0). Let’s split data training validation set: Let’s generate tf.data.Dataset objects dataframe: Dataset yields tuple (input, target) input dictionary features target value 0 1: Let’s batch datasets:","code":"file_url = (     \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\" ) dataframe = pd.read_csv(file_url) print(dataframe.shape) dataframe.head() val_dataframe = dataframe.sample(frac=0.2, random_state=1337) train_dataframe = dataframe.drop(val_dataframe.index)  print(     \"Using %d samples for training and %d for validation\"     % (len(train_dataframe), len(val_dataframe)) ) def dataframe_to_dataset(dataframe):     dataframe = dataframe.copy()     labels = dataframe.pop(\"target\")     ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))     ds = ds.shuffle(buffer_size=len(dataframe))     return ds   train_ds = dataframe_to_dataset(train_dataframe) val_ds = dataframe_to_dataset(val_dataframe) for x, y in train_ds.take(1):     print(\"Input:\", x)     print(\"Target:\", y) train_ds = train_ds.batch(32) val_ds = val_ds.batch(32)"},{"path":"https://keras.posit.co/articles/examples/structured_data_classification_with_feature_space.html","id":"configuring-a-featurespace","dir":"Articles > Examples","previous_headings":"","what":"Configuring a FeatureSpace","title":"Structured data classification with FeatureSpace","text":"configure feature preprocessed, instantiate keras.utils.FeatureSpace, pass dictionary maps name features string describes feature type. “integer categorical” features \"FBS\", one “string categorical” feature (\"thal\"), numerical features, ’d like normalize – except \"age\", ’d like discretize number bins. also use crosses argument capture feature interactions categorical features, say, create additional features represent value co-occurrences categorical features. can compute feature crosses like arbitrary sets categorical features – just tuples two features. resulting co-occurences hashed fixed-sized vector, don’t need worry whether co-occurence space large.","code":"feature_space = FeatureSpace(     features={         # Categorical features encoded as integers         \"sex\": \"integer_categorical\",         \"cp\": \"integer_categorical\",         \"fbs\": \"integer_categorical\",         \"restecg\": \"integer_categorical\",         \"exang\": \"integer_categorical\",         \"ca\": \"integer_categorical\",         # Categorical feature encoded as string         \"thal\": \"string_categorical\",         # Numerical features to discretize         \"age\": \"float_discretized\",         # Numerical features to normalize         \"trestbps\": \"float_normalized\",         \"chol\": \"float_normalized\",         \"thalach\": \"float_normalized\",         \"oldpeak\": \"float_normalized\",         \"slope\": \"float_normalized\",     },     # We create additional features by hashing     # value co-occurrences for the     # following groups of categorical features.     crosses=[(\"sex\", \"age\"), (\"thal\", \"ca\")],     # The hashing space for these co-occurrences     # wil be 32-dimensional.     crossing_dim=32,     # Our utility will one-hot encode all categorical     # features and concat all features into a single     # vector (one vector per sample).     output_mode=\"concat\", )"},{"path":"https://keras.posit.co/articles/examples/structured_data_classification_with_feature_space.html","id":"further-customizing-a-featurespace","dir":"Articles > Examples","previous_headings":"","what":"Further customizing a FeatureSpace","title":"Structured data classification with FeatureSpace","text":"Specifying feature type via string name quick easy, sometimes may want configure preprocessing feature. instance, case, categorical features don’t large set possible values – ’s handful values per feature (e.g. 1 0 feature \"FBS\"), possible values represented training set. result, don’t need reserve index represent “vocabulary” values features – default behavior. , just specify num_oov_indices=0 features tell feature preprocessor skip “vocabulary” indexing. customizations access include specifying number bins discretizing features type \"float_discretized\", dimensionality hashing space feature crossing.","code":"feature_space = FeatureSpace(     features={         # Categorical features encoded as integers         \"sex\": FeatureSpace.integer_categorical(num_oov_indices=0),         \"cp\": FeatureSpace.integer_categorical(num_oov_indices=0),         \"fbs\": FeatureSpace.integer_categorical(num_oov_indices=0),         \"restecg\": FeatureSpace.integer_categorical(num_oov_indices=0),         \"exang\": FeatureSpace.integer_categorical(num_oov_indices=0),         \"ca\": FeatureSpace.integer_categorical(num_oov_indices=0),         # Categorical feature encoded as string         \"thal\": FeatureSpace.string_categorical(num_oov_indices=0),         # Numerical features to discretize         \"age\": FeatureSpace.float_discretized(num_bins=30),         # Numerical features to normalize         \"trestbps\": FeatureSpace.float_normalized(),         \"chol\": FeatureSpace.float_normalized(),         \"thalach\": FeatureSpace.float_normalized(),         \"oldpeak\": FeatureSpace.float_normalized(),         \"slope\": FeatureSpace.float_normalized(),     },     # Specify feature cross with a custom crossing dim.     crosses=[         FeatureSpace.cross(feature_names=(\"sex\", \"age\"), crossing_dim=64),         FeatureSpace.cross(             feature_names=(\"thal\", \"ca\"),             crossing_dim=16,         ),     ],     output_mode=\"concat\", )"},{"path":"https://keras.posit.co/articles/examples/structured_data_classification_with_feature_space.html","id":"adapt-the-featurespace-to-the-training-data","dir":"Articles > Examples","previous_headings":"","what":"Adapt the FeatureSpace to the training data","title":"Structured data classification with FeatureSpace","text":"start using FeatureSpace build model, adapt training data. adapt(), FeatureSpace : Index set possible values categorical features. Compute mean variance numerical features normalize. Compute value boundaries different bins numerical features discretize. Note adapt() called tf.data.Dataset yields dicts feature values – labels. point, FeatureSpace can called dict raw feature values, return single concatenate vector sample, combining encoded features feature crosses.","code":"train_ds_with_no_labels = train_ds.map(lambda x, _: x) feature_space.adapt(train_ds_with_no_labels) for x, _ in train_ds.take(1):     preprocessed_x = feature_space(x)     print(\"preprocessed_x.shape:\", preprocessed_x.shape)     print(\"preprocessed_x.dtype:\", preprocessed_x.dtype)"},{"path":"https://keras.posit.co/articles/examples/structured_data_classification_with_feature_space.html","id":"two-ways-to-manage-preprocessing-as-part-of-the-tf-data-pipeline-or-in-the-model-itself","dir":"Articles > Examples","previous_headings":"","what":"Two ways to manage preprocessing: as part of the tf.data pipeline, or in the model itself","title":"Structured data classification with FeatureSpace","text":"two ways can leverage FeatureSpace:","code":""},{"path":"https://keras.posit.co/articles/examples/structured_data_classification_with_feature_space.html","id":"asynchronous-preprocessing-in-tf-data","dir":"Articles > Examples","previous_headings":"Two ways to manage preprocessing: as part of the tf.data pipeline, or in the model itself","what":"Asynchronous preprocessing in tf.data","title":"Structured data classification with FeatureSpace","text":"can make part data pipeline, model. enables asynchronous parallel preprocessing data CPU hits model. ’re training GPU TPU, want speed preprocessing. Usually, always right thing training.","code":""},{"path":"https://keras.posit.co/articles/examples/structured_data_classification_with_feature_space.html","id":"synchronous-preprocessing-in-the-model","dir":"Articles > Examples","previous_headings":"Two ways to manage preprocessing: as part of the tf.data pipeline, or in the model itself","what":"Synchronous preprocessing in the model","title":"Structured data classification with FeatureSpace","text":"can make part model. means model expect dicts raw feature values, preprocessing batch done synchronously (blocking manner) rest forward pass. want end--end model can process raw feature values – keep mind model able run CPU, since types feature preprocessing (e.g. string preprocessing) GPU TPU compatible. GPU / TPU performance-sensitive settings. general, want -model preprocessing inference CPU. case, apply FeatureSpace tf.data pipeline training, inference end--end model includes FeatureSpace. Let’s create training validation dataset preprocessed batches:","code":"preprocessed_train_ds = train_ds.map(     lambda x, y: (feature_space(x), y), num_parallel_calls=tf.data.AUTOTUNE ) preprocessed_train_ds = preprocessed_train_ds.prefetch(tf.data.AUTOTUNE)  preprocessed_val_ds = val_ds.map(     lambda x, y: (feature_space(x), y), num_parallel_calls=tf.data.AUTOTUNE ) preprocessed_val_ds = preprocessed_val_ds.prefetch(tf.data.AUTOTUNE)"},{"path":"https://keras.posit.co/articles/examples/structured_data_classification_with_feature_space.html","id":"build-a-model","dir":"Articles > Examples","previous_headings":"","what":"Build a model","title":"Structured data classification with FeatureSpace","text":"Time build model – rather two models: training model expects preprocessed features (one sample = one vector) inference model expects raw features (one sample = dict raw feature values)","code":"dict_inputs = feature_space.get_inputs() encoded_features = feature_space.get_encoded_features()  x = keras.layers.Dense(32, activation=\"relu\")(encoded_features) x = keras.layers.Dropout(0.5)(x) predictions = keras.layers.Dense(1, activation=\"sigmoid\")(x)  training_model = keras.Model(inputs=encoded_features, outputs=predictions) training_model.compile(     optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"] )  inference_model = keras.Model(inputs=dict_inputs, outputs=predictions)"},{"path":"https://keras.posit.co/articles/examples/structured_data_classification_with_feature_space.html","id":"train-the-model","dir":"Articles > Examples","previous_headings":"","what":"Train the model","title":"Structured data classification with FeatureSpace","text":"Let’s train model 50 epochs. Note feature preprocessing happening part tf.data pipeline, part model. quickly get 80% validation accuracy.","code":"training_model.fit(     preprocessed_train_ds,     epochs=20,     validation_data=preprocessed_val_ds,     verbose=2, )"},{"path":"https://keras.posit.co/articles/examples/structured_data_classification_with_feature_space.html","id":"inference-on-new-data-with-the-end-to-end-model","dir":"Articles > Examples","previous_headings":"","what":"Inference on new data with the end-to-end model","title":"Structured data classification with FeatureSpace","text":"Now, can use inference model (includes FeatureSpace) make predictions based dicts raw features values, follows:","code":"sample = {     \"age\": 60,     \"sex\": 1,     \"cp\": 1,     \"trestbps\": 145,     \"chol\": 233,     \"fbs\": 1,     \"restecg\": 2,     \"thalach\": 150,     \"exang\": 0,     \"oldpeak\": 2.3,     \"slope\": 3,     \"ca\": 0,     \"thal\": \"fixed\", }  input_dict = {     name: tf.convert_to_tensor([value]) for name, value in sample.items() } predictions = inference_model.predict(input_dict)  print(     f\"This particular patient had a {100 * predictions[0][0]:.2f}% probability \"     \"of having a heart disease, as evaluated by our model.\" )"},{"path":"https://keras.posit.co/articles/examples/subclassing_conv_layers.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Customizing the convolution operation of a Conv2D layer","text":"may sometimes need implement custom versions convolution layers like Conv1D Conv2D. Keras enables without implementing entire layer scratch: can reuse base convolution layer just customize convolution op via convolution_op() method. method introduced Keras 2.7. using convolution_op() API, ensure running Keras version 2.7.0 greater.","code":""},{"path":"https://keras.posit.co/articles/examples/subclassing_conv_layers.html","id":"a-simple-standardizedconv2d-implementation","dir":"Articles > Examples","previous_headings":"","what":"A Simple StandardizedConv2D implementation","title":"Customizing the convolution operation of a Conv2D layer","text":"two ways use Conv.convolution_op() API. first way override convolution_op() method convolution layer subclass. Using approach, can quickly implement StandardizedConv2D shown . way use Conv.convolution_op() API directly call convolution_op() method call() method convolution layer subclass. comparable class implemented using approach shown .","code":"import tensorflow as tf import keras as keras from keras import layers import numpy as np   class StandardizedConv2DWithOverride(layers.Conv2D):     def convolution_op(self, inputs, kernel):         mean, var = tf.nn.moments(kernel, axes=[0, 1, 2], keepdims=True)         return tf.nn.conv2d(             inputs,             (kernel - mean) / tf.sqrt(var + 1e-10),             padding=\"valid\",             strides=list(self.strides),             name=self.__class__.__name__,         ) class StandardizedConv2DWithCall(layers.Conv2D):     def call(self, inputs):         mean, var = tf.nn.moments(self.kernel, axes=[0, 1, 2], keepdims=True)         result = self.convolution_op(             inputs, (self.kernel - mean) / tf.sqrt(var + 1e-10)         )         if self.use_bias:             result = result + self.bias         return result"},{"path":"https://keras.posit.co/articles/examples/subclassing_conv_layers.html","id":"example-usage","dir":"Articles > Examples","previous_headings":"","what":"Example Usage","title":"Customizing the convolution operation of a Conv2D layer","text":"layers work drop-replacements Conv2D. following demonstration performs classification MNIST dataset.","code":"# Model / data parameters num_classes = 10 input_shape = (28, 28, 1)  # the data, split between train and test sets (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()  # Scale images to the [0, 1] range x_train = x_train.astype(\"float32\") / 255 x_test = x_test.astype(\"float32\") / 255 # Make sure images have shape (28, 28, 1) x_train = np.expand_dims(x_train, -1) x_test = np.expand_dims(x_test, -1) print(\"x_train shape:\", x_train.shape) print(x_train.shape[0], \"train samples\") print(x_test.shape[0], \"test samples\")  # convert class vectors to binary class matrices y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes)  model = keras.Sequential(     [         keras.layers.Input(shape=input_shape),         StandardizedConv2DWithCall(32, kernel_size=(3, 3), activation=\"relu\"),         layers.MaxPooling2D(pool_size=(2, 2)),         StandardizedConv2DWithOverride(             64, kernel_size=(3, 3), activation=\"relu\"         ),         layers.MaxPooling2D(pool_size=(2, 2)),         layers.Flatten(),         layers.Dropout(0.5),         layers.Dense(num_classes, activation=\"softmax\"),     ] )  model.summary() batch_size = 128 epochs = 5  model.compile(     loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"] )  model.fit(     x_train, y_train, batch_size=batch_size, epochs=5, validation_split=0.1 )"},{"path":"https://keras.posit.co/articles/examples/subclassing_conv_layers.html","id":"conclusion","dir":"Articles > Examples","previous_headings":"","what":"Conclusion","title":"Customizing the convolution operation of a Conv2D layer","text":"Conv.convolution_op() API provides easy readable way implement custom convolution layers. StandardizedConvolution implementation using API quite terse, consisting four lines code.","code":""},{"path":"https://keras.posit.co/articles/examples/super_resolution_sub_pixel.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Image Super-Resolution using an Efficient Sub-Pixel CNN","text":"ESPCN (Efficient Sub-Pixel CNN), proposed Shi, 2016 model reconstructs high-resolution version image given low-resolution version. leverages efficient “sub-pixel convolution” layers, learns array image upscaling filters. code example, implement model paper train small dataset, BSDS500. BSDS500.","code":""},{"path":"https://keras.posit.co/articles/examples/super_resolution_sub_pixel.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Image Super-Resolution using an Efficient Sub-Pixel CNN","text":"","code":"import keras as keras from keras import layers from keras import ops from keras.utils import load_img from keras.utils import array_to_img from keras.utils import img_to_array from keras.preprocessing import image_dataset_from_directory import tensorflow as tf  #  only for data preprocessing  import math import numpy as np  from IPython.display import display"},{"path":[]},{"path":"https://keras.posit.co/articles/examples/super_resolution_sub_pixel.html","id":"download-dataset","dir":"Articles > Examples","previous_headings":"Load data: BSDS500 dataset","what":"Download dataset","title":"Image Super-Resolution using an Efficient Sub-Pixel CNN","text":"use built-keras.utils.get_file utility retrieve dataset. create training validation datasets via image_dataset_from_directory. rescale images take values range [0, 1]. Let’s visualize sample images: prepare dataset test image paths use visual evaluation end example.","code":"dataset_url = \"http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/BSR/BSR_bsds500.tgz\" data_dir = keras.utils.get_file(origin=dataset_url, fname=\"BSR\", untar=True) root_dir = os.path.join(data_dir, \"BSDS500/data\") crop_size = 300 upscale_factor = 3 input_size = crop_size // upscale_factor batch_size = 8  train_ds = image_dataset_from_directory(     root_dir,     batch_size=batch_size,     image_size=(crop_size, crop_size),     validation_split=0.2,     subset=\"training\",     seed=1337,     label_mode=None, )  valid_ds = image_dataset_from_directory(     root_dir,     batch_size=batch_size,     image_size=(crop_size, crop_size),     validation_split=0.2,     subset=\"validation\",     seed=1337,     label_mode=None, ) def scaling(input_image):     input_image = input_image / 255.0     return input_image   # Scale from (0, 255) to (0, 1) train_ds = train_ds.map(scaling) valid_ds = valid_ds.map(scaling) for batch in train_ds.take(1):     for img in batch:         display(array_to_img(img)) dataset = os.path.join(root_dir, \"images\") test_path = os.path.join(dataset, \"test\")  test_img_paths = sorted(     [         os.path.join(test_path, fname)         for fname in os.listdir(test_path)         if fname.endswith(\".jpg\")     ] )"},{"path":"https://keras.posit.co/articles/examples/super_resolution_sub_pixel.html","id":"crop-and-resize-images","dir":"Articles > Examples","previous_headings":"","what":"Crop and resize images","title":"Image Super-Resolution using an Efficient Sub-Pixel CNN","text":"Let’s process image data. First, convert images RGB color space YUV colour space. input data (low-resolution images), crop image, retrieve y channel (luninance), resize area method (use BICUBIC use PIL). consider luminance channel YUV color space humans sensitive luminance change. target data (high-resolution images), just crop image retrieve y channel. Let’s take look input target data.","code":"# Use TF Ops to process. def process_input(input, input_size, upscale_factor):     input = tf.image.rgb_to_yuv(input)     last_dimension_axis = len(input.shape) - 1     y, u, v = tf.split(input, 3, axis=last_dimension_axis)     return tf.image.resize(y, [input_size, input_size], method=\"area\")   def process_target(input):     input = tf.image.rgb_to_yuv(input)     last_dimension_axis = len(input.shape) - 1     y, u, v = tf.split(input, 3, axis=last_dimension_axis)     return y   train_ds = train_ds.map(     lambda x: (process_input(x, input_size, upscale_factor), process_target(x)) ) train_ds = train_ds.prefetch(buffer_size=32)  valid_ds = valid_ds.map(     lambda x: (process_input(x, input_size, upscale_factor), process_target(x)) ) valid_ds = valid_ds.prefetch(buffer_size=32) for batch in train_ds.take(1):     for img in batch[0]:         display(array_to_img(img))     for img in batch[1]:         display(array_to_img(img))"},{"path":"https://keras.posit.co/articles/examples/super_resolution_sub_pixel.html","id":"build-a-model","dir":"Articles > Examples","previous_headings":"","what":"Build a model","title":"Image Super-Resolution using an Efficient Sub-Pixel CNN","text":"Compared paper, add one layer use relu activation function instead tanh. achieves better performance even though train model fewer epochs.","code":"class DepthToSpace(layers.Layer):     def __init__(self, block_size):         super().__init__()         self.block_size = block_size      def call(self, input):         batch, height, width, depth = ops.shape(input)         depth = depth // (self.block_size**2)          x = ops.reshape(             input, [batch, height, width, self.block_size, self.block_size, depth]         )         x = ops.transpose(x, [0, 1, 3, 2, 4, 5])         x = ops.reshape(             x, [batch, height * self.block_size, width * self.block_size, depth]         )         return x   def get_model(upscale_factor=3, channels=1):     conv_args = {         \"activation\": \"relu\",         \"kernel_initializer\": \"orthogonal\",         \"padding\": \"same\",     }     inputs = keras.Input(shape=(None, None, channels))     x = layers.Conv2D(64, 5, **conv_args)(inputs)     x = layers.Conv2D(64, 3, **conv_args)(x)     x = layers.Conv2D(32, 3, **conv_args)(x)     x = layers.Conv2D(channels * (upscale_factor**2), 3, **conv_args)(x)     outputs = DepthToSpace(upscale_factor)(x)      return keras.Model(inputs, outputs)"},{"path":"https://keras.posit.co/articles/examples/super_resolution_sub_pixel.html","id":"define-utility-functions","dir":"Articles > Examples","previous_headings":"","what":"Define utility functions","title":"Image Super-Resolution using an Efficient Sub-Pixel CNN","text":"need define several utility functions monitor results: plot_results plot save image. get_lowres_image convert image low-resolution version. upscale_image turn low-resolution image high-resolution version reconstructed model. function, use y channel YUV color space input model combine output channels obtain RGB image.","code":"import matplotlib.pyplot as plt from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes from mpl_toolkits.axes_grid1.inset_locator import mark_inset import PIL   def plot_results(img, prefix, title):     \"\"\"Plot the result with zoom-in area.\"\"\"     img_array = img_to_array(img)     img_array = img_array.astype(\"float32\") / 255.0      # Create a new figure with a default 111 subplot.     fig, ax = plt.subplots()     im = ax.imshow(img_array[::-1], origin=\"lower\")      plt.title(title)     # zoom-factor: 2.0, location: upper-left     axins = zoomed_inset_axes(ax, 2, loc=2)     axins.imshow(img_array[::-1], origin=\"lower\")      # Specify the limits.     x1, x2, y1, y2 = 200, 300, 100, 200     # Apply the x-limits.     axins.set_xlim(x1, x2)     # Apply the y-limits.     axins.set_ylim(y1, y2)      plt.yticks(visible=False)     plt.xticks(visible=False)      # Make the line.     mark_inset(ax, axins, loc1=1, loc2=3, fc=\"none\", ec=\"blue\")     plt.savefig(str(prefix) + \"-\" + title + \".png\")     plt.show()   def get_lowres_image(img, upscale_factor):     \"\"\"Return low-resolution image to use as model input.\"\"\"     return img.resize(         (img.size[0] // upscale_factor, img.size[1] // upscale_factor),         PIL.Image.BICUBIC,     )   def upscale_image(model, img):     \"\"\"Predict the result based on input image and restore the image as RGB.\"\"\"     ycbcr = img.convert(\"YCbCr\")     y, cb, cr = ycbcr.split()     y = img_to_array(y)     y = y.astype(\"float32\") / 255.0      input = np.expand_dims(y, axis=0)     out = model.predict(input)      out_img_y = out[0]     out_img_y *= 255.0      # Restore the image in RGB color space.     out_img_y = out_img_y.clip(0, 255)     out_img_y = out_img_y.reshape((np.shape(out_img_y)[0], np.shape(out_img_y)[1]))     out_img_y = PIL.Image.fromarray(np.uint8(out_img_y), mode=\"L\")     out_img_cb = cb.resize(out_img_y.size, PIL.Image.BICUBIC)     out_img_cr = cr.resize(out_img_y.size, PIL.Image.BICUBIC)     out_img = PIL.Image.merge(\"YCbCr\", (out_img_y, out_img_cb, out_img_cr)).convert(         \"RGB\"     )     return out_img"},{"path":"https://keras.posit.co/articles/examples/super_resolution_sub_pixel.html","id":"define-callbacks-to-monitor-training","dir":"Articles > Examples","previous_headings":"","what":"Define callbacks to monitor training","title":"Image Super-Resolution using an Efficient Sub-Pixel CNN","text":"ESPCNCallback object compute display PSNR metric. main metric use evaluate super-resolution performance. Define ModelCheckpoint EarlyStopping callbacks.","code":"class ESPCNCallback(keras.callbacks.Callback):     def __init__(self):         super().__init__()         self.test_img = get_lowres_image(load_img(test_img_paths[0]), upscale_factor)      # Store PSNR value in each epoch.     def on_epoch_begin(self, epoch, logs=None):         self.psnr = []      def on_epoch_end(self, epoch, logs=None):         print(\"Mean PSNR for epoch: %.2f\" % (np.mean(self.psnr)))         if epoch % 20 == 0:             prediction = upscale_image(self.model, self.test_img)             plot_results(prediction, \"epoch-\" + str(epoch), \"prediction\")      def on_test_batch_end(self, batch, logs=None):         self.psnr.append(10 * math.log10(1 / logs[\"loss\"])) early_stopping_callback = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=10)  checkpoint_filepath = \"/tmp/checkpoint.keras\"  model_checkpoint_callback = keras.callbacks.ModelCheckpoint(     filepath=checkpoint_filepath,     save_weights_only=False,     monitor=\"loss\",     mode=\"min\",     save_best_only=True, )  model = get_model(upscale_factor=upscale_factor, channels=1) model.summary()  callbacks = [ESPCNCallback(), early_stopping_callback, model_checkpoint_callback] loss_fn = keras.losses.MeanSquaredError() optimizer = keras.optimizers.Adam(learning_rate=0.001)"},{"path":"https://keras.posit.co/articles/examples/super_resolution_sub_pixel.html","id":"train-the-model","dir":"Articles > Examples","previous_headings":"","what":"Train the model","title":"Image Super-Resolution using an Efficient Sub-Pixel CNN","text":"","code":"epochs = 100  model.compile(     optimizer=optimizer,     loss=loss_fn, )  model.fit(     train_ds, epochs=epochs, callbacks=callbacks, validation_data=valid_ds, verbose=2 )  # The model weights (that are considered the best) are loaded into the model. model.load_weights(checkpoint_filepath)"},{"path":"https://keras.posit.co/articles/examples/super_resolution_sub_pixel.html","id":"run-model-prediction-and-plot-the-results","dir":"Articles > Examples","previous_headings":"","what":"Run model prediction and plot the results","title":"Image Super-Resolution using an Efficient Sub-Pixel CNN","text":"Let’s compute reconstructed version images save results.","code":"total_bicubic_psnr = 0.0 total_test_psnr = 0.0  for index, test_img_path in enumerate(test_img_paths[50:60]):     img = load_img(test_img_path)     lowres_input = get_lowres_image(img, upscale_factor)     w = lowres_input.size[0] * upscale_factor     h = lowres_input.size[1] * upscale_factor     highres_img = img.resize((w, h))     prediction = upscale_image(model, lowres_input)     lowres_img = lowres_input.resize((w, h))     lowres_img_arr = img_to_array(lowres_img)     highres_img_arr = img_to_array(highres_img)     predict_img_arr = img_to_array(prediction)     bicubic_psnr = tf.image.psnr(lowres_img_arr, highres_img_arr, max_val=255)     test_psnr = tf.image.psnr(predict_img_arr, highres_img_arr, max_val=255)      total_bicubic_psnr += bicubic_psnr     total_test_psnr += test_psnr      print(         \"PSNR of low resolution image and high resolution image is %.4f\" % bicubic_psnr     )     print(\"PSNR of predict and high resolution is %.4f\" % test_psnr)     plot_results(lowres_img, index, \"lowres\")     plot_results(highres_img, index, \"highres\")     plot_results(prediction, index, \"prediction\")  print(\"Avg. PSNR of lowres images is %.4f\" % (total_bicubic_psnr / 10)) print(\"Avg. PSNR of reconstructions is %.4f\" % (total_test_psnr / 10))"},{"path":"https://keras.posit.co/articles/examples/supervised_contrastive_learning.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Supervised Contrastive Learning","text":"Supervised Contrastive Learning (Prannay Khosla et al.) training methodology outperforms supervised training crossentropy classification tasks. Essentially, training image classification model Supervised Contrastive Learning performed two phases: Training encoder learn produce vector representations input images representations images class similar compared representations images different classes. Training classifier top frozen encoder.","code":""},{"path":"https://keras.posit.co/articles/examples/supervised_contrastive_learning.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Supervised Contrastive Learning","text":"","code":"import keras from keras import ops from keras import layers from keras.applications.resnet_v2 import ResNet50V2"},{"path":"https://keras.posit.co/articles/examples/supervised_contrastive_learning.html","id":"prepare-the-data","dir":"Articles > Examples","previous_headings":"","what":"Prepare the data","title":"Supervised Contrastive Learning","text":"","code":"num_classes = 10 input_shape = (32, 32, 3)  # Load the train and test data splits (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()  # Display shapes of train and test datasets print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\") print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"},{"path":"https://keras.posit.co/articles/examples/supervised_contrastive_learning.html","id":"using-image-data-augmentation","dir":"Articles > Examples","previous_headings":"","what":"Using image data augmentation","title":"Supervised Contrastive Learning","text":"","code":"data_augmentation = keras.Sequential(     [         layers.Normalization(),         layers.RandomFlip(\"horizontal\"),         layers.RandomRotation(0.02),     ] )  # Setting the state of the normalization layer. data_augmentation.layers[0].adapt(x_train)"},{"path":"https://keras.posit.co/articles/examples/supervised_contrastive_learning.html","id":"build-the-encoder-model","dir":"Articles > Examples","previous_headings":"","what":"Build the encoder model","title":"Supervised Contrastive Learning","text":"encoder model takes image input turns 2048-dimensional feature vector.","code":"def create_encoder():     resnet = ResNet50V2(         include_top=False, weights=None, input_shape=input_shape, pooling=\"avg\"     )      inputs = keras.Input(shape=input_shape)     augmented = data_augmentation(inputs)     outputs = resnet(augmented)     model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-encoder\")     return model   encoder = create_encoder() encoder.summary()  learning_rate = 0.001 batch_size = 265 hidden_units = 512 projection_units = 128 num_epochs = 50 dropout_rate = 0.5 temperature = 0.05"},{"path":"https://keras.posit.co/articles/examples/supervised_contrastive_learning.html","id":"build-the-classification-model","dir":"Articles > Examples","previous_headings":"","what":"Build the classification model","title":"Supervised Contrastive Learning","text":"classification model adds fully-connected layer top encoder, plus softmax layer target classes.","code":"def create_classifier(encoder, trainable=True):     for layer in encoder.layers:         layer.trainable = trainable      inputs = keras.Input(shape=input_shape)     features = encoder(inputs)     features = layers.Dropout(dropout_rate)(features)     features = layers.Dense(hidden_units, activation=\"relu\")(features)     features = layers.Dropout(dropout_rate)(features)     outputs = layers.Dense(num_classes, activation=\"softmax\")(features)      model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-classifier\")     model.compile(         optimizer=keras.optimizers.Adam(learning_rate),         loss=keras.losses.SparseCategoricalCrossentropy(),         metrics=[keras.metrics.SparseCategoricalAccuracy()],     )     return model"},{"path":"https://keras.posit.co/articles/examples/supervised_contrastive_learning.html","id":"define-npairs-loss-function","dir":"Articles > Examples","previous_headings":"","what":"Define npairs loss function","title":"Supervised Contrastive Learning","text":"","code":"def npairs_loss(y_true, y_pred):     \"\"\"Computes the npairs loss between `y_true` and `y_pred`.      Npairs loss expects paired data where a pair is composed of samples from     the same labels and each pairs in the minibatch have different labels.     The loss takes each row of the pair-wise similarity matrix, `y_pred`,     as logits and the remapped multi-class labels, `y_true`, as labels.       See:     http://www.nec-labs.com/uploads/images/Department-Images/MediaAnalytics/papers/nips16_npairmetriclearning.pdf      Args:       y_true: Ground truth values, of shape `[batch_size]` of multi-class         labels.       y_pred: Predicted values of shape `[batch_size, batch_size]` of         similarity matrix between embedding matrices.      Returns:       npairs_loss: float scalar.     \"\"\"     y_pred = ops.cast(y_pred, \"float32\")     y_true = ops.cast(y_true, y_pred.dtype)     y_true = ops.cast(ops.equal(y_true, ops.transpose(y_true)), y_pred.dtype)     y_true /= ops.sum(y_true, 1, keepdims=True)     loss = ops.categorical_crossentropy(y_true, y_pred, from_logits=True)      return ops.mean(loss)"},{"path":"https://keras.posit.co/articles/examples/supervised_contrastive_learning.html","id":"experiment-1-train-the-baseline-classification-model","dir":"Articles > Examples","previous_headings":"","what":"Experiment 1: Train the baseline classification model","title":"Supervised Contrastive Learning","text":"experiment, baseline classifier trained usual, .e., encoder classifier parts trained together single model minimize crossentropy loss.","code":"encoder = create_encoder() classifier = create_classifier(encoder) classifier.summary()  history = classifier.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs)  accuracy = classifier.evaluate(x_test, y_test)[1] print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"},{"path":"https://keras.posit.co/articles/examples/supervised_contrastive_learning.html","id":"experiment-2-use-supervised-contrastive-learning","dir":"Articles > Examples","previous_headings":"","what":"Experiment 2: Use supervised contrastive learning","title":"Supervised Contrastive Learning","text":"experiment, model trained two phases. first phase, encoder pretrained optimize supervised contrastive loss, described Prannay Khosla et al.. second phase, classifier trained using trained encoder weights freezed; weights fully-connected layers softmax optimized.","code":""},{"path":"https://keras.posit.co/articles/examples/supervised_contrastive_learning.html","id":"supervised-contrastive-learning-loss-function","dir":"Articles > Examples","previous_headings":"Experiment 2: Use supervised contrastive learning","what":"1. Supervised contrastive learning loss function","title":"Supervised Contrastive Learning","text":"","code":"class SupervisedContrastiveLoss(keras.losses.Loss):     def __init__(self, temperature=1, name=None):         super().__init__(name=name)         self.temperature = temperature      def __call__(self, labels, feature_vectors, sample_weight=None):         # Normalize feature vectors         feature_vectors_normalized = keras.utils.normalize(feature_vectors, axis=1, order=2)         # Compute logits         logits = ops.divide(             ops.matmul(                 feature_vectors_normalized, ops.transpose(feature_vectors_normalized)             ),             self.temperature,         )         return npairs_loss(ops.squeeze(labels), logits)   def add_projection_head(encoder):     inputs = keras.Input(shape=input_shape)     features = encoder(inputs)     outputs = layers.Dense(projection_units, activation=\"relu\")(features)     model = keras.Model(         inputs=inputs, outputs=outputs, name=\"cifar-encoder_with_projection-head\"     )     return model"},{"path":"https://keras.posit.co/articles/examples/supervised_contrastive_learning.html","id":"pretrain-the-encoder","dir":"Articles > Examples","previous_headings":"Experiment 2: Use supervised contrastive learning","what":"2. Pretrain the encoder","title":"Supervised Contrastive Learning","text":"","code":"encoder = create_encoder()  encoder_with_projection_head = add_projection_head(encoder) encoder_with_projection_head.compile(     optimizer=keras.optimizers.Adam(learning_rate),     loss=SupervisedContrastiveLoss(temperature), )  encoder_with_projection_head.summary()  history = encoder_with_projection_head.fit(     x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs )"},{"path":"https://keras.posit.co/articles/examples/supervised_contrastive_learning.html","id":"train-the-classifier-with-the-frozen-encoder","dir":"Articles > Examples","previous_headings":"Experiment 2: Use supervised contrastive learning","what":"3. Train the classifier with the frozen encoder","title":"Supervised Contrastive Learning","text":"get improved test accuracy.","code":"classifier = create_classifier(encoder, trainable=False)  history = classifier.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs)  accuracy = classifier.evaluate(x_test, y_test)[1] print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"},{"path":"https://keras.posit.co/articles/examples/supervised_contrastive_learning.html","id":"conclusion","dir":"Articles > Examples","previous_headings":"","what":"Conclusion","title":"Supervised Contrastive Learning","text":"shown experiments, using supervised contrastive learning technique outperformed conventional technique terms test accuracy. Note training budget (.e., number epochs) given technique. Supervised contrastive learning pays encoder involves complex architecture, like ResNet, multi-class problems many labels. addition, large batch sizes multi-layer projection heads improve effectiveness. See Supervised Contrastive Learning paper details.","code":""},{"path":"https://keras.posit.co/articles/examples/swim_transformers.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Image classification with Swin Transformers","text":"","code":"import matplotlib.pyplot as plt import numpy as np import tensorflow as tf import keras as keras from keras import layers"},{"path":"https://keras.posit.co/articles/examples/swim_transformers.html","id":"prepare-the-data","dir":"Articles > Examples","previous_headings":"","what":"Prepare the data","title":"Image classification with Swin Transformers","text":"load CIFAR-100 dataset tf.keras.datasets, normalize images, convert integer labels one-hot encoded vectors.","code":"num_classes = 100 input_shape = (32, 32, 3)  (x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 y_train = keras.utils.numerical_utils.to_categorical(y_train, num_classes) y_test = keras.utils.numerical_utils.to_categorical(y_test, num_classes) print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\") print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")  plt.figure(figsize=(10, 10)) for i in range(25):     plt.subplot(5, 5, i + 1)     plt.xticks([])     plt.yticks([])     plt.grid(False)     plt.imshow(x_train[i]) plt.show()"},{"path":"https://keras.posit.co/articles/examples/swim_transformers.html","id":"configure-the-hyperparameters","dir":"Articles > Examples","previous_headings":"","what":"Configure the hyperparameters","title":"Image classification with Swin Transformers","text":"key parameter pick patch_size, size input patches. order use pixel individual input, can set patch_size (1, 1). , take inspiration original paper settings training ImageNet-1K, keeping original settings example.","code":"patch_size = (2, 2)  # 2-by-2 sized patches dropout_rate = 0.03  # Dropout rate num_heads = 8  # Attention heads embed_dim = 64  # Embedding dimension num_mlp = 256  # MLP layer size qkv_bias = True  # Convert embedded patches to query, key, and values with a learnable additive value window_size = 2  # Size of attention window shift_size = 1  # Size of shifting window image_dimension = 32  # Initial image size  num_patch_x = input_shape[0] // patch_size[0] num_patch_y = input_shape[1] // patch_size[1]  learning_rate = 1e-3 batch_size = 128 num_epochs = 1 validation_split = 0.1 weight_decay = 0.0001 label_smoothing = 0.1"},{"path":"https://keras.posit.co/articles/examples/swim_transformers.html","id":"helper-functions","dir":"Articles > Examples","previous_headings":"","what":"Helper functions","title":"Image classification with Swin Transformers","text":"create two helper functions help us get sequence patches image, merge patches, apply dropout.","code":"def window_partition(x, window_size):     _, height, width, channels = x.shape     patch_num_y = height // window_size     patch_num_x = width // window_size     x = tf.reshape(         x,         shape=(             -1,             patch_num_y,             window_size,             patch_num_x,             window_size,             channels,         ),     )     x = tf.transpose(x, (0, 1, 3, 2, 4, 5))     windows = tf.reshape(x, shape=(-1, window_size, window_size, channels))     return windows   def window_reverse(windows, window_size, height, width, channels):     patch_num_y = height // window_size     patch_num_x = width // window_size     x = tf.reshape(         windows,         shape=(             -1,             patch_num_y,             patch_num_x,             window_size,             window_size,             channels,         ),     )     x = tf.transpose(x, perm=(0, 1, 3, 2, 4, 5))     x = tf.reshape(x, shape=(-1, height, width, channels))     return x   class DropPath(layers.Layer):     def __init__(self, drop_prob=None, **kwargs):         super().__init__(**kwargs)         self.drop_prob = drop_prob      def call(self, x):         input_shape = tf.shape(x)         batch_size = input_shape[0]         rank = x.shape.rank         shape = (batch_size,) + (1,) * (rank - 1)         random_tensor = (1 - self.drop_prob) + tf.random.uniform(             shape, dtype=x.dtype         )         path_mask = tf.floor(random_tensor)         output = tf.math.divide(x, 1 - self.drop_prob) * path_mask         return output"},{"path":"https://keras.posit.co/articles/examples/swim_transformers.html","id":"window-based-multi-head-self-attention","dir":"Articles > Examples","previous_headings":"","what":"Window based multi-head self-attention","title":"Image classification with Swin Transformers","text":"Usually Transformers perform global self-attention, relationships token tokens computed. global computation leads quadratic complexity respect number tokens. , original paper suggests, compute self-attention within local windows, non-overlapping manner. Global self-attention leads quadratic computational complexity number patches, whereas window-based self-attention leads linear complexity easily scalable.","code":"class WindowAttention(layers.Layer):     def __init__(         self,         dim,         window_size,         num_heads,         qkv_bias=True,         dropout_rate=0.0,         **kwargs,     ):         super().__init__(**kwargs)         self.dim = dim         self.window_size = window_size         self.num_heads = num_heads         self.scale = (dim // num_heads) ** -0.5         self.qkv = layers.Dense(dim * 3, use_bias=qkv_bias)         self.dropout = layers.Dropout(dropout_rate)         self.proj = layers.Dense(dim)      def build(self, input_shape):         num_window_elements = (2 * self.window_size[0] - 1) * (             2 * self.window_size[1] - 1         )         self.relative_position_bias_table = self.add_weight(             shape=(num_window_elements, self.num_heads),             initializer=tf.initializers.Zeros(),             trainable=True,         )         coords_h = np.arange(self.window_size[0])         coords_w = np.arange(self.window_size[1])         coords_matrix = np.meshgrid(coords_h, coords_w, indexing=\"ij\")         coords = np.stack(coords_matrix)         coords_flatten = coords.reshape(2, -1)         relative_coords = (             coords_flatten[:, :, None] - coords_flatten[:, None, :]         )         relative_coords = relative_coords.transpose([1, 2, 0])         relative_coords[:, :, 0] += self.window_size[0] - 1         relative_coords[:, :, 1] += self.window_size[1] - 1         relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1         relative_position_index = relative_coords.sum(-1)          self.relative_position_index = tf.Variable(             initial_value=lambda: tf.convert_to_tensor(relative_position_index),             trainable=False,         )      def call(self, x, mask=None):         _, size, channels = x.shape         head_dim = channels // self.num_heads         x_qkv = self.qkv(x)         x_qkv = tf.reshape(x_qkv, shape=(-1, size, 3, self.num_heads, head_dim))         x_qkv = tf.transpose(x_qkv, perm=(2, 0, 3, 1, 4))         q, k, v = x_qkv[0], x_qkv[1], x_qkv[2]         q = q * self.scale         k = tf.transpose(k, perm=(0, 1, 3, 2))         attn = q @ k          num_window_elements = self.window_size[0] * self.window_size[1]         relative_position_index_flat = tf.reshape(             self.relative_position_index, shape=(-1,)         )         relative_position_bias = tf.gather(             self.relative_position_bias_table, relative_position_index_flat         )         relative_position_bias = tf.reshape(             relative_position_bias,             shape=(num_window_elements, num_window_elements, -1),         )         relative_position_bias = tf.transpose(             relative_position_bias, perm=(2, 0, 1)         )         attn = attn + tf.expand_dims(relative_position_bias, axis=0)          if mask is not None:             nW = mask.shape[0]             mask_float = tf.cast(                 tf.expand_dims(tf.expand_dims(mask, axis=1), axis=0), tf.float32             )             attn = (                 tf.reshape(attn, shape=(-1, nW, self.num_heads, size, size))                 + mask_float             )             attn = tf.reshape(attn, shape=(-1, self.num_heads, size, size))             attn = keras.activations.softmax(attn, axis=-1)         else:             attn = keras.activations.softmax(attn, axis=-1)         attn = self.dropout(attn)          x_qkv = attn @ v         x_qkv = tf.transpose(x_qkv, perm=(0, 2, 1, 3))         x_qkv = tf.reshape(x_qkv, shape=(-1, size, channels))         x_qkv = self.proj(x_qkv)         x_qkv = self.dropout(x_qkv)         return x_qkv"},{"path":"https://keras.posit.co/articles/examples/swim_transformers.html","id":"the-complete-swin-transformer-model","dir":"Articles > Examples","previous_headings":"","what":"The complete Swin Transformer model","title":"Image classification with Swin Transformers","text":"Finally, put together complete Swin Transformer replacing standard multi-head attention (MHA) shifted windows attention. suggested original paper, create model comprising shifted window-based MHA layer, followed 2-layer MLP GELU nonlinearity , applying LayerNormalization MSA layer MLP, residual connection layers. Notice create simple MLP 2 Dense 2 Dropout layers. Often see models using ResNet-50 MLP quite standard literature. However paper authors use 2-layer MLP GELU nonlinearity .","code":"class SwinTransformer(layers.Layer):     def __init__(         self,         dim,         num_patch,         num_heads,         window_size=7,         shift_size=0,         num_mlp=1024,         qkv_bias=True,         dropout_rate=0.0,         **kwargs,     ):         super().__init__(**kwargs)          self.dim = dim  # number of input dimensions         self.num_patch = num_patch  # number of embedded patches         self.num_heads = num_heads  # number of attention heads         self.window_size = window_size  # size of window         self.shift_size = shift_size  # size of window shift         self.num_mlp = num_mlp  # number of MLP nodes          self.norm1 = layers.LayerNormalization(epsilon=1e-5)         self.attn = WindowAttention(             dim,             window_size=(self.window_size, self.window_size),             num_heads=num_heads,             qkv_bias=qkv_bias,             dropout_rate=dropout_rate,         )         self.drop_path = DropPath(dropout_rate)         self.norm2 = layers.LayerNormalization(epsilon=1e-5)          self.mlp = keras.Sequential(             [                 layers.Dense(num_mlp),                 layers.Activation(keras.activations.gelu),                 layers.Dropout(dropout_rate),                 layers.Dense(dim),                 layers.Dropout(dropout_rate),             ]         )          if min(self.num_patch) < self.window_size:             self.shift_size = 0             self.window_size = min(self.num_patch)      def build(self, input_shape):         if self.shift_size == 0:             self.attn_mask = None         else:             height, width = self.num_patch             h_slices = (                 slice(0, -self.window_size),                 slice(-self.window_size, -self.shift_size),                 slice(-self.shift_size, None),             )             w_slices = (                 slice(0, -self.window_size),                 slice(-self.window_size, -self.shift_size),                 slice(-self.shift_size, None),             )             mask_array = np.zeros((1, height, width, 1))             count = 0             for h in h_slices:                 for w in w_slices:                     mask_array[:, h, w, :] = count                     count += 1             mask_array = tf.convert_to_tensor(mask_array)              # mask array to windows             mask_windows = window_partition(mask_array, self.window_size)             mask_windows = tf.reshape(                 mask_windows, shape=[-1, self.window_size * self.window_size]             )             attn_mask = tf.expand_dims(mask_windows, axis=1) - tf.expand_dims(                 mask_windows, axis=2             )             attn_mask = tf.where(attn_mask != 0, -100.0, attn_mask)             attn_mask = tf.where(attn_mask == 0, 0.0, attn_mask)             self.attn_mask = tf.Variable(                 initial_value=attn_mask, trainable=False             )      def call(self, x):         height, width = self.num_patch         _, num_patches_before, channels = x.shape         x_skip = x         x = self.norm1(x)         x = tf.reshape(x, shape=(-1, height, width, channels))         if self.shift_size > 0:             shifted_x = tf.roll(                 x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2]             )         else:             shifted_x = x          x_windows = window_partition(shifted_x, self.window_size)         x_windows = tf.reshape(             x_windows, shape=(-1, self.window_size * self.window_size, channels)         )         attn_windows = self.attn(x_windows, mask=self.attn_mask)          attn_windows = tf.reshape(             attn_windows,             shape=(-1, self.window_size, self.window_size, channels),         )         shifted_x = window_reverse(             attn_windows, self.window_size, height, width, channels         )         if self.shift_size > 0:             x = tf.roll(                 shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2]             )         else:             x = shifted_x          x = tf.reshape(x, shape=(-1, height * width, channels))         x = self.drop_path(x)         x = x_skip + x         x_skip = x         x = self.norm2(x)         x = self.mlp(x)         x = self.drop_path(x)         x = x_skip + x         return x"},{"path":[]},{"path":"https://keras.posit.co/articles/examples/swim_transformers.html","id":"extract-and-embed-patches","dir":"Articles > Examples","previous_headings":"Model training and evaluation","what":"Extract and embed patches","title":"Image classification with Swin Transformers","text":"first create 3 layers help us extract, embed merge patches images top later use Swin Transformer class built.","code":"class PatchExtract(layers.Layer):     def __init__(self, patch_size, **kwargs):         super().__init__(**kwargs)         self.patch_size_x = patch_size[0]         self.patch_size_y = patch_size[0]      def call(self, images):         batch_size = tf.shape(images)[0]         patches = tf.image.extract_patches(             images=images,             sizes=(1, self.patch_size_x, self.patch_size_y, 1),             strides=(1, self.patch_size_x, self.patch_size_y, 1),             rates=(1, 1, 1, 1),             padding=\"VALID\",         )         patch_dim = patches.shape[-1]         patch_num = patches.shape[1]         return tf.reshape(             patches, (batch_size, patch_num * patch_num, patch_dim)         )   class PatchEmbedding(layers.Layer):     def __init__(self, num_patch, embed_dim, **kwargs):         super().__init__(**kwargs)         self.num_patch = num_patch         self.proj = layers.Dense(embed_dim)         self.pos_embed = layers.Embedding(             input_dim=num_patch, output_dim=embed_dim         )      def call(self, patch):         pos = tf.range(start=0, limit=self.num_patch, delta=1)         return self.proj(patch) + self.pos_embed(pos)   class PatchMerging(keras.layers.Layer):     def __init__(self, num_patch, embed_dim):         super().__init__()         self.num_patch = num_patch         self.embed_dim = embed_dim         self.linear_trans = layers.Dense(2 * embed_dim, use_bias=False)      def call(self, x):         height, width = self.num_patch         _, _, C = x.shape         x = tf.reshape(x, shape=(-1, height, width, C))         x0 = x[:, 0::2, 0::2, :]         x1 = x[:, 1::2, 0::2, :]         x2 = x[:, 0::2, 1::2, :]         x3 = x[:, 1::2, 1::2, :]         x = tf.concat((x0, x1, x2, x3), axis=-1)         x = tf.reshape(x, shape=(-1, (height // 2) * (width // 2), 4 * C))         return self.linear_trans(x)"},{"path":"https://keras.posit.co/articles/examples/swim_transformers.html","id":"build-the-model","dir":"Articles > Examples","previous_headings":"Model training and evaluation","what":"Build the model","title":"Image classification with Swin Transformers","text":"put together Swin Transformer model.","code":"input = layers.Input(input_shape) x = layers.RandomCrop(image_dimension, image_dimension)(input) x = layers.RandomFlip(\"horizontal\")(x) x = PatchExtract(patch_size)(x) x = PatchEmbedding(num_patch_x * num_patch_y, embed_dim)(x) x = SwinTransformer(     dim=embed_dim,     num_patch=(num_patch_x, num_patch_y),     num_heads=num_heads,     window_size=window_size,     shift_size=0,     num_mlp=num_mlp,     qkv_bias=qkv_bias,     dropout_rate=dropout_rate, )(x) x = SwinTransformer(     dim=embed_dim,     num_patch=(num_patch_x, num_patch_y),     num_heads=num_heads,     window_size=window_size,     shift_size=shift_size,     num_mlp=num_mlp,     qkv_bias=qkv_bias,     dropout_rate=dropout_rate, )(x) x = PatchMerging((num_patch_x, num_patch_y), embed_dim=embed_dim)(x) x = layers.GlobalAveragePooling1D()(x) output = layers.Dense(num_classes, activation=\"softmax\")(x)"},{"path":"https://keras.posit.co/articles/examples/swim_transformers.html","id":"train-on-cifar-100","dir":"Articles > Examples","previous_headings":"Model training and evaluation","what":"Train on CIFAR-100","title":"Image classification with Swin Transformers","text":"train model CIFAR-100. , train model 40 epochs keep training time short example. practice, train 150 epochs reach convergence. Let’s visualize training progress model. Let’s display final results training CIFAR-100. Swin Transformer model just trained just 152K parameters, gets us ~75% test top-5 accuracy within just 40 epochs without signs overfitting well seen graph. means can train network longer (perhaps bit regularization) obtain even better performance. performance can improved additional techniques like cosine decay learning rate schedule, data augmentation techniques. experimenting, tried training model 150 epochs slightly higher dropout greater embedding dimensions pushes performance ~72% test accuracy CIFAR-100 can see screenshot. authors present top-1 accuracy 87.3% ImageNet. authors also present number experiments study input sizes, optimizers etc. affect final performance model. authors present using model object detection, semantic segmentation instance segmentation well report competitive results . strongly advised also check original paper. example takes inspiration official PyTorch TensorFlow implementations.","code":"model = keras.Model(input, output) model.compile(     loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),     optimizer=keras.optimizers.AdamW(         learning_rate=learning_rate, weight_decay=weight_decay     ),     metrics=[         keras.metrics.CategoricalAccuracy(name=\"accuracy\"),         keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),     ], )  history = model.fit(     x_train,     y_train,     batch_size=batch_size,     epochs=num_epochs,     validation_split=validation_split, ) plt.plot(history.history[\"loss\"], label=\"train_loss\") plt.plot(history.history[\"val_loss\"], label=\"val_loss\") plt.xlabel(\"Epochs\") plt.ylabel(\"Loss\") plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14) plt.legend() plt.grid() plt.show() loss, accuracy, top_5_accuracy = model.evaluate(x_test, y_test) print(f\"Test loss: {round(loss, 2)}\") print(f\"Test accuracy: {round(accuracy * 100, 2)}%\") print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")"},{"path":"https://keras.posit.co/articles/examples/tabtransformer.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Structured data learning with TabTransformer","text":"example demonstrates structured data classification using TabTransformer, deep tabular data modeling architecture supervised semi-supervised learning. TabTransformer built upon self-attention based Transformers. Transformer layers transform embeddings categorical features robust contextual embeddings achieve higher predictive accuracy.","code":""},{"path":"https://keras.posit.co/articles/examples/tabtransformer.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Structured data learning with TabTransformer","text":"","code":"import keras as keras from keras import layers from keras import ops  import math import numpy as np import pandas as pd from tensorflow import data as tf_data import matplotlib.pyplot as plt from functools import partial"},{"path":"https://keras.posit.co/articles/examples/tabtransformer.html","id":"prepare-the-data","dir":"Articles > Examples","previous_headings":"","what":"Prepare the data","title":"Structured data learning with TabTransformer","text":"example uses United States Census Income Dataset provided UC Irvine Machine Learning Repository. task binary classification predict whether person likely making USD 50,000 year. dataset includes 48,842 instances 14 input features: 5 numerical features 9 categorical features. First, let’s load dataset UCI Machine Learning Repository Pandas DataFrame: Remove first record (valid data example) trailing ‘dot’ class labels. Now store training test data separate CSV files.","code":"CSV_HEADER = [     \"age\",     \"workclass\",     \"fnlwgt\",     \"education\",     \"education_num\",     \"marital_status\",     \"occupation\",     \"relationship\",     \"race\",     \"gender\",     \"capital_gain\",     \"capital_loss\",     \"hours_per_week\",     \"native_country\",     \"income_bracket\", ]  train_data_url = (     \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\" ) train_data = pd.read_csv(train_data_url, header=None, names=CSV_HEADER)  test_data_url = (     \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\" ) test_data = pd.read_csv(test_data_url, header=None, names=CSV_HEADER)  print(f\"Train dataset shape: {train_data.shape}\") print(f\"Test dataset shape: {test_data.shape}\") test_data = test_data[1:] test_data.income_bracket = test_data.income_bracket.apply(     lambda value: value.replace(\".\", \"\") ) train_data_file = \"train_data.csv\" test_data_file = \"test_data.csv\"  train_data.to_csv(train_data_file, index=False, header=False) test_data.to_csv(test_data_file, index=False, header=False)"},{"path":"https://keras.posit.co/articles/examples/tabtransformer.html","id":"define-dataset-metadata","dir":"Articles > Examples","previous_headings":"","what":"Define dataset metadata","title":"Structured data learning with TabTransformer","text":", define metadata dataset useful reading parsing data input features, encoding input features respect types.","code":"# A list of the numerical feature names. NUMERIC_FEATURE_NAMES = [     \"age\",     \"education_num\",     \"capital_gain\",     \"capital_loss\",     \"hours_per_week\", ] # A dictionary of the categorical features and their vocabulary. CATEGORICAL_FEATURES_WITH_VOCABULARY = {     \"workclass\": sorted(list(train_data[\"workclass\"].unique())),     \"education\": sorted(list(train_data[\"education\"].unique())),     \"marital_status\": sorted(list(train_data[\"marital_status\"].unique())),     \"occupation\": sorted(list(train_data[\"occupation\"].unique())),     \"relationship\": sorted(list(train_data[\"relationship\"].unique())),     \"race\": sorted(list(train_data[\"race\"].unique())),     \"gender\": sorted(list(train_data[\"gender\"].unique())),     \"native_country\": sorted(list(train_data[\"native_country\"].unique())), } # Name of the column to be used as instances weight. WEIGHT_COLUMN_NAME = \"fnlwgt\" # A list of the categorical feature names. CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys()) # A list of all the input features. FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES # A list of column default values for each feature. COLUMN_DEFAULTS = [     [0.0] if feature_name in NUMERIC_FEATURE_NAMES + [WEIGHT_COLUMN_NAME] else [\"NA\"]     for feature_name in CSV_HEADER ] # The name of the target feature. TARGET_FEATURE_NAME = \"income_bracket\" # A list of the labels of the target features. TARGET_LABELS = [\" <=50K\", \" >50K\"]"},{"path":"https://keras.posit.co/articles/examples/tabtransformer.html","id":"configure-the-hyperparameters","dir":"Articles > Examples","previous_headings":"","what":"Configure the hyperparameters","title":"Structured data learning with TabTransformer","text":"hyperparameters includes model architecture training configurations.","code":"LEARNING_RATE = 0.001 WEIGHT_DECAY = 0.0001 DROPOUT_RATE = 0.2 BATCH_SIZE = 265 NUM_EPOCHS = 15  NUM_TRANSFORMER_BLOCKS = 3  # Number of transformer blocks. NUM_HEADS = 4  # Number of attention heads. EMBEDDING_DIMS = 16  # Embedding dimensions of the categorical features. MLP_HIDDEN_UNITS_FACTORS = [     2,     1, ]  # MLP hidden layer units, as factors of the number of inputs. NUM_MLP_BLOCKS = 2  # Number of MLP blocks in the baseline model."},{"path":"https://keras.posit.co/articles/examples/tabtransformer.html","id":"implement-data-reading-pipeline","dir":"Articles > Examples","previous_headings":"","what":"Implement data reading pipeline","title":"Structured data learning with TabTransformer","text":"define input function reads parses file, converts features labels atf.data.Dataset training evaluation.","code":"target_label_lookup = layers.StringLookup(     vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0 )   def prepare_example(features, target):     target_index = target_label_lookup(target)     weights = features.pop(WEIGHT_COLUMN_NAME)     return features, target_index, weights   lookup_dict = {} for feature_name in CATEGORICAL_FEATURE_NAMES:     vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]     # Create a lookup to convert a string values to an integer indices.     # Since we are not using a mask token, nor expecting any out of vocabulary     # (oov) token, we set mask_token to None and num_oov_indices to 0.     lookup = layers.StringLookup(         vocabulary=vocabulary, mask_token=None, num_oov_indices=0     )     lookup_dict[feature_name] = lookup   def encode_categorical(batch_x, batch_y, weights):     for feature_name in CATEGORICAL_FEATURE_NAMES:         batch_x[feature_name] = lookup_dict[feature_name](batch_x[feature_name])      return batch_x, batch_y, weights   def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):     dataset = (         tf.data.experimental.make_csv_dataset(             csv_file_path,             batch_size=batch_size,             column_names=CSV_HEADER,             column_defaults=COLUMN_DEFAULTS,             label_name=TARGET_FEATURE_NAME,             num_epochs=1,             header=False,             na_value=\"?\",             shuffle=shuffle,         )         .map(prepare_example, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)         .map(encode_categorical)     )     return dataset.cache()"},{"path":"https://keras.posit.co/articles/examples/tabtransformer.html","id":"implement-a-training-and-evaluation-procedure","dir":"Articles > Examples","previous_headings":"","what":"Implement a training and evaluation procedure","title":"Structured data learning with TabTransformer","text":"","code":"def run_experiment(     model,     train_data_file,     test_data_file,     num_epochs,     learning_rate,     weight_decay,     batch_size, ):     optimizer = keras.optimizers.AdamW(         learning_rate=learning_rate, weight_decay=weight_decay     )      model.compile(         optimizer=optimizer,         loss=keras.losses.BinaryCrossentropy(),         metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\")],     )      train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True)     validation_dataset = get_dataset_from_csv(test_data_file, batch_size)      print(\"Start training the model...\")     history = model.fit(         train_dataset, epochs=num_epochs, validation_data=validation_dataset     )     print(\"Model training finished\")      _, accuracy = model.evaluate(validation_dataset, verbose=0)      print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")      return history"},{"path":"https://keras.posit.co/articles/examples/tabtransformer.html","id":"create-model-inputs","dir":"Articles > Examples","previous_headings":"","what":"Create model inputs","title":"Structured data learning with TabTransformer","text":"Now, define inputs models dictionary, key feature name, value keras.layers.Input tensor corresponding feature shape data type.","code":"def create_model_inputs():     inputs = {}     for feature_name in FEATURE_NAMES:         if feature_name in NUMERIC_FEATURE_NAMES:             inputs[feature_name] = layers.Input(                 name=feature_name, shape=(), dtype=\"float32\"             )         else:             inputs[feature_name] = layers.Input(                 name=feature_name, shape=(), dtype=\"float32\"             )     return inputs"},{"path":"https://keras.posit.co/articles/examples/tabtransformer.html","id":"encode-features","dir":"Articles > Examples","previous_headings":"","what":"Encode features","title":"Structured data learning with TabTransformer","text":"encode_inputs method returns encoded_categorical_feature_list numerical_feature_list. encode categorical features embeddings, using fixed embedding_dims features, regardless vocabulary sizes. required Transformer model.","code":"def encode_inputs(inputs, embedding_dims):     encoded_categorical_feature_list = []     numerical_feature_list = []      for feature_name in inputs:         if feature_name in CATEGORICAL_FEATURE_NAMES:             vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]             # Create a lookup to convert a string values to an integer indices.             # Since we are not using a mask token, nor expecting any out of vocabulary             # (oov) token, we set mask_token to None and num_oov_indices to 0.              # Convert the string input values into integer indices.              # Create an embedding layer with the specified dimensions.             embedding = layers.Embedding(                 input_dim=len(vocabulary), output_dim=embedding_dims             )              # Convert the index values to embedding representations.             encoded_categorical_feature = embedding(inputs[feature_name])             encoded_categorical_feature_list.append(encoded_categorical_feature)          else:             # Use the numerical features as-is.             numerical_feature = ops.expand_dims(inputs[feature_name], -1)             numerical_feature_list.append(numerical_feature)      return encoded_categorical_feature_list, numerical_feature_list"},{"path":"https://keras.posit.co/articles/examples/tabtransformer.html","id":"implement-an-mlp-block","dir":"Articles > Examples","previous_headings":"","what":"Implement an MLP block","title":"Structured data learning with TabTransformer","text":"","code":"def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):     mlp_layers = []     for units in hidden_units:         mlp_layers.append(normalization_layer()),         mlp_layers.append(layers.Dense(units, activation=activation))         mlp_layers.append(layers.Dropout(dropout_rate))      return keras.Sequential(mlp_layers, name=name)"},{"path":"https://keras.posit.co/articles/examples/tabtransformer.html","id":"experiment-1-a-baseline-model","dir":"Articles > Examples","previous_headings":"","what":"Experiment 1: a baseline model","title":"Structured data learning with TabTransformer","text":"first experiment, create simple multi-layer feed-forward network. Let’s train evaluate baseline model: baseline linear model achieves ~81% validation accuracy.","code":"def create_baseline_model(     embedding_dims, num_mlp_blocks, mlp_hidden_units_factors, dropout_rate ):     # Create model inputs.     inputs = create_model_inputs()     # encode features.     encoded_categorical_feature_list, numerical_feature_list = encode_inputs(         inputs, embedding_dims     )     # Concatenate all features.     features = layers.concatenate(         encoded_categorical_feature_list + numerical_feature_list     )     # Compute Feedforward layer units.     feedforward_units = [features.shape[-1]]      # Create several feedforwad layers with skip connections.     for layer_idx in range(num_mlp_blocks):         features = create_mlp(             hidden_units=feedforward_units,             dropout_rate=dropout_rate,             activation=keras.activations.gelu,             normalization_layer=layers.LayerNormalization,             name=f\"feedforward_{layer_idx}\",         )(features)      # Compute MLP hidden_units.     mlp_hidden_units = [         factor * features.shape[-1] for factor in mlp_hidden_units_factors     ]     # Create final MLP.     features = create_mlp(         hidden_units=mlp_hidden_units,         dropout_rate=dropout_rate,         activation=keras.activations.selu,         normalization_layer=layers.BatchNormalization,         name=\"MLP\",     )(features)      # Add a sigmoid as a binary classifer.     outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)     model = keras.Model(inputs=inputs, outputs=outputs)     return model   baseline_model = create_baseline_model(     embedding_dims=EMBEDDING_DIMS,     num_mlp_blocks=NUM_MLP_BLOCKS,     mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,     dropout_rate=DROPOUT_RATE, )  print(\"Total model weights:\", baseline_model.count_params()) keras.utils.plot_model(baseline_model, show_shapes=True, rankdir=\"LR\") history = run_experiment(     model=baseline_model,     train_data_file=train_data_file,     test_data_file=test_data_file,     num_epochs=NUM_EPOCHS,     learning_rate=LEARNING_RATE,     weight_decay=WEIGHT_DECAY,     batch_size=BATCH_SIZE, )"},{"path":"https://keras.posit.co/articles/examples/tabtransformer.html","id":"experiment-2-tabtransformer","dir":"Articles > Examples","previous_headings":"","what":"Experiment 2: TabTransformer","title":"Structured data learning with TabTransformer","text":"TabTransformer architecture works follows: categorical features encoded embeddings, using embedding_dims. means value categorical feature embedding vector. column embedding, one embedding vector categorical feature, added (point-wise) categorical feature embedding. embedded categorical features fed stack Transformer blocks. Transformer block consists multi-head self-attention layer followed feed-forward layer. outputs final Transformer layer, contextual embeddings categorical features, concatenated input numerical features, fed final MLP block. softmax classifer applied end model. paper discusses addition concatenation column embedding Appendix: Experiment Model Details section. architecture TabTransformer shown , presented paper.  Let’s train evaluate TabTransformer model: TabTransformer model achieves ~85% validation accuracy. Note , default parameter configurations, baseline TabTransformer similar number trainable weights: 109,629 92,151 respectively, use training hyperparameters.","code":"def create_tabtransformer_classifier(     num_transformer_blocks,     num_heads,     embedding_dims,     mlp_hidden_units_factors,     dropout_rate,     use_column_embedding=False, ):     # Create model inputs.     inputs = create_model_inputs()     # encode features.     encoded_categorical_feature_list, numerical_feature_list = encode_inputs(         inputs, embedding_dims     )     # Stack categorical feature embeddings for the Tansformer.     encoded_categorical_features = ops.stack(encoded_categorical_feature_list, axis=1)     # Concatenate numerical features.     numerical_features = layers.concatenate(numerical_feature_list)      # Add column embedding to categorical feature embeddings.     if use_column_embedding:         num_columns = encoded_categorical_features.shape[1]         column_embedding = layers.Embedding(             input_dim=num_columns, output_dim=embedding_dims         )         column_indices = ops.arange(start=0, stop=num_columns, step=1)         encoded_categorical_features = encoded_categorical_features + column_embedding(             column_indices         )      # Create multiple layers of the Transformer block.     for block_idx in range(num_transformer_blocks):         # Create a multi-head attention layer.         attention_output = layers.MultiHeadAttention(             num_heads=num_heads,             key_dim=embedding_dims,             dropout=dropout_rate,             name=f\"multihead_attention_{block_idx}\",         )(encoded_categorical_features, encoded_categorical_features)         # Skip connection 1.         x = layers.Add(name=f\"skip_connection1_{block_idx}\")(             [attention_output, encoded_categorical_features]         )         # Layer normalization 1.         x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)         # Feedforward.         feedforward_output = create_mlp(             hidden_units=[embedding_dims],             dropout_rate=dropout_rate,             activation=keras.activations.gelu,             normalization_layer=partial(                 layers.LayerNormalization, epsilon=1e-6             ),  # using partial to provide keyword arguments before initialization             name=f\"feedforward_{block_idx}\",         )(x)         # Skip connection 2.         x = layers.Add(name=f\"skip_connection2_{block_idx}\")([feedforward_output, x])         # Layer normalization 2.         encoded_categorical_features = layers.LayerNormalization(             name=f\"layer_norm2_{block_idx}\", epsilon=1e-6         )(x)      # Flatten the \"contextualized\" embeddings of the categorical features.     categorical_features = layers.Flatten()(encoded_categorical_features)     # Apply layer normalization to the numerical features.     numerical_features = layers.LayerNormalization(epsilon=1e-6)(numerical_features)     # Prepare the input for the final MLP block.     features = layers.concatenate([categorical_features, numerical_features])      # Compute MLP hidden_units.     mlp_hidden_units = [         factor * features.shape[-1] for factor in mlp_hidden_units_factors     ]     # Create final MLP.     features = create_mlp(         hidden_units=mlp_hidden_units,         dropout_rate=dropout_rate,         activation=keras.activations.selu,         normalization_layer=layers.BatchNormalization,         name=\"MLP\",     )(features)      # Add a sigmoid as a binary classifer.     outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)     model = keras.Model(inputs=inputs, outputs=outputs)     return model   tabtransformer_model = create_tabtransformer_classifier(     num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,     num_heads=NUM_HEADS,     embedding_dims=EMBEDDING_DIMS,     mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,     dropout_rate=DROPOUT_RATE, )  print(\"Total model weights:\", tabtransformer_model.count_params()) keras.utils.plot_model(tabtransformer_model, show_shapes=True, rankdir=\"LR\") history = run_experiment(     model=tabtransformer_model,     train_data_file=train_data_file,     test_data_file=test_data_file,     num_epochs=NUM_EPOCHS,     learning_rate=LEARNING_RATE,     weight_decay=WEIGHT_DECAY,     batch_size=BATCH_SIZE, )"},{"path":"https://keras.posit.co/articles/examples/tabtransformer.html","id":"conclusion","dir":"Articles > Examples","previous_headings":"","what":"Conclusion","title":"Structured data learning with TabTransformer","text":"TabTransformer significantly outperforms MLP recent deep networks tabular data matching performance tree-based ensemble models. TabTransformer can learned end--end supervised training using labeled examples. scenario labeled examples large number unlabeled examples, pre-training procedure can employed train Transformer layers using unlabeled data. followed fine-tuning pre-trained Transformer layers along top MLP layer using labeled data. Example available HuggingFace.","code":""},{"path":"https://keras.posit.co/articles/examples/tensorflow_numpy_models.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Writing Keras Models With TensorFlow NumPy","text":"NumPy hugely successful Python linear algebra library. TensorFlow recently launched tf_numpy, TensorFlow implementation large subset NumPy API. Thanks tf_numpy, can write Keras layers models NumPy style! TensorFlow NumPy API full integration TensorFlow ecosystem. Features automatic differentiation, TensorBoard, Keras model callbacks, TPU distribution model exporting supported. Let’s run examples.","code":""},{"path":"https://keras.posit.co/articles/examples/tensorflow_numpy_models.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Writing Keras Models With TensorFlow NumPy","text":"TensorFlow NumPy requires TensorFlow 2.5 later. Optionally, can call tnp.experimental_enable_numpy_behavior() enable type promotion TensorFlow. allows TNP closely follow NumPy standard. test models use Boston housing prices regression dataset.","code":"import tensorflow as tf import tensorflow.experimental.numpy as tnp import keras as keras from keras import layers tnp.experimental_enable_numpy_behavior() (x_train, y_train), (x_test, y_test) = keras.datasets.boston_housing.load_data(     path=\"boston_housing.npz\", test_split=0.2, seed=113 ) input_dim = x_train.shape[1]   def evaluate_model(model: keras.Model):     [loss, percent_error] = model.evaluate(x_test, y_test, verbose=0)     print(\"Mean absolute percent error before training: \", percent_error)     model.fit(x_train, y_train, epochs=200, verbose=0)     [loss, percent_error] = model.evaluate(x_test, y_test, verbose=0)     print(\"Mean absolute percent error after training:\", percent_error)"},{"path":"https://keras.posit.co/articles/examples/tensorflow_numpy_models.html","id":"subclassing-keras-model-with-tnp","dir":"Articles > Examples","previous_headings":"","what":"Subclassing keras.Model with TNP","title":"Writing Keras Models With TensorFlow NumPy","text":"flexible way make use Keras API subclass keras.Model class. Subclassing Model class gives ability fully customize occurs training loop. makes subclassing Model popular option researchers. example, implement Model subclass performs regression boston housing dataset using TNP API. Note differentiation gradient descent handled automatically using TNP API alongside keras. First let’s define simple TNPForwardFeedRegressionNetwork class. Just like Keras model can utilize supported optimizer, loss, metrics callbacks want. Let’s see model performs! Great! model seems effectively learning solve problem hand. can also write custom loss function using TNP.","code":"class TNPForwardFeedRegressionNetwork(keras.Model):     def __init__(self, blocks=None, **kwargs):         super().__init__(**kwargs)         if not isinstance(blocks, list):             raise ValueError(f\"blocks must be a list, got blocks={blocks}\")         self.blocks = blocks         self.block_weights = None         self.biases = None      def build(self, input_shape):         current_shape = input_shape[1]         self.block_weights = []         self.biases = []         for i, block in enumerate(self.blocks):             self.block_weights.append(                 self.add_weight(                     shape=(current_shape, block),                     trainable=True,                     name=f\"block-{i}\",                     initializer=\"glorot_normal\",                 )             )             self.biases.append(                 self.add_weight(                     shape=(block,),                     trainable=True,                     name=f\"bias-{i}\",                     initializer=\"zeros\",                 )             )             current_shape = block          self.linear_layer = self.add_weight(             shape=(current_shape, 1),             name=\"linear_projector\",             trainable=True,             initializer=\"glorot_normal\",         )      def call(self, inputs):         activations = inputs         for w, b in zip(self.block_weights, self.biases):             activations = tnp.matmul(activations, w) + b             # ReLu activation function             activations = tnp.maximum(activations, 0.0)          return tnp.matmul(activations, self.linear_layer) model = TNPForwardFeedRegressionNetwork(blocks=[3, 3]) model.compile(     optimizer=\"adam\",     loss=\"mean_squared_error\",     metrics=[keras.metrics.MeanAbsolutePercentageError()], ) evaluate_model(model) def tnp_mse(y_true, y_pred):     return tnp.mean(tnp.square(y_true - y_pred), axis=0)   keras.backend.clear_session() model = TNPForwardFeedRegressionNetwork(blocks=[3, 3]) model.compile(     optimizer=\"adam\",     loss=tnp_mse,     metrics=[keras.metrics.MeanAbsolutePercentageError()], ) evaluate_model(model)"},{"path":"https://keras.posit.co/articles/examples/tensorflow_numpy_models.html","id":"implementing-a-keras-layer-based-model-with-tnp","dir":"Articles > Examples","previous_headings":"","what":"Implementing a Keras Layer Based Model with TNP","title":"Writing Keras Models With TensorFlow NumPy","text":"desired, TNP can also used layer oriented Keras code structure. Let’s implement model, using layered approach! can also seamlessly switch TNP layers native Keras layers! Keras API offers wide variety layers. ability use alongside NumPy code can huge time saver projects.","code":"def tnp_relu(x):     return tnp.maximum(x, 0)   class TNPDense(keras.layers.Layer):     def __init__(self, units, activation=None):         super().__init__()         self.units = units         self.activation = activation      def build(self, input_shape):         self.w = self.add_weight(             name=\"weights\",             shape=(input_shape[1], self.units),             initializer=\"random_normal\",             trainable=True,         )         self.bias = self.add_weight(             name=\"bias\",             shape=(self.units,),             initializer=\"zeros\",             trainable=True,         )      def call(self, inputs):         outputs = tnp.matmul(inputs, self.w) + self.bias         if self.activation:             return self.activation(outputs)         return outputs   def create_layered_tnp_model():     return keras.Sequential(         [             TNPDense(3, activation=tnp_relu),             TNPDense(3, activation=tnp_relu),             TNPDense(1),         ]     )   model = create_layered_tnp_model() model.compile(     optimizer=\"adam\",     loss=\"mean_squared_error\",     metrics=[keras.metrics.MeanAbsolutePercentageError()], ) model.build((None, input_dim)) model.summary()  evaluate_model(model) def create_mixed_model():     return keras.Sequential(         [             TNPDense(3, activation=tnp_relu),             # The model will have no issue using a normal Dense layer             layers.Dense(3, activation=\"relu\"),             # ... or switching back to tnp layers!             TNPDense(1),         ]     )   model = create_mixed_model() model.compile(     optimizer=\"adam\",     loss=\"mean_squared_error\",     metrics=[keras.metrics.MeanAbsolutePercentageError()], ) model.build((None, input_dim)) model.summary()  evaluate_model(model)"},{"path":"https://keras.posit.co/articles/examples/tensorflow_numpy_models.html","id":"distribution-strategy","dir":"Articles > Examples","previous_headings":"","what":"Distribution Strategy","title":"Writing Keras Models With TensorFlow NumPy","text":"TensorFlow NumPy Keras integrate TensorFlow Distribution Strategies. makes simple perform distributed training across multiple GPUs, even entire TPU Pod.","code":"gpus = tf.config.list_logical_devices(\"GPU\") if gpus:     strategy = tf.distribute.MirroredStrategy(gpus) else:     # We can fallback to a no-op CPU strategy.     strategy = tf.distribute.get_strategy() print(\"Running with strategy:\", str(strategy.__class__.__name__))  with strategy.scope():     model = create_layered_tnp_model()     model.compile(         optimizer=\"adam\",         loss=\"mean_squared_error\",         metrics=[keras.metrics.MeanAbsolutePercentageError()],     )     model.build((None, input_dim))     model.summary()     evaluate_model(model)"},{"path":"https://keras.posit.co/articles/examples/tensorflow_numpy_models.html","id":"tensorboard-integration","dir":"Articles > Examples","previous_headings":"","what":"TensorBoard Integration","title":"Writing Keras Models With TensorFlow NumPy","text":"One many benefits using Keras API ability monitor training TensorBoard. Using TensorFlow NumPy API alongside Keras allows easily leverage TensorBoard. load TensorBoard Jupyter notebook, can run following magic: load TensorBoard Jupyter notebook can use %tensorboard magic: TensorBoard monitor metrics examine training curve. TensorBoard also allows explore computation graph used models. ability introspect models can valuable debugging.","code":"keras.backend.clear_session() %load_ext tensorboard models = [     (         TNPForwardFeedRegressionNetwork(blocks=[3, 3]),         \"TNPForwardFeedRegressionNetwork\",     ),     (create_layered_tnp_model(), \"layered_tnp_model\"),     (create_mixed_model(), \"mixed_model\"), ] for model, model_name in models:     model.compile(         optimizer=\"adam\",         loss=\"mean_squared_error\",         metrics=[keras.metrics.MeanAbsolutePercentageError()],     )     model.fit(         x_train,         y_train,         epochs=200,         verbose=0,         callbacks=[keras.callbacks.TensorBoard(log_dir=f\"logs/{model_name}\")],     ) %tensorboard --logdir logs"},{"path":"https://keras.posit.co/articles/examples/tensorflow_numpy_models.html","id":"conclusion","dir":"Articles > Examples","previous_headings":"","what":"Conclusion","title":"Writing Keras Models With TensorFlow NumPy","text":"Porting existing NumPy code Keras models using tensorflow_numpy API easy! integrating Keras gain ability use existing Keras callbacks, metrics optimizers, easily distribute training use Tensorboard. Migrating complex model, ResNet, TensorFlow NumPy API great follow learning exercise. Several open source NumPy ResNet implementations available online.","code":""},{"path":"https://keras.posit.co/articles/examples/text_classification_from_scratch.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Text classification from scratch","text":"example shows text classification starting raw text (set text files disk). demonstrate workflow IMDB sentiment classification dataset (unprocessed version). use TextVectorization layer word splitting & indexing.","code":""},{"path":"https://keras.posit.co/articles/examples/text_classification_from_scratch.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Text classification from scratch","text":"","code":"library(keras3) library(tfdatasets) ## ## Attaching package: 'tfdatasets' ## The following object is masked from 'package:keras3': ## ##     shape library(tensorflow) ## ## Attaching package: 'tensorflow' ## The following objects are masked from 'package:keras3': ## ##     set_random_seed, shape"},{"path":"https://keras.posit.co/articles/examples/text_classification_from_scratch.html","id":"load-the-data-imdb-movie-review-sentiment-classification","dir":"Articles > Examples","previous_headings":"","what":"Load the data: IMDB movie review sentiment classification","title":"Text classification from scratch","text":"Let’s download data inspect structure. aclImdb folder contains train test subfolder: aclImdb/train/pos aclImdb/train/neg folders contain text files, represents one review (either positive negative): interested pos neg subfolders, let’s delete subfolder text files : can use utility text_dataset_from_directory generate labeled tf_dataset object set text files disk filed class-specific folders. Let’s use generate training, validation, test datasets. validation training datasets generated two subsets train directory, 20% samples going validation dataset 80% going training dataset. validation dataset addition test dataset useful tuning hyperparameters, model architecture, test dataset used. putting model real world however, retrained using available training data (without creating validation dataset), performance maximized. using validation_split & subset arguments, make sure either specify random seed, pass shuffle=False, validation & training splits get overlap. Let’s preview samples:","code":"if (!dir.exists(\"datasets/aclImdb\")) {   dir.create(\"datasets\")   download.file(     \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",     \"datasets/aclImdb_v1.tar.gz\"   )   untar(\"datasets/aclImdb_v1.tar.gz\", exdir = \"datasets\")   unlink(\"datasets/aclImdb/train/unsup\", recursive = TRUE) } if (!dir.exists(\"datasets\")) dir.create(\"datasets\") download.file(   \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",   \"datasets/aclImdb_v1.tar.gz\" ) untar(\"datasets/aclImdb_v1.tar.gz\", exdir = \"datasets\") head(list.files(\"datasets/aclImdb/test\")) ## [1] \"labeledBow.feat\" \"neg\"             \"pos\"             \"urls_neg.txt\" ## [5] \"urls_pos.txt\" head(list.files(\"datasets/aclImdb/train\")) ## [1] \"labeledBow.feat\" \"neg\"             \"pos\"             \"unsupBow.feat\" ## [5] \"urls_neg.txt\"    \"urls_pos.txt\" cat(readLines(\"datasets/aclImdb/train/pos/6248_7.txt\")) ## Warning in readLines(\"datasets/aclImdb/train/pos/6248_7.txt\"): incomplete final ## line found on 'datasets/aclImdb/train/pos/6248_7.txt' ## Being an Austrian myself this has been a straight knock in my face. Fortunately I don't live nowhere near the place where this movie takes place but unfortunately it portrays everything that the rest of Austria hates about Viennese people (or people close to that region). And it is very easy to read that this is exactly the directors intention: to let your head sink into your hands and say \"Oh my god, how can THAT be possible!\". No, not with me, the (in my opinion) totally exaggerated uncensored swinger club scene is not necessary, I watch porn, sure, but in this context I was rather disgusted than put in the right context.<br /><br />This movie tells a story about how misled people who suffer from lack of education or bad company try to survive and live in a world of redundancy and boring horizons. A girl who is treated like a whore by her super-jealous boyfriend (and still keeps coming back), a female teacher who discovers her masochism by putting the life of her super-cruel \"lover\" on the line, an old couple who has an almost mathematical daily cycle (she is the \"official replacement\" of his ex wife), a couple that has just divorced and has the ex husband suffer under the acts of his former wife obviously having a relationship with her masseuse and finally a crazy hitchhiker who asks her drivers the most unusual questions and stretches their nerves by just being super-annoying.<br /><br />After having seen it you feel almost nothing. You're not even shocked, sad, depressed or feel like doing anything... Maybe that's why I gave it 7 points, it made me react in a way I never reacted before. If that's good or bad is up to you! unlink(\"datasets/aclImdb/train/unsup\", recursive = TRUE) batch_size <- 32  raw_train_ds <- text_dataset_from_directory(     \"datasets/aclImdb/train\",     batch_size=batch_size,     validation_split=0.2,     subset=\"training\",     seed=1337, ) ## Found 25000 files belonging to 2 classes. ## Using 20000 files for training. raw_val_ds <- text_dataset_from_directory(     \"datasets/aclImdb/train\",     batch_size=batch_size,     validation_split=0.2,     subset=\"validation\",     seed=1337, ) ## Found 25000 files belonging to 2 classes. ## Using 5000 files for validation. raw_test_ds <- text_dataset_from_directory(     \"datasets/aclImdb/test\", batch_size=batch_size ) ## Found 25000 files belonging to 2 classes. length(raw_train_ds) ## [1] 625 length(raw_val_ds) ## [1] 157 length(raw_test_ds) ## [1] 782 # It's important to take a look at your raw data to ensure your normalization # and tokenization will work as expected. We can do that by taking a few # examples from the training set and looking at them. # This is one of the places where eager execution shines: # we can just evaluate these tensors using .numpy() # instead of needing to evaluate them in a Session/Graph context. batch <- iter_next(as_iterator(raw_train_ds)) str(batch) ## List of 2 ##  $ :<tf.Tensor: shape=(32), dtype=string, numpy=…> ##  $ :<tf.Tensor: shape=(32), dtype=int32, numpy=…> c(text_batch, label_batch) %<-% batch for (i in 1:3) {   print(text_batch[i])   print(label_batch[i]) } ## tf.Tensor(b\"I have read the novel Reaper of Ben Mezrich a fews years ago and last night I accidentally came to see this adaption.<br /><br />Although it's been years since I read the story the first time, the differences between the novel and the movie are humongous. Very important elements, which made the whole thing plausible are just written out or changed to bad.<br /><br />If the plot sounds interesting to you: go and get the novel. Its much, much, much better.<br /><br />Still 4 out of 10 since it was hard to stop watching because of the great basic plot by Ben Mezrich.\", shape=(), dtype=string) ## tf.Tensor(0, shape=(), dtype=int32) ## tf.Tensor(b'After seeing all the Jesse James, Quantrill, jayhawkers,etc films in the fifties, it is quite a thrill to see this film with a new perspective by director Ang Lee. The scene of the attack of Lawrence, Kansas is awesome. The romantic relationship between Jewel and Toby Mcguire turns out to be one of the best parts and Jonathan Rhys-Meyers is outstanding as the bad guy. All the time this film makes you feel the horror of war, and the desperate situation of the main characters who do not know if they are going to survive the next hours. Definitely worth seeing.', shape=(), dtype=string) ## tf.Tensor(1, shape=(), dtype=int32) ## tf.Tensor(b'AG was an excellent presentation of drama, suspense and thriller that is so rare to American TV. Sheriff Lucas gave many a viewer the willies. We rooted for Caleb as he strove to resist the overtures of Sheriff Lucas. We became engrossed and fearful upon learning of the unthinkable connection between these two characters. The manipulations which weekly gave cause to fear what Lucas would do next were truly surprising. This show lived up to the \"Gothic\" moniker in ways American entertainment has so seldom attempted, much less mastered. The suits definitely made a big mistake in not supporting this show. This show puts shame to the current glut of \"reality\" shows- which are so less than satisfying viewing.The call for a DVD box set is well based. This show is quality viewing for a discerning market hungry for quality viewing. A public that is tiring of over-saturation of mind-numbing reality fare will welcome this gem of real storytelling. Bring on the DVD box set!!', shape=(), dtype=string) ## tf.Tensor(1, shape=(), dtype=int32)"},{"path":"https://keras.posit.co/articles/examples/text_classification_from_scratch.html","id":"prepare-the-data","dir":"Articles > Examples","previous_headings":"","what":"Prepare the data","title":"Text classification from scratch","text":"particular, remove <br /> tags.","code":"# Having looked at our data above, we see that the raw text contains HTML break # tags of the form '<br />'. These tags will not be removed by the default # standardizer (which doesn't strip HTML). Because of this, we will need to # create a custom standardization function. custom_standardization_fn <- function(string_tensor) {   string_tensor %>%     tf$strings$lower() %>% # convert to all lowercase     tf$strings$regex_replace(\"<br />\", \" \") %>% # remove '<br />' HTML tag     tf$strings$regex_replace(\"[[:punct:]]\", \"\") # remove punctuation }   # Model constants. max_features <- 20000 embedding_dim <- 128 sequence_length <- 500  # Now that we have our custom standardization, we can instantiate our text # vectorization layer. We are using this layer to normalize, split, and map # strings to integers, so we set our 'output_mode' to 'int'. # Note that we're using the default split function, # and the custom standardization defined above. # We also set an explicit maximum sequence length, since the CNNs later in our # model won't support ragged sequences. vectorize_layer <- layer_text_vectorization(     standardize = custom_standardization_fn,     max_tokens = max_features,     output_mode = \"int\",     output_sequence_length = sequence_length, )  # Now that the vectorize_layer has been created, call `adapt` on a text-only # dataset to create the vocabulary. You don't have to batch, but for very large # datasets this means you're not keeping spare copies of the dataset in memory.  # Let's make a text-only dataset (no labels): text_ds <- raw_train_ds %>%   dataset_map(function(x, y) x) # Let's call `adapt`: vectorize_layer %>% adapt(text_ds)"},{"path":"https://keras.posit.co/articles/examples/text_classification_from_scratch.html","id":"two-options-to-vectorize-the-data","dir":"Articles > Examples","previous_headings":"","what":"Two options to vectorize the data","title":"Text classification from scratch","text":"2 ways can use text vectorization layer: Option 1: Make part model, obtain model processes raw strings, like : Option 2: Apply text dataset obtain dataset word indices, feed model expects integer sequences inputs. important difference two option 2 enables asynchronous CPU processing buffering data training GPU. ’re training model GPU, probably want go option get best performance. . export model production, ’d ship model accepts raw strings input, like code snippet option 1 . can done training. last section.","code":"x <- layer_input(shape = c(1L), dtype = \"string\", name = 'text') %>%   vectorize_layer() %>%   layer_embedding(max_features + 1, embedding_dim) vectorize_text <- function(text, label) {   text <- text %>%     tf$expand_dims(-1L) %>%     vectorize_layer()   list(text, label) }  # Vectorize the data. train_ds <- raw_train_ds %>% dataset_map(vectorize_text) val_ds   <- raw_val_ds   %>% dataset_map(vectorize_text) test_ds  <- raw_test_ds  %>% dataset_map(vectorize_text)  # Do async prefetching / buffering of the data for best performance on GPU. train_ds <- train_ds %>%   dataset_cache() %>%   dataset_prefetch(buffer_size = 10) val_ds <- val_ds %>%   dataset_cache() %>%   dataset_prefetch(buffer_size = 10) test_ds <- test_ds %>%   dataset_cache() %>%   dataset_prefetch(buffer_size = 10)"},{"path":"https://keras.posit.co/articles/examples/text_classification_from_scratch.html","id":"build-a-model","dir":"Articles > Examples","previous_headings":"","what":"Build a model","title":"Text classification from scratch","text":"choose simple 1D convnet starting Embedding layer.","code":"# A integer input for vocab indices. inputs <- layer_input(shape = list(NULL), dtype = \"int64\")  predictions <- inputs %>%   # Next, we add a layer to map those vocab indices into a space of dimensionality   # 'embedding_dim'.   layer_embedding(max_features, embedding_dim) %>%   layer_dropout(0.5) %>%   # Conv1D + global max pooling   layer_conv_1d(128, 7, padding = \"valid\", activation = \"relu\", strides = 3) %>%   layer_conv_1d(128, 7, padding = \"valid\", activation = \"relu\", strides = 3) %>%   layer_global_max_pooling_1d() %>%   # We add a vanilla hidden layer:   layer_dense(128, activation = \"relu\") %>%   layer_dropout(0.5) %>%   # We project onto a single unit output layer, and squash it with a sigmoid:   layer_dense(1, activation = \"sigmoid\", name = \"predictions\")  model <- keras_model(inputs, predictions)  summary(model) ## Model: \"functional_1\" ## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓ ## ┃ Layer (type)                    ┃ Output Shape              ┃    Param # ┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩ ## │ input_layer (InputLayer)        │ (None, None)              │          0 │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ embedding_1 (Embedding)         │ (None, None, 128)         │  2,560,000 │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ dropout_1 (Dropout)             │ (None, None, 128)         │          0 │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ conv1d_1 (Conv1D)               │ (None, None, 128)         │    114,816 │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ conv1d (Conv1D)                 │ (None, None, 128)         │    114,816 │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ global_max_pooling1d            │ (None, 128)               │          0 │ ## │ (GlobalMaxPooling1D)            │                           │            │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ dense (Dense)                   │ (None, 128)               │     16,512 │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ dropout (Dropout)               │ (None, 128)               │          0 │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ predictions (Dense)             │ (None, 1)                 │        129 │ ## └─────────────────────────────────┴───────────────────────────┴────────────┘ ##  Total params: 2,806,273 (10.71 MB) ##  Trainable params: 2,806,273 (10.71 MB) ##  Non-trainable params: 0 (0.00 B) # Compile the model with binary crossentropy loss and an adam optimizer. model %>% compile(   loss = \"binary_crossentropy\",   optimizer = \"adam\",   metrics = \"accuracy\" )"},{"path":"https://keras.posit.co/articles/examples/text_classification_from_scratch.html","id":"train-the-model","dir":"Articles > Examples","previous_headings":"","what":"Train the model","title":"Text classification from scratch","text":"","code":"epochs <- 3  # Fit the model using the train and test datasets. model %>% fit(train_ds, validation_data = val_ds, epochs = epochs) ## Epoch 1/3 ## 625/625 - 15s - 24ms/step - accuracy: 0.7258 - loss: 0.4922 - val_accuracy: 0.8668 - val_loss: 0.3134 ## Epoch 2/3 ## 625/625 - 14s - 22ms/step - accuracy: 0.9089 - loss: 0.2278 - val_accuracy: 0.8814 - val_loss: 0.3005 ## Epoch 3/3 ## 625/625 - 14s - 23ms/step - accuracy: 0.9550 - loss: 0.1226 - val_accuracy: 0.8774 - val_loss: 0.3681"},{"path":"https://keras.posit.co/articles/examples/text_classification_from_scratch.html","id":"evaluate-the-model-on-the-test-set","dir":"Articles > Examples","previous_headings":"","what":"Evaluate the model on the test set","title":"Text classification from scratch","text":"","code":"model %>% evaluate(test_ds) ## 782/782 - 6s - 7ms/step - accuracy: 0.8633 - loss: 0.3975 ##            loss compile_metrics ##       0.3974522       0.8632513"},{"path":"https://keras.posit.co/articles/examples/text_classification_from_scratch.html","id":"make-an-end-to-end-model","dir":"Articles > Examples","previous_headings":"","what":"Make an end-to-end model","title":"Text classification from scratch","text":"want obtain model capable processing raw strings, can simply create new model (using weights just trained):","code":"# A string input inputs <- layer_input(shape = c(1), dtype = \"string\") # Turn strings into vocab indices indices <- vectorize_layer(inputs) # Turn vocab indices into predictions outputs <- model(indices)  # Our end to end model end_to_end_model <- keras_model(inputs, outputs) end_to_end_model %>% compile(     loss = \"binary_crossentropy\",     optimizer = \"adam\",     metrics = c(\"accuracy\") )  # Test it with `raw_test_ds`, which yields raw strings end_to_end_model %>% evaluate(raw_test_ds) ## 782/782 - 6s - 8ms/step - accuracy: 0.8633 - loss: 0.3978 ##            loss compile_metrics ##       0.3977900       0.8632513"},{"path":"https://keras.posit.co/articles/examples/text_generation_gpt.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"GPT text generation from scratch with KerasNLP","text":"example, use KerasNLP build scaled Generative Pre-Trained (GPT) model. GPT Transformer-based model allows generate sophisticated text prompt. train model simplebooks-92 corpus, dataset made several novels. good dataset example since small vocabulary high word frequency, beneficial training model parameters. example combines concepts Text generation miniature GPT KerasNLP abstractions. demonstrate KerasNLP tokenization, layers metrics simplify training process, show generate output text using KerasNLP sampling utilities. Note: running example Colab, make sure enable GPU runtime faster training. example requires KerasNLP. can install via following command: pip install keras-nlp","code":""},{"path":"https://keras.posit.co/articles/examples/text_generation_gpt.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"GPT text generation from scratch with KerasNLP","text":"","code":"import os  os.environ[\"KERAS_BACKEND\"] = \"jax\"  import keras_nlp import keras as keras  import tensorflow.data as tf_data import tensorflow.strings as tf_strings"},{"path":"https://keras.posit.co/articles/examples/text_generation_gpt.html","id":"settings-hyperparameters","dir":"Articles > Examples","previous_headings":"","what":"Settings & hyperparameters","title":"GPT text generation from scratch with KerasNLP","text":"","code":"# Data BATCH_SIZE = 64 SEQ_LEN = 128 MIN_TRAINING_SEQ_LEN = 450  # Model EMBED_DIM = 256 FEED_FORWARD_DIM = 256 NUM_HEADS = 3 NUM_LAYERS = 2 VOCAB_SIZE = 5000  # Limits parameters in model.  # Training EPOCHS = 6  # Inference NUM_TOKENS_TO_GENERATE = 80"},{"path":"https://keras.posit.co/articles/examples/text_generation_gpt.html","id":"load-the-data","dir":"Articles > Examples","previous_headings":"","what":"Load the data","title":"GPT text generation from scratch with KerasNLP","text":"Now, let’s download dataset! SimpleBooks dataset consists 1,573 Gutenberg books, one smallest vocabulary size word-level tokens ratio. vocabulary size ~98k, third WikiText-103’s, around number tokens (~100M). makes easy fit small model.","code":"keras.utils.get_file(     origin=\"https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\",     extract=True, ) dir = os.path.expanduser(\"~/.keras/datasets/simplebooks/\")  # Load simplebooks-92 train set and filter out short lines. raw_train_ds = (     tf_data.TextLineDataset(dir + \"simplebooks-92-raw/train.txt\")     .filter(lambda x: tf_strings.length(x) > MIN_TRAINING_SEQ_LEN)     .batch(BATCH_SIZE)     .shuffle(buffer_size=256) )  # Load simplebooks-92 validation set and filter out short lines. raw_val_ds = (     tf_data.TextLineDataset(dir + \"simplebooks-92-raw/valid.txt\")     .filter(lambda x: tf_strings.length(x) > MIN_TRAINING_SEQ_LEN)     .batch(BATCH_SIZE) )"},{"path":"https://keras.posit.co/articles/examples/text_generation_gpt.html","id":"train-the-tokenizer","dir":"Articles > Examples","previous_headings":"","what":"Train the tokenizer","title":"GPT text generation from scratch with KerasNLP","text":"train tokenizer training dataset vocabulary size VOCAB_SIZE, tuned hyperparameter. want limit vocabulary much possible, see later large effect number model parameters. also don’t want include vocabulary terms, many --vocabulary (OOV) sub-words. addition, three tokens reserved vocabulary: \"[PAD]\" padding sequences SEQ_LEN. token index 0 reserved_tokens vocab, since WordPieceTokenizer (layers) consider 0/vocab[0] default padding. \"[UNK]\" OOV sub-words, match default oov_token=\"[UNK]\" WordPieceTokenizer. \"[BOS]\" stands beginning sentence, technically token representing beginning line training data.","code":"# Train tokenizer vocabulary vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(     raw_train_ds,     vocabulary_size=VOCAB_SIZE,     lowercase=True,     reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\"], )"},{"path":"https://keras.posit.co/articles/examples/text_generation_gpt.html","id":"load-tokenizer","dir":"Articles > Examples","previous_headings":"","what":"Load tokenizer","title":"GPT text generation from scratch with KerasNLP","text":"use vocabulary data initialize keras_nlp.tokenizers.WordPieceTokenizer. WordPieceTokenizer efficient implementation WordPiece algorithm used BERT models. strip, lower-case irreversible preprocessing operations.","code":"tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(     vocabulary=vocab,     sequence_length=SEQ_LEN,     lowercase=True, )"},{"path":"https://keras.posit.co/articles/examples/text_generation_gpt.html","id":"tokenize-data","dir":"Articles > Examples","previous_headings":"","what":"Tokenize data","title":"GPT text generation from scratch with KerasNLP","text":"preprocess dataset tokenizing splitting features labels.","code":"# packer adds a start token start_packer = keras_nlp.layers.StartEndPacker(     sequence_length=SEQ_LEN,     start_value=tokenizer.token_to_id(\"[BOS]\"), )   def preprocess(inputs):     outputs = tokenizer(inputs)     features = start_packer(outputs)     labels = outputs     return features, labels   # Tokenize and split into train and label sequences. train_ds = raw_train_ds.map(     preprocess, num_parallel_calls=tf_data.AUTOTUNE ).prefetch(tf_data.AUTOTUNE) val_ds = raw_val_ds.map(     preprocess, num_parallel_calls=tf_data.AUTOTUNE ).prefetch(tf_data.AUTOTUNE)"},{"path":"https://keras.posit.co/articles/examples/text_generation_gpt.html","id":"build-the-model","dir":"Articles > Examples","previous_headings":"","what":"Build the model","title":"GPT text generation from scratch with KerasNLP","text":"create scaled GPT model following layers: One keras_nlp.layers.TokenAndPositionEmbedding layer, combines embedding token position. Multiple keras_nlp.layers.TransformerDecoder layers, default causal masking. layer cross-attention run decoder sequence . One final dense linear layer Let’s take look model summary - large majority parameters token_and_position_embedding output dense layer! means vocabulary size (VOCAB_SIZE) large effect size model, number Transformer decoder layers (NUM_LAYERS) doesn’t affect much.","code":"inputs = keras.layers.Input(shape=(None,), dtype=\"int32\") # Embedding. embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(     vocabulary_size=VOCAB_SIZE,     sequence_length=SEQ_LEN,     embedding_dim=EMBED_DIM,     mask_zero=True, ) x = embedding_layer(inputs) # Transformer decoders. for _ in range(NUM_LAYERS):     decoder_layer = keras_nlp.layers.TransformerDecoder(         num_heads=NUM_HEADS,         intermediate_dim=FEED_FORWARD_DIM,     )     x = decoder_layer(x)  # Giving one argument only skips cross-attention. # Output. outputs = keras.layers.Dense(VOCAB_SIZE)(x) model = keras.Model(inputs=inputs, outputs=outputs) loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True) perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0) model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[perplexity]) model.summary()"},{"path":"https://keras.posit.co/articles/examples/text_generation_gpt.html","id":"training","dir":"Articles > Examples","previous_headings":"","what":"Training","title":"GPT text generation from scratch with KerasNLP","text":"Now model, let’s train fit() method.","code":"model.fit(train_ds, validation_data=val_ds, verbose=2, epochs=EPOCHS)"},{"path":"https://keras.posit.co/articles/examples/text_generation_gpt.html","id":"inference","dir":"Articles > Examples","previous_headings":"","what":"Inference","title":"GPT text generation from scratch with KerasNLP","text":"trained model, can test gauge performance. can seed model input sequence starting \"[BOS]\" token, progressively sample model making predictions subsequent token loop. start lets build prompt shape model inputs, containing \"[BOS]\" token. use keras_nlp.samplers module inference, requires callback function wrapping model just trained. wrapper calls model returns logit predictions current token generating. Note: two pieces advanced functionality available defining callback. first ability take cache states computed previous generation steps, can used speed generation. second ability output final dense “hidden state” generated token. used keras_nlp.samplers.ContrastiveSampler, avoids repetition penalizing repeated hidden states. optional, ignore now. Creating wrapper function complex part using functions. Now ’s done, let’s test different utilities, starting greedy search.","code":"# The \"packer\" layers adds the [BOS] token for us. prompt_tokens = start_packer(tokenizer([\"\"])) prompt_tokens def next(prompt, cache, index):     logits = model(prompt)[:, index - 1, :]     # Ignore hidden states for now; only needed for contrastive search.     hidden_states = None     return logits, hidden_states, cache"},{"path":"https://keras.posit.co/articles/examples/text_generation_gpt.html","id":"greedy-search","dir":"Articles > Examples","previous_headings":"Inference","what":"Greedy search","title":"GPT text generation from scratch with KerasNLP","text":"greedily pick probable token timestep. words, get argmax model output. can see, greedy search starts making sense, quickly starts repeating . common problem text generation can fixed probabilistic text generation utilities shown later !","code":"sampler = keras_nlp.samplers.GreedySampler() output_tokens = sampler(     next=next,     prompt=prompt_tokens,     index=1,  # Start sampling immediately after the [BOS] token. ) txt = tokenizer.detokenize(output_tokens) print(f\"Greedy search generated text: \\n{txt}\\n\")"},{"path":"https://keras.posit.co/articles/examples/text_generation_gpt.html","id":"beam-search","dir":"Articles > Examples","previous_headings":"Inference","what":"Beam search","title":"GPT text generation from scratch with KerasNLP","text":"high-level, beam search keeps track num_beams probable sequences timestep, predicts best next token sequences. improvement greedy search since stores possibilities. However, less efficient greedy search since compute store multiple potential sequences. Note: beam search num_beams=1 identical greedy search. Similar greedy search, beam search quickly starts repeating , since still deterministic method.","code":"sampler = keras_nlp.samplers.BeamSampler(num_beams=10) output_tokens = sampler(     next=next,     prompt=prompt_tokens,     index=1, ) txt = tokenizer.detokenize(output_tokens) print(f\"Beam search generated text: \\n{txt}\\n\")"},{"path":"https://keras.posit.co/articles/examples/text_generation_gpt.html","id":"random-search","dir":"Articles > Examples","previous_headings":"Inference","what":"Random search","title":"GPT text generation from scratch with KerasNLP","text":"Random search first probabilistic method. time step, samples next token using softmax probabilities provided model. Voilà, repetitions! However, random search, may see nonsensical words appearing since word vocabulary chance appearing sampling method. fixed next search utility, top-k search.","code":"sampler = keras_nlp.samplers.RandomSampler() output_tokens = sampler(     next=next,     prompt=prompt_tokens,     index=1, ) txt = tokenizer.detokenize(output_tokens) print(f\"Random search generated text: \\n{txt}\\n\")"},{"path":"https://keras.posit.co/articles/examples/text_generation_gpt.html","id":"top-k-search","dir":"Articles > Examples","previous_headings":"Inference","what":"Top-K search","title":"GPT text generation from scratch with KerasNLP","text":"Similar random search, sample next token probability distribution provided model. difference , select top k probable tokens, distribute probability mass sampling. way, won’t sampling low probability tokens, hence less nonsensical words!","code":"sampler = keras_nlp.samplers.TopKSampler(k=10) output_tokens = sampler(     next=next,     prompt=prompt_tokens,     index=1, ) txt = tokenizer.detokenize(output_tokens) print(f\"Top-K search generated text: \\n{txt}\\n\")"},{"path":"https://keras.posit.co/articles/examples/text_generation_gpt.html","id":"top-p-search","dir":"Articles > Examples","previous_headings":"Inference","what":"Top-P search","title":"GPT text generation from scratch with KerasNLP","text":"Even top-k search, something improve upon. top-k search, number k fixed, means selects number tokens probability distribution. Consider two scenarios, one probability mass concentrated 2 words another probability mass evenly concentrated across 10. choose k=2 k=10? one size fits k . top-p search comes ! Instead choosing k, choose probability p want probabilities top tokens sum . way, can dynamically adjust k based probability distribution. setting p=0.9, 90% probability mass concentrated top 2 tokens, can filter top 2 tokens sample . instead 90% distributed 10 tokens, similarly filter top 10 tokens sample .","code":"sampler = keras_nlp.samplers.TopPSampler(p=0.5) output_tokens = sampler(     next=next,     prompt=prompt_tokens,     index=1, ) txt = tokenizer.detokenize(output_tokens) print(f\"Top-P search generated text: \\n{txt}\\n\")"},{"path":"https://keras.posit.co/articles/examples/text_generation_gpt.html","id":"using-callbacks-for-text-generation","dir":"Articles > Examples","previous_headings":"Inference","what":"Using callbacks for text generation","title":"GPT text generation from scratch with KerasNLP","text":"can also wrap utilities callback, allows print prediction sequence every epoch model! example callback top-k search:","code":"class TopKTextGenerator(keras.callbacks.Callback):     \"\"\"A callback to generate text from a trained model using top-k.\"\"\"      def __init__(self, k):         self.sampler = keras_nlp.samplers.TopKSampler(k)      def on_epoch_end(self, epoch, logs=None):         output_tokens = self.sampler(             next=next,             prompt=prompt_tokens,             index=1,         )         txt = tokenizer.detokenize(output_tokens)         print(f\"Top-K search generated text: \\n{txt}\\n\")   text_generation_callback = TopKTextGenerator(k=10) # Dummy training loop to demonstrate callback. model.fit(     train_ds.take(1), verbose=2, epochs=2, callbacks=[text_generation_callback] )"},{"path":"https://keras.posit.co/articles/examples/text_generation_gpt.html","id":"conclusion","dir":"Articles > Examples","previous_headings":"","what":"Conclusion","title":"GPT text generation from scratch with KerasNLP","text":"recap, example, use KerasNLP layers train sub-word vocabulary, tokenize training data, create miniature GPT model, perform inference text generation library. like understand Transformers work, learn training full GPT model, readings: Attention Need Vaswani et al., 2017 GPT-3 Paper Brown et al., 2020","code":""},{"path":"https://keras.posit.co/articles/examples/text_generation_with_miniature_gpt.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Text generation with a miniature GPT","text":"example demonstrates implement autoregressive language model using miniature version GPT model. model consists single Transformer block causal masking attention layer. use text IMDB sentiment classification dataset training generate new movie reviews given prompt. using script dataset, make sure least 1 million words. example run tf-nightly>=2.3.0-dev20200531 TensorFlow 2.3 higher. References: GPT GPT-2 GPT-3","code":""},{"path":"https://keras.posit.co/articles/examples/text_generation_with_miniature_gpt.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Text generation with a miniature GPT","text":"","code":"# We set the backend to TensorFlow. The code works with # both `tensorflow` and `torch`. It does not work with JAX # due to the behavior of `jax.numpy.tile` in a jit scope # (used in `causal_attention_mask()`: `tile` in JAX does # not support a dynamic `reps` argument. # You can make the code work in JAX by wrapping the # inside of the `causal_attention_mask` function in # a decorator to prevent jit compilation: # `with jax.ensure_compile_time_eval():`. import os os.environ['KERAS_BACKEND'] = 'tensorflow'  import keras as keras from keras import layers from keras import ops from keras.layers import TextVectorization import numpy as np import os import string import random import tensorflow import tensorflow.data as tf_data import tensorflow.strings as tf_strings"},{"path":"https://keras.posit.co/articles/examples/text_generation_with_miniature_gpt.html","id":"implement-a-transformer-block-as-a-layer","dir":"Articles > Examples","previous_headings":"","what":"Implement a Transformer block as a layer","title":"Text generation with a miniature GPT","text":"","code":"def causal_attention_mask(batch_size, n_dest, n_src, dtype):     \"\"\"     Mask the upper half of the dot product matrix in self attention.     This prevents flow of information from future tokens to current token.     1's in the lower triangle, counting from the lower right corner.     \"\"\"     i = ops.arange(n_dest)[:, None]     j = ops.arange(n_src)     m = i >= j - n_src + n_dest     mask = ops.cast(m, dtype)     mask = ops.reshape(mask, [1, n_dest, n_src])     mult = ops.concatenate(         [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])], 0     )     return ops.tile(mask, mult)   class TransformerBlock(layers.Layer):     def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):         super().__init__()         self.att = layers.MultiHeadAttention(num_heads, embed_dim)         self.ffn = keras.Sequential(             [                 layers.Dense(ff_dim, activation=\"relu\"),                 layers.Dense(embed_dim),             ]         )         self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)         self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)         self.dropout1 = layers.Dropout(rate)         self.dropout2 = layers.Dropout(rate)      def call(self, inputs):         input_shape = ops.shape(inputs)         batch_size = input_shape[0]         seq_len = input_shape[1]         causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, \"bool\")         attention_output = self.att(inputs, inputs, attention_mask=causal_mask)         attention_output = self.dropout1(attention_output)         out1 = self.layernorm1(inputs + attention_output)         ffn_output = self.ffn(out1)         ffn_output = self.dropout2(ffn_output)         return self.layernorm2(out1 + ffn_output)"},{"path":"https://keras.posit.co/articles/examples/text_generation_with_miniature_gpt.html","id":"implement-an-embedding-layer","dir":"Articles > Examples","previous_headings":"","what":"Implement an embedding layer","title":"Text generation with a miniature GPT","text":"Create two separate embedding layers: one tokens one token index (positions).","code":"class TokenAndPositionEmbedding(layers.Layer):     def __init__(self, maxlen, vocab_size, embed_dim):         super().__init__()         self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)         self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)      def call(self, x):         maxlen = ops.shape(x)[-1]         positions = ops.arange(0, maxlen, 1)         positions = self.pos_emb(positions)         x = self.token_emb(x)         return x + positions"},{"path":"https://keras.posit.co/articles/examples/text_generation_with_miniature_gpt.html","id":"implement-the-miniature-gpt-model","dir":"Articles > Examples","previous_headings":"","what":"Implement the miniature GPT model","title":"Text generation with a miniature GPT","text":"","code":"vocab_size = 20000  # Only consider the top 20k words maxlen = 80  # Max sequence size embed_dim = 256  # Embedding size for each token num_heads = 2  # Number of attention heads feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer   def create_model():     inputs = layers.Input(shape=(maxlen,), dtype=\"int32\")     embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)     x = embedding_layer(inputs)     transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)     x = transformer_block(x)     outputs = layers.Dense(vocab_size)(x)     model = keras.Model(inputs=inputs, outputs=[outputs, x])     loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)     model.compile(         \"adam\",         loss=[loss_fn, None],     )  # No loss and optimization based on word embeddings from transformer block     return model"},{"path":"https://keras.posit.co/articles/examples/text_generation_with_miniature_gpt.html","id":"prepare-the-data-for-word-level-language-modelling","dir":"Articles > Examples","previous_headings":"","what":"Prepare the data for word-level language modelling","title":"Text generation with a miniature GPT","text":"Download IMDB dataset combine training validation sets text generation task. curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz tar -xf aclImdb_v1.tar.gz","code":"batch_size = 128  # The dataset contains each review in a separate text file # The text files are present in four different folders # Create a list all files filenames = [] directories = [     \"aclImdb/train/pos\",     \"aclImdb/train/neg\",     \"aclImdb/test/pos\",     \"aclImdb/test/neg\", ] for dir in directories:     for f in os.listdir(dir):         filenames.append(os.path.join(dir, f))  print(f\"{len(filenames)} files\")  # Create a dataset from text files random.shuffle(filenames) text_ds = tf_data.TextLineDataset(filenames) text_ds = text_ds.shuffle(buffer_size=256) text_ds = text_ds.batch(batch_size)   def custom_standardization(input_string):     \"\"\"Remove html line-break tags and handle punctuation\"\"\"     lowercased = tf_strings.lower(input_string)     stripped_html = tf_strings.regex_replace(lowercased, \"<br />\", \" \")     return tf_strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")   # Create a vectorization layer and adapt it to the text vectorize_layer = TextVectorization(     standardize=custom_standardization,     max_tokens=vocab_size - 1,     output_mode=\"int\",     output_sequence_length=maxlen + 1, ) vectorize_layer.adapt(text_ds) vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices   def prepare_lm_inputs_labels(text):     \"\"\"     Shift word sequences by 1 position so that the target for position (i) is     word at position (i+1). The model will use all words up till position (i)     to predict the next word.     \"\"\"     text = tensorflow.expand_dims(text, -1)     tokenized_sentences = vectorize_layer(text)     x = tokenized_sentences[:, :-1]     y = tokenized_sentences[:, 1:]     return x, y   text_ds = text_ds.map(prepare_lm_inputs_labels, num_parallel_calls=tf_data.AUTOTUNE) text_ds = text_ds.prefetch(tf_data.AUTOTUNE)"},{"path":"https://keras.posit.co/articles/examples/text_generation_with_miniature_gpt.html","id":"implement-a-keras-callback-for-generating-text","dir":"Articles > Examples","previous_headings":"","what":"Implement a Keras callback for generating text","title":"Text generation with a miniature GPT","text":"","code":"class TextGenerator(keras.callbacks.Callback):     \"\"\"A callback to generate text from a trained model.     1. Feed some starting prompt to the model     2. Predict probabilities for the next token     3. Sample the next token and add it to the next input      Arguments:         max_tokens: Integer, the number of tokens to be generated after prompt.         start_tokens: List of integers, the token indices for the starting prompt.         index_to_word: List of strings, obtained from the TextVectorization layer.         top_k: Integer, sample from the `top_k` token predictions.         print_every: Integer, print after this many epochs.     \"\"\"      def __init__(         self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1     ):         self.max_tokens = max_tokens         self.start_tokens = start_tokens         self.index_to_word = index_to_word         self.print_every = print_every         self.k = top_k      def sample_from(self, logits):         logits, indices = ops.top_k(logits, k=self.k, sorted=True)         indices = np.asarray(indices).astype(\"int32\")         preds = keras.activations.softmax(ops.expand_dims(logits, 0))[0]         preds = np.asarray(preds).astype(\"float32\")         return np.random.choice(indices, p=preds)      def detokenize(self, number):         return self.index_to_word[number]      def on_epoch_end(self, epoch, logs=None):         start_tokens = [_ for _ in self.start_tokens]         if (epoch + 1) % self.print_every != 0:             return         num_tokens_generated = 0         tokens_generated = []         while num_tokens_generated <= self.max_tokens:             pad_len = maxlen - len(start_tokens)             sample_index = len(start_tokens) - 1             if pad_len < 0:                 x = start_tokens[:maxlen]                 sample_index = maxlen - 1             elif pad_len > 0:                 x = start_tokens + [0] * pad_len             else:                 x = start_tokens             x = np.array([x])             y, _ = self.model.predict(x)             sample_token = self.sample_from(y[0][sample_index])             tokens_generated.append(sample_token)             start_tokens.append(sample_token)             num_tokens_generated = len(tokens_generated)         txt = \" \".join(             [self.detokenize(_) for _ in self.start_tokens + tokens_generated]         )         print(f\"generated text:\\n{txt}\\n\")   # Tokenize starting prompt word_to_index = {} for index, word in enumerate(vocab):     word_to_index[word] = index  start_prompt = \"this movie is\" start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()] num_tokens_generated = 40 text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)"},{"path":"https://keras.posit.co/articles/examples/text_generation_with_miniature_gpt.html","id":"train-the-model","dir":"Articles > Examples","previous_headings":"","what":"Train the model","title":"Text generation with a miniature GPT","text":"Note: code preferably run GPU.","code":"model = create_model()  model.fit(text_ds, verbose=2, epochs=25, callbacks=[text_gen_callback])"},{"path":"https://keras.posit.co/articles/examples/tf_serving.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Serving TensorFlow models with TFServing","text":"build machine learning model, next step serve . may want exposing model endpoint service. many frameworks can use , TensorFlow ecosystem solution called TensorFlow Serving. TensorFlow Serving GitHub page: TensorFlow Serving flexible, high-performance serving system machine learning models, designed production environments. deals inference aspect machine learning, taking models training managing lifetimes, providing clients versioned access via high-performance, reference-counted lookup table. TensorFlow Serving provides ---box integration TensorFlow models, can easily extended serve types models data.” note features: can serve multiple models, multiple versions model simultaneously exposes gRPC well HTTP inference endpoints allows deployment new model versions without changing client code supports canarying new versions /B testing experimental models adds minimal latency inference time due efficient, low-overhead implementation features scheduler groups individual inference requests batches joint execution GPU, configurable latency controls supports many servables: Tensorflow models, embeddings, vocabularies, feature transformations even non-Tensorflow-based machine learning models guide creates simple MobileNet model using Keras applications API, serves TensorFlow Serving. focus TensorFlow Serving, rather modeling training TensorFlow. Note: can find Colab notebook full working code link.","code":""},{"path":"https://keras.posit.co/articles/examples/tf_serving.html","id":"dependencies","dir":"Articles > Examples","previous_headings":"","what":"Dependencies","title":"Serving TensorFlow models with TFServing","text":"","code":"import os import json import shutil import requests import numpy as np import tensorflow as tf import keras as keras import matplotlib.pyplot as plt"},{"path":"https://keras.posit.co/articles/examples/tf_serving.html","id":"model","dir":"Articles > Examples","previous_headings":"","what":"Model","title":"Serving TensorFlow models with TFServing","text":"load pre-trained MobileNet Keras applications, model going serve.","code":"model = keras.applications.MobileNet()"},{"path":"https://keras.posit.co/articles/examples/tf_serving.html","id":"preprocessing","dir":"Articles > Examples","previous_headings":"","what":"Preprocessing","title":"Serving TensorFlow models with TFServing","text":"models don’t work box raw data, usually require kind preprocessing step adjust data model requirements, case MobileNet can see API page requires three basic steps input images: Pixel values normalized [0, 1] range Pixel values scaled [-1, 1] range Images shape (224, 224, 3) meaning (height, width, channels) can following function: note regarding preprocessing postprocessing using “keras.applications” API models available Keras applications API also provide preprocess_input decode_predictions functions, functions respectively responsible preprocessing postprocessing model, already contains logic necessary steps. recommended way process inputs outputs using Keras applications models. guide, using present advantages custom signatures clearer way.","code":"def preprocess(image, mean=0.5, std=0.5, shape=(224, 224)):     \"\"\"Scale, normalize and resizes images.\"\"\"     image = image / 255.0  # Scale     image = (image - mean) / std  # Normalize     image = tf.image.resize(image, shape)  # Resize     return image"},{"path":"https://keras.posit.co/articles/examples/tf_serving.html","id":"postprocessing","dir":"Articles > Examples","previous_headings":"","what":"Postprocessing","title":"Serving TensorFlow models with TFServing","text":"context models output values need extra processing meet user requirements, instance, user want know logits values class given image, user wants know class belongs. model, translates following transformations top model outputs: Get index class highest prediction Get name class index Now let’s download banana picture see everything comes together.","code":"# Download human-readable labels for ImageNet. imagenet_labels_url = \"https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt\" response = requests.get(imagenet_labels_url) # Skiping backgroung class labels = [x for x in response.text.split(\"\\n\") if x != \"\"][1:] # Convert the labels to the TensorFlow data format tf_labels = tf.constant(labels, dtype=tf.string)   def postprocess(prediction, labels=tf_labels):     \"\"\"Convert from probs to labels.\"\"\"     indices = tf.argmax(prediction, axis=-1)  # Index with highest prediction     label = tf.gather(params=labels, indices=indices)  # Class name     return label response = requests.get(\"https://i.imgur.com/j9xCCzn.jpeg\", stream=True)  with open(\"banana.jpeg\", \"wb\") as f:     shutil.copyfileobj(response.raw, f)  sample_img = plt.imread(\"./banana.jpeg\") print(f\"Original image shape: {sample_img.shape}\") print(f\"Original image pixel range: ({sample_img.min()}, {sample_img.max()})\") plt.imshow(sample_img) plt.show()  preprocess_img = preprocess(sample_img) print(f\"Preprocessed image shape: {preprocess_img.shape}\") print(     f\"Preprocessed image pixel range: ({preprocess_img.numpy().min()},\",     f\"{preprocess_img.numpy().max()})\", )  batched_img = tf.expand_dims(preprocess_img, axis=0) batched_img = tf.cast(batched_img, tf.float32) print(f\"Batched image shape: {batched_img.shape}\")  model_outputs = model(batched_img) print(f\"Model output shape: {model_outputs.shape}\") print(f\"Predicted class: {postprocess(model_outputs)}\")"},{"path":"https://keras.posit.co/articles/examples/tf_serving.html","id":"save-the-model","dir":"Articles > Examples","previous_headings":"","what":"Save the model","title":"Serving TensorFlow models with TFServing","text":"load trained model TensorFlow Serving, first need save SavedModel format. create protobuf file well-defined directory hierarchy, include version number. TensorFlow Serving allows us select version model, “servable” want use make inference requests. version exported different sub-directory given path.","code":"model_dir = \"./model\" model_version = 1 model_export_path = f\"{model_dir}/{model_version}\"  tf.saved_model.save(     model,     export_dir=model_export_path, )  print(f\"SavedModel files: {os.listdir(model_export_path)}\")"},{"path":"https://keras.posit.co/articles/examples/tf_serving.html","id":"examine-your-saved-model","dir":"Articles > Examples","previous_headings":"","what":"Examine your saved model","title":"Serving TensorFlow models with TFServing","text":"’ll use command line utility saved_model_cli look MetaGraphDefs (models) SignatureDefs (methods can call) SavedModel. See discussion SavedModel CLI TensorFlow Guide. saved_model_cli show –dir {model_export_path} –tag_set serve –signature_def serving_default tells us lot model! instance, can see inputs 4D shape (-1, 224, 224, 3) means (batch_size, height, width, channels), also note model requires specific image shape (224, 224, 3) means may need reshape images sending model. can also see model’s outputs (-1, 1000) shape logits 1000 classes ImageNet dataset. information doesn’t tell us everything, like fact pixel values needs [-1, 1] range, ’s great start.","code":""},{"path":[]},{"path":"https://keras.posit.co/articles/examples/tf_serving.html","id":"install-tfserving","dir":"Articles > Examples","previous_headings":"Serve your model with TensorFlow Serving","what":"Install TFServing","title":"Serving TensorFlow models with TFServing","text":"’re preparing install TensorFlow Serving using Aptitude since Colab runs Debian environment. ’ll add tensorflow-model-server package list packages Aptitude knows . Note ’re running root. Note: example running TensorFlow Serving natively, can also run Docker container, one easiest ways get started using TensorFlow Serving.","code":"wget 'http://storage.googleapis.com/tensorflow-serving-apt/pool/tensorflow-model-server-universal-2.8.0/t/tensorflow-model-server-universal/tensorflow-model-server-universal_2.8.0_all.deb' dpkg -i tensorflow-model-server-universal_2.8.0_all.deb"},{"path":"https://keras.posit.co/articles/examples/tf_serving.html","id":"start-running-tensorflow-serving","dir":"Articles > Examples","previous_headings":"Serve your model with TensorFlow Serving","what":"Start running TensorFlow Serving","title":"Serving TensorFlow models with TFServing","text":"start running TensorFlow Serving load model. loads, can start making inference requests using REST. important parameters: port: port ’ll use gRPC requests. rest_api_port: port ’ll use REST requests. model_name: ’ll use URL REST requests. can anything. model_base_path: path directory ’ve saved model. Check TFServing API reference get parameters available. outputs: outputs:","code":"# Environment variable with the path to the model os.environ[\"MODEL_DIR\"] = f\"{model_dir}\" %%bash --bg nohup tensorflow_model_server \\   --port=8500 \\   --rest_api_port=8501 \\   --model_name=model \\   --model_base_path=$MODEL_DIR >server.log 2>&1 # We can check the logs to the server to help troubleshooting !cat server.log [warn] getaddrinfo: address family for nodename not supported [evhttp_server.cc : 245] NET_LOG: Entering the event loop ... # Now we can check if tensorflow is in the active services !sudo lsof -i -P -n | grep LISTEN node         7 root   21u  IPv6  19100      0t0  TCP *:8080 (LISTEN) kernel_ma   34 root    7u  IPv4  18874      0t0  TCP 172.28.0.12:6000 (LISTEN) colab-fil   63 root    5u  IPv4  17975      0t0  TCP *:3453 (LISTEN) colab-fil   63 root    6u  IPv6  17976      0t0  TCP *:3453 (LISTEN) jupyter-n   81 root    6u  IPv4  18092      0t0  TCP 172.28.0.12:9000 (LISTEN) python3    101 root   23u  IPv4  18252      0t0  TCP 127.0.0.1:44915 (LISTEN) python3    132 root    3u  IPv4  20548      0t0  TCP 127.0.0.1:15264 (LISTEN) python3    132 root    4u  IPv4  20549      0t0  TCP 127.0.0.1:37977 (LISTEN) python3    132 root    9u  IPv4  20662      0t0  TCP 127.0.0.1:40689 (LISTEN) tensorflo 1101 root    5u  IPv4  35543      0t0  TCP *:8500 (LISTEN) tensorflo 1101 root   12u  IPv4  35548      0t0  TCP *:8501 (LISTEN)"},{"path":"https://keras.posit.co/articles/examples/tf_serving.html","id":"make-a-request-to-your-model-in-tensorflow-serving","dir":"Articles > Examples","previous_headings":"","what":"Make a request to your model in TensorFlow Serving","title":"Serving TensorFlow models with TFServing","text":"Now let’s create JSON object inference request, see well model classifies :","code":""},{"path":[]},{"path":"https://keras.posit.co/articles/examples/tf_serving.html","id":"newest-version-of-the-servable","dir":"Articles > Examples","previous_headings":"Make a request to your model in TensorFlow Serving > REST API","what":"Newest version of the servable","title":"Serving TensorFlow models with TFServing","text":"’ll send predict request POST server’s REST endpoint, pass example. ’ll ask server give us latest version servable specifying particular version. outputs:","code":"data = json.dumps(     {         \"signature_name\": \"serving_default\",         \"instances\": batched_img.numpy().tolist(),     } ) url = \"http://localhost:8501/v1/models/model:predict\"   def predict_rest(json_data, url):     json_response = requests.post(url, data=json_data)     response = json.loads(json_response.text)     rest_outputs = np.array(response[\"predictions\"])     return rest_outputs rest_outputs = predict_rest(data, url)  print(f\"REST output shape: {rest_outputs.shape}\") print(f\"Predicted class: {postprocess(rest_outputs)}\") REST output shape: (1, 1000) Predicted class: [b'banana']"},{"path":"https://keras.posit.co/articles/examples/tf_serving.html","id":"grpc-api","dir":"Articles > Examples","previous_headings":"Make a request to your model in TensorFlow Serving","what":"gRPC API","title":"Serving TensorFlow models with TFServing","text":"gRPC based Remote Procedure Call (RPC) model technology implementing RPC APIs uses HTTP 2.0 underlying transport protocol. gRPC usually preferred low-latency, highly scalable, distributed systems. wanna know REST vs gRPC tradeoffs, checkout article. outputs:","code":"import grpc  # Create a channel that will be connected to the gRPC port of the container channel = grpc.insecure_channel(\"localhost:8500\") pip install -q tensorflow_serving_api from tensorflow_serving.apis import predict_pb2, prediction_service_pb2_grpc  # Create a stub made for prediction # This stub will be used to send the gRPCrequest to the TF Server stub = prediction_service_pb2_grpc.PredictionServiceStub(channel) # Get the serving_input key loaded_model = tf.saved_model.load(model_export_path) input_name = list(     loaded_model.signatures[\"serving_default\"]     .structured_input_signature[1]     .keys() )[0] def predict_grpc(data, input_name, stub):     # Create a gRPC request made for prediction     request = predict_pb2.PredictRequest()      # Set the name of the model, for this use case it is \"model\"     request.model_spec.name = \"model\"      # Set which signature is used to format the gRPC query     # here the default one \"serving_default\"     request.model_spec.signature_name = \"serving_default\"      # Set the input as the data     # tf.make_tensor_proto turns a TensorFlow tensor into a Protobuf tensor     request.inputs[input_name].CopyFrom(tf.make_tensor_proto(data.numpy().tolist()))      # Send the gRPC request to the TF Server     result = stub.Predict(request)     return result  grpc_outputs = predict_grpc(batched_img, input_name, stub) grpc_outputs = np.array([grpc_outputs.outputs['predictions'].float_val])  print(f\"gRPC output shape: {grpc_outputs.shape}\") print(f\"Predicted class: {postprocess(grpc_outputs)}\") gRPC output shape: (1, 1000) Predicted class: [b'banana']"},{"path":"https://keras.posit.co/articles/examples/tf_serving.html","id":"custom-signature","dir":"Articles > Examples","previous_headings":"","what":"Custom signature","title":"Serving TensorFlow models with TFServing","text":"Note model always need preprocess postprocess samples get desired output, can get quite tricky maintaining serving several models developed large team, one might require different processing logic. TensorFlow allows us customize model graph embed processing logic, makes model serving much easier, different ways achieve , since going server models using TFServing can customize model graph straight serving signature. can just use following code export model already contains preprocessing postprocessing logic default signature, allows model make predictions raw data. saved_model_cli show –dir {model_sig_export_path} –tag_set serve –signature_def serving_default Note model different signature, input still 4D now (-1, -1, -1, 3) shape, means supports images height width size. output also different shape, longer outputs 1000-long logits. can test model’s prediction using specific signature using API :","code":"def export_model(model, labels):     @tf.function(         input_signature=[tf.TensorSpec([None, None, None, 3], tf.float32)]     )     def serving_fn(image):         processed_img = preprocess(image)         probs = model(processed_img)         label = postprocess(probs)         return {\"label\": label}      return serving_fn   model_sig_version = 2 model_sig_export_path = f\"{model_dir}/{model_sig_version}\"  tf.saved_model.save(     model,     export_dir=model_sig_export_path,     signatures={\"serving_default\": export_model(model, labels)}, ) batched_raw_img = tf.expand_dims(sample_img, axis=0) batched_raw_img = tf.cast(batched_raw_img, tf.float32)  loaded_model = tf.saved_model.load(model_sig_export_path) loaded_model.signatures[\"serving_default\"](**{\"image\": batched_raw_img})"},{"path":"https://keras.posit.co/articles/examples/tf_serving.html","id":"prediction-using-a-particular-version-of-the-servable","dir":"Articles > Examples","previous_headings":"","what":"Prediction using a particular version of the servable","title":"Serving TensorFlow models with TFServing","text":"Now let’s specify particular version servable. Note saved model custom signature used different folder, first model saved folder /1 (version 1), one custom signature folder /2 (version 2). default, TFServing serve models share base parent folder.","code":""},{"path":"https://keras.posit.co/articles/examples/tf_serving.html","id":"rest-api-1","dir":"Articles > Examples","previous_headings":"Prediction using a particular version of the servable","what":"REST API","title":"Serving TensorFlow models with TFServing","text":"outputs:","code":"data = json.dumps(     {         \"signature_name\": \"serving_default\",         \"instances\": batched_raw_img.numpy().tolist(),     } ) url_sig = \"http://localhost:8501/v1/models/model/versions/2:predict\" print(f\"REST output shape: {rest_outputs.shape}\") print(f\"Predicted class: {rest_outputs}\") REST output shape: (1,) Predicted class: ['banana']"},{"path":"https://keras.posit.co/articles/examples/tf_serving.html","id":"grpc-api-1","dir":"Articles > Examples","previous_headings":"Prediction using a particular version of the servable","what":"gRPC API","title":"Serving TensorFlow models with TFServing","text":"outputs:","code":"channel = grpc.insecure_channel(\"localhost:8500\") stub = prediction_service_pb2_grpc.PredictionServiceStub(channel) input_name = list(     loaded_model.signatures[\"serving_default\"]     .structured_input_signature[1]     .keys() )[0] grpc_outputs = predict_grpc(batched_raw_img, input_name, stub) grpc_outputs = np.array([grpc_outputs.outputs['label'].string_val])  print(f\"gRPC output shape: {grpc_outputs.shape}\") print(f\"Predicted class: {grpc_outputs}\") gRPC output shape: (1, 1) Predicted class: [[b'banana']]"},{"path":"https://keras.posit.co/articles/examples/tf_serving.html","id":"additional-resources","dir":"Articles > Examples","previous_headings":"","what":"Additional resources","title":"Serving TensorFlow models with TFServing","text":"Colab notebook full working code Train serve TensorFlow model TensorFlow Serving - TensorFlow blog TensorFlow Serving playlist - TensorFlow YouTube channel","code":""},{"path":"https://keras.posit.co/articles/examples/timeseries_anomaly_detection.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Timeseries anomaly detection using an Autoencoder","text":"script demonstrates can use reconstruction convolutional autoencoder model detect anomalies timeseries data.","code":""},{"path":"https://keras.posit.co/articles/examples/timeseries_anomaly_detection.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Timeseries anomaly detection using an Autoencoder","text":"","code":"import numpy as np import pandas as pd import keras as keras from keras import layers from matplotlib import pyplot as plt"},{"path":"https://keras.posit.co/articles/examples/timeseries_anomaly_detection.html","id":"load-the-data","dir":"Articles > Examples","previous_headings":"","what":"Load the data","title":"Timeseries anomaly detection using an Autoencoder","text":"use Numenta Anomaly Benchmark(NAB) dataset. provides artificial timeseries data containing labeled anomalous periods behavior. Data ordered, timestamped, single-valued metrics. use art_daily_small_noise.csv file training art_daily_jumpsup.csv file testing. simplicity dataset allows us demonstrate anomaly detection effectively.","code":"master_url_root = \"https://raw.githubusercontent.com/numenta/NAB/master/data/\"  df_small_noise_url_suffix = \"artificialNoAnomaly/art_daily_small_noise.csv\" df_small_noise_url = master_url_root + df_small_noise_url_suffix df_small_noise = pd.read_csv(     df_small_noise_url, parse_dates=True, index_col=\"timestamp\" )  df_daily_jumpsup_url_suffix = \"artificialWithAnomaly/art_daily_jumpsup.csv\" df_daily_jumpsup_url = master_url_root + df_daily_jumpsup_url_suffix df_daily_jumpsup = pd.read_csv(     df_daily_jumpsup_url, parse_dates=True, index_col=\"timestamp\" )"},{"path":"https://keras.posit.co/articles/examples/timeseries_anomaly_detection.html","id":"quick-look-at-the-data","dir":"Articles > Examples","previous_headings":"","what":"Quick look at the data","title":"Timeseries anomaly detection using an Autoencoder","text":"","code":"print(df_small_noise.head())  print(df_daily_jumpsup.head())"},{"path":[]},{"path":"https://keras.posit.co/articles/examples/timeseries_anomaly_detection.html","id":"timeseries-data-without-anomalies","dir":"Articles > Examples","previous_headings":"Visualize the data","what":"Timeseries data without anomalies","title":"Timeseries anomaly detection using an Autoencoder","text":"use following data training.","code":"fig, ax = plt.subplots() df_small_noise.plot(legend=False, ax=ax) plt.show()"},{"path":"https://keras.posit.co/articles/examples/timeseries_anomaly_detection.html","id":"timeseries-data-with-anomalies","dir":"Articles > Examples","previous_headings":"Visualize the data","what":"Timeseries data with anomalies","title":"Timeseries anomaly detection using an Autoencoder","text":"use following data testing see sudden jump data detected anomaly.","code":"fig, ax = plt.subplots() df_daily_jumpsup.plot(legend=False, ax=ax) plt.show()"},{"path":"https://keras.posit.co/articles/examples/timeseries_anomaly_detection.html","id":"prepare-training-data","dir":"Articles > Examples","previous_headings":"","what":"Prepare training data","title":"Timeseries anomaly detection using an Autoencoder","text":"Get data values training timeseries data file normalize value data. value every 5 mins 14 days. 24 * 60 / 5 = 288 timesteps per day 288 * 14 = 4032 data points total","code":"# Normalize and save the mean and std we get, # for normalizing test data. training_mean = df_small_noise.mean() training_std = df_small_noise.std() df_training_value = (df_small_noise - training_mean) / training_std print(\"Number of training samples:\", len(df_training_value))"},{"path":"https://keras.posit.co/articles/examples/timeseries_anomaly_detection.html","id":"create-sequences","dir":"Articles > Examples","previous_headings":"Prepare training data","what":"Create sequences","title":"Timeseries anomaly detection using an Autoencoder","text":"Create sequences combining TIME_STEPS contiguous data values training data.","code":"TIME_STEPS = 288   # Generated training sequences for use in the model. def create_sequences(values, time_steps=TIME_STEPS):     output = []     for i in range(len(values) - time_steps + 1):         output.append(values[i : (i + time_steps)])     return np.stack(output)   x_train = create_sequences(df_training_value.values) print(\"Training input shape: \", x_train.shape)"},{"path":"https://keras.posit.co/articles/examples/timeseries_anomaly_detection.html","id":"build-a-model","dir":"Articles > Examples","previous_headings":"","what":"Build a model","title":"Timeseries anomaly detection using an Autoencoder","text":"build convolutional reconstruction autoencoder model. model take input shape (batch_size, sequence_length, num_features) return output shape. case, sequence_length 288 num_features 1.","code":"model = keras.Sequential(     [         layers.Input(shape=(x_train.shape[1], x_train.shape[2])),         layers.Conv1D(             filters=32,             kernel_size=7,             padding=\"same\",             strides=2,             activation=\"relu\",         ),         layers.Dropout(rate=0.2),         layers.Conv1D(             filters=16,             kernel_size=7,             padding=\"same\",             strides=2,             activation=\"relu\",         ),         layers.Conv1DTranspose(             filters=16,             kernel_size=7,             padding=\"same\",             strides=2,             activation=\"relu\",         ),         layers.Dropout(rate=0.2),         layers.Conv1DTranspose(             filters=32,             kernel_size=7,             padding=\"same\",             strides=2,             activation=\"relu\",         ),         layers.Conv1DTranspose(filters=1, kernel_size=7, padding=\"same\"),     ] ) model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\") model.summary()"},{"path":"https://keras.posit.co/articles/examples/timeseries_anomaly_detection.html","id":"train-the-model","dir":"Articles > Examples","previous_headings":"","what":"Train the model","title":"Timeseries anomaly detection using an Autoencoder","text":"Please note using x_train input target since reconstruction model. Let’s plot training validation loss see training went.","code":"history = model.fit(     x_train,     x_train,     epochs=50,     batch_size=128,     validation_split=0.1,     callbacks=[         keras.callbacks.EarlyStopping(             monitor=\"val_loss\", patience=5, mode=\"min\"         )     ], ) plt.plot(history.history[\"loss\"], label=\"Training Loss\") plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\") plt.legend() plt.show()"},{"path":"https://keras.posit.co/articles/examples/timeseries_anomaly_detection.html","id":"detecting-anomalies","dir":"Articles > Examples","previous_headings":"","what":"Detecting anomalies","title":"Timeseries anomaly detection using an Autoencoder","text":"detect anomalies determining well model can reconstruct input data. Find MAE loss training samples. Find max MAE loss value. worst model performed trying reconstruct sample. make threshold anomaly detection. reconstruction loss sample greater threshold value can infer model seeing pattern isn’t familiar . label sample anomaly.","code":"# Get train MAE loss. x_train_pred = model.predict(x_train) train_mae_loss = np.mean(np.abs(x_train_pred - x_train), axis=1)  plt.hist(train_mae_loss, bins=50) plt.xlabel(\"Train MAE loss\") plt.ylabel(\"No of samples\") plt.show()  # Get reconstruction loss threshold. threshold = np.max(train_mae_loss) print(\"Reconstruction error threshold: \", threshold)"},{"path":"https://keras.posit.co/articles/examples/timeseries_anomaly_detection.html","id":"compare-recontruction","dir":"Articles > Examples","previous_headings":"Detecting anomalies","what":"Compare recontruction","title":"Timeseries anomaly detection using an Autoencoder","text":"Just fun, let’s see model recontructed first sample. 288 timesteps day 1 training dataset.","code":"# Checking how the first sequence is learnt plt.plot(x_train[0]) plt.plot(x_train_pred[0]) plt.show()"},{"path":"https://keras.posit.co/articles/examples/timeseries_anomaly_detection.html","id":"prepare-test-data","dir":"Articles > Examples","previous_headings":"Detecting anomalies","what":"Prepare test data","title":"Timeseries anomaly detection using an Autoencoder","text":"","code":"df_test_value = (df_daily_jumpsup - training_mean) / training_std fig, ax = plt.subplots() df_test_value.plot(legend=False, ax=ax) plt.show()  # Create sequences from test values. x_test = create_sequences(df_test_value.values) print(\"Test input shape: \", x_test.shape)  # Get test MAE loss. x_test_pred = model.predict(x_test) test_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1) test_mae_loss = test_mae_loss.reshape((-1))  plt.hist(test_mae_loss, bins=50) plt.xlabel(\"test MAE loss\") plt.ylabel(\"No of samples\") plt.show()  # Detect all the samples which are anomalies. anomalies = test_mae_loss > threshold print(\"Number of anomaly samples: \", np.sum(anomalies)) print(\"Indices of anomaly samples: \", np.where(anomalies))"},{"path":"https://keras.posit.co/articles/examples/timeseries_anomaly_detection.html","id":"plot-anomalies","dir":"Articles > Examples","previous_headings":"","what":"Plot anomalies","title":"Timeseries anomaly detection using an Autoencoder","text":"now know samples data anomalies. , find corresponding timestamps original test data. using following method : Let’s say time_steps = 3 10 training values. x_train look like : 0, 1, 2 1, 2, 3 2, 3, 4 3, 4, 5 4, 5, 6 5, 6, 7 6, 7, 8 7, 8, 9 except initial final time_steps-1 data values, appear time_steps number samples. , know samples [(3, 4, 5), (4, 5, 6), (5, 6, 7)] anomalies, can say data point 5 anomaly. Let’s overlay anomalies original test data plot.","code":"# data i is an anomaly if samples [(i - timesteps + 1) to (i)] are anomalies anomalous_data_indices = [] for data_idx in range(TIME_STEPS - 1, len(df_test_value) - TIME_STEPS + 1):     if np.all(anomalies[data_idx - TIME_STEPS + 1 : data_idx]):         anomalous_data_indices.append(data_idx) df_subset = df_daily_jumpsup.iloc[anomalous_data_indices] fig, ax = plt.subplots() df_daily_jumpsup.plot(legend=False, ax=ax) df_subset.plot(legend=False, ax=ax, color=\"r\") plt.show()"},{"path":"https://keras.posit.co/articles/examples/timeseries_classification_from_scratch.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Timeseries classification from scratch","text":"example shows timeseries classification scratch, starting raw CSV timeseries files disk. demonstrate workflow FordA dataset UCR/UEA archive.","code":""},{"path":"https://keras.posit.co/articles/examples/timeseries_classification_from_scratch.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Timeseries classification from scratch","text":"","code":"import keras as keras import numpy as np import matplotlib.pyplot as plt"},{"path":[]},{"path":"https://keras.posit.co/articles/examples/timeseries_classification_from_scratch.html","id":"dataset-description","dir":"Articles > Examples","previous_headings":"Load the data: the FordA dataset","what":"Dataset description","title":"Timeseries classification from scratch","text":"dataset using called FordA. data comes UCR archive. dataset contains 3601 training instances another 1320 testing instances. timeseries corresponds measurement engine noise captured motor sensor. task, goal automatically detect presence specific issue engine. problem balanced binary classification task. full description dataset can found .","code":""},{"path":"https://keras.posit.co/articles/examples/timeseries_classification_from_scratch.html","id":"read-the-tsv-data","dir":"Articles > Examples","previous_headings":"Load the data: the FordA dataset","what":"Read the TSV data","title":"Timeseries classification from scratch","text":"use FordA_TRAIN file training FordA_TEST file testing. simplicity dataset allows us demonstrate effectively use ConvNets timeseries classification. file, first column corresponds label.","code":"def readucr(filename):     data = np.loadtxt(filename, delimiter=\"\\t\")     y = data[:, 0]     x = data[:, 1:]     return x, y.astype(int)   root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"  x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\") x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")"},{"path":"https://keras.posit.co/articles/examples/timeseries_classification_from_scratch.html","id":"visualize-the-data","dir":"Articles > Examples","previous_headings":"","what":"Visualize the data","title":"Timeseries classification from scratch","text":"visualize one timeseries example class dataset.","code":"classes = np.unique(np.concatenate((y_train, y_test), axis=0))  plt.figure() for c in classes:     c_x_train = x_train[y_train == c]     plt.plot(c_x_train[0], label=\"class \" + str(c)) plt.legend(loc=\"best\") plt.show() plt.close()"},{"path":"https://keras.posit.co/articles/examples/timeseries_classification_from_scratch.html","id":"standardize-the-data","dir":"Articles > Examples","previous_headings":"","what":"Standardize the data","title":"Timeseries classification from scratch","text":"timeseries already single length (500). However, values usually various ranges. ideal neural network; general seek make input values normalized. specific dataset, data already z-normalized: timeseries sample mean equal zero standard deviation equal one. type normalization common timeseries classification problems, see Bagnall et al. (2016). Note timeseries data used univariate, meaning one channel per timeseries example. therefore transform timeseries multivariate one one channel using simple reshaping via numpy. allow us construct model easily applicable multivariate time series. Finally, order use sparse_categorical_crossentropy, count number classes beforehand. Now shuffle training set using validation_split option later training. Standardize labels positive integers. expected labels 0 1.","code":"x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1)) x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1)) num_classes = len(np.unique(y_train)) idx = np.random.permutation(len(x_train)) x_train = x_train[idx] y_train = y_train[idx] y_train[y_train == -1] = 0 y_test[y_test == -1] = 0"},{"path":"https://keras.posit.co/articles/examples/timeseries_classification_from_scratch.html","id":"build-a-model","dir":"Articles > Examples","previous_headings":"","what":"Build a model","title":"Timeseries classification from scratch","text":"build Fully Convolutional Neural Network originally proposed paper. implementation based TF 2 version provided . following hyperparameters (kernel_size, filters, usage BatchNorm) found via random search using KerasTuner.","code":"def make_model(input_shape):     input_layer = keras.layers.Input(input_shape)      conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(         input_layer     )     conv1 = keras.layers.BatchNormalization()(conv1)     conv1 = keras.layers.ReLU()(conv1)      conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(         conv1     )     conv2 = keras.layers.BatchNormalization()(conv2)     conv2 = keras.layers.ReLU()(conv2)      conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(         conv2     )     conv3 = keras.layers.BatchNormalization()(conv3)     conv3 = keras.layers.ReLU()(conv3)      gap = keras.layers.GlobalAveragePooling1D()(conv3)      output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(gap)      return keras.models.Model(inputs=input_layer, outputs=output_layer)   model = make_model(input_shape=x_train.shape[1:]) keras.utils.plot_model(model, show_shapes=True)"},{"path":"https://keras.posit.co/articles/examples/timeseries_classification_from_scratch.html","id":"train-the-model","dir":"Articles > Examples","previous_headings":"","what":"Train the model","title":"Timeseries classification from scratch","text":"","code":"epochs = 500 batch_size = 32  callbacks = [     keras.callbacks.ModelCheckpoint(         \"best_model.keras\", save_best_only=True, monitor=\"val_loss\"     ),     keras.callbacks.ReduceLROnPlateau(         monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001     ),     keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1), ] model.compile(     optimizer=\"adam\",     loss=\"sparse_categorical_crossentropy\",     metrics=[\"sparse_categorical_accuracy\"], ) history = model.fit(     x_train,     y_train,     batch_size=batch_size,     epochs=epochs,     callbacks=callbacks,     validation_split=0.2,     verbose=1, )"},{"path":"https://keras.posit.co/articles/examples/timeseries_classification_from_scratch.html","id":"evaluate-model-on-test-data","dir":"Articles > Examples","previous_headings":"","what":"Evaluate model on test data","title":"Timeseries classification from scratch","text":"","code":"model = keras.models.load_model(\"best_model.keras\")  test_loss, test_acc = model.evaluate(x_test, y_test)  print(\"Test accuracy\", test_acc) print(\"Test loss\", test_loss)"},{"path":"https://keras.posit.co/articles/examples/timeseries_classification_from_scratch.html","id":"plot-the-models-training-and-validation-loss","dir":"Articles > Examples","previous_headings":"","what":"Plot the model’s training and validation loss","title":"Timeseries classification from scratch","text":"can see training accuracy reaches almost 0.95 100 epochs. However, observing validation accuracy can see network still needs training reaches almost 0.97 validation training accuracy 200 epochs. Beyond 200th epoch, continue training, validation accuracy start decreasing training accuracy continue increasing: model starts overfitting.","code":"metric = \"sparse_categorical_accuracy\" plt.figure() plt.plot(history.history[metric]) plt.plot(history.history[\"val_\" + metric]) plt.title(\"model \" + metric) plt.ylabel(metric, fontsize=\"large\") plt.xlabel(\"epoch\", fontsize=\"large\") plt.legend([\"train\", \"val\"], loc=\"best\") plt.show() plt.close()"},{"path":"https://keras.posit.co/articles/examples/timeseries_classification_transformer.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Timeseries classification with a Transformer model","text":"Transformer architecture Attention Need, applied timeseries instead natural language. example requires TensorFlow 2.4 higher.","code":""},{"path":"https://keras.posit.co/articles/examples/timeseries_classification_transformer.html","id":"load-the-dataset","dir":"Articles > Examples","previous_headings":"","what":"Load the dataset","title":"Timeseries classification with a Transformer model","text":"going use dataset preprocessing TimeSeries Classification Scratch example.","code":"import numpy as np   def readucr(filename):     data = np.loadtxt(filename, delimiter=\"\\t\")     y = data[:, 0]     x = data[:, 1:]     return x, y.astype(int)   root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"  x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\") x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")  x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1)) x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))  n_classes = len(np.unique(y_train))  idx = np.random.permutation(len(x_train)) x_train = x_train[idx] y_train = y_train[idx]  y_train[y_train == -1] = 0 y_test[y_test == -1] = 0"},{"path":"https://keras.posit.co/articles/examples/timeseries_classification_transformer.html","id":"build-the-model","dir":"Articles > Examples","previous_headings":"","what":"Build the model","title":"Timeseries classification with a Transformer model","text":"model processes tensor shape (batch size, sequence length, features), sequence length number time steps features input timeseries. can replace classification RNN layers one: inputs fully compatible! include residual connections, layer normalization, dropout. resulting layer can stacked multiple times. projection layers implemented keras.layers.Conv1D. main part model now complete. can stack multiple transformer_encoder blocks can also proceed add final Multi-Layer Perceptron classification head. Apart stack Dense layers, need reduce output tensor TransformerEncoder part model vector features data point current batch. common way achieve use pooling layer. example, GlobalAveragePooling1D layer sufficient.","code":"import keras as keras from keras import layers def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):     # Attention and Normalization     x = layers.MultiHeadAttention(         key_dim=head_size, num_heads=num_heads, dropout=dropout     )(inputs, inputs)     x = layers.Dropout(dropout)(x)     x = layers.LayerNormalization(epsilon=1e-6)(x)     res = x + inputs      # Feed Forward Part     x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)     x = layers.Dropout(dropout)(x)     x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)     x = layers.LayerNormalization(epsilon=1e-6)(x)     return x + res def build_model(     input_shape,     head_size,     num_heads,     ff_dim,     num_transformer_blocks,     mlp_units,     dropout=0,     mlp_dropout=0, ):     inputs = keras.Input(shape=input_shape)     x = inputs     for _ in range(num_transformer_blocks):         x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)      x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)     for dim in mlp_units:         x = layers.Dense(dim, activation=\"relu\")(x)         x = layers.Dropout(mlp_dropout)(x)     outputs = layers.Dense(n_classes, activation=\"softmax\")(x)     return keras.Model(inputs, outputs)"},{"path":"https://keras.posit.co/articles/examples/timeseries_classification_transformer.html","id":"train-and-evaluate","dir":"Articles > Examples","previous_headings":"","what":"Train and evaluate","title":"Timeseries classification with a Transformer model","text":"","code":"input_shape = x_train.shape[1:]  model = build_model(     input_shape,     head_size=256,     num_heads=4,     ff_dim=4,     num_transformer_blocks=4,     mlp_units=[128],     mlp_dropout=0.4,     dropout=0.25, )  model.compile(     loss=\"sparse_categorical_crossentropy\",     optimizer=keras.optimizers.Adam(learning_rate=1e-4),     metrics=[\"sparse_categorical_accuracy\"], ) model.summary()  callbacks = [     keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True) ]  model.fit(     x_train,     y_train,     validation_split=0.2,     epochs=2,     batch_size=64,     callbacks=callbacks, )  model.evaluate(x_test, y_test, verbose=1)"},{"path":"https://keras.posit.co/articles/examples/timeseries_classification_transformer.html","id":"conclusions","dir":"Articles > Examples","previous_headings":"","what":"Conclusions","title":"Timeseries classification with a Transformer model","text":"110-120 epochs (25s Colab), model reaches training accuracy ~0.95, validation accuracy ~84 testing accuracy ~85, without hyperparameter tuning. model less 100k parameters. course, parameter count accuracy improved hyperparameter search sophisticated learning rate schedule, different optimizer. can use trained model hosted Hugging Face Hub try demo Hugging Face Spaces.","code":""},{"path":"https://keras.posit.co/articles/examples/timeseries_traffic_forecasting.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Traffic forecasting using graph neural networks and LSTM","text":"example shows forecast traffic condition using graph neural networks LSTM. Specifically, interested predicting future values traffic speed given history traffic speed collection road segments. One popular method solve problem consider road segment’s traffic speed separate timeseries predict future values timeseries using past values timeseries. method, however, ignores dependency traffic speed one road segment neighboring segments. able take account complex interactions traffic speed collection neighboring roads, can define traffic network graph consider traffic speed signal graph. example, implement neural network architecture can process timeseries data graph. first show process data create tf.data.Dataset forecasting graphs. , implement model uses graph convolution LSTM layers perform forecasting graph. data processing model architecture inspired paper: Yu, Bing, Haoteng Yin, Zhanxing Zhu. “Spatio-temporal graph convolutional networks: deep learning framework traffic forecasting.” Proceedings 27th International Joint Conference Artificial Intelligence, 2018. (github)","code":""},{"path":"https://keras.posit.co/articles/examples/timeseries_traffic_forecasting.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Traffic forecasting using graph neural networks and LSTM","text":"","code":"import pandas as pd import numpy as np import os import typing import matplotlib.pyplot as plt  import tensorflow as tf import keras as keras from keras import layers from keras.utils import timeseries_dataset_from_array"},{"path":[]},{"path":"https://keras.posit.co/articles/examples/timeseries_traffic_forecasting.html","id":"data-description","dir":"Articles > Examples","previous_headings":"Data preparation","what":"Data description","title":"Traffic forecasting using graph neural networks and LSTM","text":"use real-world traffic speed dataset named PeMSD7. use version collected prepared Yu et al., 2018 available . data consists two files: PeMSD7_W_228.csv contains distances 228 stations across District 7 California. PeMSD7_V_228.csv contains traffic speed collected stations weekdays May June 2012. full description dataset can found Yu et al., 2018.","code":""},{"path":"https://keras.posit.co/articles/examples/timeseries_traffic_forecasting.html","id":"loading-data","dir":"Articles > Examples","previous_headings":"Data preparation","what":"Loading data","title":"Traffic forecasting using graph neural networks and LSTM","text":"","code":"url = \"https://github.com/VeritasYin/STGCN_IJCAI-18/raw/master/dataset/PeMSD7_Full.zip\" data_dir = keras.utils.get_file(origin=url, extract=True, archive_format=\"zip\") data_dir = data_dir.rstrip(\"PeMSD7_Full.zip\")  route_distances = pd.read_csv(     os.path.join(data_dir, \"PeMSD7_W_228.csv\"), header=None ).to_numpy() speeds_array = pd.read_csv(     os.path.join(data_dir, \"PeMSD7_V_228.csv\"), header=None ).to_numpy()  print(f\"route_distances shape={route_distances.shape}\") print(f\"speeds_array shape={speeds_array.shape}\")"},{"path":"https://keras.posit.co/articles/examples/timeseries_traffic_forecasting.html","id":"sub-sampling-roads","dir":"Articles > Examples","previous_headings":"Data preparation","what":"sub-sampling roads","title":"Traffic forecasting using graph neural networks and LSTM","text":"reduce problem size make training faster, work sample 26 roads 228 roads dataset. chosen roads starting road 0, choosing 5 closest roads , continuing process get 25 roads. can choose subset roads. chose roads way increase likelihood roads correlated speed timeseries. sample_routes contains IDs selected roads.","code":"sample_routes = [     0,     1,     4,     7,     8,     11,     15,     108,     109,     114,     115,     118,     120,     123,     124,     126,     127,     129,     130,     132,     133,     136,     139,     144,     147,     216, ] route_distances = route_distances[np.ix_(sample_routes, sample_routes)] speeds_array = speeds_array[:, sample_routes]  print(f\"route_distances shape={route_distances.shape}\") print(f\"speeds_array shape={speeds_array.shape}\")"},{"path":"https://keras.posit.co/articles/examples/timeseries_traffic_forecasting.html","id":"data-visualization","dir":"Articles > Examples","previous_headings":"Data preparation","what":"Data visualization","title":"Traffic forecasting using graph neural networks and LSTM","text":"timeseries traffic speed two routes: can also visualize correlation timeseries different routes. Using correlation heatmap, can see example speed routes 4, 5, 6 highly correlated.","code":"plt.figure(figsize=(18, 6)) plt.plot(speeds_array[:, [0, -1]]) plt.legend([\"route_0\", \"route_25\"]) plt.figure(figsize=(8, 8)) plt.matshow(np.corrcoef(speeds_array.T), 0) plt.xlabel(\"road number\") plt.ylabel(\"road number\")"},{"path":"https://keras.posit.co/articles/examples/timeseries_traffic_forecasting.html","id":"splitting-and-normalizing-data","dir":"Articles > Examples","previous_headings":"Data preparation","what":"Splitting and normalizing data","title":"Traffic forecasting using graph neural networks and LSTM","text":"Next, split speed values array train/validation/test sets, normalize resulting arrays:","code":"train_size, val_size = 0.5, 0.2   def preprocess(data_array: np.ndarray, train_size: float, val_size: float):     \"\"\"Splits data into train/val/test sets and normalizes the data.      Args:         data_array: ndarray of shape `(num_time_steps, num_routes)`         train_size: A float value between 0.0 and 1.0 that represent the proportion of the dataset             to include in the train split.         val_size: A float value between 0.0 and 1.0 that represent the proportion of the dataset             to include in the validation split.      Returns:         `train_array`, `val_array`, `test_array`     \"\"\"      num_time_steps = data_array.shape[0]     num_train, num_val = (         int(num_time_steps * train_size),         int(num_time_steps * val_size),     )     train_array = data_array[:num_train]     mean, std = train_array.mean(axis=0), train_array.std(axis=0)      train_array = (train_array - mean) / std     val_array = (data_array[num_train : (num_train + num_val)] - mean) / std     test_array = (data_array[(num_train + num_val) :] - mean) / std      return train_array, val_array, test_array   train_array, val_array, test_array = preprocess(     speeds_array, train_size, val_size )  print(f\"train set size: {train_array.shape}\") print(f\"validation set size: {val_array.shape}\") print(f\"test set size: {test_array.shape}\")"},{"path":"https://keras.posit.co/articles/examples/timeseries_traffic_forecasting.html","id":"creating-tensorflow-datasets","dir":"Articles > Examples","previous_headings":"Data preparation","what":"Creating TensorFlow Datasets","title":"Traffic forecasting using graph neural networks and LSTM","text":"Next, create datasets forecasting problem. forecasting problem can stated follows: given sequence road speed values times t+1, t+2, ..., t+T, want predict future values roads speed times t+T+1, ..., t+T+h. time t inputs model T vectors size N targets h vectors size N, N number roads. use Keras built-function timeseries_dataset_from_array(). function create_tf_dataset() takes input numpy.ndarray returns tf.data.Dataset. function input_sequence_length=T forecast_horizon=h. argument multi_horizon needs explanation. Assume forecast_horizon=3. multi_horizon=True model make forecast time steps t+T+1, t+T+2, t+T+3. target shape (T,3). multi_horizon=False, model make forecast time step t+T+3 target shape (T, 1). may notice input tensor batch shape (batch_size, input_sequence_length, num_routes, 1). last dimension added make model general: time step, input features raod may contain multiple timeseries. instance, one might want use temperature timeseries addition historical values speed input features. example, however, last dimension input always 1. use last 12 values speed road forecast speed 3 time steps ahead:","code":"batch_size = 64 input_sequence_length = 12 forecast_horizon = 3 multi_horizon = False   def create_tf_dataset(     data_array: np.ndarray,     input_sequence_length: int,     forecast_horizon: int,     batch_size: int = 128,     shuffle=True,     multi_horizon=True, ):     \"\"\"Creates tensorflow dataset from numpy array.      This function creates a dataset where each element is a tuple `(inputs, targets)`.     `inputs` is a Tensor     of shape `(batch_size, input_sequence_length, num_routes, 1)` containing     the `input_sequence_length` past values of the timeseries for each node.     `targets` is a Tensor of shape `(batch_size, forecast_horizon, num_routes)`     containing the `forecast_horizon`     future values of the timeseries for each node.      Args:         data_array: np.ndarray with shape `(num_time_steps, num_routes)`         input_sequence_length: Length of the input sequence (in number of timesteps).         forecast_horizon: If `multi_horizon=True`, the target will be the values of the timeseries for 1 to             `forecast_horizon` timesteps ahead. If `multi_horizon=False`, the target will be the value of the             timeseries `forecast_horizon` steps ahead (only one value).         batch_size: Number of timeseries samples in each batch.         shuffle: Whether to shuffle output samples, or instead draw them in chronological order.         multi_horizon: See `forecast_horizon`.      Returns:         A tf.data.Dataset instance.     \"\"\"      inputs = timeseries_dataset_from_array(         np.expand_dims(data_array[:-forecast_horizon], axis=-1),         None,         sequence_length=input_sequence_length,         shuffle=False,         batch_size=batch_size,     )      target_offset = (         input_sequence_length         if multi_horizon         else input_sequence_length + forecast_horizon - 1     )     target_seq_length = forecast_horizon if multi_horizon else 1     targets = timeseries_dataset_from_array(         data_array[target_offset:],         None,         sequence_length=target_seq_length,         shuffle=False,         batch_size=batch_size,     )      dataset = tf.data.Dataset.zip((inputs, targets))     if shuffle:         dataset = dataset.shuffle(100)      return dataset.prefetch(16).cache()   train_dataset, val_dataset = (     create_tf_dataset(         data_array, input_sequence_length, forecast_horizon, batch_size     )     for data_array in [train_array, val_array] )  test_dataset = create_tf_dataset(     test_array,     input_sequence_length,     forecast_horizon,     batch_size=test_array.shape[0],     shuffle=False,     multi_horizon=multi_horizon, )"},{"path":"https://keras.posit.co/articles/examples/timeseries_traffic_forecasting.html","id":"roads-graph","dir":"Articles > Examples","previous_headings":"Data preparation","what":"Roads Graph","title":"Traffic forecasting using graph neural networks and LSTM","text":"mentioned , assume road segments form graph. PeMSD7 dataset road segments distance. next step create graph adjacency matrix distances. Following Yu et al., 2018 (equation 10) assume edge two nodes graph distance corresponding roads less threshold. function compute_adjacency_matrix() returns boolean adjacency matrix 1 means edge two nodes. use following class store information graph.","code":"def compute_adjacency_matrix(     route_distances: np.ndarray, sigma2: float, epsilon: float ):     \"\"\"Computes the adjacency matrix from distances matrix.      It uses the formula in https://github.com/VeritasYin/STGCN_IJCAI-18#data-preprocessing to     compute an adjacency matrix from the distance matrix.     The implementation follows that paper.      Args:         route_distances: np.ndarray of shape `(num_routes, num_routes)`. Entry `i,j` of this array is the             distance between roads `i,j`.         sigma2: Determines the width of the Gaussian kernel applied to the square distances matrix.         epsilon: A threshold specifying if there is an edge between two nodes. Specifically, `A[i,j]=1`             if `np.exp(-w2[i,j] / sigma2) >= epsilon` and `A[i,j]=0` otherwise, where `A` is the adjacency             matrix and `w2=route_distances * route_distances`      Returns:         A boolean graph adjacency matrix.     \"\"\"     num_routes = route_distances.shape[0]     route_distances = route_distances / 10000.0     w2, w_mask = (         route_distances * route_distances,         np.ones([num_routes, num_routes]) - np.identity(num_routes),     )     return (np.exp(-w2 / sigma2) >= epsilon) * w_mask class GraphInfo:     def __init__(self, edges: typing.Tuple[list, list], num_nodes: int):         self.edges = edges         self.num_nodes = num_nodes   sigma2 = 0.1 epsilon = 0.5 adjacency_matrix = compute_adjacency_matrix(route_distances, sigma2, epsilon) node_indices, neighbor_indices = np.where(adjacency_matrix == 1) graph = GraphInfo(     edges=(node_indices.tolist(), neighbor_indices.tolist()),     num_nodes=adjacency_matrix.shape[0], ) print(     f\"number of nodes: {graph.num_nodes}, number of edges: {len(graph.edges[0])}\" )"},{"path":"https://keras.posit.co/articles/examples/timeseries_traffic_forecasting.html","id":"network-architecture","dir":"Articles > Examples","previous_headings":"","what":"Network architecture","title":"Traffic forecasting using graph neural networks and LSTM","text":"model forecasting graph consists graph convolution layer LSTM layer.","code":""},{"path":"https://keras.posit.co/articles/examples/timeseries_traffic_forecasting.html","id":"graph-convolution-layer","dir":"Articles > Examples","previous_headings":"Network architecture","what":"Graph convolution layer","title":"Traffic forecasting using graph neural networks and LSTM","text":"implementation graph convolution layer resembles implementation Keras example. Note example input layer 2D tensor shape (num_nodes,in_feat) example input layer 4D tensor shape (num_nodes, batch_size, input_seq_length, in_feat). graph convolution layer performs following steps: nodes’ representations computed self.compute_nodes_representation() multiplying input features self.weight aggregated neighbors’ messages computed self.compute_aggregated_messages() first aggregating neighbors’ representations multiplying results self.weight final output layer computed self.update() combining nodes representations neighbors’ aggregated messages","code":"class GraphConv(layers.Layer):     def __init__(         self,         in_feat,         out_feat,         graph_info: GraphInfo,         aggregation_type=\"mean\",         combination_type=\"concat\",         activation: typing.Optional[str] = None,         **kwargs,     ):         super().__init__(**kwargs)         self.in_feat = in_feat         self.out_feat = out_feat         self.graph_info = graph_info         self.aggregation_type = aggregation_type         self.combination_type = combination_type         self.weight = tf.Variable(             initial_value=keras.initializers.GlorotUniform()(                 shape=(in_feat, out_feat), dtype=\"float32\"             ),             trainable=True,         )         self.activation = layers.Activation(activation)      def aggregate(self, neighbour_representations: tf.Tensor):         aggregation_func = {             \"sum\": tf.math.unsorted_segment_sum,             \"mean\": tf.math.unsorted_segment_mean,             \"max\": tf.math.unsorted_segment_max,         }.get(self.aggregation_type)          if aggregation_func:             return aggregation_func(                 neighbour_representations,                 self.graph_info.edges[0],                 num_segments=self.graph_info.num_nodes,             )          raise ValueError(f\"Invalid aggregation type: {self.aggregation_type}\")      def compute_nodes_representation(self, features: tf.Tensor):         \"\"\"Computes each node's representation.          The nodes' representations are obtained by multiplying the features tensor with         `self.weight`. Note that         `self.weight` has shape `(in_feat, out_feat)`.          Args:             features: Tensor of shape `(num_nodes, batch_size, input_seq_len, in_feat)`          Returns:             A tensor of shape `(num_nodes, batch_size, input_seq_len, out_feat)`         \"\"\"         return tf.matmul(features, self.weight)      def compute_aggregated_messages(self, features: tf.Tensor):         neighbour_representations = tf.gather(             features, self.graph_info.edges[1]         )         aggregated_messages = self.aggregate(neighbour_representations)         return tf.matmul(aggregated_messages, self.weight)      def update(         self, nodes_representation: tf.Tensor, aggregated_messages: tf.Tensor     ):         if self.combination_type == \"concat\":             h = tf.concat([nodes_representation, aggregated_messages], axis=-1)         elif self.combination_type == \"add\":             h = nodes_representation + aggregated_messages         else:             raise ValueError(                 f\"Invalid combination type: {self.combination_type}.\"             )          return self.activation(h)      def call(self, features: tf.Tensor):         \"\"\"Forward pass.          Args:             features: tensor of shape `(num_nodes, batch_size, input_seq_len, in_feat)`          Returns:             A tensor of shape `(num_nodes, batch_size, input_seq_len, out_feat)`         \"\"\"         nodes_representation = self.compute_nodes_representation(features)         aggregated_messages = self.compute_aggregated_messages(features)         return self.update(nodes_representation, aggregated_messages)"},{"path":"https://keras.posit.co/articles/examples/timeseries_traffic_forecasting.html","id":"lstm-plus-graph-convolution","dir":"Articles > Examples","previous_headings":"Network architecture","what":"LSTM plus graph convolution","title":"Traffic forecasting using graph neural networks and LSTM","text":"applying graph convolution layer input tensor, get another tensor containing nodes’ representations time (another 4D tensor). time step, node’s representation informed information neighbors. make good forecasts, however, need information neighbors also need process information time. end, can pass node’s tensor recurrent layer. LSTMGC layer , first applies graph convolution layer inputs passes results LSTM layer.","code":"class LSTMGC(layers.Layer):     \"\"\"Layer comprising a convolution layer followed by LSTM and dense layers.\"\"\"      def __init__(         self,         in_feat,         out_feat,         lstm_units: int,         input_seq_len: int,         output_seq_len: int,         graph_info: GraphInfo,         graph_conv_params: typing.Optional[dict] = None,         **kwargs,     ):         super().__init__(**kwargs)          # graph conv layer         if graph_conv_params is None:             graph_conv_params = {                 \"aggregation_type\": \"mean\",                 \"combination_type\": \"concat\",                 \"activation\": None,             }         self.graph_conv = GraphConv(             in_feat, out_feat, graph_info, **graph_conv_params         )          self.lstm = layers.LSTM(lstm_units, activation=\"relu\")         self.dense = layers.Dense(output_seq_len)          self.input_seq_len, self.output_seq_len = input_seq_len, output_seq_len      def call(self, inputs):         \"\"\"Forward pass.          Args:             inputs: tf.Tensor of shape `(batch_size, input_seq_len, num_nodes, in_feat)`          Returns:             A tensor of shape `(batch_size, output_seq_len, num_nodes)`.         \"\"\"          # convert shape to  (num_nodes, batch_size, input_seq_len, in_feat)         inputs = tf.transpose(inputs, [2, 0, 1, 3])          gcn_out = self.graph_conv(             inputs         )  # gcn_out has shape: (num_nodes, batch_size, input_seq_len, out_feat)         shape = tf.shape(gcn_out)         num_nodes, batch_size, input_seq_len, out_feat = (             shape[0],             shape[1],             shape[2],             shape[3],         )          # LSTM takes only 3D tensors as input         gcn_out = tf.reshape(             gcn_out, (batch_size * num_nodes, input_seq_len, out_feat)         )         lstm_out = self.lstm(             gcn_out         )  # lstm_out has shape: (batch_size * num_nodes, lstm_units)          dense_output = self.dense(             lstm_out         )  # dense_output has shape: (batch_size * num_nodes, output_seq_len)         output = tf.reshape(             dense_output, (num_nodes, batch_size, self.output_seq_len)         )         return tf.transpose(             output, [1, 2, 0]         )  # returns Tensor of shape (batch_size, output_seq_len, num_nodes)"},{"path":"https://keras.posit.co/articles/examples/timeseries_traffic_forecasting.html","id":"model-training","dir":"Articles > Examples","previous_headings":"","what":"Model training","title":"Traffic forecasting using graph neural networks and LSTM","text":"","code":"in_feat = 1 batch_size = 64 epochs = 20 input_sequence_length = 12 forecast_horizon = 3 multi_horizon = False out_feat = 10 lstm_units = 64 graph_conv_params = {     \"aggregation_type\": \"mean\",     \"combination_type\": \"concat\",     \"activation\": None, }  st_gcn = LSTMGC(     in_feat,     out_feat,     lstm_units,     input_sequence_length,     forecast_horizon,     graph,     graph_conv_params, ) inputs = layers.Input((input_sequence_length, graph.num_nodes, in_feat)) outputs = st_gcn(inputs)  model = keras.models.Model(inputs, outputs) model.compile(     optimizer=keras.optimizers.RMSprop(learning_rate=0.0002),     loss=keras.losses.MeanSquaredError(), ) model.fit(     train_dataset,     validation_data=val_dataset,     epochs=epochs,     callbacks=[keras.callbacks.EarlyStopping(patience=10)], )"},{"path":"https://keras.posit.co/articles/examples/timeseries_traffic_forecasting.html","id":"making-forecasts-on-test-set","dir":"Articles > Examples","previous_headings":"","what":"Making forecasts on test set","title":"Traffic forecasting using graph neural networks and LSTM","text":"Now can use trained model make forecasts test set. , compute MAE model compare MAE naive forecasts. naive forecasts last value speed node. course, goal demonstrate method, achieve best performance. improve model’s accuracy, model hyperparameters tuned carefully. addition, several LSTMGC blocks can stacked increase representation power model.","code":"x_test, y = next(test_dataset.as_numpy_iterator()) y_pred = model.predict(x_test) plt.figure(figsize=(18, 6)) plt.plot(y[:, 0, 0]) plt.plot(y_pred[:, 0, 0]) plt.legend([\"actual\", \"forecast\"])  naive_mse, model_mse = (     np.square(x_test[:, -1, :, 0] - y[:, 0, :]).mean(),     np.square(y_pred[:, 0, :] - y[:, 0, :]).mean(), ) print(f\"naive MAE: {naive_mse}, model MAE: {model_mse}\")"},{"path":"https://keras.posit.co/articles/examples/timeseries_weather_forecasting.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Timeseries forecasting for weather prediction","text":"example requires TensorFlow 2.3 higher.","code":"import pandas as pd import matplotlib.pyplot as plt import keras as keras"},{"path":"https://keras.posit.co/articles/examples/timeseries_weather_forecasting.html","id":"climate-data-time-series","dir":"Articles > Examples","previous_headings":"","what":"Climate Data Time-Series","title":"Timeseries forecasting for weather prediction","text":"using Jena Climate dataset recorded Max Planck Institute Biogeochemistry. dataset consists 14 features temperature, pressure, humidity etc, recorded per 10 minutes. Location: Weather Station, Max Planck Institute Biogeochemistry Jena, Germany Time-frame Considered: Jan 10, 2009 - December 31, 2016 table shows column names, value formats, description.","code":"from zipfile import ZipFile  uri = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip\" zip_path = keras.utils.get_file(     origin=uri, fname=\"jena_climate_2009_2016.csv.zip\" ) zip_file = ZipFile(zip_path) zip_file.extractall() csv_path = \"jena_climate_2009_2016.csv\"  df = pd.read_csv(csv_path)"},{"path":"https://keras.posit.co/articles/examples/timeseries_weather_forecasting.html","id":"raw-data-visualization","dir":"Articles > Examples","previous_headings":"","what":"Raw Data Visualization","title":"Timeseries forecasting for weather prediction","text":"give us sense data working , feature plotted . shows distinct pattern feature time period 2009 2016. also shows anomalies present, addressed normalization. heat map shows correlation different features.","code":"titles = [     \"Pressure\",     \"Temperature\",     \"Temperature in Kelvin\",     \"Temperature (dew point)\",     \"Relative Humidity\",     \"Saturation vapor pressure\",     \"Vapor pressure\",     \"Vapor pressure deficit\",     \"Specific humidity\",     \"Water vapor concentration\",     \"Airtight\",     \"Wind speed\",     \"Maximum wind speed\",     \"Wind direction in degrees\", ]  feature_keys = [     \"p (mbar)\",     \"T (degC)\",     \"Tpot (K)\",     \"Tdew (degC)\",     \"rh (%)\",     \"VPmax (mbar)\",     \"VPact (mbar)\",     \"VPdef (mbar)\",     \"sh (g/kg)\",     \"H2OC (mmol/mol)\",     \"rho (g/m**3)\",     \"wv (m/s)\",     \"max. wv (m/s)\",     \"wd (deg)\", ]  colors = [     \"blue\",     \"orange\",     \"green\",     \"red\",     \"purple\",     \"brown\",     \"pink\",     \"gray\",     \"olive\",     \"cyan\", ]  date_time_key = \"Date Time\"   def show_raw_visualization(data):     time_data = data[date_time_key]     fig, axes = plt.subplots(         nrows=7, ncols=2, figsize=(15, 20), dpi=80, facecolor=\"w\", edgecolor=\"k\"     )     for i in range(len(feature_keys)):         key = feature_keys[i]         c = colors[i % (len(colors))]         t_data = data[key]         t_data.index = time_data         t_data.head()         ax = t_data.plot(             ax=axes[i // 2, i % 2],             color=c,             title=\"{} - {}\".format(titles[i], key),             rot=25,         )         ax.legend([titles[i]])     plt.tight_layout()   show_raw_visualization(df) def show_heatmap(data):     plt.matshow(data.corr())     plt.xticks(range(data.shape[1]), data.columns, fontsize=14, rotation=90)     plt.gca().xaxis.tick_bottom()     plt.yticks(range(data.shape[1]), data.columns, fontsize=14)      cb = plt.colorbar()     cb.ax.tick_params(labelsize=14)     plt.title(\"Feature Correlation Heatmap\", fontsize=14)     plt.show()   show_heatmap(df)"},{"path":"https://keras.posit.co/articles/examples/timeseries_weather_forecasting.html","id":"data-preprocessing","dir":"Articles > Examples","previous_headings":"","what":"Data Preprocessing","title":"Timeseries forecasting for weather prediction","text":"picking ~300,000 data points training. Observation recorded every 10 mins, means 6 times per hour. resample one point per hour since drastic change expected within 60 minutes. via sampling_rate argument timeseries_dataset_from_array utility. tracking data past 720 timestamps (720/6=120 hours). data used predict temperature 72 timestamps (72/6=12 hours). Since every feature values varying ranges, normalization confine feature values range [0, 1] training neural network. subtracting mean dividing standard deviation feature. 71.5 % data used train model, .e. 300,693 rows. split_fraction can changed alter percentage. model shown data first 5 days .e. 720 observations, sampled every hour. temperature 72 (12 hours * 6 observation per hour) observation used label. can see correlation heatmap, parameters like Relative Humidity Specific Humidity redundant. Hence using select features, .","code":"split_fraction = 0.715 train_split = int(split_fraction * int(df.shape[0])) step = 6  past = 720 future = 72 learning_rate = 0.001 batch_size = 256 epochs = 10   def normalize(data, train_split):     data_mean = data[:train_split].mean(axis=0)     data_std = data[:train_split].std(axis=0)     return (data - data_mean) / data_std print(     \"The selected parameters are:\",     \", \".join([titles[i] for i in [0, 1, 5, 7, 8, 10, 11]]), ) selected_features = [feature_keys[i] for i in [0, 1, 5, 7, 8, 10, 11]] features = df[selected_features] features.index = df[date_time_key] features.head()  features = normalize(features.values, train_split) features = pd.DataFrame(features) features.head()  train_data = features.loc[0 : train_split - 1] val_data = features.loc[train_split:]"},{"path":"https://keras.posit.co/articles/examples/timeseries_weather_forecasting.html","id":"training-dataset","dir":"Articles > Examples","previous_headings":"","what":"Training dataset","title":"Timeseries forecasting for weather prediction","text":"training dataset labels starts 792nd observation (720 + 72). timeseries_dataset_from_array function takes sequence data-points gathered equal intervals, along time series parameters length sequences/windows, spacing two sequence/windows, etc., produce batches sub-timeseries inputs targets sampled main timeseries.","code":"start = past + future end = start + train_split  x_train = train_data[[i for i in range(7)]].values y_train = features.iloc[start:end][[1]]  sequence_length = int(past / step) dataset_train = keras.preprocessing.timeseries_dataset_from_array(     x_train,     y_train,     sequence_length=sequence_length,     sampling_rate=step,     batch_size=batch_size, )"},{"path":"https://keras.posit.co/articles/examples/timeseries_weather_forecasting.html","id":"validation-dataset","dir":"Articles > Examples","previous_headings":"Training dataset","what":"Validation dataset","title":"Timeseries forecasting for weather prediction","text":"validation dataset must contain last 792 rows won’t label data records, hence 792 must subtracted end data. validation label dataset must start 792 train_split, hence must add past + future (792) label_start.","code":"x_end = len(val_data) - past - future  label_start = train_split + past + future  x_val = val_data.iloc[:x_end][[i for i in range(7)]].values y_val = features.iloc[label_start:][[1]]  dataset_val = keras.preprocessing.timeseries_dataset_from_array(     x_val,     y_val,     sequence_length=sequence_length,     sampling_rate=step,     batch_size=batch_size, )   for batch in dataset_train.take(1):     inputs, targets = batch  print(\"Input shape:\", inputs.numpy().shape) print(\"Target shape:\", targets.numpy().shape)"},{"path":"https://keras.posit.co/articles/examples/timeseries_weather_forecasting.html","id":"training","dir":"Articles > Examples","previous_headings":"Training dataset","what":"Training","title":"Timeseries forecasting for weather prediction","text":"’ll use ModelCheckpoint callback regularly save checkpoints, EarlyStopping callback interrupt training validation loss longer improving. can visualize loss function . one point, loss stops decreasing.","code":"inputs = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2])) lstm_out = keras.layers.LSTM(32)(inputs) outputs = keras.layers.Dense(1)(lstm_out)  model = keras.Model(inputs=inputs, outputs=outputs) model.compile(     optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\" ) model.summary() path_checkpoint = \"model_checkpoint.weights.h5\" es_callback = keras.callbacks.EarlyStopping(     monitor=\"val_loss\", min_delta=0, patience=5 )  modelckpt_callback = keras.callbacks.ModelCheckpoint(     monitor=\"val_loss\",     filepath=path_checkpoint,     verbose=1,     save_weights_only=True,     save_best_only=True, )  history = model.fit(     dataset_train,     epochs=epochs,     validation_data=dataset_val,     callbacks=[es_callback, modelckpt_callback], ) def visualize_loss(history, title):     loss = history.history[\"loss\"]     val_loss = history.history[\"val_loss\"]     epochs = range(len(loss))     plt.figure()     plt.plot(epochs, loss, \"b\", label=\"Training loss\")     plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")     plt.title(title)     plt.xlabel(\"Epochs\")     plt.ylabel(\"Loss\")     plt.legend()     plt.show()   visualize_loss(history, \"Training and Validation Loss\")"},{"path":"https://keras.posit.co/articles/examples/timeseries_weather_forecasting.html","id":"prediction","dir":"Articles > Examples","previous_headings":"Training dataset","what":"Prediction","title":"Timeseries forecasting for weather prediction","text":"trained model now able make predictions 5 sets values validation set. Example available HuggingFace | Trained Model | Demo | | :–: | :–: | |  |  |","code":"def show_plot(plot_data, delta, title):     labels = [\"History\", \"True Future\", \"Model Prediction\"]     marker = [\".-\", \"rx\", \"go\"]     time_steps = list(range(-(plot_data[0].shape[0]), 0))     if delta:         future = delta     else:         future = 0      plt.title(title)     for i, val in enumerate(plot_data):         if i:             plt.plot(                 future, plot_data[i], marker[i], markersize=10, label=labels[i]             )         else:             plt.plot(                 time_steps, plot_data[i].flatten(), marker[i], label=labels[i]             )     plt.legend()     plt.xlim([time_steps[0], (future + 5) * 2])     plt.xlabel(\"Time-Step\")     plt.show()     return   for x, y in dataset_val.take(5):     show_plot(         [x[0][:, 1].numpy(), y[0].numpy(), model.predict(x)[0]],         12,         \"Single Step Prediction\",     )"},{"path":"https://keras.posit.co/articles/examples/token_learner.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Learning to tokenize in Vision Transformers","text":"Vision Transformers (Dosovitskiy et al.) many Transformer-based architectures (Liu et al., Yuan et al., etc.) shown strong results image recognition. following provides brief overview components involved Vision Transformer architecture image classification: Extract small patches input images. Linearly project patches. Add positional embeddings linear projections. Run projections series Transformer (Vaswani et al.) blocks. Finally, take representation final Transformer block add classification head. take 224x224 images extract 16x16 patches, get total 196 patches (also called tokens) image. number patches increases increase resolution, leading higher memory footprint. use reduced number patches without compromise performance? Ryoo et al. investigate question TokenLearner: Adaptive Space-Time Tokenization Videos. introduce novel module called TokenLearner can help reduce number patches used Vision Transformer (ViT) adaptive manner. TokenLearner incorporated standard ViT architecture, able reduce amount compute (measured FLOPS) used model. example, implement TokenLearner module demonstrate performance mini ViT CIFAR-10 dataset. make use following references: Official TokenLearner code Image Classification ViTs keras.io TokenLearner slides NeurIPS 2021","code":""},{"path":"https://keras.posit.co/articles/examples/token_learner.html","id":"imports","dir":"Articles > Examples","previous_headings":"","what":"Imports","title":"Learning to tokenize in Vision Transformers","text":"","code":"import keras as keras from keras import layers from keras import ops from tensorflow import data as tf_data   from datetime import datetime import matplotlib.pyplot as plt import numpy as np  import math"},{"path":"https://keras.posit.co/articles/examples/token_learner.html","id":"hyperparameters","dir":"Articles > Examples","previous_headings":"","what":"Hyperparameters","title":"Learning to tokenize in Vision Transformers","text":"Please feel free change hyperparameters check results. best way develop intuition architecture experiment .","code":"# DATA BATCH_SIZE = 256 AUTO = tf_data.AUTOTUNE INPUT_SHAPE = (32, 32, 3) NUM_CLASSES = 10  # OPTIMIZER LEARNING_RATE = 1e-3 WEIGHT_DECAY = 1e-4  # TRAINING EPOCHS = 20  # AUGMENTATION IMAGE_SIZE = 48  # We will resize input images to this size. PATCH_SIZE = 6  # Size of the patches to be extracted from the input images. NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2  # ViT ARCHITECTURE LAYER_NORM_EPS = 1e-6 PROJECTION_DIM = 128 NUM_HEADS = 4 NUM_LAYERS = 4 MLP_UNITS = [     PROJECTION_DIM * 2,     PROJECTION_DIM, ]  # TOKENLEARNER NUM_TOKENS = 4"},{"path":"https://keras.posit.co/articles/examples/token_learner.html","id":"load-and-prepare-the-cifar-10-dataset","dir":"Articles > Examples","previous_headings":"","what":"Load and prepare the CIFAR-10 dataset","title":"Learning to tokenize in Vision Transformers","text":"","code":"# Load the CIFAR-10 dataset. (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data() (x_train, y_train), (x_val, y_val) = (     (x_train[:40000], y_train[:40000]),     (x_train[40000:], y_train[40000:]), ) print(f\"Training samples: {len(x_train)}\") print(f\"Validation samples: {len(x_val)}\") print(f\"Testing samples: {len(x_test)}\")  # Convert to tf.data.Dataset objects. train_ds = tf_data.Dataset.from_tensor_slices((x_train, y_train)) train_ds = train_ds.shuffle(BATCH_SIZE * 100).batch(BATCH_SIZE).prefetch(AUTO)  val_ds = tf_data.Dataset.from_tensor_slices((x_val, y_val)) val_ds = val_ds.batch(BATCH_SIZE).prefetch(AUTO)  test_ds = tf_data.Dataset.from_tensor_slices((x_test, y_test)) test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTO)"},{"path":"https://keras.posit.co/articles/examples/token_learner.html","id":"data-augmentation","dir":"Articles > Examples","previous_headings":"","what":"Data augmentation","title":"Learning to tokenize in Vision Transformers","text":"augmentation pipeline consists : Rescaling Resizing Random cropping (fixed-sized random sized) Random horizontal flipping Note image data augmentation layers apply data transformations inference time. means layers called training=False behave differently. Refer documentation details.","code":"data_augmentation = keras.Sequential(     [         layers.Rescaling(1 / 255.0),         layers.Resizing(INPUT_SHAPE[0] + 20, INPUT_SHAPE[0] + 20),         layers.RandomCrop(IMAGE_SIZE, IMAGE_SIZE),         layers.RandomFlip(\"horizontal\"),     ],     name=\"data_augmentation\", )"},{"path":"https://keras.posit.co/articles/examples/token_learner.html","id":"positional-embedding-module","dir":"Articles > Examples","previous_headings":"","what":"Positional embedding module","title":"Learning to tokenize in Vision Transformers","text":"Transformer architecture consists multi-head self attention layers fully-connected feed forward networks (MLP) main components. components permutation invariant: ’re aware feature order. overcome problem inject tokens positional information. position_embedding function adds positional information linearly projected tokens.","code":"class PatchEncoder(layers.Layer):     def __init__(self, num_patches, projection_dim):         super().__init__()         self.num_patches = num_patches         self.position_embedding = layers.Embedding(             input_dim=num_patches, output_dim=projection_dim         )      def call(self, patch):         positions = ops.expand_dims(             ops.arange(start=0, stop=self.num_patches, step=1), axis=0         )         encoded = patch + self.position_embedding(positions)         return encoded      def get_config(self):         config = super().get_config()         config.update({\"num_patches\": self.num_patches})         return config"},{"path":"https://keras.posit.co/articles/examples/token_learner.html","id":"mlp-block-for-transformer","dir":"Articles > Examples","previous_headings":"","what":"MLP block for Transformer","title":"Learning to tokenize in Vision Transformers","text":"serves Fully Connected Feed Forward block Transformer.","code":"def mlp(x, dropout_rate, hidden_units):     # Iterate over the hidden units and     # add Dense => Dropout.     for units in hidden_units:         x = layers.Dense(units, activation=ops.gelu)(x)         x = layers.Dropout(dropout_rate)(x)     return x"},{"path":"https://keras.posit.co/articles/examples/token_learner.html","id":"tokenlearner-module","dir":"Articles > Examples","previous_headings":"","what":"TokenLearner module","title":"Learning to tokenize in Vision Transformers","text":"following figure presents pictorial overview module (source). TokenLearner module takes input image-shaped tensor. passes multiple single-channel convolutional layers extracting different spatial attention maps focusing different parts input. attention maps element-wise multiplied input result aggregated pooling. pooled output can trated summary input much lesser number patches (8, example) original one (196, example). Using multiple convolution layers helps expressivity. Imposing form spatial attention helps retain relevant information inputs. components crucial make TokenLearner work, especially significantly reducing number patches.","code":"def token_learner(inputs, number_of_tokens=NUM_TOKENS):     # Layer normalize the inputs.     x = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(         inputs     )  # (B, H, W, C)      # Applying Conv2D => Reshape => Permute     # The reshape and permute is done to help with the next steps of     # multiplication and Global Average Pooling.     attention_maps = keras.Sequential(         [             # 3 layers of conv with gelu activation as suggested             # in the paper.             layers.Conv2D(                 filters=number_of_tokens,                 kernel_size=(3, 3),                 activation=ops.gelu,                 padding=\"same\",                 use_bias=False,             ),             layers.Conv2D(                 filters=number_of_tokens,                 kernel_size=(3, 3),                 activation=ops.gelu,                 padding=\"same\",                 use_bias=False,             ),             layers.Conv2D(                 filters=number_of_tokens,                 kernel_size=(3, 3),                 activation=ops.gelu,                 padding=\"same\",                 use_bias=False,             ),             # This conv layer will generate the attention maps             layers.Conv2D(                 filters=number_of_tokens,                 kernel_size=(3, 3),                 activation=\"sigmoid\",  # Note sigmoid for [0, 1] output                 padding=\"same\",                 use_bias=False,             ),             # Reshape and Permute             layers.Reshape((-1, number_of_tokens)),  # (B, H*W, num_of_tokens)             layers.Permute((2, 1)),         ]     )(         x     )  # (B, num_of_tokens, H*W)      # Reshape the input to align it with the output of the conv block.     num_filters = inputs.shape[-1]     inputs = layers.Reshape((1, -1, num_filters))(         inputs     )  # inputs == (B, 1, H*W, C)      # Element-Wise multiplication of the attention maps and the inputs     attended_inputs = (         ops.expand_dims(attention_maps, axis=-1) * inputs     )  # (B, num_tokens, H*W, C)      # Global average pooling the element wise multiplication result.     outputs = ops.mean(attended_inputs, axis=2)  # (B, num_tokens, C)     return outputs"},{"path":"https://keras.posit.co/articles/examples/token_learner.html","id":"transformer-block","dir":"Articles > Examples","previous_headings":"","what":"Transformer block","title":"Learning to tokenize in Vision Transformers","text":"","code":"def transformer(encoded_patches):     # Layer normalization 1.     x1 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(encoded_patches)      # Multi Head Self Attention layer 1.     attention_output = layers.MultiHeadAttention(         num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1     )(x1, x1)      # Skip connection 1.     x2 = layers.Add()([attention_output, encoded_patches])      # Layer normalization 2.     x3 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(x2)      # MLP layer 1.     x4 = mlp(x3, hidden_units=MLP_UNITS, dropout_rate=0.1)      # Skip connection 2.     encoded_patches = layers.Add()([x4, x2])     return encoded_patches"},{"path":"https://keras.posit.co/articles/examples/token_learner.html","id":"vit-model-with-the-tokenlearner-module","dir":"Articles > Examples","previous_headings":"","what":"ViT model with the TokenLearner module","title":"Learning to tokenize in Vision Transformers","text":"shown TokenLearner paper, almost always advantageous include TokenLearner module middle network.","code":"def create_vit_classifier(     use_token_learner=True, token_learner_units=NUM_TOKENS ):     inputs = layers.Input(shape=INPUT_SHAPE)  # (B, H, W, C)      # Augment data.     augmented = data_augmentation(inputs)      # Create patches and project the pathces.     projected_patches = layers.Conv2D(         filters=PROJECTION_DIM,         kernel_size=(PATCH_SIZE, PATCH_SIZE),         strides=(PATCH_SIZE, PATCH_SIZE),         padding=\"VALID\",     )(augmented)     _, h, w, c = projected_patches.shape     projected_patches = layers.Reshape((h * w, c))(         projected_patches     )  # (B, number_patches, projection_dim)      # Add positional embeddings to the projected patches.     encoded_patches = PatchEncoder(         num_patches=NUM_PATCHES, projection_dim=PROJECTION_DIM     )(         projected_patches     )  # (B, number_patches, projection_dim)     encoded_patches = layers.Dropout(0.1)(encoded_patches)      # Iterate over the number of layers and stack up blocks of     # Transformer.     for i in range(NUM_LAYERS):         # Add a Transformer block.         encoded_patches = transformer(encoded_patches)          # Add TokenLearner layer in the middle of the         # architecture. The paper suggests that anywhere         # between 1/2 or 3/4 will work well.         if use_token_learner and i == NUM_LAYERS // 2:             _, hh, c = encoded_patches.shape             h = int(math.sqrt(hh))             encoded_patches = layers.Reshape((h, h, c))(                 encoded_patches             )  # (B, h, h, projection_dim)             encoded_patches = token_learner(                 encoded_patches, token_learner_units             )  # (B, num_tokens, c)      # Layer normalization and Global average pooling.     representation = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(         encoded_patches     )     representation = layers.GlobalAvgPool1D()(representation)      # Classify outputs.     outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(representation)      # Create the Keras model.     model = keras.Model(inputs=inputs, outputs=outputs)     return model"},{"path":"https://keras.posit.co/articles/examples/token_learner.html","id":"training-utility","dir":"Articles > Examples","previous_headings":"","what":"Training utility","title":"Learning to tokenize in Vision Transformers","text":"","code":"def run_experiment(model):     # Initialize the AdamW optimizer.     optimizer = keras.optimizers.AdamW(         learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY     )      # Compile the model with the optimizer, loss function     # and the metrics.     model.compile(         optimizer=optimizer,         loss=\"sparse_categorical_crossentropy\",         metrics=[             keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),             keras.metrics.SparseTopKCategoricalAccuracy(                 5, name=\"top-5-accuracy\"             ),         ],     )      # Define callbacks     checkpoint_filepath = \"/tmp/checkpoint.weights.h5\"     checkpoint_callback = keras.callbacks.ModelCheckpoint(         checkpoint_filepath,         monitor=\"val_accuracy\",         save_best_only=True,         save_weights_only=True,     )      # Train the model.     _ = model.fit(         train_ds,         epochs=EPOCHS,         validation_data=val_ds,         callbacks=[checkpoint_callback],     )      model.load_weights(checkpoint_filepath)     _, accuracy, top_5_accuracy = model.evaluate(test_ds)     print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")     print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")"},{"path":"https://keras.posit.co/articles/examples/token_learner.html","id":"train-and-evaluate-a-vit-with-tokenlearner","dir":"Articles > Examples","previous_headings":"","what":"Train and evaluate a ViT with TokenLearner","title":"Learning to tokenize in Vision Transformers","text":"","code":"vit_token_learner = create_vit_classifier() run_experiment(vit_token_learner)"},{"path":"https://keras.posit.co/articles/examples/token_learner.html","id":"results","dir":"Articles > Examples","previous_headings":"","what":"Results","title":"Learning to tokenize in Vision Transformers","text":"experimented without TokenLearner inside mini ViT implemented (hyperparameters presented example). results: TokenLearner able consistently outperform mini ViT without module. also interesting notice also able outperform deeper version mini ViT (8 layers). authors also report similar observations paper attribute adaptiveness TokenLearner. One also note FLOPs count decreases considerably addition TokenLearner module. less FLOPs count TokenLearner module able deliver better results. aligns well authors’ findings. Additionally, authors introduced newer version TokenLearner smaller training data regimes. Quoting authors: Instead using 4 conv. layers small channels implement spatial attention, version uses 2 grouped conv. layers channels. also uses softmax instead sigmoid. confirmed version works better limited training data, training ImageNet1K scratch. experimented module following table summarize results: Please note used hyperparameters presented example. implementation available notebook. acknowledge results new TokenLearner module slightly expected might mitigate hyperparameter tuning. Note: compute FLOPs models used utility repository.","code":""},{"path":"https://keras.posit.co/articles/examples/token_learner.html","id":"number-of-parameters","dir":"Articles > Examples","previous_headings":"","what":"Number of parameters","title":"Learning to tokenize in Vision Transformers","text":"may noticed adding TokenLearner module increases number parameters base network. mean less efficient shown Dehghani et al.. Similar findings reported Bello et al. well. TokenLearner module helps reducing FLOPS overall network thereby helping reduce memory footprint.","code":""},{"path":"https://keras.posit.co/articles/examples/token_learner.html","id":"final-notes","dir":"Articles > Examples","previous_headings":"","what":"Final notes","title":"Learning to tokenize in Vision Transformers","text":"TokenFuser: authors paper also propose another module named TokenFuser. module helps remapping representation TokenLearner output back original spatial resolution. reuse TokenLearner ViT architecture, TokenFuser must. first learn tokens TokenLearner, build representation tokens Transformer layer remap representation original spatial resolution, can consumed TokenLearner. Note can use TokenLearner module entire ViT model paired TokenFuser. Use modules video: authors also suggest TokenFuser goes really well Vision Transformers Videos (Arnab et al.). grateful JarvisLabs Google Developers Experts program helping GPU credits. Also, thankful Michael Ryoo (first author TokenLearner) fruitful discussions.","code":""},{"path":"https://keras.posit.co/articles/examples/torchvision_keras.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Fine-tuning a pre-trained TorchVision Model","text":"TorchVision library part PyTorch project consists popular datasets, model architectures, common image transformations computer vision. example demonstrates can perform transfer learning image classification using pre-trained backbone model TorchVision Imagenette dataset using KerasCore. also demonstrate compatibility KerasCore input system consisting Torch Datasets Dataloaders.","code":""},{"path":"https://keras.posit.co/articles/examples/torchvision_keras.html","id":"references","dir":"Articles > Examples","previous_headings":"Introduction","what":"References:","title":"Fine-tuning a pre-trained TorchVision Model","text":"Customizing happens fit() PyTorch PyTorch Datasets Dataloaders Transfer learning Computer Vision using PyTorch Fine-tuning TorchVision Model using Keras","code":""},{"path":"https://keras.posit.co/articles/examples/torchvision_keras.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Fine-tuning a pre-trained TorchVision Model","text":"","code":"import os  os.environ[\"KERAS_BACKEND\"] = \"torch\"  import numpy as np import matplotlib.pyplot as plt  import torch import torch.nn as nn import torch.nn.functional as F  import torchvision from torchvision import datasets, models, transforms  import keras as keras from keras.layers import TorchModuleWrapper"},{"path":"https://keras.posit.co/articles/examples/torchvision_keras.html","id":"define-the-hyperparameters","dir":"Articles > Examples","previous_headings":"","what":"Define the Hyperparameters","title":"Fine-tuning a pre-trained TorchVision Model","text":"","code":"batch_size = 32 image_size = 224 initial_learning_rate = 1e-3 num_epochs = 5"},{"path":"https://keras.posit.co/articles/examples/torchvision_keras.html","id":"creating-the-torch-datasets-and-dataloaders","dir":"Articles > Examples","previous_headings":"","what":"Creating the Torch Datasets and Dataloaders","title":"Fine-tuning a pre-trained TorchVision Model","text":"example, train image classification model Imagenette dataset. Imagenette subset 10 easily classified classes Imagenet (tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute). Next, define pre-processing augmentation transforms TorchVision train validation sets. Finally, use TorchVision torch.utils.data packages creating dataloaders trainig validation. Let us visualize samples training dataloader.","code":"# Fetch the imagenette dataset data_dir = keras.utils.get_file(     fname=\"imagenette2-320.tgz\",     origin=\"https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz\",     extract=True, ) data_dir = data_dir.replace(\".tgz\", \"\") data_transforms = {     \"train\": transforms.Compose(         [             transforms.RandomResizedCrop(image_size),             transforms.RandomHorizontalFlip(),             transforms.ToTensor(),             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),         ]     ),     \"val\": transforms.Compose(         [             transforms.Resize(256),             transforms.CenterCrop(image_size),             transforms.ToTensor(),             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),         ]     ), } # Define the train and validation datasets image_datasets = {     x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])     for x in [\"train\", \"val\"] }  # Define the torch dataloaders corresponding to the train and validation dataset dataloaders = {     x: torch.utils.data.DataLoader(         image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4     )     for x in [\"train\", \"val\"] } dataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"val\"]} class_names = image_datasets[\"train\"].classes  # Specify the global device device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") plt.figure(figsize=(10, 10)) sample_images, sample_labels = next(iter(dataloaders[\"train\"])) sample_images = sample_images.numpy() sample_labels = sample_labels.numpy() for idx in range(9):     ax = plt.subplot(3, 3, idx + 1)     image = sample_images[idx].transpose((1, 2, 0))     mean = np.array([0.485, 0.456, 0.406])     std = np.array([0.229, 0.224, 0.225])     image = std * image + mean     image = np.clip(image, 0, 1)     plt.imshow(image)     plt.title(\"Ground Truth Label: \" + class_names[int(sample_labels[idx])])     plt.axis(\"off\")"},{"path":"https://keras.posit.co/articles/examples/torchvision_keras.html","id":"the-image-classification-model","dir":"Articles > Examples","previous_headings":"","what":"The Image Classification Model","title":"Fine-tuning a pre-trained TorchVision Model","text":"typically define model PyTorch using torch.nn.Modules act building blocks stateful computation. Let us define ResNet18 model TorchVision package torch.nn.Module pre-trained Imagenet1K dataset. Even though Keras supports PyTorch backend, mean can nest torch modules inside keras.Model, trainable variables inside Keras Model tracked exclusively via Keras Layers. KerasCore provides us feature called TorchModuleWrapper enables us exactly . TorchModuleWrapper Keras Layer accepts torch module tracks trainable variables, essentially converting torch module Keras Layer. enables us put torch modules inside Keras Model train single model.fit()! Now, build Keras functional model backbone layer.","code":"# Define the pre-trained resnet18 module from TorchVision resnet_18 = models.resnet18(weights=\"IMAGENET1K_V1\")  # We set the classification head of the pre-trained ResNet18 # module to an identity module resnet_18.fc = nn.Identity() # We set the trainable ResNet18 backbone to be a Keras Layer # using `TorchModuleWrapper` backbone = TorchModuleWrapper(resnet_18)  # We set this to `False` if you want to freeze the backbone backbone.trainable = True inputs = keras.Input(shape=(3, image_size, image_size)) x = backbone(inputs) x = keras.layers.Dropout(0.5)(x) x = keras.layers.Dense(len(class_names))(x) outputs = keras.activations.softmax(x, axis=1) model = keras.Model(inputs, outputs, name=\"ResNet18_Classifier\")  model.summary()  # Create exponential decay learning rate scheduler decay_steps = num_epochs * len(dataloaders[\"train\"]) // batch_size lr_scheduler = keras.optimizers.schedules.ExponentialDecay(     initial_learning_rate=initial_learning_rate,     decay_steps=decay_steps,     decay_rate=0.1, )  # Compile the model model.compile(     loss=\"sparse_categorical_crossentropy\",     optimizer=keras.optimizers.Adam(lr_scheduler),     metrics=[\"accuracy\"], )  # Define the backend-agnostic WandB callbacks for KerasCore callbacks = [     # Save best model checkpoints     keras.callbacks.ModelCheckpoint(         filepath=\"model.weights.h5\",         monitor=\"val_loss\",         save_best_only=True,         save_weights_only=True,     ) ]  # Train the model by calling model.fit history = model.fit(     dataloaders[\"train\"],     validation_data=dataloaders[\"val\"],     epochs=num_epochs,     callbacks=callbacks, )   def plot_history(item):     plt.plot(history.history[item], label=item)     plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)     plt.xlabel(\"Epochs\")     plt.ylabel(item)     plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)     plt.legend()     plt.grid()     plt.show()   plot_history(\"loss\") plot_history(\"accuracy\")"},{"path":"https://keras.posit.co/articles/examples/torchvision_keras.html","id":"evaluation-and-inference","dir":"Articles > Examples","previous_headings":"","what":"Evaluation and Inference","title":"Fine-tuning a pre-trained TorchVision Model","text":"Now, let us load best model weights checkpoint evaluate model. Finally, let us visualize predictions model","code":"model.load_weights(\"model.weights.h5\") _, val_accuracy = model.evaluate(dataloaders[\"val\"]) print(\"Best Validation Accuracy:\", val_accuracy) plt.figure(figsize=(10, 10)) sample_images, sample_labels = next(iter(dataloaders[\"train\"]))  # We perform inference and detach the predicted probabilities from the Torch # computation graph with a tensor that does not require gradient computation. sample_pred_probas = model(sample_images.to(\"cuda\")).detach() sample_pred_logits = keras.ops.argmax(sample_pred_probas, axis=1) sample_pred_logits = sample_pred_logits.to(\"cpu\").numpy()  sample_images = sample_images.numpy() sample_labels = sample_labels.numpy()  for idx in range(9):     ax = plt.subplot(3, 3, idx + 1)     image = sample_images[idx].transpose((1, 2, 0))     mean = np.array([0.485, 0.456, 0.406])     std = np.array([0.229, 0.224, 0.225])     image = std * image + mean     image = np.clip(image, 0, 1)     plt.imshow(image)     title = \"Ground Truth Label: \" + class_names[int(sample_labels[idx])]     title += \"\\nPredicted Label: \" + class_names[int(sample_pred_logits[idx])]     plt.title(title)     plt.axis(\"off\")"},{"path":"https://keras.posit.co/articles/examples/trainer_pattern.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Trainer pattern","text":"example shows create custom training step using “Trainer pattern”, can shared across multiple Keras models. pattern overrides train_step() method keras.Model class, allowing training loops beyond plain supervised learning. Trainer pattern can also easily adapted complex models larger custom training steps, end--end GAN model, putting custom training step Trainer class definition.","code":""},{"path":"https://keras.posit.co/articles/examples/trainer_pattern.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Trainer pattern","text":"","code":"import tensorflow as tf import keras as keras  # Load MNIST dataset and standardize the data mnist = keras.datasets.mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0"},{"path":"https://keras.posit.co/articles/examples/trainer_pattern.html","id":"define-the-trainer-class","dir":"Articles > Examples","previous_headings":"","what":"Define the Trainer class","title":"Trainer pattern","text":"custom training evaluation step can created overriding train_step() test_step() method Model subclass:","code":"class MyTrainer(keras.Model):     def __init__(self, model):         super().__init__()         self.model = model         # Create loss and metrics here.         self.loss_fn = keras.losses.SparseCategoricalCrossentropy()         self.accuracy_metric = keras.metrics.SparseCategoricalAccuracy()      @property     def metrics(self):         # List metrics here.         return [self.accuracy_metric]      def train_step(self, data):         x, y = data         with tf.GradientTape() as tape:             y_pred = self.model(x, training=True)  # Forward pass             # Compute loss value             loss = self.loss_fn(y, y_pred)          # Compute gradients         trainable_vars = self.trainable_variables         gradients = tape.gradient(loss, trainable_vars)          # Update weights         self.optimizer.apply_gradients(zip(gradients, trainable_vars))          # Update metrics         for metric in self.metrics:             metric.update_state(y, y_pred)          # Return a dict mapping metric names to current value.         return {m.name: m.result() for m in self.metrics}      def test_step(self, data):         x, y = data          # Inference step         y_pred = self.model(x, training=False)          # Update metrics         for metric in self.metrics:             metric.update_state(y, y_pred)         return {m.name: m.result() for m in self.metrics}      def call(self, x):         # Equivalent to `call()` of the wrapped keras.Model         x = self.model(x)         return x"},{"path":"https://keras.posit.co/articles/examples/trainer_pattern.html","id":"define-multiple-models-to-share-the-custom-training-step","dir":"Articles > Examples","previous_headings":"","what":"Define multiple models to share the custom training step","title":"Trainer pattern","text":"Let’s define two different models can share Trainer class custom train_step():","code":"# A model defined using Sequential API model_a = keras.models.Sequential(     [         keras.layers.Flatten(input_shape=(28, 28)),         keras.layers.Dense(256, activation=\"relu\"),         keras.layers.Dropout(0.2),         keras.layers.Dense(10, activation=\"softmax\"),     ] )  # A model defined using Functional API func_input = keras.Input(shape=(28, 28, 1)) x = keras.layers.Flatten(input_shape=(28, 28))(func_input) x = keras.layers.Dense(512, activation=\"relu\")(x) x = keras.layers.Dropout(0.4)(x) func_output = keras.layers.Dense(10, activation=\"softmax\")(x)  model_b = keras.Model(func_input, func_output)"},{"path":"https://keras.posit.co/articles/examples/trainer_pattern.html","id":"create-trainer-class-objects-from-the-models","dir":"Articles > Examples","previous_headings":"","what":"Create Trainer class objects from the models","title":"Trainer pattern","text":"","code":"trainer_1 = MyTrainer(model_a) trainer_2 = MyTrainer(model_b)"},{"path":"https://keras.posit.co/articles/examples/trainer_pattern.html","id":"compile-and-fit-the-models-to-the-mnist-dataset","dir":"Articles > Examples","previous_headings":"","what":"Compile and fit the models to the MNIST dataset","title":"Trainer pattern","text":"","code":"trainer_1.compile(optimizer=keras.optimizers.SGD()) trainer_1.fit(     x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test) )  trainer_2.compile(optimizer=keras.optimizers.Adam()) trainer_2.fit(     x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test) )"},{"path":"https://keras.posit.co/articles/examples/uk_ireland_accent_recognition.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"English speaker accent recognition using Transfer Learning","text":"following example shows use feature extraction order train model classify English accent spoken audio wave. Instead training model scratch, transfer learning enables us take advantage existing state---art deep learning models use feature extractors. process: Use TF Hub pre-trained model (Yamnet) apply part tf.data pipeline transforms audio files feature vectors. Train dense model feature vectors. Use trained model inference new audio file. Note: need install TensorFlow IO order resample audio files 16 kHz required Yamnet model. test section, ffmpeg used convert mp3 file wav. can install TensorFlow IO following command: pip install -U -q tensorflow_io","code":""},{"path":"https://keras.posit.co/articles/examples/uk_ireland_accent_recognition.html","id":"configuration","dir":"Articles > Examples","previous_headings":"","what":"Configuration","title":"English speaker accent recognition using Transfer Learning","text":"","code":"SEED = 1337 EPOCHS = 1 BATCH_SIZE = 64 VALIDATION_RATIO = 0.1 MODEL_NAME = \"uk_irish_accent_recognition\"  # Location where the dataset will be downloaded. # By default (None), keras.utils.get_file will use ~/.keras/ as the CACHE_DIR CACHE_DIR = None  # The location of the dataset URL_PATH = \"https://www.openslr.org/resources/83/\"  # List of datasets compressed files that contain the audio files zip_files = {     0: \"irish_english_male.zip\",     1: \"midlands_english_female.zip\",     2: \"midlands_english_male.zip\",     3: \"northern_english_female.zip\",     4: \"northern_english_male.zip\",     5: \"scottish_english_female.zip\",     6: \"scottish_english_male.zip\",     7: \"southern_english_female.zip\",     8: \"southern_english_male.zip\",     9: \"welsh_english_female.zip\",     10: \"welsh_english_male.zip\", }  # We see that there are 2 compressed files for each accent (except Irish): # - One for male speakers # - One for female speakers # However, we will be using a gender agnostic dataset.  # List of gender agnostic categories gender_agnostic_categories = [     \"ir\",  # Irish     \"mi\",  # Midlands     \"no\",  # Northern     \"sc\",  # Scottish     \"so\",  # Southern     \"we\",  # Welsh ]  class_names = [     \"Irish\",     \"Midlands\",     \"Northern\",     \"Scottish\",     \"Southern\",     \"Welsh\",     \"Not a speech\", ]"},{"path":"https://keras.posit.co/articles/examples/uk_ireland_accent_recognition.html","id":"imports","dir":"Articles > Examples","previous_headings":"","what":"Imports","title":"English speaker accent recognition using Transfer Learning","text":"","code":"import os  os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  import io import csv import numpy as np import pandas as pd import tensorflow as tf import tensorflow_hub as hub import tensorflow_io as tfio import keras as keras import matplotlib.pyplot as plt import seaborn as sns from scipy import stats from IPython.display import Audio   # Set all random seeds in order to get reproducible results keras.utils.set_random_seed(SEED)  # Where to download the dataset DATASET_DESTINATION = os.path.join(     CACHE_DIR if CACHE_DIR else \"~/.keras/\", \"datasets\" )"},{"path":"https://keras.posit.co/articles/examples/uk_ireland_accent_recognition.html","id":"yamnet-model","dir":"Articles > Examples","previous_headings":"","what":"Yamnet Model","title":"English speaker accent recognition using Transfer Learning","text":"Yamnet audio event classifier trained AudioSet dataset predict audio events AudioSet ontology. available TensorFlow Hub. Yamnet accepts 1-D tensor audio samples sample rate 16 kHz. output, model returns 3-tuple: Scores shape (N, 521) representing scores 521 classes. Embeddings shape (N, 1024). log-mel spectrogram entire audio frame. use embeddings, features extracted audio samples, input dense model. detailed information Yamnet, please refer TensorFlow Hub page.","code":"yamnet_model = hub.load(\"https://tfhub.dev/google/yamnet/1\")"},{"path":"https://keras.posit.co/articles/examples/uk_ireland_accent_recognition.html","id":"dataset","dir":"Articles > Examples","previous_headings":"","what":"Dataset","title":"English speaker accent recognition using Transfer Learning","text":"dataset used Crowdsourced high-quality UK Ireland English Dialect speech data set consists total 17,877 high-quality audio wav files. dataset includes 31 hours recording 120 volunteers self-identify native speakers Southern England, Midlands, Northern England, Wales, Scotland Ireland. info, please refer link following paper: Open-source Multi-speaker Corpora English Accents British Isles","code":""},{"path":"https://keras.posit.co/articles/examples/uk_ireland_accent_recognition.html","id":"download-the-data","dir":"Articles > Examples","previous_headings":"","what":"Download the data","title":"English speaker accent recognition using Transfer Learning","text":"","code":"# CSV file that contains information about the dataset. For each entry, we have: # - ID # - wav file name # - transcript line_index_file = keras.utils.get_file(     fname=\"line_index_file\", origin=URL_PATH + \"line_index_all.csv\" )  # Download the list of compressed files that contain the audio wav files for i in zip_files:     fname = zip_files[i].split(\".\")[0]     url = URL_PATH + zip_files[i]      zip_file = keras.utils.get_file(fname=fname, origin=url, extract=True)     os.remove(zip_file)"},{"path":"https://keras.posit.co/articles/examples/uk_ireland_accent_recognition.html","id":"load-the-data-in-a-dataframe","dir":"Articles > Examples","previous_headings":"","what":"Load the data in a Dataframe","title":"English speaker accent recognition using Transfer Learning","text":"3 columns (ID, filename transcript), interested filename column order read audio file. ignore two. Let’s now preprocess dataset : Adjusting filename (removing leading space & adding “.wav” extension filename). Creating label using first 2 characters filename indicate accent. Shuffling samples.","code":"dataframe = pd.read_csv(     line_index_file,     names=[\"id\", \"filename\", \"transcript\"],     usecols=[\"filename\"], ) dataframe.head() # The purpose of this function is to preprocess the dataframe by applying the following: # - Cleaning the filename from a leading space # - Generating a label column that is gender agnostic i.e. #   welsh english male and welsh english female for example are both labeled as #   welsh english # - Add extension .wav to the filename # - Shuffle samples def preprocess_dataframe(dataframe):     # Remove leading space in filename column     dataframe[\"filename\"] = dataframe.apply(         lambda row: row[\"filename\"].strip(), axis=1     )      # Create gender agnostic labels based on the filename first 2 letters     dataframe[\"label\"] = dataframe.apply(         lambda row: gender_agnostic_categories.index(row[\"filename\"][:2]),         axis=1,     )      # Add the file path to the name     dataframe[\"filename\"] = dataframe.apply(         lambda row: os.path.join(DATASET_DESTINATION, row[\"filename\"] + \".wav\"),         axis=1,     )      # Shuffle the samples     dataframe = dataframe.sample(frac=1, random_state=SEED).reset_index(         drop=True     )      return dataframe   dataframe = preprocess_dataframe(dataframe) dataframe.head()"},{"path":"https://keras.posit.co/articles/examples/uk_ireland_accent_recognition.html","id":"prepare-training-validation-sets","dir":"Articles > Examples","previous_headings":"","what":"Prepare training & validation sets","title":"English speaker accent recognition using Transfer Learning","text":"Let’s split samples creating training validation sets.","code":"split = int(len(dataframe) * (1 - VALIDATION_RATIO)) train_df = dataframe[:split] valid_df = dataframe[split:]  print(     f\"We have {train_df.shape[0]} training samples & {valid_df.shape[0]} validation ones\" )"},{"path":"https://keras.posit.co/articles/examples/uk_ireland_accent_recognition.html","id":"prepare-a-tensorflow-dataset","dir":"Articles > Examples","previous_headings":"","what":"Prepare a TensorFlow Dataset","title":"English speaker accent recognition using Transfer Learning","text":"Next, need create tf.data.Dataset. done creating dataframe_to_dataset function following: Create dataset using filenames labels. Get Yamnet embeddings calling another function filepath_to_embeddings. Apply caching, reshuffling setting batch size. filepath_to_embeddings following: Load audio file. Resample audio 16 kHz. Generate scores embeddings Yamnet model. Since Yamnet generates multiple samples audio file, function also duplicates label generated samples score=0 (speech) whereas sets label others ‘’ indicating audio segment speech won’t label one accents. load_16k_audio_file copied following tutorial Transfer learning YAMNet environmental sound classification","code":"@tf.function def load_16k_audio_wav(filename):     # Read file content     file_content = tf.io.read_file(filename)      # Decode audio wave     audio_wav, sample_rate = tf.audio.decode_wav(         file_content, desired_channels=1     )     audio_wav = tf.squeeze(audio_wav, axis=-1)     sample_rate = tf.cast(sample_rate, dtype=tf.int64)      # Resample to 16k     audio_wav = tfio.audio.resample(         audio_wav, rate_in=sample_rate, rate_out=16000     )      return audio_wav   def filepath_to_embeddings(filename, label):     # Load 16k audio wave     audio_wav = load_16k_audio_wav(filename)      # Get audio embeddings & scores.     # The embeddings are the audio features extracted using transfer learning     # while scores will be used to identify time slots that are not speech     # which will then be gathered into a specific new category 'other'     scores, embeddings, _ = yamnet_model(audio_wav)      # Number of embeddings in order to know how many times to repeat the label     embeddings_num = tf.shape(embeddings)[0]     labels = tf.repeat(label, embeddings_num)      # Change labels for time-slots that are not speech into a new category 'other'     labels = tf.where(         tf.argmax(scores, axis=1) == 0, label, len(class_names) - 1     )      # Using one-hot in order to use AUC     return (embeddings, tf.one_hot(labels, len(class_names)))   def dataframe_to_dataset(dataframe, batch_size=64):     dataset = tf.data.Dataset.from_tensor_slices(         (dataframe[\"filename\"], dataframe[\"label\"])     )      dataset = dataset.map(         lambda x, y: filepath_to_embeddings(x, y),         num_parallel_calls=tf.data.experimental.AUTOTUNE,     ).unbatch()      return dataset.cache().batch(batch_size).prefetch(tf.data.AUTOTUNE)   train_ds = dataframe_to_dataset(train_df) valid_ds = dataframe_to_dataset(valid_df)"},{"path":"https://keras.posit.co/articles/examples/uk_ireland_accent_recognition.html","id":"build-the-model","dir":"Articles > Examples","previous_headings":"","what":"Build the model","title":"English speaker accent recognition using Transfer Learning","text":"model use consists : input layer embedding output Yamnet classifier. 4 dense hidden layers 4 dropout layers. output dense layer. model’s hyperparameters selected using KerasTuner.","code":"keras.backend.clear_session()   def build_and_compile_model():     inputs = keras.layers.Input(shape=(1024,), name=\"embedding\")      x = keras.layers.Dense(256, activation=\"relu\", name=\"dense_1\")(inputs)     x = keras.layers.Dropout(0.15, name=\"dropout_1\")(x)      x = keras.layers.Dense(384, activation=\"relu\", name=\"dense_2\")(x)     x = keras.layers.Dropout(0.2, name=\"dropout_2\")(x)      x = keras.layers.Dense(192, activation=\"relu\", name=\"dense_3\")(x)     x = keras.layers.Dropout(0.25, name=\"dropout_3\")(x)      x = keras.layers.Dense(384, activation=\"relu\", name=\"dense_4\")(x)     x = keras.layers.Dropout(0.2, name=\"dropout_4\")(x)      outputs = keras.layers.Dense(         len(class_names), activation=\"softmax\", name=\"ouput\"     )(x)      model = keras.Model(         inputs=inputs, outputs=outputs, name=\"accent_recognition\"     )      model.compile(         optimizer=keras.optimizers.Adam(learning_rate=1.9644e-5),         loss=keras.losses.CategoricalCrossentropy(),         metrics=[\"accuracy\", keras.metrics.AUC(name=\"auc\")],     )      return model   model = build_and_compile_model() model.summary()"},{"path":"https://keras.posit.co/articles/examples/uk_ireland_accent_recognition.html","id":"class-weights-calculation","dir":"Articles > Examples","previous_headings":"","what":"Class weights calculation","title":"English speaker accent recognition using Transfer Learning","text":"Since dataset quite unbalanced, wil use class_weight argument training. Getting class weights little tricky even though know number audio files class, represent number samples class since Yamnet transforms audio file multiple audio samples 0.96 seconds . every audio file split number samples proportional length. Therefore, get weights, calculate number samples class preprocessing Yamnet.","code":"class_counts = tf.zeros(shape=(len(class_names),), dtype=tf.int32)  for x, y in iter(train_ds):     class_counts = class_counts + tf.math.bincount(         tf.cast(tf.math.argmax(y, axis=1), tf.int32), minlength=len(class_names)     )  class_weight = {     i: tf.math.reduce_sum(class_counts).numpy() / class_counts[i].numpy()     for i in range(len(class_counts)) }  print(class_weight)"},{"path":"https://keras.posit.co/articles/examples/uk_ireland_accent_recognition.html","id":"callbacks","dir":"Articles > Examples","previous_headings":"","what":"Callbacks","title":"English speaker accent recognition using Transfer Learning","text":"use Keras callbacks order : Stop whenever validation AUC stops improving. Save best model. Call TensorBoard order later view training validation logs.","code":"early_stopping_cb = keras.callbacks.EarlyStopping(     monitor=\"val_auc\", patience=10, restore_best_weights=True )  model_checkpoint_cb = keras.callbacks.ModelCheckpoint(     MODEL_NAME + \".keras\", monitor=\"val_auc\", save_best_only=True )  tensorboard_cb = keras.callbacks.TensorBoard(     os.path.join(os.curdir, \"logs\", model.name) )  callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"},{"path":"https://keras.posit.co/articles/examples/uk_ireland_accent_recognition.html","id":"training","dir":"Articles > Examples","previous_headings":"","what":"Training","title":"English speaker accent recognition using Transfer Learning","text":"","code":"history = model.fit(     train_ds,     epochs=EPOCHS,     validation_data=valid_ds,     class_weight=class_weight,     callbacks=callbacks,     verbose=2, )"},{"path":"https://keras.posit.co/articles/examples/uk_ireland_accent_recognition.html","id":"results","dir":"Articles > Examples","previous_headings":"","what":"Results","title":"English speaker accent recognition using Transfer Learning","text":"Let’s plot training validation AUC accuracy.","code":"fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))  axs[0].plot(range(EPOCHS), history.history[\"accuracy\"], label=\"Training\") axs[0].plot(range(EPOCHS), history.history[\"val_accuracy\"], label=\"Validation\") axs[0].set_xlabel(\"Epochs\") axs[0].set_title(\"Training & Validation Accuracy\") axs[0].legend() axs[0].grid(True)  axs[1].plot(range(EPOCHS), history.history[\"auc\"], label=\"Training\") axs[1].plot(range(EPOCHS), history.history[\"val_auc\"], label=\"Validation\") axs[1].set_xlabel(\"Epochs\") axs[1].set_title(\"Training & Validation AUC\") axs[1].legend() axs[1].grid(True)  plt.show()"},{"path":"https://keras.posit.co/articles/examples/uk_ireland_accent_recognition.html","id":"evaluation","dir":"Articles > Examples","previous_headings":"","what":"Evaluation","title":"English speaker accent recognition using Transfer Learning","text":"Let’s try compare model’s performance Yamnet’s using one Yamnet metrics (d-prime) Yamnet achieved d-prime value 2.318. Let’s check model’s performance. can see model achieves following results:","code":"train_loss, train_acc, train_auc = model.evaluate(train_ds) valid_loss, valid_acc, valid_auc = model.evaluate(valid_ds) # The following function calculates the d-prime score from the AUC def d_prime(auc):     standard_normal = stats.norm()     d_prime = standard_normal.ppf(auc) * np.sqrt(2.0)     return d_prime   print(     \"train d-prime: {0:.3f}, validation d-prime: {1:.3f}\".format(         d_prime(train_auc), d_prime(valid_auc)     ) )"},{"path":"https://keras.posit.co/articles/examples/uk_ireland_accent_recognition.html","id":"confusion-matrix","dir":"Articles > Examples","previous_headings":"","what":"Confusion Matrix","title":"English speaker accent recognition using Transfer Learning","text":"Let’s now plot confusion matrix validation dataset. confusion matrix lets us see, every class, many samples correctly classified, also classes samples confused . allows us calculate precision recall every class.","code":"# Create x and y tensors x_valid = None y_valid = None  for x, y in iter(valid_ds):     if x_valid is None:         x_valid = x.numpy()         y_valid = y.numpy()     else:         x_valid = np.concatenate((x_valid, x.numpy()), axis=0)         y_valid = np.concatenate((y_valid, y.numpy()), axis=0)  # Generate predictions y_pred = model.predict(x_valid)  # Calculate confusion matrix confusion_mtx = tf.math.confusion_matrix(     np.argmax(y_valid, axis=1), np.argmax(y_pred, axis=1) )  # Plot the confusion matrix plt.figure(figsize=(10, 8)) sns.heatmap(     confusion_mtx,     xticklabels=class_names,     yticklabels=class_names,     annot=True,     fmt=\"g\", ) plt.xlabel(\"Prediction\") plt.ylabel(\"Label\") plt.title(\"Validation Confusion Matrix\") plt.show()"},{"path":"https://keras.posit.co/articles/examples/uk_ireland_accent_recognition.html","id":"precision-recall","dir":"Articles > Examples","previous_headings":"","what":"Precision & recall","title":"English speaker accent recognition using Transfer Learning","text":"every class: Recall ratio correctly classified samples .e. shows many samples specific class, model able detect. ratio diagonal elements sum elements row. Precision shows accuracy classifier. ratio correctly predicted samples among ones classified belonging class. ratio diagonal elements sum elements column.","code":"for i, label in enumerate(class_names):     precision = confusion_mtx[i, i] / np.sum(confusion_mtx[:, i])     recall = confusion_mtx[i, i] / np.sum(confusion_mtx[i, :])     print(         \"{0:15} Precision:{1:.2f}%; Recall:{2:.2f}%\".format(             label, precision * 100, recall * 100         )     )"},{"path":"https://keras.posit.co/articles/examples/uk_ireland_accent_recognition.html","id":"run-inference-on-test-data","dir":"Articles > Examples","previous_headings":"","what":"Run inference on test data","title":"English speaker accent recognition using Transfer Learning","text":"Let’s now run test single audio file. Let’s check example Scottish Voice : Download mp3 file. Convert 16k wav file. Run model wav file. Plot results. function yamnet_class_names_from_csv copied slightly changed Yamnet Notebook. Let’s run model audio file: Listen audio function copied Yamnet notebook adjusted need. function plots following: Audio waveform Mel spectrogram Predictions every time step","code":"filename = \"audio-sample-Stuart\" url = \"https://www.thescottishvoice.org.uk/files/cm/files/\"  if os.path.exists(filename + \".wav\") == False:     print(f\"Downloading {filename}.mp3 from {url}\")     command = f\"wget {url}{filename}.mp3\"     os.system(command)      print(f\"Converting mp3 to wav and resampling to 16 kHZ\")     command = (         f\"ffmpeg -hide_banner -loglevel panic -y -i {filename}.mp3 -acodec \"         f\"pcm_s16le -ac 1 -ar 16000 {filename}.wav\"     )     os.system(command)  filename = filename + \".wav\" def yamnet_class_names_from_csv(yamnet_class_map_csv_text):     \"\"\"Returns list of class names corresponding to score vector.\"\"\"     yamnet_class_map_csv = io.StringIO(yamnet_class_map_csv_text)     yamnet_class_names = [         name for (class_index, mid, name) in csv.reader(yamnet_class_map_csv)     ]     yamnet_class_names = yamnet_class_names[1:]  # Skip CSV header     return yamnet_class_names   yamnet_class_map_path = yamnet_model.class_map_path().numpy() yamnet_class_names = yamnet_class_names_from_csv(     tf.io.read_file(yamnet_class_map_path).numpy().decode(\"utf-8\") )   def calculate_number_of_non_speech(scores):     number_of_non_speech = tf.math.reduce_sum(         tf.where(             tf.math.argmax(scores, axis=1, output_type=tf.int32) != 0, 1, 0         )     )      return number_of_non_speech   def filename_to_predictions(filename):     # Load 16k audio wave     audio_wav = load_16k_audio_wav(filename)      # Get audio embeddings & scores.     scores, embeddings, mel_spectrogram = yamnet_model(audio_wav)      print(         \"Out of {} samples, {} are not speech\".format(             scores.shape[0], calculate_number_of_non_speech(scores)         )     )      # Predict the output of the accent recognition model with embeddings as input     predictions = model.predict(embeddings)      return audio_wav, predictions, mel_spectrogram audio_wav, predictions, mel_spectrogram = filename_to_predictions(filename)  infered_class = class_names[predictions.mean(axis=0).argmax()] print(f\"The main accent is: {infered_class} English\") Audio(audio_wav, rate=16000) plt.figure(figsize=(10, 6))  # Plot the waveform. plt.subplot(3, 1, 1) plt.plot(audio_wav) plt.xlim([0, len(audio_wav)])  # Plot the log-mel spectrogram (returned by the model). plt.subplot(3, 1, 2) plt.imshow(     mel_spectrogram.numpy().T,     aspect=\"auto\",     interpolation=\"nearest\",     origin=\"lower\", )  # Plot and label the model output scores for the top-scoring classes. mean_predictions = np.mean(predictions, axis=0)  top_class_indices = np.argsort(mean_predictions)[::-1] plt.subplot(3, 1, 3) plt.imshow(     predictions[:, top_class_indices].T,     aspect=\"auto\",     interpolation=\"nearest\",     cmap=\"gray_r\", )  # patch_padding = (PATCH_WINDOW_SECONDS / 2) / PATCH_HOP_SECONDS # values from the model documentation patch_padding = (0.025 / 2) / 0.01 plt.xlim([-patch_padding - 0.5, predictions.shape[0] + patch_padding - 0.5]) # Label the top_N classes. yticks = range(0, len(class_names), 1) plt.yticks(yticks, [class_names[top_class_indices[x]] for x in yticks]) _ = plt.ylim(-0.5 + np.array([len(class_names), 0]))"},{"path":"https://keras.posit.co/articles/examples/vae.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Variational AutoEncoder","text":"","code":"import numpy as np import tensorflow as tf import keras as keras from keras import layers"},{"path":"https://keras.posit.co/articles/examples/vae.html","id":"create-a-sampling-layer","dir":"Articles > Examples","previous_headings":"","what":"Create a sampling layer","title":"Variational AutoEncoder","text":"","code":"class Sampling(layers.Layer):     \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"      def call(self, inputs):         z_mean, z_log_var = inputs         batch = tf.shape(z_mean)[0]         dim = tf.shape(z_mean)[1]         epsilon = tf.random.normal(shape=(batch, dim))         return z_mean + tf.exp(0.5 * z_log_var) * epsilon"},{"path":"https://keras.posit.co/articles/examples/vae.html","id":"build-the-encoder","dir":"Articles > Examples","previous_headings":"","what":"Build the encoder","title":"Variational AutoEncoder","text":"","code":"latent_dim = 2  encoder_inputs = keras.Input(shape=(28, 28, 1)) x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(     encoder_inputs ) x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x) x = layers.Flatten()(x) x = layers.Dense(16, activation=\"relu\")(x) z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x) z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x) z = Sampling()([z_mean, z_log_var]) encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\") encoder.summary()"},{"path":"https://keras.posit.co/articles/examples/vae.html","id":"build-the-decoder","dir":"Articles > Examples","previous_headings":"","what":"Build the decoder","title":"Variational AutoEncoder","text":"","code":"latent_inputs = keras.Input(shape=(latent_dim,)) x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs) x = layers.Reshape((7, 7, 64))(x) x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(     x ) x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(     x ) decoder_outputs = layers.Conv2DTranspose(     1, 3, activation=\"sigmoid\", padding=\"same\" )(x) decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\") decoder.summary()"},{"path":"https://keras.posit.co/articles/examples/vae.html","id":"define-the-vae-as-a-model-with-a-custom-train_step","dir":"Articles > Examples","previous_headings":"","what":"Define the VAE as a Model with a custom train_step","title":"Variational AutoEncoder","text":"","code":"class VAE(keras.Model):     def __init__(self, encoder, decoder, **kwargs):         super().__init__(**kwargs)         self.encoder = encoder         self.decoder = decoder         self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")         self.reconstruction_loss_tracker = keras.metrics.Mean(             name=\"reconstruction_loss\"         )         self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")      @property     def metrics(self):         return [             self.total_loss_tracker,             self.reconstruction_loss_tracker,             self.kl_loss_tracker,         ]      def train_step(self, data):         with tf.GradientTape() as tape:             z_mean, z_log_var, z = self.encoder(data)             reconstruction = self.decoder(z)             reconstruction_loss = tf.reduce_mean(                 tf.reduce_sum(                     keras.losses.binary_crossentropy(data, reconstruction),                     axis=(1, 2),                 )             )             kl_loss = -0.5 * (                 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)             )             kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))             total_loss = reconstruction_loss + kl_loss         grads = tape.gradient(total_loss, self.trainable_weights)         self.optimizer.apply_gradients(zip(grads, self.trainable_weights))         self.total_loss_tracker.update_state(total_loss)         self.reconstruction_loss_tracker.update_state(reconstruction_loss)         self.kl_loss_tracker.update_state(kl_loss)         return {             \"loss\": self.total_loss_tracker.result(),             \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),             \"kl_loss\": self.kl_loss_tracker.result(),         }"},{"path":"https://keras.posit.co/articles/examples/vae.html","id":"train-the-vae","dir":"Articles > Examples","previous_headings":"","what":"Train the VAE","title":"Variational AutoEncoder","text":"","code":"(x_train, _), (x_test, _) = keras.datasets.mnist.load_data() mnist_digits = np.concatenate([x_train, x_test], axis=0) mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255  vae = VAE(encoder, decoder) vae.compile(optimizer=keras.optimizers.Adam()) vae.fit(mnist_digits, epochs=30, batch_size=128)"},{"path":"https://keras.posit.co/articles/examples/vae.html","id":"display-a-grid-of-sampled-digits","dir":"Articles > Examples","previous_headings":"","what":"Display a grid of sampled digits","title":"Variational AutoEncoder","text":"","code":"import matplotlib.pyplot as plt   def plot_latent_space(vae, n=30, figsize=15):     # display a n*n 2D manifold of digits     digit_size = 28     scale = 1.0     figure = np.zeros((digit_size * n, digit_size * n))     # linearly spaced coordinates corresponding to the 2D plot     # of digit classes in the latent space     grid_x = np.linspace(-scale, scale, n)     grid_y = np.linspace(-scale, scale, n)[::-1]      for i, yi in enumerate(grid_y):         for j, xi in enumerate(grid_x):             z_sample = np.array([[xi, yi]])             x_decoded = vae.decoder.predict(z_sample)             digit = x_decoded[0].reshape(digit_size, digit_size)             figure[                 i * digit_size : (i + 1) * digit_size,                 j * digit_size : (j + 1) * digit_size,             ] = digit      plt.figure(figsize=(figsize, figsize))     start_range = digit_size // 2     end_range = n * digit_size + start_range     pixel_range = np.arange(start_range, end_range, digit_size)     sample_range_x = np.round(grid_x, 1)     sample_range_y = np.round(grid_y, 1)     plt.xticks(pixel_range, sample_range_x)     plt.yticks(pixel_range, sample_range_y)     plt.xlabel(\"z[0]\")     plt.ylabel(\"z[1]\")     plt.imshow(figure, cmap=\"Greys_r\")     plt.show()   plot_latent_space(vae)"},{"path":"https://keras.posit.co/articles/examples/vae.html","id":"display-how-the-latent-space-clusters-different-digit-classes","dir":"Articles > Examples","previous_headings":"","what":"Display how the latent space clusters different digit classes","title":"Variational AutoEncoder","text":"","code":"def plot_label_clusters(vae, data, labels):     # display a 2D plot of the digit classes in the latent space     z_mean, _, _ = vae.encoder.predict(data, verbose=0)     plt.figure(figsize=(12, 10))     plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)     plt.colorbar()     plt.xlabel(\"z[0]\")     plt.ylabel(\"z[1]\")     plt.show()   (x_train, y_train), _ = keras.datasets.mnist.load_data() x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255  plot_label_clusters(vae, x_train, y_train)"},{"path":"https://keras.posit.co/articles/examples/video_transformers.html","id":"data-collection","dir":"Articles > Examples","previous_headings":"","what":"Data collection","title":"Video Classification with Transformers","text":"done predecessor example, using subsampled version UCF101 dataset, well-known benchmark dataset. case want operate larger subsample even entire dataset, please refer notebook. wget -q https://github.com/sayakpaul/Action-Recognition--TensorFlow/releases/download/v1.0.0/ucf101_top5.tar.gz tar -xf ucf101_top5.tar.gz","code":""},{"path":"https://keras.posit.co/articles/examples/video_transformers.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Video Classification with Transformers","text":"","code":"import os  os.environ[\"KERAS_BACKEND\"] = \"jax\"  # @param [\"tensorflow\", \"jax\", \"torch\"]  import keras as keras from keras import layers from keras.applications.densenet import DenseNet121  from tensorflow_docs.vis import embed  import matplotlib.pyplot as plt import pandas as pd import numpy as np import imageio import cv2"},{"path":"https://keras.posit.co/articles/examples/video_transformers.html","id":"define-hyperparameters","dir":"Articles > Examples","previous_headings":"","what":"Define hyperparameters","title":"Video Classification with Transformers","text":"","code":"MAX_SEQ_LENGTH = 20 NUM_FEATURES = 1024 IMG_SIZE = 128  EPOCHS = 5"},{"path":"https://keras.posit.co/articles/examples/video_transformers.html","id":"data-preparation","dir":"Articles > Examples","previous_headings":"","what":"Data preparation","title":"Video Classification with Transformers","text":"mostly following data preparation steps example, except following changes: reduce image size 128x128 instead 224x224 speed computation. Instead using pre-trained InceptionV3 network, use pre-trained DenseNet121 feature extraction. directly pad shorter videos length MAX_SEQ_LENGTH. First, let’s load DataFrames. Calling prepare_all_videos() train_df test_df takes ~20 minutes complete. reason, save time, download already preprocessed NumPy arrays: !wget -q https://git.io/JZmf4 -O top5_data_prepared.tar.gz !tar -xf top5_data_prepared.tar.gz","code":"train_df = pd.read_csv(\"train.csv\") test_df = pd.read_csv(\"test.csv\")  print(f\"Total videos for training: {len(train_df)}\") print(f\"Total videos for testing: {len(test_df)}\")  center_crop_layer = layers.CenterCrop(IMG_SIZE, IMG_SIZE)   def crop_center(frame):     cropped = center_crop_layer(frame[None, ...])     cropped = keras.ops.convert_to_numpy(cropped)     cropped = keras.ops.squeeze(cropped)     return cropped   # Following method is modified from this tutorial: # https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub def load_video(path, max_frames=0, offload_to_cpu=False):     cap = cv2.VideoCapture(path)     frames = []     try:         while True:             ret, frame = cap.read()             if not ret:                 break             frame = frame[:, :, [2, 1, 0]]             frame = crop_center(frame)             if offload_to_cpu and keras.backend.backend() == \"torch\":                 frame = frame.to(\"cpu\")             frames.append(frame)              if len(frames) == max_frames:                 break     finally:         cap.release()     if offload_to_cpu and keras.backend.backend() == \"torch\":         return np.array([frame.to(\"cpu\").numpy() for frame in frames])     return np.array(frames)   def build_feature_extractor():     feature_extractor = DenseNet121(         weights=\"imagenet\",         include_top=False,         pooling=\"avg\",         input_shape=(IMG_SIZE, IMG_SIZE, 3),     )     preprocess_input = keras.applications.densenet.preprocess_input      inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))     preprocessed = preprocess_input(inputs)      outputs = feature_extractor(preprocessed)     return keras.Model(inputs, outputs, name=\"feature_extractor\")   feature_extractor = build_feature_extractor()   # Label preprocessing with StringLookup. label_processor = keras.layers.StringLookup(     num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]), mask_token=None ) print(label_processor.get_vocabulary())   def prepare_all_videos(df, root_dir):     num_samples = len(df)     video_paths = df[\"video_name\"].values.tolist()     labels = df[\"tag\"].values     labels = label_processor(labels[..., None]).numpy()      # `frame_features` are what we will feed to our sequence model.     frame_features = np.zeros(         shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"     )      # For each video.     for idx, path in enumerate(video_paths):         # Gather all its frames and add a batch dimension.         frames = load_video(os.path.join(root_dir, path))          # Pad shorter videos.         if len(frames) < MAX_SEQ_LENGTH:             diff = MAX_SEQ_LENGTH - len(frames)             padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))             frames = np.concatenate(frames, padding)          frames = frames[None, ...]          # Initialize placeholder to store the features of the current video.         temp_frame_features = np.zeros(             shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"         )          # Extract features from the frames of the current video.         for i, batch in enumerate(frames):             video_length = batch.shape[0]             length = min(MAX_SEQ_LENGTH, video_length)             for j in range(length):                 if np.mean(batch[j, :]) > 0.0:                     temp_frame_features[i, j, :] = feature_extractor.predict(                         batch[None, j, :]                     )                  else:                     temp_frame_features[i, j, :] = 0.0          frame_features[idx,] = temp_frame_features.squeeze()      return frame_features, labels train_data, train_labels = np.load(\"train_data.npy\"), np.load(     \"train_labels.npy\" ) test_data, test_labels = np.load(\"test_data.npy\"), np.load(\"test_labels.npy\")  print(f\"Frame features in train set: {train_data.shape}\")"},{"path":"https://keras.posit.co/articles/examples/video_transformers.html","id":"building-the-transformer-based-model","dir":"Articles > Examples","previous_headings":"","what":"Building the Transformer-based model","title":"Video Classification with Transformers","text":"building top code shared book chapter Deep Learning Python (Second ed.) François Chollet. First, self-attention layers form basic blocks Transformer order-agnostic. Since videos ordered sequences frames, need Transformer model take account order information. via positional encoding. simply embed positions frames present inside videos Embedding layer. add positional embeddings precomputed CNN feature maps. Now, can create subclassed layer Transformer.","code":"class PositionalEmbedding(layers.Layer):     def __init__(self, sequence_length, output_dim, **kwargs):         super().__init__(**kwargs)         self.position_embeddings = layers.Embedding(             input_dim=sequence_length, output_dim=output_dim         )         self.sequence_length = sequence_length         self.output_dim = output_dim      def call(self, inputs):         # The inputs are of shape: `(batch_size, frames, num_features)`         inputs = keras.backend.cast(inputs, self.compute_dtype)         length = keras.backend.shape(inputs)[1]         positions = keras.ops.numpy.arange(start=0, stop=length, step=1)         embedded_positions = self.position_embeddings(positions)         return inputs + embedded_positions class TransformerEncoder(layers.Layer):     def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):         super().__init__(**kwargs)         self.embed_dim = embed_dim         self.dense_dim = dense_dim         self.num_heads = num_heads         self.attention = layers.MultiHeadAttention(             num_heads=num_heads, key_dim=embed_dim, dropout=0.3         )         self.dense_proj = keras.Sequential(             [                 layers.Dense(dense_dim, activation=keras.activations.gelu),                 layers.Dense(embed_dim),             ]         )         self.layernorm_1 = layers.LayerNormalization()         self.layernorm_2 = layers.LayerNormalization()      def call(self, inputs, mask=None):         attention_output = self.attention(inputs, inputs, attention_mask=mask)         proj_input = self.layernorm_1(inputs + attention_output)         proj_output = self.dense_proj(proj_input)         return self.layernorm_2(proj_input + proj_output)"},{"path":"https://keras.posit.co/articles/examples/video_transformers.html","id":"utility-functions-for-training","dir":"Articles > Examples","previous_headings":"","what":"Utility functions for training","title":"Video Classification with Transformers","text":"","code":"def get_compiled_model(shape):     sequence_length = MAX_SEQ_LENGTH     embed_dim = NUM_FEATURES     dense_dim = 4     num_heads = 1     classes = len(label_processor.get_vocabulary())      inputs = keras.Input(shape=shape)     x = PositionalEmbedding(         sequence_length, embed_dim, name=\"frame_position_embedding\"     )(inputs)     x = TransformerEncoder(         embed_dim, dense_dim, num_heads, name=\"transformer_layer\"     )(x)     x = layers.GlobalMaxPooling1D()(x)     x = layers.Dropout(0.5)(x)     outputs = layers.Dense(classes, activation=\"softmax\")(x)     model = keras.Model(inputs, outputs)      model.compile(         optimizer=\"adam\",         loss=\"sparse_categorical_crossentropy\",         metrics=[\"accuracy\"],     )     return model   def run_experiment():     filepath = \"/tmp/video_classifier.weights.h5\"     checkpoint = keras.callbacks.ModelCheckpoint(         filepath, save_weights_only=True, save_best_only=True, verbose=1     )      model = get_compiled_model(train_data.shape[1:])     history = model.fit(         train_data,         train_labels,         validation_split=0.15,         epochs=EPOCHS,         callbacks=[checkpoint],     )      model.load_weights(filepath)     _, accuracy = model.evaluate(test_data, test_labels)     print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")      return model"},{"path":"https://keras.posit.co/articles/examples/video_transformers.html","id":"model-training-and-inference","dir":"Articles > Examples","previous_headings":"","what":"Model training and inference","title":"Video Classification with Transformers","text":"Note: model ~4.23 Million parameters, way sequence model (99918 parameters) used prequel example. kind Transformer model works best larger dataset longer pre-training schedule. performance model far optimal, trained small dataset.","code":"trained_model = run_experiment() def prepare_single_video(frames):     frame_features = np.zeros(         shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"     )      # Pad shorter videos.     if len(frames) < MAX_SEQ_LENGTH:         diff = MAX_SEQ_LENGTH - len(frames)         padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))         frames = np.concatenate(frames, padding)      frames = frames[None, ...]      # Extract features from the frames of the current video.     for i, batch in enumerate(frames):         video_length = batch.shape[0]         length = min(MAX_SEQ_LENGTH, video_length)         for j in range(length):             if np.mean(batch[j, :]) > 0.0:                 frame_features[i, j, :] = feature_extractor.predict(                     batch[None, j, :]                 )             else:                 frame_features[i, j, :] = 0.0      return frame_features   def predict_action(path):     class_vocab = label_processor.get_vocabulary()      frames = load_video(os.path.join(\"test\", path), offload_to_cpu=True)     frame_features = prepare_single_video(frames)     probabilities = trained_model.predict(frame_features)[0]      plot_x_axis, plot_y_axis = [], []      for i in np.argsort(probabilities)[::-1]:         plot_x_axis.append(class_vocab[i])         plot_y_axis.append(probabilities[i])         print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")      plt.bar(plot_x_axis, plot_y_axis, label=plot_x_axis)     plt.xlabel(\"class_label\")     plt.xlabel(\"Probability\")     plt.show()      return frames   # This utility is for visualization. # Referenced from: # https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub def to_gif(images):     converted_images = images.astype(np.uint8)     imageio.mimsave(\"animation.gif\", converted_images, fps=10)     return embed.embed_file(\"animation.gif\")   test_video = np.random.choice(test_df[\"video_name\"].values.tolist()) print(f\"Test video path: {test_video}\") test_frames = predict_action(test_video) to_gif(test_frames[:MAX_SEQ_LENGTH])"},{"path":"https://keras.posit.co/articles/examples/visualizing_what_convnets_learn.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Visualizing what convnets learn","text":"example, look sort visual patterns image classification models learn. ’ll using ResNet50V2 model, trained ImageNet dataset. process simple: create input images maximize activation specific filters target layer (picked somewhere middle model: layer conv3_block4_out). images represent visualization pattern filter responds .","code":""},{"path":"https://keras.posit.co/articles/examples/visualizing_what_convnets_learn.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"Visualizing what convnets learn","text":"","code":"import os  os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  import keras as keras   import numpy as np import tensorflow as tf  # The dimensions of our input image img_width = 180 img_height = 180 # Our target layer: we will visualize the filters from this layer. # See `model.summary()` for list of layer names, if you want to change this. layer_name = \"conv3_block4_out\""},{"path":"https://keras.posit.co/articles/examples/visualizing_what_convnets_learn.html","id":"build-a-feature-extraction-model","dir":"Articles > Examples","previous_headings":"","what":"Build a feature extraction model","title":"Visualizing what convnets learn","text":"","code":"# Build a ResNet50V2 model loaded with pre-trained ImageNet weights model = keras.applications.ResNet50V2(weights=\"imagenet\", include_top=False)  # Set up a model that returns the activation values for our target layer layer = model.get_layer(name=layer_name) feature_extractor = keras.Model(inputs=model.inputs, outputs=layer.output)"},{"path":"https://keras.posit.co/articles/examples/visualizing_what_convnets_learn.html","id":"set-up-the-gradient-ascent-process","dir":"Articles > Examples","previous_headings":"","what":"Set up the gradient ascent process","title":"Visualizing what convnets learn","text":"“loss” maximize simply mean activation specific filter target layer. avoid border effects, exclude border pixels. gradient ascent function simply computes gradients loss regard input image, update update image move towards state activate target filter strongly.","code":"def compute_loss(input_image, filter_index):     activation = feature_extractor(input_image)     # We avoid border artifacts by only involving non-border pixels in the loss.     filter_activation = activation[:, 2:-2, 2:-2, filter_index]     return tf.reduce_mean(filter_activation) @tf.function def gradient_ascent_step(img, filter_index, learning_rate):     with tf.GradientTape() as tape:         tape.watch(img)         loss = compute_loss(img, filter_index)     # Compute gradients.     grads = tape.gradient(loss, img)     # Normalize gradients.     grads = tf.math.l2_normalize(grads)     img += learning_rate * grads     return loss, img"},{"path":"https://keras.posit.co/articles/examples/visualizing_what_convnets_learn.html","id":"set-up-the-end-to-end-filter-visualization-loop","dir":"Articles > Examples","previous_headings":"","what":"Set up the end-to-end filter visualization loop","title":"Visualizing what convnets learn","text":"process follow: Start random image close “gray” (.e. visually netural) Repeatedly apply gradient ascent step function defined Convert resulting input image back displayable form, normalizing , center-cropping , restricting [0, 255] range. Let’s try filter 0 target layer: input maximizes response filter 0 target layer look like:","code":"def initialize_image():     # We start from a gray image with some random noise     img = tf.random.uniform((1, img_width, img_height, 3))     # ResNet50V2 expects inputs in the range [-1, +1].     # Here we scale our random inputs to [-0.125, +0.125]     return (img - 0.5) * 0.25   def visualize_filter(filter_index):     # We run gradient ascent for 20 steps     iterations = 30     learning_rate = 10.0     img = initialize_image()     for iteration in range(iterations):         loss, img = gradient_ascent_step(img, filter_index, learning_rate)      # Decode the resulting input image     img = deprocess_image(img[0].numpy())     return loss, img   def deprocess_image(img):     # Normalize array: center on 0., ensure variance is 0.15     img -= img.mean()     img /= img.std() + 1e-5     img *= 0.15      # Center crop     img = img[25:-25, 25:-25, :]      # Clip to [0, 1]     img += 0.5     img = np.clip(img, 0, 1)      # Convert to RGB array     img *= 255     img = np.clip(img, 0, 255).astype(\"uint8\")     return img from IPython.display import Image, display  loss, img = visualize_filter(0) keras.utils.save_img(\"0.png\", img) display(Image(\"0.png\"))"},{"path":"https://keras.posit.co/articles/examples/visualizing_what_convnets_learn.html","id":"visualize-the-first-64-filters-in-the-target-layer","dir":"Articles > Examples","previous_headings":"","what":"Visualize the first 64 filters in the target layer","title":"Visualizing what convnets learn","text":"Now, let’s make 8x8 grid first 64 filters target layer get feel range different visual patterns model learned. Image classification models see world decomposing inputs “vector basis” texture filters . See also old blog post analysis interpretation. Example available HuggingFace.","code":"# Compute image inputs that maximize per-filter activations # for the first 64 filters of our target layer all_imgs = [] for filter_index in range(64):     print(\"Processing filter %d\" % (filter_index,))     loss, img = visualize_filter(filter_index)     all_imgs.append(img)  # Build a black picture with enough space for # our 8 x 8 filters of size 128 x 128, with a 5px margin in between margin = 5 n = 8 cropped_width = img_width - 25 * 2 cropped_height = img_height - 25 * 2 width = n * cropped_width + (n - 1) * margin height = n * cropped_height + (n - 1) * margin stitched_filters = np.zeros((width, height, 3))  # Fill the picture with our saved filters for i in range(n):     for j in range(n):         img = all_imgs[i * n + j]         stitched_filters[             (cropped_width + margin) * i : (cropped_width + margin) * i + cropped_width,             (cropped_height + margin) * j : (cropped_height + margin) * j             + cropped_height,             :,         ] = img keras.utils.save_img(\"stiched_filters.png\", stitched_filters)  from IPython.display import Image, display  display(Image(\"stiched_filters.png\"))"},{"path":"https://keras.posit.co/articles/examples/wgan_gp.html","id":"wasserstein-gan-wgan-with-gradient-penalty-gp","dir":"Articles > Examples","previous_headings":"","what":"Wasserstein GAN (WGAN) with Gradient Penalty (GP)","title":"WGAN-GP overriding `Model.train_step`","text":"original Wasserstein GAN leverages Wasserstein distance produce value function better theoretical properties value function used original GAN paper. WGAN requires discriminator (aka critic) lie within space 1-Lipschitz functions. authors proposed idea weight clipping achieve constraint. Though weight clipping works, can problematic way enforce 1-Lipschitz constraint can cause undesirable behavior, e.g. deep WGAN discriminator (critic) often fails converge. WGAN-GP method proposes alternative weight clipping ensure smooth training. Instead clipping weights, authors proposed “gradient penalty” adding loss term keeps L2 norm discriminator gradients close 1.","code":""},{"path":"https://keras.posit.co/articles/examples/wgan_gp.html","id":"setup","dir":"Articles > Examples","previous_headings":"","what":"Setup","title":"WGAN-GP overriding `Model.train_step`","text":"","code":"import tensorflow as tf import keras as keras from keras import layers"},{"path":"https://keras.posit.co/articles/examples/wgan_gp.html","id":"prepare-the-fashion-mnist-data","dir":"Articles > Examples","previous_headings":"","what":"Prepare the Fashion-MNIST data","title":"WGAN-GP overriding `Model.train_step`","text":"demonstrate train WGAN-GP, using Fashion-MNIST dataset. sample dataset 28x28 grayscale image associated label 10 classes (e.g. trouser, pullover, sneaker, etc.)","code":"IMG_SHAPE = (28, 28, 1) BATCH_SIZE = 512  # Size of the noise vector noise_dim = 128  fashion_mnist = keras.datasets.fashion_mnist (train_images, train_labels), (     test_images,     test_labels, ) = fashion_mnist.load_data() print(f\"Number of examples: {len(train_images)}\") print(f\"Shape of the images in the dataset: {train_images.shape[1:]}\")  # Reshape each sample to (28, 28, 1) and normalize the pixel values in the [-1, 1] range train_images = train_images.reshape(train_images.shape[0], *IMG_SHAPE).astype(     \"float32\" ) train_images = (train_images - 127.5) / 127.5"},{"path":"https://keras.posit.co/articles/examples/wgan_gp.html","id":"create-the-discriminator-the-critic-in-the-original-wgan","dir":"Articles > Examples","previous_headings":"","what":"Create the discriminator (the critic in the original WGAN)","title":"WGAN-GP overriding `Model.train_step`","text":"samples dataset (28, 28, 1) shape. using strided convolutions, can result shape odd dimensions. example, (28, 28) -> Conv_s2 -> (14, 14) -> Conv_s2 -> (7, 7) -> Conv_s2 ->(3, 3). peforming upsampling generator part network, won’t get input shape original images aren’t careful. avoid , something much simpler: - discriminator: “zero pad” input change shape (32, 32, 1) sample; - Ihe generator: crop final output match shape input shape.","code":"def conv_block(     x,     filters,     activation,     kernel_size=(3, 3),     strides=(1, 1),     padding=\"same\",     use_bias=True,     use_bn=False,     use_dropout=False,     drop_value=0.5, ):     x = layers.Conv2D(         filters,         kernel_size,         strides=strides,         padding=padding,         use_bias=use_bias,     )(x)     if use_bn:         x = layers.BatchNormalization()(x)     x = activation(x)     if use_dropout:         x = layers.Dropout(drop_value)(x)     return x   def get_discriminator_model():     img_input = layers.Input(shape=IMG_SHAPE)     # Zero pad the input to make the input images size to (32, 32, 1).     x = layers.ZeroPadding2D((2, 2))(img_input)     x = conv_block(         x,         64,         kernel_size=(5, 5),         strides=(2, 2),         use_bn=False,         use_bias=True,         activation=layers.LeakyReLU(0.2),         use_dropout=False,         drop_value=0.3,     )     x = conv_block(         x,         128,         kernel_size=(5, 5),         strides=(2, 2),         use_bn=False,         activation=layers.LeakyReLU(0.2),         use_bias=True,         use_dropout=True,         drop_value=0.3,     )     x = conv_block(         x,         256,         kernel_size=(5, 5),         strides=(2, 2),         use_bn=False,         activation=layers.LeakyReLU(0.2),         use_bias=True,         use_dropout=True,         drop_value=0.3,     )     x = conv_block(         x,         512,         kernel_size=(5, 5),         strides=(2, 2),         use_bn=False,         activation=layers.LeakyReLU(0.2),         use_bias=True,         use_dropout=False,         drop_value=0.3,     )      x = layers.Flatten()(x)     x = layers.Dropout(0.2)(x)     x = layers.Dense(1)(x)      d_model = keras.models.Model(img_input, x, name=\"discriminator\")     return d_model   d_model = get_discriminator_model() d_model.summary()"},{"path":"https://keras.posit.co/articles/examples/wgan_gp.html","id":"create-the-generator","dir":"Articles > Examples","previous_headings":"","what":"Create the generator","title":"WGAN-GP overriding `Model.train_step`","text":"","code":"def upsample_block(     x,     filters,     activation,     kernel_size=(3, 3),     strides=(1, 1),     up_size=(2, 2),     padding=\"same\",     use_bn=False,     use_bias=True,     use_dropout=False,     drop_value=0.3, ):     x = layers.UpSampling2D(up_size)(x)     x = layers.Conv2D(         filters,         kernel_size,         strides=strides,         padding=padding,         use_bias=use_bias,     )(x)      if use_bn:         x = layers.BatchNormalization()(x)      if activation:         x = activation(x)     if use_dropout:         x = layers.Dropout(drop_value)(x)     return x   def get_generator_model():     noise = layers.Input(shape=(noise_dim,))     x = layers.Dense(4 * 4 * 256, use_bias=False)(noise)     x = layers.BatchNormalization()(x)     x = layers.LeakyReLU(0.2)(x)      x = layers.Reshape((4, 4, 256))(x)     x = upsample_block(         x,         128,         layers.LeakyReLU(0.2),         strides=(1, 1),         use_bias=False,         use_bn=True,         padding=\"same\",         use_dropout=False,     )     x = upsample_block(         x,         64,         layers.LeakyReLU(0.2),         strides=(1, 1),         use_bias=False,         use_bn=True,         padding=\"same\",         use_dropout=False,     )     x = upsample_block(         x,         1,         layers.Activation(\"tanh\"),         strides=(1, 1),         use_bias=False,         use_bn=True,     )     # At this point, we have an output which has the same shape as the input, (32, 32, 1).     # We will use a Cropping2D layer to make it (28, 28, 1).     x = layers.Cropping2D((2, 2))(x)      g_model = keras.models.Model(noise, x, name=\"generator\")     return g_model   g_model = get_generator_model() g_model.summary()"},{"path":"https://keras.posit.co/articles/examples/wgan_gp.html","id":"create-the-wgan-gp-model","dir":"Articles > Examples","previous_headings":"","what":"Create the WGAN-GP model","title":"WGAN-GP overriding `Model.train_step`","text":"Now defined generator discriminator, ’s time implement WGAN-GP model. also override train_step training.","code":"class WGAN(keras.Model):     def __init__(         self,         discriminator,         generator,         latent_dim,         discriminator_extra_steps=3,         gp_weight=10.0,     ):         super().__init__()         self.discriminator = discriminator         self.generator = generator         self.latent_dim = latent_dim         self.d_steps = discriminator_extra_steps         self.gp_weight = gp_weight      def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):         super().compile()         self.d_optimizer = d_optimizer         self.g_optimizer = g_optimizer         self.d_loss_fn = d_loss_fn         self.g_loss_fn = g_loss_fn      def gradient_penalty(self, batch_size, real_images, fake_images):         \"\"\"Calculates the gradient penalty.          This loss is calculated on an interpolated image         and added to the discriminator loss.         \"\"\"         # Get the interpolated image         alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)         diff = fake_images - real_images         interpolated = real_images + alpha * diff          with tf.GradientTape() as gp_tape:             gp_tape.watch(interpolated)             # 1. Get the discriminator output for this interpolated image.             pred = self.discriminator(interpolated, training=True)          # 2. Calculate the gradients w.r.t to this interpolated image.         grads = gp_tape.gradient(pred, [interpolated])[0]         # 3. Calculate the norm of the gradients.         norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))         gp = tf.reduce_mean((norm - 1.0) ** 2)         return gp      def train_step(self, real_images):         if isinstance(real_images, tuple):             real_images = real_images[0]          # Get the batch size         batch_size = tf.shape(real_images)[0]          # For each batch, we are going to perform the         # following steps as laid out in the original paper:         # 1. Train the generator and get the generator loss         # 2. Train the discriminator and get the discriminator loss         # 3. Calculate the gradient penalty         # 4. Multiply this gradient penalty with a constant weight factor         # 5. Add the gradient penalty to the discriminator loss         # 6. Return the generator and discriminator losses as a loss dictionary          # Train the discriminator first. The original paper recommends training         # the discriminator for `x` more steps (typically 5) as compared to         # one step of the generator. Here we will train it for 3 extra steps         # as compared to 5 to reduce the training time.         for i in range(self.d_steps):             # Get the latent vector             random_latent_vectors = tf.random.normal(                 shape=(batch_size, self.latent_dim)             )             with tf.GradientTape() as tape:                 # Generate fake images from the latent vector                 fake_images = self.generator(                     random_latent_vectors, training=True                 )                 # Get the logits for the fake images                 fake_logits = self.discriminator(fake_images, training=True)                 # Get the logits for the real images                 real_logits = self.discriminator(real_images, training=True)                  # Calculate the discriminator loss using the fake and real image logits                 d_cost = self.d_loss_fn(                     real_img=real_logits, fake_img=fake_logits                 )                 # Calculate the gradient penalty                 gp = self.gradient_penalty(batch_size, real_images, fake_images)                 # Add the gradient penalty to the original discriminator loss                 d_loss = d_cost + gp * self.gp_weight              # Get the gradients w.r.t the discriminator loss             d_gradient = tape.gradient(                 d_loss, self.discriminator.trainable_variables             )             # Update the weights of the discriminator using the discriminator optimizer             self.d_optimizer.apply_gradients(                 zip(d_gradient, self.discriminator.trainable_variables)             )          # Train the generator         # Get the latent vector         random_latent_vectors = tf.random.normal(             shape=(batch_size, self.latent_dim)         )         with tf.GradientTape() as tape:             # Generate fake images using the generator             generated_images = self.generator(                 random_latent_vectors, training=True             )             # Get the discriminator logits for fake images             gen_img_logits = self.discriminator(generated_images, training=True)             # Calculate the generator loss             g_loss = self.g_loss_fn(gen_img_logits)          # Get the gradients w.r.t the generator loss         gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)         # Update the weights of the generator using the generator optimizer         self.g_optimizer.apply_gradients(             zip(gen_gradient, self.generator.trainable_variables)         )         return {\"d_loss\": d_loss, \"g_loss\": g_loss}"},{"path":"https://keras.posit.co/articles/examples/wgan_gp.html","id":"create-a-keras-callback-that-periodically-saves-generated-images","dir":"Articles > Examples","previous_headings":"","what":"Create a Keras callback that periodically saves generated images","title":"WGAN-GP overriding `Model.train_step`","text":"","code":"class GANMonitor(keras.callbacks.Callback):     def __init__(self, num_img=6, latent_dim=128):         self.num_img = num_img         self.latent_dim = latent_dim      def on_epoch_end(self, epoch, logs=None):         random_latent_vectors = tf.random.normal(             shape=(self.num_img, self.latent_dim)         )         generated_images = self.model.generator(random_latent_vectors)         generated_images = (generated_images * 127.5) + 127.5          for i in range(self.num_img):             img = generated_images[i].numpy()             img = keras.utils.array_to_img(img)             img.save(\"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))"},{"path":"https://keras.posit.co/articles/examples/wgan_gp.html","id":"train-the-end-to-end-model","dir":"Articles > Examples","previous_headings":"","what":"Train the end-to-end model","title":"WGAN-GP overriding `Model.train_step`","text":"Display last generated images: Example available HuggingFace.","code":"# Instantiate the optimizer for both networks # (learning_rate=0.0002, beta_1=0.5 are recommended) generator_optimizer = keras.optimizers.Adam(     learning_rate=0.0002, beta_1=0.5, beta_2=0.9 ) discriminator_optimizer = keras.optimizers.Adam(     learning_rate=0.0002, beta_1=0.5, beta_2=0.9 )   # Define the loss functions for the discriminator, # which should be (fake_loss - real_loss). # We will add the gradient penalty later to this loss function. def discriminator_loss(real_img, fake_img):     real_loss = tf.reduce_mean(real_img)     fake_loss = tf.reduce_mean(fake_img)     return fake_loss - real_loss   # Define the loss functions for the generator. def generator_loss(fake_img):     return -tf.reduce_mean(fake_img)   # Set the number of epochs for trainining. epochs = 20  # Instantiate the customer `GANMonitor` Keras callback. cbk = GANMonitor(num_img=3, latent_dim=noise_dim)  # Get the wgan model wgan = WGAN(     discriminator=d_model,     generator=g_model,     latent_dim=noise_dim,     discriminator_extra_steps=3, )  # Compile the wgan model wgan.compile(     d_optimizer=discriminator_optimizer,     g_optimizer=generator_optimizer,     g_loss_fn=generator_loss,     d_loss_fn=discriminator_loss, )  # Start training wgan.fit(train_images, batch_size=BATCH_SIZE, epochs=epochs, callbacks=[cbk]) from IPython.display import Image, display  display(Image(\"generated_img_0_19.png\")) display(Image(\"generated_img_1_19.png\")) display(Image(\"generated_img_2_19.png\"))"},{"path":"https://keras.posit.co/articles/examples/zero_dce.html","id":"introduction","dir":"Articles > Examples","previous_headings":"","what":"Introduction","title":"Zero-DCE for low-light image enhancement","text":"Zero-Reference Deep Curve Estimation Zero-DCE formulates low-light image enhancement task estimating image-specific tonal curve deep neural network. example, train lightweight deep network, DCE-Net, estimate pixel-wise high-order tonal curves dynamic range adjustment given image. Zero-DCE takes low-light image input produces high-order tonal curves output. curves used pixel-wise adjustment dynamic range input obtain enhanced image. curve estimation process done way maintains range enhanced image preserves contrast neighboring pixels. curve estimation inspired curves adjustment used photo editing software Adobe Photoshop users can adjust points throughout image’s tonal range. Zero-DCE appealing relaxed assumptions regard reference images: require input/output image pairs training. achieved set carefully formulated non-reference loss functions, implicitly measure enhancement quality guide training network.","code":""},{"path":"https://keras.posit.co/articles/examples/zero_dce.html","id":"references","dir":"Articles > Examples","previous_headings":"Introduction","what":"References","title":"Zero-DCE for low-light image enhancement","text":"Zero-Reference Deep Curve Estimation Low-Light Image Enhancement Curves adjustment Adobe Photoshop","code":""},{"path":"https://keras.posit.co/articles/examples/zero_dce.html","id":"downloading-loldataset","dir":"Articles > Examples","previous_headings":"","what":"Downloading LOLDataset","title":"Zero-DCE for low-light image enhancement","text":"LoL Dataset created low-light image enhancement. provides 485 images training 15 testing. image pair dataset consists low-light input image corresponding well-exposed reference image. wget https://huggingface.co/datasets/geekyrakshit/LoL-Dataset/resolve/main/lol_dataset.zip unzip -q lol_dataset.zip && rm lol_dataset.zip","code":"import os import random import numpy as np from glob import glob from PIL import Image, ImageOps import matplotlib.pyplot as plt  import keras as keras from keras import layers  import tensorflow as tf"},{"path":"https://keras.posit.co/articles/examples/zero_dce.html","id":"creating-a-tensorflow-dataset","dir":"Articles > Examples","previous_headings":"","what":"Creating a TensorFlow Dataset","title":"Zero-DCE for low-light image enhancement","text":"use 300 low-light images LoL Dataset training set training, use remaining 185 low-light images validation. resize images size 256 x 256 used training validation. Note order train DCE-Net, require corresponding enhanced images.","code":"IMAGE_SIZE = 256 BATCH_SIZE = 16 MAX_TRAIN_IMAGES = 400   def load_data(image_path):     image = tf.io.read_file(image_path)     image = tf.image.decode_png(image, channels=3)     image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])     image = image / 255.0     return image   def data_generator(low_light_images):     dataset = tf.data.Dataset.from_tensor_slices((low_light_images))     dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)     dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)     return dataset   train_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[     :MAX_TRAIN_IMAGES ] val_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[     MAX_TRAIN_IMAGES: ] test_low_light_images = sorted(glob(\"./lol_dataset/eval15/low/*\"))   train_dataset = data_generator(train_low_light_images) val_dataset = data_generator(val_low_light_images)  print(\"Train Dataset:\", train_dataset) print(\"Validation Dataset:\", val_dataset)"},{"path":"https://keras.posit.co/articles/examples/zero_dce.html","id":"the-zero-dce-framework","dir":"Articles > Examples","previous_headings":"","what":"The Zero-DCE Framework","title":"Zero-DCE for low-light image enhancement","text":"goal DCE-Net estimate set best-fitting light-enhancement curves (LE-curves) given input image. framework maps pixels input’s RGB channels applying curves iteratively obtain final enhanced image.","code":""},{"path":"https://keras.posit.co/articles/examples/zero_dce.html","id":"understanding-light-enhancement-curves","dir":"Articles > Examples","previous_headings":"The Zero-DCE Framework","what":"Understanding light-enhancement curves","title":"Zero-DCE for low-light image enhancement","text":"ligh-enhancement curve kind curve can map low-light image enhanced version automatically, self-adaptive curve parameters solely dependent input image. designing curve, three objectives taken account: pixel value enhanced image normalized range [0,1], order avoid information loss induced overflow truncation. monotonous, preserve contrast neighboring pixels. shape curve simple possible, curve differentiable allow backpropagation. light-enhancement curve separately applied three RGB channels instead solely illumination channel. three-channel adjustment can better preserve inherent color reduce risk -saturation.","code":""},{"path":"https://keras.posit.co/articles/examples/zero_dce.html","id":"dce-net","dir":"Articles > Examples","previous_headings":"The Zero-DCE Framework","what":"DCE-Net","title":"Zero-DCE for low-light image enhancement","text":"DCE-Net lightweight deep neural network learns mapping input image best-fitting curve parameter maps. input DCE-Net low-light image outputs set pixel-wise curve parameter maps corresponding higher-order curves. plain CNN seven convolutional layers symmetrical concatenation. layer consists 32 convolutional kernels size 3×3 stride 1 followed ReLU activation function. last convolutional layer followed Tanh activation function, produces 24 parameter maps 8 iterations, iteration requires three curve parameter maps three channels.","code":"def build_dce_net():     input_img = keras.Input(shape=[None, None, 3])     conv1 = layers.Conv2D(         32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"     )(input_img)     conv2 = layers.Conv2D(         32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"     )(conv1)     conv3 = layers.Conv2D(         32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"     )(conv2)     conv4 = layers.Conv2D(         32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"     )(conv3)     int_con1 = layers.Concatenate(axis=-1)([conv4, conv3])     conv5 = layers.Conv2D(         32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"     )(int_con1)     int_con2 = layers.Concatenate(axis=-1)([conv5, conv2])     conv6 = layers.Conv2D(         32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"     )(int_con2)     int_con3 = layers.Concatenate(axis=-1)([conv6, conv1])     x_r = layers.Conv2D(         24, (3, 3), strides=(1, 1), activation=\"tanh\", padding=\"same\"     )(int_con3)     return keras.Model(inputs=input_img, outputs=x_r)"},{"path":"https://keras.posit.co/articles/examples/zero_dce.html","id":"loss-functions","dir":"Articles > Examples","previous_headings":"","what":"Loss functions","title":"Zero-DCE for low-light image enhancement","text":"enable zero-reference learning DCE-Net, use set differentiable zero-reference losses allow us evaluate quality enhanced images.","code":""},{"path":"https://keras.posit.co/articles/examples/zero_dce.html","id":"color-constancy-loss","dir":"Articles > Examples","previous_headings":"Loss functions","what":"Color constancy loss","title":"Zero-DCE for low-light image enhancement","text":"color constancy loss used correct potential color deviations enhanced image.","code":"def color_constancy_loss(x):     mean_rgb = tf.reduce_mean(x, axis=(1, 2), keepdims=True)     mr, mg, mb = (         mean_rgb[:, :, :, 0],         mean_rgb[:, :, :, 1],         mean_rgb[:, :, :, 2],     )     d_rg = tf.square(mr - mg)     d_rb = tf.square(mr - mb)     d_gb = tf.square(mb - mg)     return tf.sqrt(tf.square(d_rg) + tf.square(d_rb) + tf.square(d_gb))"},{"path":"https://keras.posit.co/articles/examples/zero_dce.html","id":"exposure-loss","dir":"Articles > Examples","previous_headings":"Loss functions","what":"Exposure loss","title":"Zero-DCE for low-light image enhancement","text":"restrain -/-exposed regions, use exposure control loss. measures distance average intensity value local region preset well-exposedness level (set 0.6).","code":"def exposure_loss(x, mean_val=0.6):     x = tf.reduce_mean(x, axis=3, keepdims=True)     mean = tf.nn.avg_pool2d(x, ksize=16, strides=16, padding=\"VALID\")     return tf.reduce_mean(tf.square(mean - mean_val))"},{"path":"https://keras.posit.co/articles/examples/zero_dce.html","id":"illumination-smoothness-loss","dir":"Articles > Examples","previous_headings":"Loss functions","what":"Illumination smoothness loss","title":"Zero-DCE for low-light image enhancement","text":"preserve monotonicity relations neighboring pixels, illumination smoothness loss added curve parameter map.","code":"def illumination_smoothness_loss(x):     batch_size = tf.shape(x)[0]     h_x = tf.shape(x)[1]     w_x = tf.shape(x)[2]     count_h = (tf.shape(x)[2] - 1) * tf.shape(x)[3]     count_w = tf.shape(x)[2] * (tf.shape(x)[3] - 1)     h_tv = tf.reduce_sum(tf.square((x[:, 1:, :, :] - x[:, : h_x - 1, :, :])))     w_tv = tf.reduce_sum(tf.square((x[:, :, 1:, :] - x[:, :, : w_x - 1, :])))     batch_size = tf.cast(batch_size, dtype=tf.float32)     count_h = tf.cast(count_h, dtype=tf.float32)     count_w = tf.cast(count_w, dtype=tf.float32)     return 2 * (h_tv / count_h + w_tv / count_w) / batch_size"},{"path":"https://keras.posit.co/articles/examples/zero_dce.html","id":"spatial-consistency-loss","dir":"Articles > Examples","previous_headings":"Loss functions","what":"Spatial consistency loss","title":"Zero-DCE for low-light image enhancement","text":"spatial consistency loss encourages spatial coherence enhanced image preserving contrast neighboring regions across input image enhanced version.","code":"class SpatialConsistencyLoss(keras.losses.Loss):     def __init__(self, **kwargs):         super().__init__(reduction=\"none\")          self.left_kernel = tf.constant(             [[[[0, 0, 0]], [[-1, 1, 0]], [[0, 0, 0]]]], dtype=tf.float32         )         self.right_kernel = tf.constant(             [[[[0, 0, 0]], [[0, 1, -1]], [[0, 0, 0]]]], dtype=tf.float32         )         self.up_kernel = tf.constant(             [[[[0, -1, 0]], [[0, 1, 0]], [[0, 0, 0]]]], dtype=tf.float32         )         self.down_kernel = tf.constant(             [[[[0, 0, 0]], [[0, 1, 0]], [[0, -1, 0]]]], dtype=tf.float32         )      def call(self, y_true, y_pred):         original_mean = tf.reduce_mean(y_true, 3, keepdims=True)         enhanced_mean = tf.reduce_mean(y_pred, 3, keepdims=True)         original_pool = tf.nn.avg_pool2d(             original_mean, ksize=4, strides=4, padding=\"VALID\"         )         enhanced_pool = tf.nn.avg_pool2d(             enhanced_mean, ksize=4, strides=4, padding=\"VALID\"         )          d_original_left = tf.nn.conv2d(             original_pool,             self.left_kernel,             strides=[1, 1, 1, 1],             padding=\"SAME\",         )         d_original_right = tf.nn.conv2d(             original_pool,             self.right_kernel,             strides=[1, 1, 1, 1],             padding=\"SAME\",         )         d_original_up = tf.nn.conv2d(             original_pool, self.up_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"         )         d_original_down = tf.nn.conv2d(             original_pool,             self.down_kernel,             strides=[1, 1, 1, 1],             padding=\"SAME\",         )          d_enhanced_left = tf.nn.conv2d(             enhanced_pool,             self.left_kernel,             strides=[1, 1, 1, 1],             padding=\"SAME\",         )         d_enhanced_right = tf.nn.conv2d(             enhanced_pool,             self.right_kernel,             strides=[1, 1, 1, 1],             padding=\"SAME\",         )         d_enhanced_up = tf.nn.conv2d(             enhanced_pool, self.up_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"         )         d_enhanced_down = tf.nn.conv2d(             enhanced_pool,             self.down_kernel,             strides=[1, 1, 1, 1],             padding=\"SAME\",         )          d_left = tf.square(d_original_left - d_enhanced_left)         d_right = tf.square(d_original_right - d_enhanced_right)         d_up = tf.square(d_original_up - d_enhanced_up)         d_down = tf.square(d_original_down - d_enhanced_down)         return d_left + d_right + d_up + d_down"},{"path":"https://keras.posit.co/articles/examples/zero_dce.html","id":"deep-curve-estimation-model","dir":"Articles > Examples","previous_headings":"Loss functions","what":"Deep curve estimation model","title":"Zero-DCE for low-light image enhancement","text":"implement Zero-DCE framework Keras subclassed model.","code":"class ZeroDCE(keras.Model):     def __init__(self, **kwargs):         super().__init__(**kwargs)         self.dce_model = build_dce_net()      def compile(self, learning_rate, **kwargs):         super().compile(**kwargs)         self.optimizer = keras.optimizers.Adam(learning_rate=learning_rate)         self.spatial_constancy_loss = SpatialConsistencyLoss(reduction=\"none\")         self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")         self.illumination_smoothness_loss_tracker = keras.metrics.Mean(             name=\"illumination_smoothness_loss\"         )         self.spatial_constancy_loss_tracker = keras.metrics.Mean(             name=\"spatial_constancy_loss\"         )         self.color_constancy_loss_tracker = keras.metrics.Mean(             name=\"color_constancy_loss\"         )         self.exposure_loss_tracker = keras.metrics.Mean(name=\"exposure_loss\")      @property     def metrics(self):         return [             self.total_loss_tracker,             self.illumination_smoothness_loss_tracker,             self.spatial_constancy_loss_tracker,             self.color_constancy_loss_tracker,             self.exposure_loss_tracker,         ]      def get_enhanced_image(self, data, output):         r1 = output[:, :, :, :3]         r2 = output[:, :, :, 3:6]         r3 = output[:, :, :, 6:9]         r4 = output[:, :, :, 9:12]         r5 = output[:, :, :, 12:15]         r6 = output[:, :, :, 15:18]         r7 = output[:, :, :, 18:21]         r8 = output[:, :, :, 21:24]         x = data + r1 * (tf.square(data) - data)         x = x + r2 * (tf.square(x) - x)         x = x + r3 * (tf.square(x) - x)         enhanced_image = x + r4 * (tf.square(x) - x)         x = enhanced_image + r5 * (tf.square(enhanced_image) - enhanced_image)         x = x + r6 * (tf.square(x) - x)         x = x + r7 * (tf.square(x) - x)         enhanced_image = x + r8 * (tf.square(x) - x)         return enhanced_image      def call(self, data):         dce_net_output = self.dce_model(data)         return self.get_enhanced_image(data, dce_net_output)      def compute_losses(self, data, output):         enhanced_image = self.get_enhanced_image(data, output)         loss_illumination = 200 * illumination_smoothness_loss(output)         loss_spatial_constancy = tf.reduce_mean(             self.spatial_constancy_loss(enhanced_image, data)         )         loss_color_constancy = 5 * tf.reduce_mean(             color_constancy_loss(enhanced_image)         )         loss_exposure = 10 * tf.reduce_mean(exposure_loss(enhanced_image))         total_loss = (             loss_illumination             + loss_spatial_constancy             + loss_color_constancy             + loss_exposure         )          return {             \"total_loss\": total_loss,             \"illumination_smoothness_loss\": loss_illumination,             \"spatial_constancy_loss\": loss_spatial_constancy,             \"color_constancy_loss\": loss_color_constancy,             \"exposure_loss\": loss_exposure,         }      def train_step(self, data):         with tf.GradientTape() as tape:             output = self.dce_model(data)             losses = self.compute_losses(data, output)          gradients = tape.gradient(             losses[\"total_loss\"], self.dce_model.trainable_weights         )         self.optimizer.apply_gradients(             zip(gradients, self.dce_model.trainable_weights)         )          self.total_loss_tracker.update_state(losses[\"total_loss\"])         self.illumination_smoothness_loss_tracker.update_state(             losses[\"illumination_smoothness_loss\"]         )         self.spatial_constancy_loss_tracker.update_state(             losses[\"spatial_constancy_loss\"]         )         self.color_constancy_loss_tracker.update_state(             losses[\"color_constancy_loss\"]         )         self.exposure_loss_tracker.update_state(losses[\"exposure_loss\"])          return {metric.name: metric.result() for metric in self.metrics}      def test_step(self, data):         output = self.dce_model(data)         losses = self.compute_losses(data, output)          self.total_loss_tracker.update_state(losses[\"total_loss\"])         self.illumination_smoothness_loss_tracker.update_state(             losses[\"illumination_smoothness_loss\"]         )         self.spatial_constancy_loss_tracker.update_state(             losses[\"spatial_constancy_loss\"]         )         self.color_constancy_loss_tracker.update_state(             losses[\"color_constancy_loss\"]         )         self.exposure_loss_tracker.update_state(losses[\"exposure_loss\"])          return {metric.name: metric.result() for metric in self.metrics}      def save_weights(         self, filepath, overwrite=True, save_format=None, options=None     ):         \"\"\"While saving the weights, we simply save the weights of the DCE-Net\"\"\"         self.dce_model.save_weights(             filepath,             overwrite=overwrite,             save_format=save_format,             options=options,         )      def load_weights(         self, filepath, by_name=False, skip_mismatch=False, options=None     ):         \"\"\"While loading the weights, we simply load the weights of the DCE-Net\"\"\"         self.dce_model.load_weights(             filepath=filepath,             by_name=by_name,             skip_mismatch=skip_mismatch,             options=options,         )"},{"path":"https://keras.posit.co/articles/examples/zero_dce.html","id":"training","dir":"Articles > Examples","previous_headings":"","what":"Training","title":"Zero-DCE for low-light image enhancement","text":"","code":"zero_dce_model = ZeroDCE() zero_dce_model.compile(learning_rate=1e-4) history = zero_dce_model.fit(     train_dataset, validation_data=val_dataset, epochs=100 )   def plot_result(item):     plt.plot(history.history[item], label=item)     plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)     plt.xlabel(\"Epochs\")     plt.ylabel(item)     plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)     plt.legend()     plt.grid()     plt.show()   plot_result(\"total_loss\") plot_result(\"illumination_smoothness_loss\") plot_result(\"spatial_constancy_loss\") plot_result(\"color_constancy_loss\") plot_result(\"exposure_loss\")"},{"path":"https://keras.posit.co/articles/examples/zero_dce.html","id":"inference","dir":"Articles > Examples","previous_headings":"","what":"Inference","title":"Zero-DCE for low-light image enhancement","text":"","code":"def plot_results(images, titles, figure_size=(12, 12)):     fig = plt.figure(figsize=figure_size)     for i in range(len(images)):         fig.add_subplot(1, len(images), i + 1).set_title(titles[i])         _ = plt.imshow(images[i])         plt.axis(\"off\")     plt.show()   def infer(original_image):     image = keras.utils.img_to_array(original_image)     image = image.astype(\"float32\") / 255.0     image = np.expand_dims(image, axis=0)     output_image = zero_dce_model(image)     output_image = tf.cast((output_image[0, :, :, :] * 255), dtype=np.uint8)     output_image = Image.fromarray(output_image.numpy())     return output_image"},{"path":"https://keras.posit.co/articles/examples/zero_dce.html","id":"inference-on-test-images","dir":"Articles > Examples","previous_headings":"Inference","what":"Inference on test images","title":"Zero-DCE for low-light image enhancement","text":"compare test images LOLDataset enhanced MIRNet images enhanced via PIL.ImageOps.autocontrast() function. can use trained model hosted Hugging Face Hub try demo Hugging Face Spaces.","code":"for val_image_file in test_low_light_images:     original_image = Image.open(val_image_file)     enhanced_image = infer(original_image)     plot_results(         [original_image, ImageOps.autocontrast(original_image), enhanced_image],         [\"Original\", \"PIL Autocontrast\", \"Enhanced\"],         (20, 12),     )"},{"path":"https://keras.posit.co/articles/functional_api.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"The Functional API","text":"","code":"library(keras3)"},{"path":"https://keras.posit.co/articles/functional_api.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"The Functional API","text":"Keras functional API way create models flexible sequential API. functional API can handle models non-linear topology, shared layers, even multiple inputs outputs. main idea deep learning model usually directed acyclic graph (DAG) layers. functional API way build graphs layers. Consider following model: basic graph three layers. build model using functional API, start creating input node: shape data set 784-dimensional vector. batch size always omitted since shape sample specified. , example, image input shape (32, 32, 3), use: inputs returned contains information shape dtype input data feed model. ’s shape: ’s dtype: create new node graph layers calling layer inputs object: “layer call” action like drawing arrow “inputs” layer created. ’re “passing” inputs dense layer, get x output. Let’s add layers graph layers: point, can create Model specifying inputs outputs graph layers: Let’s check model summary looks like: can also plot model graph: , optionally, display input output shapes layer plotted graph: figure code almost identical. code version, connection arrows replaced call operation. “graph layers” intuitive mental image deep learning model, functional API way create models closely mirrors .","code":"(input: 784-dimensional vectors)        ↧ [Dense (64 units, relu activation)]        ↧ [Dense (64 units, relu activation)]        ↧ [Dense (10 units, softmax activation)]        ↧ (output: logits of a probability distribution over 10 classes) inputs <- layer_input(shape = c(784)) # Just for demonstration purposes. img_inputs <- layer_input(shape = c(32, 32, 3)) shape(inputs) ## shape(NA, 784) inputs$dtype ## [1] \"float32\" dense <- layer_dense(units = 64, activation=\"relu\") x <- dense(inputs) outputs <- x %>%   layer_dense(units = 64, activation = \"relu\") %>%   layer_dense(units = 10) model <- keras_model(inputs = inputs, outputs = outputs, name = \"mnist_model\") summary(model) ## Model: \"mnist_model\" ## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓ ## ┃ Layer (type)                      ┃ Output Shape                ┃    Param # ┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩ ## │ input_layer (InputLayer)          │ (None, 784)                 │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ dense (Dense)                     │ (None, 64)                  │     50,240 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ dense_2 (Dense)                   │ (None, 64)                  │      4,160 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ dense_1 (Dense)                   │ (None, 10)                  │        650 │ ## └───────────────────────────────────┴─────────────────────────────┴────────────┘ ##  Total params: 55,050 (215.04 KB) ##  Trainable params: 55,050 (215.04 KB) ##  Non-trainable params: 0 (0.00 B) plot(model) plot(model, show_shapes = TRUE)"},{"path":"https://keras.posit.co/articles/functional_api.html","id":"training-evaluation-and-inference","dir":"Articles","previous_headings":"","what":"Training, evaluation, and inference","title":"The Functional API","text":"Training, evaluation, inference work exactly way models built using functional API Sequential models. Model class offers built-training loop (fit() method) built-evaluation loop (evaluate() method). Note can easily customize loops implement training routines beyond supervised learning (e.g. GANs). , load MNIST image data, reshape vectors, fit model data (monitoring performance validation split), evaluate model test data: reading, see training evaluation guide.","code":"c(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_mnist()  x_train <- array_reshape(x_train, c(60000, 784)) / 255 x_test <- array_reshape(x_test, c(10000, 784)) / 255  model %>% compile(   loss = loss_sparse_categorical_crossentropy(from_logits = TRUE),   optimizer = optimizer_rmsprop(),   metrics = \"accuracy\" )  history <- model %>% fit(     x_train, y_train, batch_size = 64, epochs = 2, validation_split = 0.2 ) ## Epoch 1/2 ## 750/750 - 1s - 1ms/step - accuracy: 0.9015 - loss: 0.3513 - val_accuracy: 0.9442 - val_loss: 0.1921 ## Epoch 2/2 ## 750/750 - 1s - 839us/step - accuracy: 0.9508 - loss: 0.1643 - val_accuracy: 0.9581 - val_loss: 0.1423 test_scores <- model %>% evaluate(x_test, y_test, verbose=2) ## 313/313 - 0s - 355us/step - accuracy: 0.9578 - loss: 0.1338 cat(\"Test loss:\", test_scores[\"loss\"]) ## Test loss: ## Error in cat(\"Test loss:\", test_scores[\"loss\"]): argument 2 (type 'list') cannot be handled by 'cat' cat(\"Test accuracy:\", test_scores[\"compile_metrics\"]) ## Test accuracy: ## Error in cat(\"Test accuracy:\", test_scores[\"compile_metrics\"]): argument 2 (type 'list') cannot be handled by 'cat'"},{"path":"https://keras.posit.co/articles/functional_api.html","id":"save-and-serialize","dir":"Articles","previous_headings":"","what":"Save and serialize","title":"The Functional API","text":"Saving model serialization work way models built using functional API Sequential models. standard way save functional model call model.save() save entire model single file. can later recreate model file, even code built model longer available. saved file includes : - model architecture - model weight values (learned training) - model training config, (passed compile()) - optimizer state, (restart training left ) details, read model serialization & saving guide.","code":"model$save(\"my_model.keras\") rm(model) # Recreate the exact same model purely from the file: model <- keras$models$load_model(\"my_model.keras\")"},{"path":"https://keras.posit.co/articles/functional_api.html","id":"use-the-same-graph-of-layers-to-define-multiple-models","dir":"Articles","previous_headings":"","what":"Use the same graph of layers to define multiple models","title":"The Functional API","text":"functional API, models created specifying inputs outputs graph layers. means single graph layers can used generate multiple models. example , use stack layers instantiate two models: encoder model turns image inputs 16-dimensional vectors, end--end autoencoder model training. , decoding architecture strictly symmetrical encoding architecture, output shape input shape (28, 28, 1). reverse conv_2d layer conv_2d_transpose layer, reverse max_pooling_2d layer upsampling_2d layer.","code":"encoder_input <- layer_input(shape = c(28, 28, 1), name=\"img\") encoder_output <- encoder_input %>%   layer_conv_2d(16, 3, activation = \"relu\") %>%   layer_conv_2d(32, 3, activation = \"relu\") %>%   layer_max_pooling_2d(3) %>%   layer_conv_2d(32, 3, activation = \"relu\") %>%   layer_conv_2d(16, 3, activation = \"relu\") %>%   layer_global_max_pooling_2d()  encoder <- keras_model(encoder_input, encoder_output, name=\"encoder\") summary(encoder) ## Model: \"encoder\" ## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓ ## ┃ Layer (type)                      ┃ Output Shape                ┃    Param # ┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩ ## │ img (InputLayer)                  │ (None, 28, 28, 1)           │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_3 (Conv2D)                 │ (None, 26, 26, 16)          │        160 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_2 (Conv2D)                 │ (None, 24, 24, 32)          │      4,640 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ max_pooling2d (MaxPooling2D)      │ (None, 8, 8, 32)            │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_1 (Conv2D)                 │ (None, 6, 6, 32)            │      9,248 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d (Conv2D)                   │ (None, 4, 4, 16)            │      4,624 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ global_max_pooling2d              │ (None, 16)                  │          0 │ ## │ (GlobalMaxPooling2D)              │                             │            │ ## └───────────────────────────────────┴─────────────────────────────┴────────────┘ ##  Total params: 18,672 (72.94 KB) ##  Trainable params: 18,672 (72.94 KB) ##  Non-trainable params: 0 (0.00 B) decoder_output <- encoder_output %>%   layer_reshape(c(4, 4, 1)) %>%   layer_conv_2d_transpose(16, 3, activation = \"relu\") %>%   layer_conv_2d_transpose(32, 3, activation = \"relu\") %>%   layer_upsampling_2d(3) %>%   layer_conv_2d_transpose(16, 3, activation = \"relu\") %>%   layer_conv_2d_transpose(1, 3, activation = \"relu\")  autoencoder <- keras_model(encoder_input, decoder_output, name=\"autoencoder\") summary(autoencoder) ## Model: \"autoencoder\" ## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓ ## ┃ Layer (type)                      ┃ Output Shape                ┃    Param # ┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩ ## │ img (InputLayer)                  │ (None, 28, 28, 1)           │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_3 (Conv2D)                 │ (None, 26, 26, 16)          │        160 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_2 (Conv2D)                 │ (None, 24, 24, 32)          │      4,640 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ max_pooling2d (MaxPooling2D)      │ (None, 8, 8, 32)            │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_1 (Conv2D)                 │ (None, 6, 6, 32)            │      9,248 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d (Conv2D)                   │ (None, 4, 4, 16)            │      4,624 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ global_max_pooling2d              │ (None, 16)                  │          0 │ ## │ (GlobalMaxPooling2D)              │                             │            │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ reshape (Reshape)                 │ (None, 4, 4, 1)             │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_transpose_3                │ (None, 6, 6, 16)            │        160 │ ## │ (Conv2DTranspose)                 │                             │            │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_transpose_2                │ (None, 8, 8, 32)            │      4,640 │ ## │ (Conv2DTranspose)                 │                             │            │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ up_sampling2d (UpSampling2D)      │ (None, 24, 24, 32)          │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_transpose_1                │ (None, 26, 26, 16)          │      4,624 │ ## │ (Conv2DTranspose)                 │                             │            │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_transpose                  │ (None, 28, 28, 1)           │        145 │ ## │ (Conv2DTranspose)                 │                             │            │ ## └───────────────────────────────────┴─────────────────────────────┴────────────┘ ##  Total params: 28,241 (110.32 KB) ##  Trainable params: 28,241 (110.32 KB) ##  Non-trainable params: 0 (0.00 B)"},{"path":"https://keras.posit.co/articles/functional_api.html","id":"all-models-are-callable-just-like-layers","dir":"Articles","previous_headings":"","what":"All models are callable, just like layers","title":"The Functional API","text":"can treat model layer invoking Input output another layer. calling model aren’t just reusing architecture model, ’re also reusing weights. see action, ’s different take autoencoder example creates encoder model, decoder model, chains two calls obtain autoencoder model: can see, model can nested: model can contain sub-models (since model just like layer). common use case model nesting ensembling. example, ’s ensemble set models single model averages predictions:","code":"encoder_input <- layer_input(shape = c(28, 28, 1), name=\"img\") encoder_output <- encoder_input %>%   layer_conv_2d(16, 3, activation = \"relu\") %>%   layer_conv_2d(32, 3, activation = \"relu\") %>%   layer_max_pooling_2d(3) %>%   layer_conv_2d(32, 3, activation = \"relu\") %>%   layer_conv_2d(16, 3, activation = \"relu\") %>%   layer_global_max_pooling_2d()  encoder <- keras_model(encoder_input, encoder_output, name=\"encoder\") summary(encoder) ## Model: \"encoder\" ## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓ ## ┃ Layer (type)                      ┃ Output Shape                ┃    Param # ┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩ ## │ img (InputLayer)                  │ (None, 28, 28, 1)           │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_7 (Conv2D)                 │ (None, 26, 26, 16)          │        160 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_6 (Conv2D)                 │ (None, 24, 24, 32)          │      4,640 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ max_pooling2d_1 (MaxPooling2D)    │ (None, 8, 8, 32)            │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_5 (Conv2D)                 │ (None, 6, 6, 32)            │      9,248 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_4 (Conv2D)                 │ (None, 4, 4, 16)            │      4,624 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ global_max_pooling2d_1            │ (None, 16)                  │          0 │ ## │ (GlobalMaxPooling2D)              │                             │            │ ## └───────────────────────────────────┴─────────────────────────────┴────────────┘ ##  Total params: 18,672 (72.94 KB) ##  Trainable params: 18,672 (72.94 KB) ##  Non-trainable params: 0 (0.00 B) decoder_input <- layer_input(shape = c(16), name = \"encoded_img\") decoder_output <- decoder_input %>%   layer_reshape(c(4, 4, 1)) %>%   layer_conv_2d_transpose(16, 3, activation = \"relu\") %>%   layer_conv_2d_transpose(32, 3, activation = \"relu\") %>%   layer_upsampling_2d(3) %>%   layer_conv_2d_transpose(16, 3, activation = \"relu\") %>%   layer_conv_2d_transpose(1, 3, activation = \"relu\")  decoder <- keras_model(decoder_input, decoder_output, name = \"decoder\") summary(decoder) ## Model: \"decoder\" ## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓ ## ┃ Layer (type)                      ┃ Output Shape                ┃    Param # ┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩ ## │ encoded_img (InputLayer)          │ (None, 16)                  │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ reshape_1 (Reshape)               │ (None, 4, 4, 1)             │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_transpose_7                │ (None, 6, 6, 16)            │        160 │ ## │ (Conv2DTranspose)                 │                             │            │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_transpose_6                │ (None, 8, 8, 32)            │      4,640 │ ## │ (Conv2DTranspose)                 │                             │            │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ up_sampling2d_1 (UpSampling2D)    │ (None, 24, 24, 32)          │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_transpose_5                │ (None, 26, 26, 16)          │      4,624 │ ## │ (Conv2DTranspose)                 │                             │            │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_transpose_4                │ (None, 28, 28, 1)           │        145 │ ## │ (Conv2DTranspose)                 │                             │            │ ## └───────────────────────────────────┴─────────────────────────────┴────────────┘ ##  Total params: 9,569 (37.38 KB) ##  Trainable params: 9,569 (37.38 KB) ##  Non-trainable params: 0 (0.00 B) autoencoder_input <- layer_input(shape = c(28, 28, 1), name = \"img\") encoded_img <- encoder(autoencoder_input) decoded_img <- decoder(encoded_img) autoencoder = keras_model(autoencoder_input, decoded_img, name=\"autoencoder\") summary(autoencoder) ## Model: \"autoencoder\" ## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓ ## ┃ Layer (type)                      ┃ Output Shape                ┃    Param # ┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩ ## │ img (InputLayer)                  │ (None, 28, 28, 1)           │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ encoder (Functional)              │ (None, 16)                  │     18,672 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ decoder (Functional)              │ (None, 28, 28, 1)           │      9,569 │ ## └───────────────────────────────────┴─────────────────────────────┴────────────┘ ##  Total params: 28,241 (110.32 KB) ##  Trainable params: 28,241 (110.32 KB) ##  Non-trainable params: 0 (0.00 B) get_model <- function() {   inputs <- layer_input(shape = 128)   outputs <- layer_dense(inputs, 1)   keras_model(inputs, outputs) }  model1 <- get_model() model2 <- get_model() model3 <- get_model()  inputs <- layer_input(shape = 128) y1 <- model1(inputs) y2 <- model2(inputs) y3 <- model3(inputs) outputs <- layer_average(list(y1, y2, y3)) ensemble_model <- keras_model(inputs = inputs, outputs = outputs)"},{"path":[]},{"path":"https://keras.posit.co/articles/functional_api.html","id":"models-with-multiple-inputs-and-outputs","dir":"Articles","previous_headings":"Manipulate complex graph topologies","what":"Models with multiple inputs and outputs","title":"The Functional API","text":"functional API makes easy manipulate multiple inputs outputs. handled Sequential API. example, ’re building system ranking customer issue tickets priority routing correct department, model three inputs: title ticket (text input), text body ticket (text input), tags added user (categorical input) model two outputs: priority score 0 1 (scalar sigmoid output), department handle ticket (softmax output set departments). can build model lines functional API: Now plot model: compiling model, can assign different losses output. can even assign different weights loss – modulate contribution total training loss. Since output layers different names, also specify losses loss weights corresponding layer names: Train model passing lists NumPy arrays inputs targets: calling fit Dataset object, yield either list lists like list(list(title_data, body_data, tags_data), list(priority_targets, dept_targets)) list named lists like list(list(title = title_data, body = body_data, tags = tags_data), list(priority = priority_targets, department = dept_targets)). detailed explanation, refer training evaluation guide.","code":"num_tags <- 12  # Number of unique issue tags num_words <-   10000  # Size of vocabulary obtained when preprocessing text data num_departments <- 4  # Number of departments for predictions  title_input <- # Variable-length sequence of ints   layer_input(shape(NA), name = \"title\") body_input <-  # Variable-length sequence of ints   layer_input(shape(NA), name = \"body\") tags_input <-  # Binary vectors of size `num_tags`   layer_input(shape = num_tags, name = \"tags\")  # Embed each word in the title into a 64-dimensional vector title_features <- layer_embedding(title_input, num_words, 64) # Embed each word in the text into a 64-dimensional vector body_features <- layer_embedding(body_input, num_words, 64)  # Reduce sequence of embedded words in the title # into a single 128-dimensional vector title_features <- layer_lstm(title_features, 128) # Reduce sequence of embedded words in the body # into a single 32-dimensional vector body_features <- layer_lstm(body_features, 32)  # Merge all available features into a single large vector via concatenation x <- layer_concatenate(title_features, body_features, tags_input)  # Stick a logistic regression for priority prediction on top of the features priority_pred <- layer_dense(x, 1, name = \"priority\")  # Stick a department classifier on top of the features department_pred <- layer_dense(x, num_departments, name = \"department\")  # Instantiate an end-to-end model predicting both priority and department model <- keras_model(   inputs = list(title_input, body_input, tags_input),   outputs = list(priority = priority_pred, department = department_pred) ) plot(model, show_shapes = TRUE) model %>% compile(     optimizer = optimizer_rmsprop(1e-3),     loss = list(         loss_binary_crossentropy(from_logits = TRUE),         loss_categorical_crossentropy(from_logits = TRUE)     ),     loss_weights = c(1.0, 0.2) ) model %>% compile(     optimizer = optimizer_rmsprop(1e-3),     loss = list(         priority = loss_binary_crossentropy(from_logits = TRUE),         department = loss_categorical_crossentropy(from_logits = TRUE)     ),     loss_weights = list(priority = 1.0, department = 0.2) ) # Dummy input data title_data <- random_integer(c(1280, 10), 0, num_words) body_data <- random_integer(c(1280, 100), 0, num_words) tags_data <- random_integer(c(1280, num_tags), 0, 2)  # Dummy target data priority_targets <- random_normal(c(1280, 1)) dept_targets <- random_integer(c(1280, num_departments), 0, 2)  model %>% fit(     list(title = title_data, body = body_data, tags = tags_data),     list(priority = priority_targets, department = dept_targets),     epochs=2,     batch_size=32, ) ## Epoch 1/2 ## 40/40 - 2s - 58ms/step - loss: 0.3948 ## Epoch 2/2 ## 40/40 - 1s - 16ms/step - loss: 0.1971"},{"path":"https://keras.posit.co/articles/functional_api.html","id":"a-toy-resnet-model","dir":"Articles","previous_headings":"Manipulate complex graph topologies","what":"A toy ResNet model","title":"The Functional API","text":"addition models multiple inputs outputs, functional API makes easy manipulate non-linear connectivity topologies – models layers connected sequentially, Sequential API handle. common use case residual connections. Let’s build toy ResNet model CIFAR10 demonstrate : Plot model: Now train model:","code":"inputs <- layer_input(shape = c(32, 32, 3), name=\"img\") block_1_output <- inputs %>%   layer_conv_2d(filters = 32, kernel_size = 3, activation = \"relu\") %>%   layer_conv_2d(filters = 64, kernel_size = 3, activation = \"relu\") %>%   layer_max_pooling_2d(pool_size = 3)  block_2_output <- block_1_output %>%   layer_conv_2d(filters = 32, kernel_size = 3, activation = \"relu\", padding = \"same\") %>%   layer_conv_2d(filters = 64, kernel_size = 3, activation = \"relu\", padding = \"same\") %>%   layer_add(block_1_output)  block_3_output <- block_2_output %>%   layer_conv_2d(filters = 64, kernel_size = 3, activation = \"relu\", padding = \"same\") %>%   layer_conv_2d(filters = 64, kernel_size = 3, activation = \"relu\", padding = \"same\") %>%   layer_add(block_2_output)  outputs <- block_3_output %>%   layer_conv_2d(64, 3, activation = \"relu\") %>%   layer_global_average_pooling_2d() %>%   layer_dense(256, activation = \"relu\") %>%   layer_dropout(0.5) %>%   layer_dense(10)  model <- keras_model(inputs, outputs, name=\"toy_resnet\") summary(model) ## Model: \"toy_resnet\" ## ┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┓ ## ┃ Layer (type)          ┃ Output Shape      ┃  Param # ┃ Connected to          ┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━┩ ## │ img (InputLayer)      │ (None, 32, 32, 3) │        0 │ -                     │ ## ├───────────────────────┼───────────────────┼──────────┼───────────────────────┤ ## │ conv2d_9 (Conv2D)     │ (None, 30, 30,    │      896 │ img[0][0]             │ ## │                       │ 32)               │          │                       │ ## ├───────────────────────┼───────────────────┼──────────┼───────────────────────┤ ## │ conv2d_8 (Conv2D)     │ (None, 28, 28,    │   18,496 │ conv2d_9[0][0]        │ ## │                       │ 64)               │          │                       │ ## ├───────────────────────┼───────────────────┼──────────┼───────────────────────┤ ## │ max_pooling2d_2       │ (None, 9, 9, 64)  │        0 │ conv2d_8[0][0]        │ ## │ (MaxPooling2D)        │                   │          │                       │ ## ├───────────────────────┼───────────────────┼──────────┼───────────────────────┤ ## │ conv2d_11 (Conv2D)    │ (None, 9, 9, 32)  │   18,464 │ max_pooling2d_2[0][0] │ ## ├───────────────────────┼───────────────────┼──────────┼───────────────────────┤ ## │ conv2d_10 (Conv2D)    │ (None, 9, 9, 64)  │   18,496 │ conv2d_11[0][0]       │ ## ├───────────────────────┼───────────────────┼──────────┼───────────────────────┤ ## │ add (Add)             │ (None, 9, 9, 64)  │        0 │ conv2d_10[0][0],      │ ## │                       │                   │          │ max_pooling2d_2[0][0] │ ## ├───────────────────────┼───────────────────┼──────────┼───────────────────────┤ ## │ conv2d_13 (Conv2D)    │ (None, 9, 9, 64)  │   36,928 │ add[0][0]             │ ## ├───────────────────────┼───────────────────┼──────────┼───────────────────────┤ ## │ conv2d_12 (Conv2D)    │ (None, 9, 9, 64)  │   36,928 │ conv2d_13[0][0]       │ ## ├───────────────────────┼───────────────────┼──────────┼───────────────────────┤ ## │ add_1 (Add)           │ (None, 9, 9, 64)  │        0 │ conv2d_12[0][0],      │ ## │                       │                   │          │ add[0][0]             │ ## ├───────────────────────┼───────────────────┼──────────┼───────────────────────┤ ## │ conv2d_14 (Conv2D)    │ (None, 7, 7, 64)  │   36,928 │ add_1[0][0]           │ ## ├───────────────────────┼───────────────────┼──────────┼───────────────────────┤ ## │ global_average_pooli… │ (None, 64)        │        0 │ conv2d_14[0][0]       │ ## │ (GlobalAveragePoolin… │                   │          │                       │ ## ├───────────────────────┼───────────────────┼──────────┼───────────────────────┤ ## │ dense_7 (Dense)       │ (None, 256)       │   16,640 │ global_average_pooli… │ ## ├───────────────────────┼───────────────────┼──────────┼───────────────────────┤ ## │ dropout (Dropout)     │ (None, 256)       │        0 │ dense_7[0][0]         │ ## ├───────────────────────┼───────────────────┼──────────┼───────────────────────┤ ## │ dense_6 (Dense)       │ (None, 10)        │    2,570 │ dropout[0][0]         │ ## └───────────────────────┴───────────────────┴──────────┴───────────────────────┘ ##  Total params: 186,346 (727.91 KB) ##  Trainable params: 186,346 (727.91 KB) ##  Non-trainable params: 0 (0.00 B) plot(model, show_shapes = TRUE) c(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_cifar10()  x_train <- x_train / 255.0 x_test <- x_test / 255.0  model %>% compile(     optimizer = optimizer_rmsprop(1e-3),     loss = loss_sparse_categorical_crossentropy(from_logits = TRUE),     metrics = \"acc\" ) # We restrict the data to the first 1000 samples so as to limit the # guide render time. # Try to train on the entire dataset until convergence! model %>% fit(     x_train[1:1000,,,],     y_train[1:1000,],     batch_size=64,     epochs=1,     validation_split=0.2 ) ## 13/13 - 2s - 119ms/step - acc: 0.1200 - loss: 2.3110 - val_acc: 0.1150 - val_loss: 2.3192"},{"path":"https://keras.posit.co/articles/functional_api.html","id":"shared-layers","dir":"Articles","previous_headings":"","what":"Shared layers","title":"The Functional API","text":"Another good use functional API models use shared layers. Shared layers layer instances reused multiple times model – learn features correspond multiple paths graph--layers. Shared layers often used encode inputs similar spaces (say, two different pieces text feature similar vocabulary). enable sharing information across different inputs, make possible train model less data. given word seen one inputs, benefit processing inputs pass shared layer. share layer functional API, call layer instance multiple times. instance, ’s Embedding layer shared across two different text inputs:","code":"# Embedding for 1000 unique words mapped to 128-dimensional vectors shared_embedding <- layer_embedding(input_dim = 1000, output_dim = 128)  # Variable-length sequence of integers text_input_a <- layer_input(shape = shape(NULL), dtype=\"int32\")  # Variable-length sequence of integers text_input_b <- layer_input(shape = shape(NULL), dtype=\"int32\")  # Reuse the same layer to encode both inputs encoded_input_a <- shared_embedding(text_input_a) encoded_input_b <- shared_embedding(text_input_b)"},{"path":"https://keras.posit.co/articles/functional_api.html","id":"extract-and-reuse-nodes-in-the-graph-of-layers","dir":"Articles","previous_headings":"","what":"Extract and reuse nodes in the graph of layers","title":"The Functional API","text":"graph layers manipulating static data structure, can accessed inspected. able plot functional models images. also means can access activations intermediate layers (“nodes” graph) reuse elsewhere – useful something like feature extraction. Let’s look example. VGG19 model weights pretrained ImageNet: intermediate activations model, obtained querying graph data structure: Use features create new feature-extraction model returns values intermediate layer activations: comes handy tasks like neural style transfer, among things.","code":"vgg19 <- application_vgg19() features_list <- lapply(vgg19$layers, function(x) x$output) feat_extraction_model <- keras_model(inputs = vgg19$input, outputs = features_list)  img <- random_normal(c(1, 224, 224, 3)) extracted_features <- feat_extraction_model(img)"},{"path":"https://keras.posit.co/articles/functional_api.html","id":"extend-the-api-using-custom-layers","dir":"Articles","previous_headings":"","what":"Extend the API using custom layers","title":"The Functional API","text":"keras includes wide range built-layers, example: Convolutional layers: conv_1d, conv_2d, conv_3d, conv_2d_transpose Pooling layers: max_pooling_1d, max_pooling_2d, max_pooling_3d, average_pooling_3d RNN layers: gru, lstm, conv_lstm_2d batch_normalization, dropout, embedding, etc. don’t find need, ’s easy extend API creating layers. layers subclass Layer class implement: call method, specifies computation done layer. build method, creates weights layer (just style convention since can create weights initialize, well). learn creating layers scratch, read custom layers models guide. following basic implementation keras.layers.Dense: serialization support custom layer, define get_config() method returns constructor arguments layer instance: Optionally, implement class method from_config(cls, config) used recreating layer instance given config dictionary. default implementation from_config :","code":"custom_dense <- new_layer_class(   \"CustomDense\",   initialize = function(units = 32) {     super$initialize()     self$units <- as.integer(units)   },   build = function(input_shape) {     self$w <- self$add_weight(       shape = shape(input_shape[[2]], self$units),       initializer = \"random_normal\",       trainable = TRUE,     )     self$b <- self$add_weight(       shape = shape(self$units),       initializer=\"random_normal\",       trainable = TRUE     )   },   call = function(inputs) {     op_matmul(inputs, self$w) + self$b   } )  inputs <- layer_input(c(4)) outputs = custom_dense(inputs, 10)  model <- keras_model(inputs, outputs) custom_dense <- new_layer_class(   \"CustomDense\",   initialize = function(units = 32, ...) {     super$initialize()     self$units <- as.integer(units)   },   build = function(input_shape) {     self$w <- self$add_weight(       shape = shape(input_shape[[2]], self$units),       initializer = \"random_normal\",       trainable = TRUE,     )     self$b <- self$add_weight(       shape = shape(self$units),       initializer=\"random_normal\",       trainable = TRUE     )   },   call = function(inputs) {     op_matmul(inputs, self$w) + self$b   },   get_config = function() {     list(units = self$units)   } )  inputs <- layer_input(c(4)) outputs = custom_dense(inputs, 10)  model <- keras_model(inputs, outputs) config <- get_config(model)  new_model <- from_config(config, custom_objects=list(CustomDense=custom_dense)) ## Could not deserialize class 'CustomDense' because its parent module ## <r-globalenv> cannot be imported. Full object config: {'module': ## '<r-globalenv>', 'class_name': 'CustomDense', 'config': {'units': 10}, ## 'registered_name': 'CustomDense', 'build_config': {'input_shape': [None, 4]}, ## 'name': 'custom_dense_1', 'inbound_nodes': [{'args': [{'class_name': ## '__keras_tensor__', 'config': {'shape': [None, 4], 'dtype': 'float32', ## 'keras_history': ['input_layer_10', 0, 0]}}], 'kwargs': {}}]} from_config <- function(cls, config) {   do.call(cls, config) }"},{"path":"https://keras.posit.co/articles/functional_api.html","id":"when-to-use-the-functional-api","dir":"Articles","previous_headings":"","what":"When to use the functional API","title":"The Functional API","text":"use Keras functional API create new model, just subclass Model class directly? general, functional API higher-level, easier safer, number features subclassed models support. However, model subclassing provides greater flexibility building models easily expressible directed acyclic graphs layers. example, implement Tree-RNN functional API subclass Model directly. -depth look differences functional API model subclassing, read Symbolic Imperative APIs TensorFlow 2.0?.","code":""},{"path":"https://keras.posit.co/articles/functional_api.html","id":"functional-api-strengths","dir":"Articles","previous_headings":"When to use the functional API","what":"Functional API strengths:","title":"The Functional API","text":"following properties also true Sequential models (also data structures), true subclassed models (Python bytecode, data structures).","code":""},{"path":"https://keras.posit.co/articles/functional_api.html","id":"less-verbose","dir":"Articles","previous_headings":"When to use the functional API > Functional API strengths:","what":"Less verbose","title":"The Functional API","text":"super$initialize(...), call = function(self, ...):, etc. Compare: subclassed version:","code":"inputs <- layer_input(shape=shape(32)) outputs <- inputs %>%   layer_dense(64, activation = \"relu\") %>%   layer_dense(10) mlp <- keras_model(inputs, outputs) MLP <- new_model_class(   \"MLP\",   initialize = function(...) {     super$initialize(...)     self$dense_1 <- layer_dense(units = 64, activation = \"relu\")     self$dense_2 <- layer_dense(units = 10)   },   call = function(inputs) {     x <- self$dense_1(inputs)     self$dense_2(x)   } )  # Instantiate the model. mlp = MLP() # Necessary to create the model's state. # The model doesn't have a state until it's called at least once. out <- mlp(op_zeros(c(1, 32)))"},{"path":"https://keras.posit.co/articles/functional_api.html","id":"model-validation-while-defining-its-connectivity-graph","dir":"Articles","previous_headings":"When to use the functional API > Functional API strengths:","what":"Model validation while defining its connectivity graph","title":"The Functional API","text":"functional API, input specification (shape dtype) created advance (using Input). Every time call layer, layer checks specification passed matches assumptions, raise helpful error message . guarantees model can build functional API run. debugging – convergence-related debugging – happens statically model construction execution time. similar type checking compiler.","code":""},{"path":"https://keras.posit.co/articles/functional_api.html","id":"a-functional-model-is-plottable-and-inspectable","dir":"Articles","previous_headings":"When to use the functional API > Functional API strengths:","what":"A functional model is plottable and inspectable","title":"The Functional API","text":"can plot model graph, can easily access intermediate nodes graph. example, extract reuse activations intermediate layers (seen previous example):","code":"features_list <- lapply(vgg19$layers, function(x) x$output) feat_extraction_model <- keras_model(inputs = vgg19$input, outputs = features_list)"},{"path":"https://keras.posit.co/articles/functional_api.html","id":"a-functional-model-can-be-serialized-or-cloned","dir":"Articles","previous_headings":"When to use the functional API > Functional API strengths:","what":"A functional model can be serialized or cloned","title":"The Functional API","text":"functional model data structure rather piece code, safely serializable can saved single file allows recreate exact model without access original code. See serialization & saving guide. serialize subclassed model, necessary implementer specify get_config() from_config() method model level.","code":""},{"path":[]},{"path":"https://keras.posit.co/articles/functional_api.html","id":"it-does-not-support-dynamic-architectures","dir":"Articles","previous_headings":"When to use the functional API > Functional API weakness:","what":"It does not support dynamic architectures","title":"The Functional API","text":"functional API treats models DAGs layers. true deep learning architectures, – example, recursive networks Tree RNNs follow assumption implemented functional API.","code":""},{"path":"https://keras.posit.co/articles/functional_api.html","id":"mix-and-match-api-styles","dir":"Articles","previous_headings":"","what":"Mix-and-match API styles","title":"The Functional API","text":"Choosing functional API Model subclassing isn’t binary decision restricts one category models. models keras API can interact , whether ’re Sequential models, functional models, subclassed models written scratch. can always use functional model Sequential model part subclassed model layer: can use subclassed layer model functional API long implements call method follows one following patterns: call(self, inputs, ...) – inputs tensor nested structure tensors (e.g. list tensors), ... non-tensor arguments (non-inputs). call(self, inputs, training=None, ...) – training boolean indicating whether layer behave training mode inference mode. call(self, inputs, mask=NULL, ...) – mask boolean mask tensor (useful RNNs, instance). call(self, inputs, training=NULL, mask=NULL, ...) – course, can masking training-specific behavior time. Additionally, implement get_config method custom Layer model, functional models create still serializable cloneable. ’s quick example custom RNN, written scratch, used functional model:","code":"units <- 32 timesteps <- 10 input_dim <- 5  # Define a Functional model inputs <- layer_input(shape(NULL, units)) outputs <- inputs %>%   layer_global_average_pooling_1d() %>%   layer_dense(units = 1)  model <- keras_model(inputs, outputs)  custom_rnn <- new_layer_class(   \"custom_rnn\",   initialize = function(...) {     super$initialize(...)     self$units <- units     self$projection_1 <- layer_dense(units = units, activation = \"tanh\")     self$projection_2 <- layer_dense(units = units, activation = \"tanh\")     self$classifier <- model   },   call = function(inputs, ...) {     outputs <- list()     state <- op_zeros(c(shape(inputs)[[1]], self$units))     for (t in 1:(shape(inputs)[[2]])) {       x <- inputs[, t, ]       h <- self$projection_1(x)       y <- h + self$projection_2(state)       state <- y       outputs[[t]] <- y     }     features <- op_stack(outputs, axis = 2)     self$classifier(features)   } )  rnn_model <- custom_rnn() out <- rnn_model(op_zeros(c(1, timesteps, input_dim))) units <- 32 timesteps <- 10 input_dim <- 5 batch_size <- 16  custom_rnn <- new_layer_class(   \"custom_rnn\",   initialize = function(...) {     super$initialize(...)     self$units <- units     self$projection_1 <- layer_dense(units = units, activation = \"tanh\")     self$projection_2 <- layer_dense(units = units, activation = \"tanh\")     self$classifier <- layer_dense(units = 1)   },   call = function(inputs, ...) {     outputs <- list()     state <- op_zeros(c(shape(inputs)[[1]], self$units))     for (t in 1:(shape(inputs)[[2]])) {       x <- inputs[, t, ]       h <- self$projection_1(x)       y <- h + self$projection_2(state)       state <- y       outputs[[t]] <- y     }     features <- op_stack(outputs, axis = 2)     self$classifier(features)   } )  # Note that you specify a static batch size for the inputs with the `batch_shape` # arg, because the inner computation of `custom_rnn` requires a static batch size # (when you create the `state` zeros tensor). inputs <- layer_input(batch_shape = as.integer(c(batch_size, timesteps, input_dim))) outputs <- inputs %>%   layer_conv_1d(filters = 32, kernel_size = 3) %>%   custom_rnn()  model <- keras_model(inputs, outputs) out <- model(op_zeros(c(1, 10, 32))) ## Input 0 of layer \"functional_23\" is incompatible with the layer: expected ## shape=(None, 10, 5), found shape=(1, 10, 32)"},{"path":"https://keras.posit.co/articles/getting_started_with_keras_core.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Getting started with Keras Core","text":"Keras Core full implementation Keras API works TensorFlow, JAX, PyTorch interchangeably. notebook walk key Keras Core workflows. First, let’s install Keras Core:","code":"library(keras3) keras3::install_keras()"},{"path":"https://keras.posit.co/articles/getting_started_with_keras_core.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Getting started with Keras Core","text":"’re going using JAX backend – can edit string \"tensorflow\", \"jax\", \"torch\", whole document run just ! entire guide backend-agnostic.","code":"config_set_backend(\"tensorflow\") ## Error in config_set_backend(\"tensorflow\"): The keras backend must be set before keras has inititialized. Please restart the R session # config_set_backend(\"jax\")"},{"path":"https://keras.posit.co/articles/getting_started_with_keras_core.html","id":"a-first-example-a-mnist-convnet","dir":"Articles","previous_headings":"","what":"A first example: A MNIST convnet","title":"Getting started with Keras Core","text":"Let’s start Hello World ML: training convnet classify MNIST digits. ’s data: ’s model. Different model-building options Keras offers include: Sequential API (use ) Functional API (typical) Writing models via subclassing (advanced use cases) ’s model summary: use compile() method specify optimizer, loss function, metrics monitor. Note JAX TensorFlow backends, XLA compilation turned default. Let’s train evaluate model. ’ll set aside validation split 15% data training monitor generalization unseen data. training, saving model end epoch. can also save model latest state like : reload like : Next, can query predictions class probabilities predict(): ’s basics!","code":"# Load the data and split it between train and test sets c(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_mnist()  # Scale images to the [0, 1] range x_train %<>% {. / 255} x_test %<>% {. / 255}  # Make sure images have shape (28, 28, 1) x_train %<>% array_reshape(c(dim(.), 1)) x_test %<>% array_reshape(c(dim(.), 1))  str(rlang::dots_list( .named = TRUE,   x_train, y_train, x_test, y_test)) ## List of 4 ##  $ x_train: num [1:60000, 1:28, 1:28, 1] 0 0 0 0 0 0 0 0 0 0 ... ##  $ y_train: int [1:60000(1d)] 5 0 4 1 9 2 1 3 1 4 ... ##  $ x_test : num [1:10000, 1:28, 1:28, 1] 0 0 0 0 0 0 0 0 0 0 ... ##  $ y_test : int [1:10000(1d)] 7 2 1 0 4 1 4 9 5 9 ... cat(\"Training samples:\", nrow(x_train), \"\\n\") ## Training samples: 60000 cat(\"    Test samples:\", nrow(x_test), \"\\n\") ##     Test samples: 10000 # Model parameters num_classes <- 10 input_shape <- c(28, 28, 1)  model <- keras_model_sequential(input_shape = input_shape) %>%   layer_conv_2d(64, kernel_size = c(3, 3), activation = 'relu') %>%   layer_conv_2d(64, kernel_size = c(3, 3), activation = 'relu') %>%   layer_max_pooling_2d(pool_size = c(2, 2)) %>%   layer_conv_2d(128, kernel_size = c(3, 3), activation = 'relu') %>%   layer_conv_2d(128, kernel_size = c(3, 3), activation = 'relu') %>%   layer_global_average_pooling_2d() %>%   layer_dropout(0.5) %>%   layer_dense(num_classes, activation = 'softmax') model ## Model: \"sequential\" ## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓ ## ┃ Layer (type)                      ┃ Output Shape                ┃    Param # ┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩ ## │ conv2d_3 (Conv2D)                 │ (None, 26, 26, 64)          │        640 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_2 (Conv2D)                 │ (None, 24, 24, 64)          │     36,928 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ max_pooling2d (MaxPooling2D)      │ (None, 12, 12, 64)          │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_1 (Conv2D)                 │ (None, 10, 10, 128)         │     73,856 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d (Conv2D)                   │ (None, 8, 8, 128)           │    147,584 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ global_average_pooling2d          │ (None, 128)                 │          0 │ ## │ (GlobalAveragePooling2D)          │                             │            │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ dropout (Dropout)                 │ (None, 128)                 │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ dense (Dense)                     │ (None, 10)                  │      1,290 │ ## └───────────────────────────────────┴─────────────────────────────┴────────────┘ ##  Total params: 260,298 (1016.79 KB) ##  Trainable params: 260,298 (1016.79 KB) ##  Non-trainable params: 0 (0.00 B) model |> compile(   loss = loss_sparse_categorical_crossentropy(),   optimizer = optimizer_adam(learning_rate = 1e-3),   metrics = c(metric_sparse_categorical_accuracy(name = \"acc\")) ) batch_size <- 128 epochs <- 2  model |> fit(   x_train,   y_train,   batch_size = batch_size,   epochs = epochs,   validation_split = 0.15,   callbacks = c(     callback_model_checkpoint(filepath = \"model_at_epoch_{epoch}.keras\"),     callback_early_stopping(monitor = \"val_loss\", patience = 2)   ) ) ## Epoch 1/2 ## 399/399 - 66s - 166ms/step - acc: 0.7364 - loss: 0.7758 - val_acc: 0.9496 - val_loss: 0.1792 ## Epoch 2/2 ## 399/399 - 63s - 157ms/step - acc: 0.9337 - loss: 0.2230 - val_acc: 0.9748 - val_loss: 0.0858 score <- model |> evaluate(x_test, y_test, verbose=0) model$save(\"final_model.keras\") model <- keras$saving$load_model(\"final_model.keras\") predictions <- model |> predict(x_test) ## 313/313 - 3s - 9ms/step"},{"path":"https://keras.posit.co/articles/getting_started_with_keras_core.html","id":"writing-cross-framework-custom-components","dir":"Articles","previous_headings":"","what":"Writing cross-framework custom components","title":"Getting started with Keras Core","text":"Keras Core enables write custom Layers, Models, Metrics, Losses, Optimizers work across TensorFlow, JAX, PyTorch codebase. Let’s take look custom layers first. ’re already familiar writing custom layers tf.keras – well, nothing changed. Except one thing: instead using functions tf namespace, use functions keras3::k_*. k_* namespace contains: implementation NumPy API, e.g. k_stack k_matmul. set neural network specific ops absent NumPy, k_conv k_binary_crossentropy. Let’s make custom Dense layer works backends: Next, let’s make custom Dropout layer relies keras.random namespace: Next, let’s write custom subclassed model uses two custom layers: Let’s compile fit :","code":"layer_my_dense <- new_layer_class(   classname = \"MyDense\",    initialize = function(self, units, activation = NULL, name = NULL) {     super$initialize(name = name)     self$units <- units     self$activation <- keras$activations$get(activation)   },    build = function(self, input_shape) {     input_dim <- tail(input_shape, 1)     self$w <- self$add_weight(       shape = c(input_dim, self$units),       initializer = initializer_glorot_normal(),       name = \"kernel\",       trainable = TRUE     )      self$b <- self$add_weight(       shape = c(self$units),       initializer = initializer_zeros(),       name = \"bias\",       trainable = TRUE     )   },    call = function(self, inputs) {     # Use k_* Ops to create backend-agnostic layers/metrics/etc.     x <- (inputs %*% self$w) + self$b     self$activation(x)   } ) layer_my_dropout <- new_layer_class(   classname = \"MyDropout\",    initialize = function(self, rate, name = NULL) {     super$initialize(name = name)     self$rate <- rate     # Use seed_generator for managing RNG state.     # It is a state element and its seed variable is     # tracked as part of `layer$variables`.     self$seed_generator <- random_seed_generator(1337)   },    call = function(self, inputs) {     # Use `k_random` for random ops.     random_dropout(inputs, self$rate, seed = self$seed_generator)   } ) model_my_model <- new_model_class(   classname = \"MyModel\",    initialize = function(self, num_classes) {     super$initialize()     self$conv_base <- keras_model_sequential() %>%       layer_conv_2d(64, kernel_size = c(3, 3), activation = \"relu\") %>%       layer_conv_2d(64, kernel_size = c(3, 3), activation = \"relu\") %>%       layer_max_pooling_2d(pool_size = c(2, 2)) %>%       layer_conv_2d(128, kernel_size = c(3, 3), activation = \"relu\") %>%       layer_conv_2d(128, kernel_size = c(3, 3), activation = \"relu\") %>%       layer_global_average_pooling_2d()     self$dp <- layer_my_dropout(rate = 0.5)     self$dense <- layer_my_dense(units = num_classes, activation = \"softmax\")   },    call = function(self, x) {     x <- self$conv_base(x)   } ) model <- model_my_model(num_classes = 10) model |> compile(   loss = loss_sparse_categorical_crossentropy(),   optimizer = optimizer_adam(learning_rate = 1e-3),   metrics = list(     metric_sparse_categorical_accuracy(name = \"acc\")   ) )  model |> fit(   x_train,   y_train,   batch_size = batch_size,   epochs = 1,  # For speed   validation_split = 0.15 ) ## 399/399 - 68s - 171ms/step - acc: 0.6040 - loss: nan - val_acc: 0.8556 - val_loss: nan"},{"path":"https://keras.posit.co/articles/getting_started_with_keras_core.html","id":"training-models-on-arbitrary-data-sources","dir":"Articles","previous_headings":"","what":"Training models on arbitrary data sources","title":"Getting started with Keras Core","text":"Keras models can trained evaluated wide variety data sources, independently backend ’re using. includes: R NumPy arrays Dataframes TensorFlow tf.data.Dataset objects (R package tfdatasets) PyTorch DataLoader objects work whether ’re using TensorFlow, JAX, PyTorch Keras backend. Let’s try PyTorch DataLoaders: Now let’s try tfdatasets:","code":"import torch  # Create a TensorDataset train_torch_dataset = torch.utils.data.TensorDataset(     torch.from_numpy(x_train), torch.from_numpy(y_train) ) val_torch_dataset = torch.utils.data.TensorDataset(     torch.from_numpy(x_test), torch.from_numpy(y_test) )  # Create a DataLoader train_dataloader = torch.utils.data.DataLoader(     train_torch_dataset, batch_size=batch_size, shuffle=True ) val_dataloader = torch.utils.data.DataLoader(     val_torch_dataset, batch_size=batch_size, shuffle=False )  model = MyModel(num_classes=10) model.compile(     loss=keras.losses.SparseCategoricalCrossentropy(),     optimizer=keras.optimizers.Adam(learning_rate=1e-3),     metrics=[         keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),     ], ) model.fit(train_dataloader, epochs=1, validation_data=val_dataloader) library(tensorflow) ## ## Attaching package: 'tensorflow' ## The following object is masked from 'package:keras': ## ##     set_random_seed library(tfdatasets)  train_dataset <- tensor_slices_dataset(list(x_train, y_train)) |>   dataset_batch(batch_size) %>%   dataset_prefetch(buffer_size = tf$data$AUTOTUNE)  test_dataset <- tensor_slices_dataset(list(x_test, y_test)) %>%   dataset_batch(batch_size) %>%   dataset_prefetch(buffer_size = tf$data$AUTOTUNE)  model <- model_my_model(num_classes = 10) model |> compile(   loss = loss_sparse_categorical_crossentropy(),   optimizer = optimizer_adam(learning_rate = 1e-3),   metrics = list(     metric_sparse_categorical_accuracy(name = \"acc\")   ) ) model |> fit(train_dataset, epochs = 1, validation_data = test_dataset) ## Error: The tensorflow backend is required for direct tensor input to models"},{"path":"https://keras.posit.co/articles/getting_started_with_keras_core.html","id":"further-reading","dir":"Articles","previous_headings":"","what":"Further reading","title":"Getting started with Keras Core","text":"concludes short overview new multi-backend capabilities Keras Core. Next, can learn :","code":""},{"path":"https://keras.posit.co/articles/getting_started_with_keras_core.html","id":"how-to-customize-what-happens-in-fit","dir":"Articles","previous_headings":"Further reading","what":"How to customize what happens in fit()","title":"Getting started with Keras Core","text":"Want implement non-standard training algorithm (e.g. GAN training routine) still want benefit power usability fit()? ’s really easy customize fit() support arbitrary use cases. Customizing happens fit() TensorFlow Customizing happens fit() JAX Customizing happens fit() PyTorch","code":""},{"path":"https://keras.posit.co/articles/getting_started_with_keras_core.html","id":"how-to-write-custom-training-loops","dir":"Articles","previous_headings":"","what":"How to write custom training loops","title":"Getting started with Keras Core","text":"Writing training loop scratch TensorFlow Writing training loop scratch JAX Writing training loop scratch PyTorch","code":""},{"path":"https://keras.posit.co/articles/getting_started_with_keras_core.html","id":"how-to-distribute-training","dir":"Articles","previous_headings":"","what":"How to distribute training","title":"Getting started with Keras Core","text":"Guide distributed training TensorFlow JAX distributed training example PyTorch distributed training example Enjoy library! 🚀","code":""},{"path":"https://keras.posit.co/articles/guides/custom_train_step_in_jax.html","id":"introduction","dir":"Articles > Guides","previous_headings":"","what":"Introduction","title":"Customizing what happens in `fit()` with JAX","text":"’re supervised learning, can use fit() everything works smoothly. need take control every little detail, can write training loop entirely scratch. need custom training algorithm, still want benefit convenient features fit(), callbacks, built-distribution support, step fusing? core principle Keras progressive disclosure complexity. always able get lower-level workflows gradual way. shouldn’t fall cliff high-level functionality doesn’t exactly match use case. able gain control small details retaining commensurate amount high-level convenience. need customize fit() , override training step function Model class. function called fit() every batch data. able call fit() usual – running learning algorithm. Note pattern prevent building models Functional API. can whether ’re building Sequential models, Functional API models, subclassed models. Let’s see works.","code":""},{"path":"https://keras.posit.co/articles/guides/custom_train_step_in_jax.html","id":"setup","dir":"Articles > Guides","previous_headings":"","what":"Setup","title":"Customizing what happens in `fit()` with JAX","text":"","code":"import os  # This guide can only be run with the JAX backend. os.environ[\"KERAS_BACKEND\"] = \"jax\"  import jax import keras as keras import numpy as np"},{"path":"https://keras.posit.co/articles/guides/custom_train_step_in_jax.html","id":"a-first-simple-example","dir":"Articles > Guides","previous_headings":"","what":"A first simple example","title":"Customizing what happens in `fit()` with JAX","text":"Let’s start simple example: create new class subclasses keras.Model. implement fully-stateless compute_loss_and_updates() method compute loss well updated values non-trainable variables model. Internally, calls stateless_call() built-compute_loss(). implement fully-stateless train_step() method compute current metric values (including loss) well updated values trainable variables, optimizer variables, metric variables. Note can also take account sample_weight argument : Unpacking data x, y, sample_weight = data Passing sample_weight compute_loss() Passing sample_weight alongside y y_pred metrics stateless_update_state() Let’s try :","code":"class CustomModel(keras.Model):     def compute_loss_and_updates(         self,         trainable_variables,         non_trainable_variables,         x,         y,         training=False,     ):         y_pred, non_trainable_variables = self.stateless_call(             trainable_variables,             non_trainable_variables,             x,             training=training,         )         loss = self.compute_loss(x, y, y_pred)         return loss, (y_pred, non_trainable_variables)      def train_step(self, state, data):         (             trainable_variables,             non_trainable_variables,             optimizer_variables,             metrics_variables,         ) = state         x, y = data          # Get the gradient function.         grad_fn = jax.value_and_grad(             self.compute_loss_and_updates, has_aux=True         )          # Compute the gradients.         (loss, (y_pred, non_trainable_variables)), grads = grad_fn(             trainable_variables,             non_trainable_variables,             x,             y,             training=True,         )          # Update trainable variables and optimizer variables.         (             trainable_variables,             optimizer_variables,         ) = self.optimizer.stateless_apply(             optimizer_variables, grads, trainable_variables         )          # Update metrics.         new_metrics_vars = []         for metric in self.metrics:             this_metric_vars = metrics_variables[                 len(new_metrics_vars) : len(new_metrics_vars)                 + len(metric.variables)             ]             if metric.name == \"loss\":                 this_metric_vars = metric.stateless_update_state(                     this_metric_vars, loss                 )             else:                 this_metric_vars = metric.stateless_update_state(                     this_metric_vars, y, y_pred                 )             logs = metric.stateless_result(this_metric_vars)             new_metrics_vars += this_metric_vars          # Return metric logs and updated state variables.         state = (             trainable_variables,             non_trainable_variables,             optimizer_variables,             new_metrics_vars,         )         return logs, state # Construct and compile an instance of CustomModel inputs = keras.Input(shape=(32,)) outputs = keras.layers.Dense(1)(inputs) model = CustomModel(inputs, outputs) model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])  # Just use `fit` as usual x = np.random.random((1000, 32)) y = np.random.random((1000, 1)) model.fit(x, y, epochs=3)"},{"path":"https://keras.posit.co/articles/guides/custom_train_step_in_jax.html","id":"going-lower-level","dir":"Articles > Guides","previous_headings":"","what":"Going lower-level","title":"Customizing what happens in `fit()` with JAX","text":"Naturally, just skip passing loss function compile(), instead everything manually train_step. Likewise metrics. ’s lower-level example, uses compile() configure optimizer:","code":"class CustomModel(keras.Model):     def __init__(self, *args, **kwargs):         super().__init__(*args, **kwargs)         self.loss_tracker = keras.metrics.Mean(name=\"loss\")         self.mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")         self.loss_fn = keras.losses.MeanSquaredError()      def compute_loss_and_updates(         self,         trainable_variables,         non_trainable_variables,         x,         y,         training=False,     ):         y_pred, non_trainable_variables = self.stateless_call(             trainable_variables,             non_trainable_variables,             x,             training=training,         )         loss = self.loss_fn(y, y_pred)         return loss, (y_pred, non_trainable_variables)      def train_step(self, state, data):         (             trainable_variables,             non_trainable_variables,             optimizer_variables,             metrics_variables,         ) = state         x, y = data          # Get the gradient function.         grad_fn = jax.value_and_grad(             self.compute_loss_and_updates, has_aux=True         )          # Compute the gradients.         (loss, (y_pred, non_trainable_variables)), grads = grad_fn(             trainable_variables,             non_trainable_variables,             x,             y,             training=True,         )          # Update trainable variables and optimizer variables.         (             trainable_variables,             optimizer_variables,         ) = self.optimizer.stateless_apply(             optimizer_variables, grads, trainable_variables         )          # Update metrics.         loss_tracker_vars = metrics_variables[             : len(self.loss_tracker.variables)         ]         mae_metric_vars = metrics_variables[len(self.loss_tracker.variables) :]          loss_tracker_vars = self.loss_tracker.stateless_update_state(             loss_tracker_vars, loss         )         mae_metric_vars = self.mae_metric.stateless_update_state(             mae_metric_vars, y, y_pred         )          logs = {}         logs[self.loss_tracker.name] = self.loss_tracker.stateless_result(             loss_tracker_vars         )         logs[self.mae_metric.name] = self.mae_metric.stateless_result(             mae_metric_vars         )          new_metrics_vars = loss_tracker_vars + mae_metric_vars          # Return metric logs and updated state variables.         state = (             trainable_variables,             non_trainable_variables,             optimizer_variables,             new_metrics_vars,         )         return logs, state      @property     def metrics(self):         # We list our `Metric` objects here so that `reset_states()` can be         # called automatically at the start of each epoch         # or at the start of `evaluate()`.         return [self.loss_tracker, self.mae_metric]   # Construct an instance of CustomModel inputs = keras.Input(shape=(32,)) outputs = keras.layers.Dense(1)(inputs) model = CustomModel(inputs, outputs)  # We don't passs a loss or metrics here. model.compile(optimizer=\"adam\")  # Just use `fit` as usual -- you can use callbacks, etc. x = np.random.random((1000, 32)) y = np.random.random((1000, 1)) model.fit(x, y, epochs=5)"},{"path":"https://keras.posit.co/articles/guides/custom_train_step_in_jax.html","id":"providing-your-own-evaluation-step","dir":"Articles > Guides","previous_headings":"","what":"Providing your own evaluation step","title":"Customizing what happens in `fit()` with JAX","text":"want calls model.evaluate()? override test_step exactly way. ’s looks like: ’s !","code":"class CustomModel(keras.Model):     def test_step(self, state, data):         # Unpack the data.         x, y = data         (             trainable_variables,             non_trainable_variables,             metrics_variables,         ) = state          # Compute predictions and loss.         y_pred, non_trainable_variables = self.stateless_call(             trainable_variables,             non_trainable_variables,             x,             training=False,         )         loss = self.compute_loss(x, y, y_pred)          # Update metrics.         new_metrics_vars = []         for metric in self.metrics:             this_metric_vars = metrics_variables[                 len(new_metrics_vars) : len(new_metrics_vars)                 + len(metric.variables)             ]             if metric.name == \"loss\":                 this_metric_vars = metric.stateless_update_state(                     this_metric_vars, loss                 )             else:                 this_metric_vars = metric.stateless_update_state(                     this_metric_vars, y, y_pred                 )             logs = metric.stateless_result(this_metric_vars)             new_metrics_vars += this_metric_vars          # Return metric logs and updated state variables.         state = (             trainable_variables,             non_trainable_variables,             new_metrics_vars,         )         return logs, state   # Construct an instance of CustomModel inputs = keras.Input(shape=(32,)) outputs = keras.layers.Dense(1)(inputs) model = CustomModel(inputs, outputs) model.compile(loss=\"mse\", metrics=[\"mae\"])  # Evaluate with our custom test_step x = np.random.random((1000, 32)) y = np.random.random((1000, 1)) model.evaluate(x, y)"},{"path":"https://keras.posit.co/articles/guides/custom_train_step_in_tensorflow.html","id":"introduction","dir":"Articles > Guides","previous_headings":"","what":"Introduction","title":"Customizing what happens in `fit()` with TensorFlow","text":"’re supervised learning, can use fit() everything works smoothly. need take control every little detail, can write training loop entirely scratch. need custom training algorithm, still want benefit convenient features fit(), callbacks, built-distribution support, step fusing? core principle Keras progressive disclosure complexity. always able get lower-level workflows gradual way. shouldn’t fall cliff high-level functionality doesn’t exactly match use case. able gain control small details retaining commensurate amount high-level convenience. need customize fit() , override training step function Model class. function called fit() every batch data. able call fit() usual – running learning algorithm. Note pattern prevent building models Functional API. can whether ’re building Sequential models, Functional API models, subclassed models. Let’s see works.","code":""},{"path":"https://keras.posit.co/articles/guides/custom_train_step_in_tensorflow.html","id":"setup","dir":"Articles > Guides","previous_headings":"","what":"Setup","title":"Customizing what happens in `fit()` with TensorFlow","text":"","code":"import os  # This guide can only be run with the TF backend. os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  import tensorflow as tf import keras as keras from keras import layers import numpy as np"},{"path":"https://keras.posit.co/articles/guides/custom_train_step_in_tensorflow.html","id":"a-first-simple-example","dir":"Articles > Guides","previous_headings":"","what":"A first simple example","title":"Customizing what happens in `fit()` with TensorFlow","text":"Let’s start simple example: create new class subclasses keras.Model. just override method train_step(self, data). return dictionary mapping metric names (including loss) current value. input argument data gets passed fit training data: pass NumPy arrays, calling fit(x, y, ...), data tuple (x, y) pass tf.data.Dataset, calling fit(dataset, ...), data gets yielded dataset batch. body train_step() method, implement regular training update, similar already familiar . Importantly, compute loss via self.compute_loss(), wraps loss(es) function(s) passed compile(). Similarly, call metric.update_state(y, y_pred) metrics self.metrics, update state metrics passed compile(), query results self.metrics end retrieve current value. Let’s try :","code":"class CustomModel(keras.Model):     def train_step(self, data):         # Unpack the data. Its structure depends on your model and         # on what you pass to `fit()`.         x, y = data          with tf.GradientTape() as tape:             y_pred = self(x, training=True)  # Forward pass             # Compute the loss value             # (the loss function is configured in `compile()`)             loss = self.compute_loss(y=y, y_pred=y_pred)          # Compute gradients         trainable_vars = self.trainable_variables         gradients = tape.gradient(loss, trainable_vars)          # Update weights         self.optimizer.apply(gradients, trainable_vars)          # Update metrics (includes the metric that tracks the loss)         for metric in self.metrics:             if metric.name == \"loss\":                 metric.update_state(loss)             else:                 metric.update_state(y, y_pred)          # Return a dict mapping metric names to current value         return {m.name: m.result() for m in self.metrics} # Construct and compile an instance of CustomModel inputs = keras.Input(shape=(32,)) outputs = keras.layers.Dense(1)(inputs) model = CustomModel(inputs, outputs) model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])  # Just use `fit` as usual x = np.random.random((1000, 32)) y = np.random.random((1000, 1)) model.fit(x, y, epochs=3)"},{"path":"https://keras.posit.co/articles/guides/custom_train_step_in_tensorflow.html","id":"going-lower-level","dir":"Articles > Guides","previous_headings":"","what":"Going lower-level","title":"Customizing what happens in `fit()` with TensorFlow","text":"Naturally, just skip passing loss function compile(), instead everything manually train_step. Likewise metrics. ’s lower-level example, uses compile() configure optimizer: start creating Metric instances track loss MAE score (__init__()). implement custom train_step() updates state metrics (calling update_state() ), query (via result()) return current average value, displayed progress bar pass callback. Note need call reset_states() metrics epoch! Otherwise calling result() return average since start training, whereas usually work per-epoch averages. Thankfully, framework can us: just list metric want reset metrics property model. model call reset_states() object listed beginning fit() epoch beginning call evaluate().","code":"class CustomModel(keras.Model):     def __init__(self, *args, **kwargs):         super().__init__(*args, **kwargs)         self.loss_tracker = keras.metrics.Mean(name=\"loss\")         self.mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")         self.loss_fn = keras.losses.MeanSquaredError()      def train_step(self, data):         x, y = data          with tf.GradientTape() as tape:             y_pred = self(x, training=True)  # Forward pass             # Compute our own loss             loss = self.loss_fn(y, y_pred)          # Compute gradients         trainable_vars = self.trainable_variables         gradients = tape.gradient(loss, trainable_vars)          # Update weights         self.optimizer.apply(gradients, trainable_vars)          # Compute our own metrics         self.loss_tracker.update_state(loss)         self.mae_metric.update_state(y, y_pred)         return {             \"loss\": self.loss_tracker.result(),             \"mae\": self.mae_metric.result(),         }      @property     def metrics(self):         # We list our `Metric` objects here so that `reset_states()` can be         # called automatically at the start of each epoch         # or at the start of `evaluate()`.         return [self.loss_tracker, self.mae_metric]   # Construct an instance of CustomModel inputs = keras.Input(shape=(32,)) outputs = keras.layers.Dense(1)(inputs) model = CustomModel(inputs, outputs)  # We don't passs a loss or metrics here. model.compile(optimizer=\"adam\")  # Just use `fit` as usual -- you can use callbacks, etc. x = np.random.random((1000, 32)) y = np.random.random((1000, 1)) model.fit(x, y, epochs=5)"},{"path":"https://keras.posit.co/articles/guides/custom_train_step_in_tensorflow.html","id":"supporting-sample_weight-class_weight","dir":"Articles > Guides","previous_headings":"","what":"Supporting sample_weight & class_weight","title":"Customizing what happens in `fit()` with TensorFlow","text":"may noticed first basic example didn’t make mention sample weighting. want support fit() arguments sample_weight class_weight, ’d simply following: Unpack sample_weight data argument Pass compute_loss & update_state (course, also just apply manually don’t rely compile() losses & metrics) ’s .","code":"class CustomModel(keras.Model):     def train_step(self, data):         # Unpack the data. Its structure depends on your model and         # on what you pass to `fit()`.         if len(data) == 3:             x, y, sample_weight = data         else:             sample_weight = None             x, y = data          with tf.GradientTape() as tape:             y_pred = self(x, training=True)  # Forward pass             # Compute the loss value.             # The loss function is configured in `compile()`.             loss = self.compute_loss(                 y=y,                 y_pred=y_pred,                 sample_weight=sample_weight,             )          # Compute gradients         trainable_vars = self.trainable_variables         gradients = tape.gradient(loss, trainable_vars)          # Update weights         self.optimizer.apply(gradients, trainable_vars)          # Update the metrics.         # Metrics are configured in `compile()`.         for metric in self.metrics:             if metric.name == \"loss\":                 metric.update_state(loss)             else:                 metric.update_state(y, y_pred, sample_weight=sample_weight)          # Return a dict mapping metric names to current value.         # Note that it will include the loss (tracked in self.metrics).         return {m.name: m.result() for m in self.metrics}   # Construct and compile an instance of CustomModel inputs = keras.Input(shape=(32,)) outputs = keras.layers.Dense(1)(inputs) model = CustomModel(inputs, outputs) model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])  # You can now use sample_weight argument x = np.random.random((1000, 32)) y = np.random.random((1000, 1)) sw = np.random.random((1000, 1)) model.fit(x, y, sample_weight=sw, epochs=3)"},{"path":"https://keras.posit.co/articles/guides/custom_train_step_in_tensorflow.html","id":"providing-your-own-evaluation-step","dir":"Articles > Guides","previous_headings":"","what":"Providing your own evaluation step","title":"Customizing what happens in `fit()` with TensorFlow","text":"want calls model.evaluate()? override test_step exactly way. ’s looks like:","code":"class CustomModel(keras.Model):     def test_step(self, data):         # Unpack the data         x, y = data         # Compute predictions         y_pred = self(x, training=False)         # Updates the metrics tracking the loss         loss = self.compute_loss(y=y, y_pred=y_pred)         # Update the metrics.         for metric in self.metrics:             if metric.name == \"loss\":                 metric.update_state(loss)             else:                 metric.update_state(y, y_pred)         # Return a dict mapping metric names to current value.         # Note that it will include the loss (tracked in self.metrics).         return {m.name: m.result() for m in self.metrics}   # Construct an instance of CustomModel inputs = keras.Input(shape=(32,)) outputs = keras.layers.Dense(1)(inputs) model = CustomModel(inputs, outputs) model.compile(loss=\"mse\", metrics=[\"mae\"])  # Evaluate with our custom test_step x = np.random.random((1000, 32)) y = np.random.random((1000, 1)) model.evaluate(x, y)"},{"path":"https://keras.posit.co/articles/guides/custom_train_step_in_tensorflow.html","id":"wrapping-up-an-end-to-end-gan-example","dir":"Articles > Guides","previous_headings":"","what":"Wrapping up: an end-to-end GAN example","title":"Customizing what happens in `fit()` with TensorFlow","text":"Let’s walk end--end example leverages everything just learned. Let’s consider: generator network meant generate 28x28x1 images. discriminator network meant classify 28x28x1 images two classes (“fake” “real”). One optimizer . loss function train discriminator. ’s feature-complete GAN class, overriding compile() use signature, implementing entire GAN algorithm 17 lines train_step: Let’s test-drive : ideas behind deep learning simple, implementation painful?","code":"# Create the discriminator discriminator = keras.Sequential(     [         keras.Input(shape=(28, 28, 1)),         layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),         layers.LeakyReLU(negative_slope=0.2),         layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),         layers.LeakyReLU(negative_slope=0.2),         layers.GlobalMaxPooling2D(),         layers.Dense(1),     ],     name=\"discriminator\", )  # Create the generator latent_dim = 128 generator = keras.Sequential(     [         keras.Input(shape=(latent_dim,)),         # We want to generate 128 coefficients to reshape into a 7x7x128 map         layers.Dense(7 * 7 * 128),         layers.LeakyReLU(negative_slope=0.2),         layers.Reshape((7, 7, 128)),         layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),         layers.LeakyReLU(negative_slope=0.2),         layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),         layers.LeakyReLU(negative_slope=0.2),         layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),     ],     name=\"generator\", ) class GAN(keras.Model):     def __init__(self, discriminator, generator, latent_dim):         super().__init__()         self.discriminator = discriminator         self.generator = generator         self.latent_dim = latent_dim         self.d_loss_tracker = keras.metrics.Mean(name=\"d_loss\")         self.g_loss_tracker = keras.metrics.Mean(name=\"g_loss\")         self.seed_generator = keras.random.SeedGenerator(1337)      @property     def metrics(self):         return [self.d_loss_tracker, self.g_loss_tracker]      def compile(self, d_optimizer, g_optimizer, loss_fn):         super().compile()         self.d_optimizer = d_optimizer         self.g_optimizer = g_optimizer         self.loss_fn = loss_fn      def train_step(self, real_images):         if isinstance(real_images, tuple):             real_images = real_images[0]         # Sample random points in the latent space         batch_size = tf.shape(real_images)[0]         random_latent_vectors = keras.random.normal(             shape=(batch_size, self.latent_dim), seed=self.seed_generator         )          # Decode them to fake images         generated_images = self.generator(random_latent_vectors)          # Combine them with real images         combined_images = tf.concat([generated_images, real_images], axis=0)          # Assemble labels discriminating real from fake images         labels = tf.concat(             [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0         )         # Add random noise to the labels - important trick!         labels += 0.05 * keras.random.uniform(             tf.shape(labels), seed=self.seed_generator         )          # Train the discriminator         with tf.GradientTape() as tape:             predictions = self.discriminator(combined_images)             d_loss = self.loss_fn(labels, predictions)         grads = tape.gradient(d_loss, self.discriminator.trainable_weights)         self.d_optimizer.apply(grads, self.discriminator.trainable_weights)          # Sample random points in the latent space         random_latent_vectors = keras.random.normal(             shape=(batch_size, self.latent_dim), seed=self.seed_generator         )          # Assemble labels that say \"all real images\"         misleading_labels = tf.zeros((batch_size, 1))          # Train the generator (note that we should *not* update the weights         # of the discriminator)!         with tf.GradientTape() as tape:             predictions = self.discriminator(                 self.generator(random_latent_vectors)             )             g_loss = self.loss_fn(misleading_labels, predictions)         grads = tape.gradient(g_loss, self.generator.trainable_weights)         self.g_optimizer.apply(grads, self.generator.trainable_weights)          # Update metrics and return their value.         self.d_loss_tracker.update_state(d_loss)         self.g_loss_tracker.update_state(g_loss)         return {             \"d_loss\": self.d_loss_tracker.result(),             \"g_loss\": self.g_loss_tracker.result(),         } # Prepare the dataset. We use both the training & test MNIST digits. batch_size = 64 (x_train, _), (x_test, _) = keras.datasets.mnist.load_data() all_digits = np.concatenate([x_train, x_test]) all_digits = all_digits.astype(\"float32\") / 255.0 all_digits = np.reshape(all_digits, (-1, 28, 28, 1)) dataset = tf.data.Dataset.from_tensor_slices(all_digits) dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)  gan = GAN(     discriminator=discriminator, generator=generator, latent_dim=latent_dim ) gan.compile(     d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),     g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),     loss_fn=keras.losses.BinaryCrossentropy(from_logits=True), )  # To limit the execution time, we only train on 100 batches. You can train on # the entire dataset. You will need about 20 epochs to get nice results. gan.fit(dataset.take(100), epochs=1)"},{"path":"https://keras.posit.co/articles/guides/custom_train_step_in_torch.html","id":"introduction","dir":"Articles > Guides","previous_headings":"","what":"Introduction","title":"Customizing what happens in `fit()` with PyTorch","text":"’re supervised learning, can use fit() everything works smoothly. need take control every little detail, can write training loop entirely scratch. need custom training algorithm, still want benefit convenient features fit(), callbacks, built-distribution support, step fusing? core principle Keras progressive disclosure complexity. always able get lower-level workflows gradual way. shouldn’t fall cliff high-level functionality doesn’t exactly match use case. able gain control small details retaining commensurate amount high-level convenience. need customize fit() , override training step function Model class. function called fit() every batch data. able call fit() usual – running learning algorithm. Note pattern prevent building models Functional API. can whether ’re building Sequential models, Functional API models, subclassed models. Let’s see works.","code":""},{"path":"https://keras.posit.co/articles/guides/custom_train_step_in_torch.html","id":"setup","dir":"Articles > Guides","previous_headings":"","what":"Setup","title":"Customizing what happens in `fit()` with PyTorch","text":"","code":"import os  # This guide can only be run with the torch backend. os.environ[\"KERAS_BACKEND\"] = \"torch\"  import torch import keras as keras from keras import layers import numpy as np"},{"path":"https://keras.posit.co/articles/guides/custom_train_step_in_torch.html","id":"a-first-simple-example","dir":"Articles > Guides","previous_headings":"","what":"A first simple example","title":"Customizing what happens in `fit()` with PyTorch","text":"Let’s start simple example: create new class subclasses keras.Model. just override method train_step(self, data). return dictionary mapping metric names (including loss) current value. input argument data gets passed fit training data: pass NumPy arrays, calling fit(x, y, ...), data tuple (x, y) pass torch.utils.data.DataLoader tf.data.Dataset, calling fit(dataset, ...), data gets yielded dataset batch. body train_step() method, implement regular training update, similar already familiar . Importantly, compute loss via self.compute_loss(), wraps loss(es) function(s) passed compile(). Similarly, call metric.update_state(y, y_pred) metrics self.metrics, update state metrics passed compile(), query results self.metrics end retrieve current value. Let’s try :","code":"class CustomModel(keras.Model):     def train_step(self, data):         # Unpack the data. Its structure depends on your model and         # on what you pass to `fit()`.         x, y = data          # Call torch.nn.Module.zero_grad() to clear the leftover gradients         # for the weights from the previous train step.         self.zero_grad()          # Compute loss         y_pred = self(x, training=True)  # Forward pass         loss = self.compute_loss(y=y, y_pred=y_pred)          # Call torch.Tensor.backward() on the loss to compute gradients         # for the weights.         loss.backward()          trainable_weights = [v for v in self.trainable_weights]         gradients = [v.value.grad for v in trainable_weights]          # Update weights         with torch.no_grad():             self.optimizer.apply(gradients, trainable_weights)          # Update metrics (includes the metric that tracks the loss)         for metric in self.metrics:             if metric.name == \"loss\":                 metric.update_state(loss)             else:                 metric.update_state(y, y_pred)          # Return a dict mapping metric names to current value         # Note that it will include the loss (tracked in self.metrics).         return {m.name: m.result() for m in self.metrics} # Construct and compile an instance of CustomModel inputs = keras.Input(shape=(32,)) outputs = keras.layers.Dense(1)(inputs) model = CustomModel(inputs, outputs) model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])  # Just use `fit` as usual x = np.random.random((1000, 32)) y = np.random.random((1000, 1)) model.fit(x, y, epochs=3)"},{"path":"https://keras.posit.co/articles/guides/custom_train_step_in_torch.html","id":"going-lower-level","dir":"Articles > Guides","previous_headings":"","what":"Going lower-level","title":"Customizing what happens in `fit()` with PyTorch","text":"Naturally, just skip passing loss function compile(), instead everything manually train_step. Likewise metrics. ’s lower-level example, uses compile() configure optimizer: start creating Metric instances track loss MAE score (__init__()). implement custom train_step() updates state metrics (calling update_state() ), query (via result()) return current average value, displayed progress bar pass callback. Note need call reset_states() metrics epoch! Otherwise calling result() return average since start training, whereas usually work per-epoch averages. Thankfully, framework can us: just list metric want reset metrics property model. model call reset_states() object listed beginning fit() epoch beginning call evaluate().","code":"class CustomModel(keras.Model):     def __init__(self, *args, **kwargs):         super().__init__(*args, **kwargs)         self.loss_tracker = keras.metrics.Mean(name=\"loss\")         self.mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")         self.loss_fn = keras.losses.MeanSquaredError()      def train_step(self, data):         x, y = data          # Call torch.nn.Module.zero_grad() to clear the leftover gradients         # for the weights from the previous train step.         self.zero_grad()          # Compute loss         y_pred = self(x, training=True)  # Forward pass         loss = self.loss_fn(y, y_pred)          # Call torch.Tensor.backward() on the loss to compute gradients         # for the weights.         loss.backward()          trainable_weights = [v for v in self.trainable_weights]         gradients = [v.value.grad for v in trainable_weights]          # Update weights         with torch.no_grad():             self.optimizer.apply(gradients, trainable_weights)          # Compute our own metrics         self.loss_tracker.update_state(loss)         self.mae_metric.update_state(y, y_pred)         return {             \"loss\": self.loss_tracker.result(),             \"mae\": self.mae_metric.result(),         }      @property     def metrics(self):         # We list our `Metric` objects here so that `reset_states()` can be         # called automatically at the start of each epoch         # or at the start of `evaluate()`.         return [self.loss_tracker, self.mae_metric]   # Construct an instance of CustomModel inputs = keras.Input(shape=(32,)) outputs = keras.layers.Dense(1)(inputs) model = CustomModel(inputs, outputs)  # We don't passs a loss or metrics here. model.compile(optimizer=\"adam\")  # Just use `fit` as usual -- you can use callbacks, etc. x = np.random.random((1000, 32)) y = np.random.random((1000, 1)) model.fit(x, y, epochs=5)"},{"path":"https://keras.posit.co/articles/guides/custom_train_step_in_torch.html","id":"supporting-sample_weight-class_weight","dir":"Articles > Guides","previous_headings":"","what":"Supporting sample_weight & class_weight","title":"Customizing what happens in `fit()` with PyTorch","text":"may noticed first basic example didn’t make mention sample weighting. want support fit() arguments sample_weight class_weight, ’d simply following: Unpack sample_weight data argument Pass compute_loss & update_state (course, also just apply manually don’t rely compile() losses & metrics) ’s .","code":"class CustomModel(keras.Model):     def train_step(self, data):         # Unpack the data. Its structure depends on your model and         # on what you pass to `fit()`.         if len(data) == 3:             x, y, sample_weight = data         else:             sample_weight = None             x, y = data          # Call torch.nn.Module.zero_grad() to clear the leftover gradients         # for the weights from the previous train step.         self.zero_grad()          # Compute loss         y_pred = self(x, training=True)  # Forward pass         loss = self.compute_loss(             y=y,             y_pred=y_pred,             sample_weight=sample_weight,         )          # Call torch.Tensor.backward() on the loss to compute gradients         # for the weights.         loss.backward()          trainable_weights = [v for v in self.trainable_weights]         gradients = [v.value.grad for v in trainable_weights]          # Update weights         with torch.no_grad():             self.optimizer.apply(gradients, trainable_weights)          # Update metrics (includes the metric that tracks the loss)         for metric in self.metrics:             if metric.name == \"loss\":                 metric.update_state(loss)             else:                 metric.update_state(y, y_pred, sample_weight=sample_weight)          # Return a dict mapping metric names to current value         # Note that it will include the loss (tracked in self.metrics).         return {m.name: m.result() for m in self.metrics}   # Construct and compile an instance of CustomModel inputs = keras.Input(shape=(32,)) outputs = keras.layers.Dense(1)(inputs) model = CustomModel(inputs, outputs) model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])  # You can now use sample_weight argument x = np.random.random((1000, 32)) y = np.random.random((1000, 1)) sw = np.random.random((1000, 1)) model.fit(x, y, sample_weight=sw, epochs=3)"},{"path":"https://keras.posit.co/articles/guides/custom_train_step_in_torch.html","id":"providing-your-own-evaluation-step","dir":"Articles > Guides","previous_headings":"","what":"Providing your own evaluation step","title":"Customizing what happens in `fit()` with PyTorch","text":"want calls model.evaluate()? override test_step exactly way. ’s looks like:","code":"class CustomModel(keras.Model):     def test_step(self, data):         # Unpack the data         x, y = data         # Compute predictions         y_pred = self(x, training=False)         # Updates the metrics tracking the loss         loss = self.compute_loss(y=y, y_pred=y_pred)         # Update the metrics.         for metric in self.metrics:             if metric.name == \"loss\":                 metric.update_state(loss)             else:                 metric.update_state(y, y_pred)         # Return a dict mapping metric names to current value.         # Note that it will include the loss (tracked in self.metrics).         return {m.name: m.result() for m in self.metrics}   # Construct an instance of CustomModel inputs = keras.Input(shape=(32,)) outputs = keras.layers.Dense(1)(inputs) model = CustomModel(inputs, outputs) model.compile(loss=\"mse\", metrics=[\"mae\"])  # Evaluate with our custom test_step x = np.random.random((1000, 32)) y = np.random.random((1000, 1)) model.evaluate(x, y)"},{"path":"https://keras.posit.co/articles/guides/custom_train_step_in_torch.html","id":"wrapping-up-an-end-to-end-gan-example","dir":"Articles > Guides","previous_headings":"","what":"Wrapping up: an end-to-end GAN example","title":"Customizing what happens in `fit()` with PyTorch","text":"Let’s walk end--end example leverages everything just learned. Let’s consider: generator network meant generate 28x28x1 images. discriminator network meant classify 28x28x1 images two classes (“fake” “real”). One optimizer . loss function train discriminator. ’s feature-complete GAN class, overriding compile() use signature, implementing entire GAN algorithm 17 lines train_step: Let’s test-drive : ideas behind deep learning simple, implementation painful?","code":"# Create the discriminator discriminator = keras.Sequential(     [         keras.Input(shape=(28, 28, 1)),         layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),         layers.LeakyReLU(negative_slope=0.2),         layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),         layers.LeakyReLU(negative_slope=0.2),         layers.GlobalMaxPooling2D(),         layers.Dense(1),     ],     name=\"discriminator\", )  # Create the generator latent_dim = 128 generator = keras.Sequential(     [         keras.Input(shape=(latent_dim,)),         # We want to generate 128 coefficients to reshape into a 7x7x128 map         layers.Dense(7 * 7 * 128),         layers.LeakyReLU(negative_slope=0.2),         layers.Reshape((7, 7, 128)),         layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),         layers.LeakyReLU(negative_slope=0.2),         layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),         layers.LeakyReLU(negative_slope=0.2),         layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),     ],     name=\"generator\", ) class GAN(keras.Model):     def __init__(self, discriminator, generator, latent_dim):         super().__init__()         self.discriminator = discriminator         self.generator = generator         self.latent_dim = latent_dim         self.d_loss_tracker = keras.metrics.Mean(name=\"d_loss\")         self.g_loss_tracker = keras.metrics.Mean(name=\"g_loss\")         self.seed_generator = keras.random.SeedGenerator(1337)         self.built = True      @property     def metrics(self):         return [self.d_loss_tracker, self.g_loss_tracker]      def compile(self, d_optimizer, g_optimizer, loss_fn):         super().compile()         self.d_optimizer = d_optimizer         self.g_optimizer = g_optimizer         self.loss_fn = loss_fn      def train_step(self, real_images):         if isinstance(real_images, tuple):             real_images = real_images[0]         # Sample random points in the latent space         batch_size = real_images.shape[0]         random_latent_vectors = keras.random.normal(             shape=(batch_size, self.latent_dim), seed=self.seed_generator         )          # Decode them to fake images         generated_images = self.generator(random_latent_vectors)          # Combine them with real images         real_images = torch.tensor(real_images)         combined_images = torch.concat([generated_images, real_images], axis=0)          # Assemble labels discriminating real from fake images         labels = torch.concat(             [torch.ones((batch_size, 1)), torch.zeros((batch_size, 1))], axis=0         )         # Add random noise to the labels - important trick!         labels += 0.05 * keras.random.uniform(             labels.shape, seed=self.seed_generator         )          # Train the discriminator         self.zero_grad()         predictions = self.discriminator(combined_images)         d_loss = self.loss_fn(labels, predictions)         d_loss.backward()         grads = [v.value.grad for v in self.discriminator.trainable_weights]         with torch.no_grad():             self.d_optimizer.apply(grads, self.discriminator.trainable_weights)          # Sample random points in the latent space         random_latent_vectors = keras.random.normal(             shape=(batch_size, self.latent_dim), seed=self.seed_generator         )          # Assemble labels that say \"all real images\"         misleading_labels = torch.zeros((batch_size, 1))          # Train the generator (note that we should *not* update the weights         # of the discriminator)!         self.zero_grad()         predictions = self.discriminator(self.generator(random_latent_vectors))         g_loss = self.loss_fn(misleading_labels, predictions)         grads = g_loss.backward()         grads = [v.value.grad for v in self.generator.trainable_weights]         with torch.no_grad():             self.g_optimizer.apply(grads, self.generator.trainable_weights)          # Update metrics and return their value.         self.d_loss_tracker.update_state(d_loss)         self.g_loss_tracker.update_state(g_loss)         return {             \"d_loss\": self.d_loss_tracker.result(),             \"g_loss\": self.g_loss_tracker.result(),         } # Prepare the dataset. We use both the training & test MNIST digits. batch_size = 64 (x_train, _), (x_test, _) = keras.datasets.mnist.load_data() all_digits = np.concatenate([x_train, x_test]) all_digits = all_digits.astype(\"float32\") / 255.0 all_digits = np.reshape(all_digits, (-1, 28, 28, 1))  # Create a TensorDataset dataset = torch.utils.data.TensorDataset(     torch.from_numpy(all_digits), torch.from_numpy(all_digits) ) # Create a DataLoader dataloader = torch.utils.data.DataLoader(     dataset, batch_size=batch_size, shuffle=True )  gan = GAN(     discriminator=discriminator, generator=generator, latent_dim=latent_dim ) gan.compile(     d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),     g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),     loss_fn=keras.losses.BinaryCrossentropy(from_logits=True), )  gan.fit(dataloader, epochs=1)"},{"path":"https://keras.posit.co/articles/guides/customizing_saving_and_serialization.html","id":"introduction","dir":"Articles > Guides","previous_headings":"","what":"Introduction","title":"Customizing Saving and Serialization","text":"guide covers advanced methods can customized Keras saving. users, methods outlined primary Serialize, save, export guide sufficient.","code":""},{"path":"https://keras.posit.co/articles/guides/customizing_saving_and_serialization.html","id":"apis","dir":"Articles > Guides","previous_headings":"Introduction","what":"APIs","title":"Customizing Saving and Serialization","text":"cover following APIs: save_assets() load_assets() save_own_variables() load_own_variables() get_build_config() build_from_config() get_compile_config() compile_from_config() restoring model, get executed following order: build_from_config() compile_from_config() load_own_variables() load_assets()","code":""},{"path":"https://keras.posit.co/articles/guides/customizing_saving_and_serialization.html","id":"setup","dir":"Articles > Guides","previous_headings":"","what":"Setup","title":"Customizing Saving and Serialization","text":"","code":"import os import numpy as np import tensorflow as tf import keras"},{"path":"https://keras.posit.co/articles/guides/customizing_saving_and_serialization.html","id":"state-saving-customization","dir":"Articles > Guides","previous_headings":"","what":"State saving customization","title":"Customizing Saving and Serialization","text":"methods determine state model’s layers saved calling model.save(). can override take full control state saving process.","code":""},{"path":"https://keras.posit.co/articles/guides/customizing_saving_and_serialization.html","id":"save_own_variables-and-load_own_variables","dir":"Articles > Guides","previous_headings":"State saving customization","what":"save_own_variables() and load_own_variables()","title":"Customizing Saving and Serialization","text":"methods save load state variables layer model.save() keras.models.load_model() called, respectively. default, state variables saved loaded weights layer (trainable non-trainable). default implementation save_own_variables(): store used methods dictionary can populated layer variables. Let’s take look example customizing . Example:","code":"def save_own_variables(self, store):     all_vars = self._trainable_weights + self._non_trainable_weights     for i, v in enumerate(all_vars):         store[f\"{i}\"] = v.numpy() @keras.utils.register_keras_serializable(package=\"my_custom_package\") class LayerWithCustomVariables(keras.layers.Dense):     def __init__(self, units, **kwargs):         super().__init__(units, **kwargs)         self.stored_variables = tf.Variable(             np.random.random((10,)), name=\"special_arr\", dtype=tf.float32         )      def save_own_variables(self, store):         super().save_own_variables(store)         # Stores the value of the `tf.Variable` upon saving         store[\"variables\"] = self.stored_variables.numpy()      def load_own_variables(self, store):         # Assigns the value of the `tf.Variable` upon loading         self.stored_variables.assign(store[\"variables\"])         # Load the remaining weights         for i, v in enumerate(self.weights):             v.assign(store[f\"{i}\"])         # Note: You must specify how all variables (including layer weights)         # are loaded in `load_own_variables.`      def call(self, inputs):         return super().call(inputs) * self.stored_variables   model = keras.Sequential([LayerWithCustomVariables(1)])  ref_input = np.random.random((8, 10)) ref_output = np.random.random((8,)) model.compile(optimizer=\"adam\", loss=\"mean_squared_error\") model.fit(ref_input, ref_output)  model.save(\"custom_vars_model.keras\") restored_model = keras.models.load_model(\"custom_vars_model.keras\")  np.testing.assert_allclose(     model.layers[0].stored_variables.numpy(),     restored_model.layers[0].stored_variables.numpy(), )"},{"path":"https://keras.posit.co/articles/guides/customizing_saving_and_serialization.html","id":"save_assets-and-load_assets","dir":"Articles > Guides","previous_headings":"State saving customization","what":"save_assets() and load_assets()","title":"Customizing Saving and Serialization","text":"methods can added model class definition store load additional information model needs. example, NLP domain layers TextVectorization layers IndexLookup layers may need store associated vocabulary (lookup table) text file upon saving. Let’s take basics workflow simple file assets.txt. Example:","code":"@keras.saving.register_keras_serializable(package=\"my_custom_package\") class LayerWithCustomAssets(keras.layers.Dense):     def __init__(self, vocab=None, *args, **kwargs):         super().__init__(*args, **kwargs)         self.vocab = vocab      def save_assets(self, inner_path):         # Writes the vocab (sentence) to text file at save time.         with open(os.path.join(inner_path, \"vocabulary.txt\"), \"w\") as f:             f.write(self.vocab)      def load_assets(self, inner_path):         # Reads the vocab (sentence) from text file at load time.         with open(os.path.join(inner_path, \"vocabulary.txt\"), \"r\") as f:             text = f.read()         self.vocab = text.replace(\"<unk>\", \"little\")   model = keras.Sequential(     [LayerWithCustomAssets(vocab=\"Mary had a <unk> lamb.\", units=5)] )  x = np.random.random((10, 10)) y = model(x)  model.save(\"custom_assets_model.keras\") restored_model = keras.models.load_model(\"custom_assets_model.keras\")  np.testing.assert_string_equal(     restored_model.layers[0].vocab, \"Mary had a little lamb.\" )"},{"path":[]},{"path":"https://keras.posit.co/articles/guides/customizing_saving_and_serialization.html","id":"get_build_config-and-build_from_config","dir":"Articles > Guides","previous_headings":"build and compile saving customization","what":"get_build_config() and build_from_config()","title":"Customizing Saving and Serialization","text":"methods work together save layer’s built states restore upon loading. default, includes build config dictionary layer’s input shape, overriding methods can used include Variables Lookup Tables can useful restore built model. Example:","code":"@keras.saving.register_keras_serializable(package=\"my_custom_package\") class LayerWithCustomBuild(keras.layers.Layer):     def __init__(self, units=32, **kwargs):         super().__init__(**kwargs)         self.units = units      def call(self, inputs):         return tf.matmul(inputs, self.w) + self.b      def get_config(self):         return dict(units=self.units, **super().get_config())      def build(self, input_shape, layer_init):         # Note the customization in overriding `build()` adds an extra argument.         # Therefore, we will need to manually call build with `layer_init` argument         # before the first execution of `call()`.         super().build(input_shape)         self.w = self.add_weight(             shape=(input_shape[-1], self.units),             initializer=layer_init,             trainable=True,         )         self.b = self.add_weight(             shape=(self.units,),             initializer=layer_init,             trainable=True,         )         self.layer_init = layer_init      def get_build_config(self):         build_config = super().get_build_config()  # only gives `input_shape`         build_config.update(             {\"layer_init\": self.layer_init}  # Stores our initializer for `build()`         )         return build_config      def build_from_config(self, config):         # Calls `build()` with the parameters at loading time         self.build(config[\"input_shape\"], config[\"layer_init\"])   custom_layer = LayerWithCustomBuild(units=16) custom_layer.build(input_shape=(8,), layer_init=\"random_normal\")  model = keras.Sequential(     [         custom_layer,         keras.layers.Dense(1, activation=\"sigmoid\"),     ] )  x = np.random.random((16, 8)) y = model(x)  model.save(\"custom_build_model.keras\") restored_model = keras.models.load_model(\"custom_build_model.keras\")  np.testing.assert_equal(restored_model.layers[0].layer_init, \"random_normal\") np.testing.assert_equal(restored_model.built, True)"},{"path":"https://keras.posit.co/articles/guides/customizing_saving_and_serialization.html","id":"get_compile_config-and-compile_from_config","dir":"Articles > Guides","previous_headings":"build and compile saving customization","what":"get_compile_config() and compile_from_config()","title":"Customizing Saving and Serialization","text":"methods work together save information model compiled (optimizers, losses, etc.) restore re-compile model information. Overriding methods can useful compiling restored model custom optimizers, custom losses, etc., need deserialized prior calling model.compile compile_from_config(). Let’s take look example . Example:","code":"@keras.saving.register_keras_serializable(package=\"my_custom_package\") def small_square_sum_loss(y_true, y_pred):     loss = tf.math.squared_difference(y_pred, y_true)     loss = loss / 10.0     loss = tf.reduce_sum(loss, axis=1)     return loss   @keras.saving.register_keras_serializable(package=\"my_custom_package\") def mean_pred(y_true, y_pred):     return tf.reduce_mean(y_pred)   @keras.saving.register_keras_serializable(package=\"my_custom_package\") class ModelWithCustomCompile(keras.Model):     def __init__(self):         super().__init__()         self.dense1 = keras.layers.Dense(8, activation=\"relu\")         self.dense2 = keras.layers.Dense(4, activation=\"softmax\")      def call(self, inputs):         x = self.dense1(inputs)         return self.dense2(x)      def compile(self, optimizer, loss_fn, metrics):         super().compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)         self.model_optimizer = optimizer         self.loss_fn = loss_fn         self.loss_metrics = metrics      def get_compile_config(self):         # These parameters will be serialized at saving time.         return {             \"model_optimizer\": self.model_optimizer,             \"loss_fn\": self.loss_fn,             \"metric\": self.loss_metrics,         }      def compile_from_config(self, config):         # Deserializes the compile parameters (important, since many are custom)         optimizer = keras.utils.deserialize_keras_object(config[\"model_optimizer\"])         loss_fn = keras.utils.deserialize_keras_object(config[\"loss_fn\"])         metrics = keras.utils.deserialize_keras_object(config[\"metric\"])          # Calls compile with the deserialized parameters         self.compile(optimizer=optimizer, loss_fn=loss_fn, metrics=metrics)   model = ModelWithCustomCompile() model.compile(     optimizer=\"SGD\", loss_fn=small_square_sum_loss, metrics=[\"accuracy\", mean_pred] )  x = np.random.random((4, 8)) y = np.random.random((4,))  model.fit(x, y)  model.save(\"custom_compile_model.keras\") restored_model = keras.models.load_model(\"custom_compile_model.keras\")  np.testing.assert_equal(model.model_optimizer, restored_model.model_optimizer) np.testing.assert_equal(model.loss_fn, restored_model.loss_fn) np.testing.assert_equal(model.loss_metrics, restored_model.loss_metrics)"},{"path":"https://keras.posit.co/articles/guides/customizing_saving_and_serialization.html","id":"conclusion","dir":"Articles > Guides","previous_headings":"","what":"Conclusion","title":"Customizing Saving and Serialization","text":"Using methods learned tutorial allows wide variety use cases, allowing saving loading complex models exotic assets state elements. recap: save_own_variables load_own_variables determine states saved loaded. save_assets load_assets can added store load additional information model needs. get_build_config build_from_config save restore model’s built states. get_compile_config compile_from_config save restore model’s compiled states.","code":""},{"path":"https://keras.posit.co/articles/guides/customizing_what_happens_in_fit.html","id":"introduction","dir":"Articles > Guides","previous_headings":"","what":"Introduction","title":"Customizing what happens in `fit()`","text":"’re supervised learning, can use fit() everything works smoothly. need write training loop scratch, can use GradientTape take control every little detail. need custom training algorithm, still want benefit convenient features fit(), callbacks, built-distribution support, step fusing? core principle Keras progressive disclosure complexity. always able get lower-level workflows gradual way. shouldn’t fall cliff high-level functionality doesn’t exactly match use case. able gain control small details retaining commensurate amount high-level convenience. need customize fit() , override training step function Model class. function called fit() every batch data. able call fit() usual – running learning algorithm. Note pattern prevent building models Functional API. can whether ’re building Sequential models, Functional API models, subclassed models. Let’s see works.","code":""},{"path":"https://keras.posit.co/articles/guides/customizing_what_happens_in_fit.html","id":"setup","dir":"Articles > Guides","previous_headings":"","what":"Setup","title":"Customizing what happens in `fit()`","text":"Requires TensorFlow 2.8 later.","code":"import tensorflow as tf import keras"},{"path":"https://keras.posit.co/articles/guides/customizing_what_happens_in_fit.html","id":"a-first-simple-example","dir":"Articles > Guides","previous_headings":"","what":"A first simple example","title":"Customizing what happens in `fit()`","text":"Let’s start simple example: create new class subclasses keras.Model. just override method train_step(self, data). return dictionary mapping metric names (including loss) current value. input argument data gets passed fit training data: pass Numpy arrays, calling fit(x, y, ...), data tuple (x, y) pass tf.data.Dataset, calling fit(dataset, ...), data gets yielded dataset batch. body train_step method, implement regular training update, similar already familiar . Importantly, compute loss via self.compute_loss(), wraps loss(es) function(s) passed compile(). Similarly, call metric.update_state(y, y_pred) metrics self.metrics, update state metrics passed compile(), query results self.metrics end retrieve current value. Let’s try :","code":"class CustomModel(keras.Model):     def train_step(self, data):         # Unpack the data. Its structure depends on your model and         # on what you pass to `fit()`.         x, y = data          with tf.GradientTape() as tape:             y_pred = self(x, training=True)  # Forward pass             # Compute the loss value             # (the loss function is configured in `compile()`)             loss = self.compute_loss(y=y, y_pred=y_pred)          # Compute gradients         trainable_vars = self.trainable_variables         gradients = tape.gradient(loss, trainable_vars)         # Update weights         self.optimizer.apply_gradients(zip(gradients, trainable_vars))         # Update metrics (includes the metric that tracks the loss)         for metric in self.metrics:             if metric.name == \"loss\":                 metric.update_state(loss)             else:                 metric.update_state(y, y_pred)         # Return a dict mapping metric names to current value         return {m.name: m.result() for m in self.metrics} import numpy as np  # Construct and compile an instance of CustomModel inputs = keras.Input(shape=(32,)) outputs = keras.layers.Dense(1)(inputs) model = CustomModel(inputs, outputs) model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])  # Just use `fit` as usual x = np.random.random((1000, 32)) y = np.random.random((1000, 1)) model.fit(x, y, epochs=3)"},{"path":"https://keras.posit.co/articles/guides/customizing_what_happens_in_fit.html","id":"going-lower-level","dir":"Articles > Guides","previous_headings":"","what":"Going lower-level","title":"Customizing what happens in `fit()`","text":"Naturally, just skip passing loss function compile(), instead everything manually train_step. Likewise metrics. ’s lower-level example, uses compile() configure optimizer: start creating Metric instances track loss MAE score (__init__()). implement custom train_step() updates state metrics (calling update_state() ), query (via result()) return current average value, displayed progress bar pass callback. Note need call reset_states() metrics epoch! Otherwise calling result() return average since start training, whereas usually work per-epoch averages. Thankfully, framework can us: just list metric want reset metrics property model. model call reset_states() object listed beginning fit() epoch beginning call evaluate().","code":"class CustomModel(keras.Model):     def __init__(self, *args, **kwargs):         super().__init__(*args, **kwargs)         self.loss_tracker = keras.metrics.Mean(name=\"loss\")         self.mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")      def train_step(self, data):         x, y = data          with tf.GradientTape() as tape:             y_pred = self(x, training=True)  # Forward pass             # Compute our own loss             loss = keras.losses.mean_squared_error(y, y_pred)          # Compute gradients         trainable_vars = self.trainable_variables         gradients = tape.gradient(loss, trainable_vars)          # Update weights         self.optimizer.apply_gradients(zip(gradients, trainable_vars))          # Compute our own metrics         self.loss_tracker.update_state(loss)         self.mae_metric.update_state(y, y_pred)         return {\"loss\": self.loss_tracker.result(), \"mae\": self.mae_metric.result()}      @property     def metrics(self):         # We list our `Metric` objects here so that `reset_states()` can be         # called automatically at the start of each epoch         # or at the start of `evaluate()`.         # If you don't implement this property, you have to call         # `reset_states()` yourself at the time of your choosing.         return [self.loss_tracker, self.mae_metric]   # Construct an instance of CustomModel inputs = keras.Input(shape=(32,)) outputs = keras.layers.Dense(1)(inputs) model = CustomModel(inputs, outputs)  # We don't passs a loss or metrics here. model.compile(optimizer=\"adam\")  # Just use `fit` as usual -- you can use callbacks, etc. x = np.random.random((1000, 32)) y = np.random.random((1000, 1)) model.fit(x, y, epochs=5)"},{"path":"https://keras.posit.co/articles/guides/customizing_what_happens_in_fit.html","id":"supporting-sample_weight-class_weight","dir":"Articles > Guides","previous_headings":"","what":"Supporting sample_weight & class_weight","title":"Customizing what happens in `fit()`","text":"may noticed first basic example didn’t make mention sample weighting. want support fit() arguments sample_weight class_weight, ’d simply following: Unpack sample_weight data argument Pass compute_loss & update_state (course, also just apply manually don’t rely compile() losses & metrics) ’s .","code":"class CustomModel(keras.Model):     def train_step(self, data):         # Unpack the data. Its structure depends on your model and         # on what you pass to `fit()`.         if len(data) == 3:             x, y, sample_weight = data         else:             sample_weight = None             x, y = data          with tf.GradientTape() as tape:             y_pred = self(x, training=True)  # Forward pass             # Compute the loss value.             # The loss function is configured in `compile()`.             loss = self.compute_loss(                 y=y,                 y_pred=y_pred,                 sample_weight=sample_weight,             )          # Compute gradients         trainable_vars = self.trainable_variables         gradients = tape.gradient(loss, trainable_vars)          # Update weights         self.optimizer.apply_gradients(zip(gradients, trainable_vars))          # Update the metrics.         # Metrics are configured in `compile()`.         for metric in self.metrics:             if metric.name == \"loss\":                 metric.update_state(loss)             else:                 metric.update_state(y, y_pred, sample_weight=sample_weight)          # Return a dict mapping metric names to current value.         # Note that it will include the loss (tracked in self.metrics).         return {m.name: m.result() for m in self.metrics}   # Construct and compile an instance of CustomModel inputs = keras.Input(shape=(32,)) outputs = keras.layers.Dense(1)(inputs) model = CustomModel(inputs, outputs) model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])  # You can now use sample_weight argument x = np.random.random((1000, 32)) y = np.random.random((1000, 1)) sw = np.random.random((1000, 1)) model.fit(x, y, sample_weight=sw, epochs=3)"},{"path":"https://keras.posit.co/articles/guides/customizing_what_happens_in_fit.html","id":"providing-your-own-evaluation-step","dir":"Articles > Guides","previous_headings":"","what":"Providing your own evaluation step","title":"Customizing what happens in `fit()`","text":"want calls model.evaluate()? override test_step exactly way. ’s looks like:","code":"class CustomModel(keras.Model):     def test_step(self, data):         # Unpack the data         x, y = data         # Compute predictions         y_pred = self(x, training=False)         # Updates the metrics tracking the loss         self.compute_loss(y=y, y_pred=y_pred)         # Update the metrics.         for metric in self.metrics:             if metric.name != \"loss\":                 metric.update_state(y, y_pred)         # Return a dict mapping metric names to current value.         # Note that it will include the loss (tracked in self.metrics).         return {m.name: m.result() for m in self.metrics}   # Construct an instance of CustomModel inputs = keras.Input(shape=(32,)) outputs = keras.layers.Dense(1)(inputs) model = CustomModel(inputs, outputs) model.compile(loss=\"mse\", metrics=[\"mae\"])  # Evaluate with our custom test_step x = np.random.random((1000, 32)) y = np.random.random((1000, 1)) model.evaluate(x, y)"},{"path":"https://keras.posit.co/articles/guides/customizing_what_happens_in_fit.html","id":"wrapping-up-an-end-to-end-gan-example","dir":"Articles > Guides","previous_headings":"","what":"Wrapping up: an end-to-end GAN example","title":"Customizing what happens in `fit()`","text":"Let’s walk end--end example leverages everything just learned. Let’s consider: generator network meant generate 28x28x1 images. discriminator network meant classify 28x28x1 images two classes (“fake” “real”). One optimizer . loss function train discriminator. ’s feature-complete GAN class, overriding compile() use signature, implementing entire GAN algorithm 17 lines train_step: Let’s test-drive : ideas behind deep learning simple, implementation painful?","code":"from tensorflow.keras import layers  # Create the discriminator discriminator = keras.Sequential(     [         keras.Input(shape=(28, 28, 1)),         layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),         layers.LeakyReLU(alpha=0.2),         layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),         layers.LeakyReLU(alpha=0.2),         layers.GlobalMaxPooling2D(),         layers.Dense(1),     ],     name=\"discriminator\", )  # Create the generator latent_dim = 128 generator = keras.Sequential(     [         keras.Input(shape=(latent_dim,)),         # We want to generate 128 coefficients to reshape into a 7x7x128 map         layers.Dense(7 * 7 * 128),         layers.LeakyReLU(alpha=0.2),         layers.Reshape((7, 7, 128)),         layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),         layers.LeakyReLU(alpha=0.2),         layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),         layers.LeakyReLU(alpha=0.2),         layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),     ],     name=\"generator\", ) class GAN(keras.Model):     def __init__(self, discriminator, generator, latent_dim):         super().__init__()         self.discriminator = discriminator         self.generator = generator         self.latent_dim = latent_dim         self.d_loss_tracker = keras.metrics.Mean(name=\"d_loss\")         self.g_loss_tracker = keras.metrics.Mean(name=\"g_loss\")      def compile(self, d_optimizer, g_optimizer, loss_fn):         super().compile()         self.d_optimizer = d_optimizer         self.g_optimizer = g_optimizer         self.loss_fn = loss_fn      def train_step(self, real_images):         if isinstance(real_images, tuple):             real_images = real_images[0]         # Sample random points in the latent space         batch_size = tf.shape(real_images)[0]         random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))          # Decode them to fake images         generated_images = self.generator(random_latent_vectors)          # Combine them with real images         combined_images = tf.concat([generated_images, real_images], axis=0)          # Assemble labels discriminating real from fake images         labels = tf.concat(             [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0         )         # Add random noise to the labels - important trick!         labels += 0.05 * tf.random.uniform(tf.shape(labels))          # Train the discriminator         with tf.GradientTape() as tape:             predictions = self.discriminator(combined_images)             d_loss = self.loss_fn(labels, predictions)         grads = tape.gradient(d_loss, self.discriminator.trainable_weights)         self.d_optimizer.apply_gradients(             zip(grads, self.discriminator.trainable_weights)         )          # Sample random points in the latent space         random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))          # Assemble labels that say \"all real images\"         misleading_labels = tf.zeros((batch_size, 1))          # Train the generator (note that we should *not* update the weights         # of the discriminator)!         with tf.GradientTape() as tape:             predictions = self.discriminator(self.generator(random_latent_vectors))             g_loss = self.loss_fn(misleading_labels, predictions)         grads = tape.gradient(g_loss, self.generator.trainable_weights)         self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))          # Update metrics and return their value.         self.d_loss_tracker.update_state(d_loss)         self.g_loss_tracker.update_state(g_loss)         return {             \"d_loss\": self.d_loss_tracker.result(),             \"g_loss\": self.g_loss_tracker.result(),         } # Prepare the dataset. We use both the training & test MNIST digits. batch_size = 64 (x_train, _), (x_test, _) = keras.datasets.mnist.load_data() all_digits = np.concatenate([x_train, x_test]) all_digits = all_digits.astype(\"float32\") / 255.0 all_digits = np.reshape(all_digits, (-1, 28, 28, 1)) dataset = tf.data.Dataset.from_tensor_slices(all_digits) dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)  gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim) gan.compile(     d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),     g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),     loss_fn=keras.losses.BinaryCrossentropy(from_logits=True), )  # To limit the execution time, we only train on 100 batches. You can train on # the entire dataset. You will need about 20 epochs to get nice results. gan.fit(dataset.take(100), epochs=1)"},{"path":"https://keras.posit.co/articles/guides/distributed_training.html","id":"introduction","dir":"Articles > Guides","previous_headings":"","what":"Introduction","title":"Multi-GPU and distributed training","text":"generally two ways distribute computation across multiple devices: Data parallelism, single model gets replicated multiple devices multiple machines. processes different batches data, merge results. exist many variants setup, differ different model replicas merge results, whether stay sync every batch whether loosely coupled, etc. Model parallelism, different parts single model run different devices, processing single batch data together. works best models naturally-parallel architecture, models feature multiple branches. guide focuses data parallelism, particular synchronous data parallelism, different replicas model stay sync batch process. Synchronicity keeps model convergence behavior identical see single-device training. Specifically, guide teaches use tf.distribute API train Keras models multiple GPUs, minimal changes code, following two setups: multiple GPUs (typically 2 8) installed single machine (single host, multi-device training). common setup researchers small-scale industry workflows. cluster many machines, hosting one multiple GPUs (multi-worker distributed training). good setup large-scale industry workflows, e.g. training high-resolution image classification models tens millions images using 20-100 GPUs.","code":""},{"path":"https://keras.posit.co/articles/guides/distributed_training.html","id":"setup","dir":"Articles > Guides","previous_headings":"","what":"Setup","title":"Multi-GPU and distributed training","text":"","code":"import tensorflow as tf import keras"},{"path":"https://keras.posit.co/articles/guides/distributed_training.html","id":"single-host-multi-device-synchronous-training","dir":"Articles > Guides","previous_headings":"","what":"Single-host, multi-device synchronous training","title":"Multi-GPU and distributed training","text":"setup, one machine several GPUs (typically 2 8). device run copy model (called replica). simplicity, follows, ’ll assume ’re dealing 8 GPUs, loss generality. works step training: current batch data (called global batch) split 8 different sub-batches (called local batches). instance, global batch 512 samples, 8 local batches 64 samples. 8 replicas independently processes local batch: run forward pass, backward pass, outputting gradient weights respect loss model local batch. weight updates originating local gradients efficiently merged across 8 replicas. done end every step, replicas always stay sync. practice, process synchronously updating weights model replicas handled level individual weight variable. done mirrored variable object. use single-host, multi-device synchronous training Keras model, use tf.distribute.MirroredStrategy API. ’s works: Instantiate MirroredStrategy, optionally configuring specific devices want use (default strategy use GPUs available). Use strategy object open scope, within scope, create Keras objects need contain variables. Typically, means creating & compiling model inside distribution scope. Train model via fit() usual. Importantly, recommend use tf.data.Dataset objects load data multi-device distributed workflow. Schematically, looks like : ’s simple end--end runnable example:","code":"# Create a MirroredStrategy. strategy = tf.distribute.MirroredStrategy() print('Number of devices: {}'.format(strategy.num_replicas_in_sync))  # Open a strategy scope. with strategy.scope():   # Everything that creates variables should be under the strategy scope.   # In general this is only model construction & `compile()`.   model = Model(...)   model.compile(...)  # Train the model on all available devices. model.fit(train_dataset, validation_data=val_dataset, ...)  # Test the model on all available devices. model.evaluate(test_dataset) def get_compiled_model():     # Make a simple 2-layer densely-connected neural network.     inputs = keras.Input(shape=(784,))     x = keras.layers.Dense(256, activation=\"relu\")(inputs)     x = keras.layers.Dense(256, activation=\"relu\")(x)     outputs = keras.layers.Dense(10)(x)     model = keras.Model(inputs, outputs)     model.compile(         optimizer=keras.optimizers.Adam(),         loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),         metrics=[keras.metrics.SparseCategoricalAccuracy()],     )     return model   def get_dataset():     batch_size = 32     num_val_samples = 10000      # Return the MNIST dataset in the form of a `tf.data.Dataset`.     (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()      # Preprocess the data (these are Numpy arrays)     x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255     x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255     y_train = y_train.astype(\"float32\")     y_test = y_test.astype(\"float32\")      # Reserve num_val_samples samples for validation     x_val = x_train[-num_val_samples:]     y_val = y_train[-num_val_samples:]     x_train = x_train[:-num_val_samples]     y_train = y_train[:-num_val_samples]     return (         tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size),         tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size),         tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size),     )   # Create a MirroredStrategy. strategy = tf.distribute.MirroredStrategy() print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))  # Open a strategy scope. with strategy.scope():     # Everything that creates variables should be under the strategy scope.     # In general this is only model construction & `compile()`.     model = get_compiled_model()  # Train the model on all available devices. train_dataset, val_dataset, test_dataset = get_dataset() model.fit(train_dataset, epochs=2, validation_data=val_dataset)  # Test the model on all available devices. model.evaluate(test_dataset)"},{"path":"https://keras.posit.co/articles/guides/distributed_training.html","id":"using-callbacks-to-ensure-fault-tolerance","dir":"Articles > Guides","previous_headings":"","what":"Using callbacks to ensure fault tolerance","title":"Multi-GPU and distributed training","text":"using distributed training, always make sure strategy recover failure (fault tolerance). simplest way handle pass ModelCheckpoint callback fit(), save model regular intervals (e.g. every 100 batches every epoch). can restart training saved model. ’s simple example:","code":"import os from tensorflow import keras  # Prepare a directory to store all the checkpoints. checkpoint_dir = \"./ckpt\" if not os.path.exists(checkpoint_dir):     os.makedirs(checkpoint_dir)   def make_or_restore_model():     # Either restore the latest model, or create a fresh one     # if there is no checkpoint available.     checkpoints = [checkpoint_dir + \"/\" + name for name in os.listdir(checkpoint_dir)]     if checkpoints:         latest_checkpoint = max(checkpoints, key=os.path.getctime)         print(\"Restoring from\", latest_checkpoint)         return keras.models.load_model(latest_checkpoint)     print(\"Creating a new model\")     return get_compiled_model()   def run_training(epochs=1):     # Create a MirroredStrategy.     strategy = tf.distribute.MirroredStrategy()      # Open a strategy scope and create/restore the model     with strategy.scope():         model = make_or_restore_model()      callbacks = [         # This callback saves a SavedModel every epoch         # We include the current epoch in the folder name.         keras.callbacks.ModelCheckpoint(             filepath=checkpoint_dir + \"/ckpt-{epoch}\", save_freq=\"epoch\"         )     ]     model.fit(         train_dataset,         epochs=epochs,         callbacks=callbacks,         validation_data=val_dataset,         verbose=2,     )   # Running the first time creates the model run_training(epochs=1)  # Calling the same function again will resume from where we left off run_training(epochs=1)"},{"path":"https://keras.posit.co/articles/guides/distributed_training.html","id":"tf-data-performance-tips","dir":"Articles > Guides","previous_headings":"","what":"tf.data performance tips","title":"Multi-GPU and distributed training","text":"distributed training, efficiency load data can often become critical. tips make sure tf.data pipelines run fast possible. Note dataset batching creating dataset, make sure batched global batch size. instance, 8 GPUs capable running batch 64 samples, call use global batch size 512. Calling dataset.cache() call .cache() dataset, data cached running first iteration data. Every subsequent iteration use cached data. cache can memory (default) local file specify. can improve performance : data expected change iteration iteration reading data remote distributed filesystem reading data local disk, data fit memory workflow significantly IO-bound (e.g. reading & decoding image files). Calling dataset.prefetch(buffer_size) almost always call .prefetch(buffer_size) creating dataset. means data pipeline run asynchronously model, new samples preprocessed stored buffer current batch samples used train model. next batch prefetched GPU memory time current batch .","code":""},{"path":"https://keras.posit.co/articles/guides/distributed_training.html","id":"multi-worker-distributed-synchronous-training","dir":"Articles > Guides","previous_headings":"","what":"Multi-worker distributed synchronous training","title":"Multi-GPU and distributed training","text":"works setup, multiple machines (called workers), one several GPUs . Much like happens single-host training, available GPU run one model replica, value variables replica kept sync batch. Importantly, current implementation assumes workers number GPUs (homogeneous cluster). use Set cluster (provide pointers ). Set appropriate TF_CONFIG environment variable worker. tells worker role communicate peers. worker, run model construction & compilation code within scope MultiWorkerMirroredStrategy object, similarly single-host training. Run evaluation code designated evaluator machine. Setting cluster First, set cluster (collective machines). machine individually setup able run model (typically, machine run Docker image) able access data source (e.g. GCS). Cluster management beyond scope guide. document help get started. can also take look Kubeflow. Setting TF_CONFIG environment variable code running worker almost code used single-host workflow (except different tf.distribute strategy object), one significant difference single-host workflow multi-worker workflow need set TF_CONFIG environment variable machine running cluster. TF_CONFIG environment variable JSON string specifies: cluster configuration, list addresses & ports machines make cluster worker’s “task”, role specific machine play within cluster. One example TF_CONFIG : multi-worker synchronous training setup, valid roles (task types) machines “worker” “evaluator”. example, 8 machines 4 GPUs , 7 workers one evaluator. workers train model, one processing sub-batches global batch. One workers (worker 0) serve “chief”, particular kind worker responsible saving logs checkpoints later reuse (typically Cloud storage location). evaluator runs continuous loop loads latest checkpoint saved chief worker, runs evaluation (asynchronously workers) writes evaluation logs (e.g. TensorBoard logs). Running code worker run training code worker (including chief) evaluation code evaluator. training code basically use single-host setup, except using MultiWorkerMirroredStrategy instead MirroredStrategy. worker run code (minus difference explained note ), including callbacks. Note: Callbacks save model checkpoints logs save different directory worker. standard practice workers save local disk (typically temporary), except worker 0, save TensorBoard logs checkpoints Cloud storage location later access & reuse. evaluator simply use MirroredStrategy (since runs single machine need communicate machines) call model.evaluate(). loading latest checkpoint saved chief worker Cloud storage location, save evaluation logs location chief logs.","code":"os.environ['TF_CONFIG'] = json.dumps({     'cluster': {         'worker': [\"localhost:12345\", \"localhost:23456\"]     },     'task': {'type': 'worker', 'index': 0} })"},{"path":"https://keras.posit.co/articles/guides/distributed_training.html","id":"example-code-running-in-a-multi-worker-setup","dir":"Articles > Guides","previous_headings":"Multi-worker distributed synchronous training","what":"Example: code running in a multi-worker setup","title":"Multi-GPU and distributed training","text":"chief (worker 0): workers: evaluator:","code":"# Set TF_CONFIG os.environ['TF_CONFIG'] = json.dumps({     'cluster': {         'worker': [\"localhost:12345\", \"localhost:23456\"]     },     'task': {'type': 'worker', 'index': 0} })  # Open a strategy scope and create/restore the model. strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() with strategy.scope():   model = make_or_restore_model()  callbacks = [     # This callback saves a SavedModel every 100 batches     keras.callbacks.ModelCheckpoint(filepath='path/to/cloud/location/ckpt',                                     save_freq=100),     keras.callbacks.TensorBoard('path/to/cloud/location/tb/') ] model.fit(train_dataset,           callbacks=callbacks,           ...) # Set TF_CONFIG worker_index = 1  # For instance os.environ['TF_CONFIG'] = json.dumps({     'cluster': {         'worker': [\"localhost:12345\", \"localhost:23456\"]     },     'task': {'type': 'worker', 'index': worker_index} })  # Open a strategy scope and create/restore the model. # You can restore from the checkpoint saved by the chief. strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() with strategy.scope():   model = make_or_restore_model()  callbacks = [     keras.callbacks.ModelCheckpoint(filepath='local/path/ckpt', save_freq=100),     keras.callbacks.TensorBoard('local/path/tb/') ] model.fit(train_dataset,           callbacks=callbacks,           ...) strategy = tf.distribute.MirroredStrategy() with strategy.scope():   model = make_or_restore_model()  # Restore from the checkpoint saved by the chief.  results = model.evaluate(val_dataset) # Then, log the results on a shared location, write TensorBoard logs, etc"},{"path":"https://keras.posit.co/articles/guides/distributed_training.html","id":"further-reading","dir":"Articles > Guides","previous_headings":"Multi-worker distributed synchronous training","what":"Further reading","title":"Multi-GPU and distributed training","text":"TensorFlow distributed training guide Tutorial multi-worker training Keras MirroredStrategy docs MultiWorkerMirroredStrategy docs Distributed training tf.keras Weights & Biases","code":""},{"path":"https://keras.posit.co/articles/guides/distributed_training_with_jax.html","id":"introduction","dir":"Articles > Guides","previous_headings":"","what":"Introduction","title":"Multi-GPU distributed training with JAX","text":"generally two ways distribute computation across multiple devices: Data parallelism, single model gets replicated multiple devices multiple machines. processes different batches data, merge results. exist many variants setup, differ different model replicas merge results, whether stay sync every batch whether loosely coupled, etc. Model parallelism, different parts single model run different devices, processing single batch data together. works best models naturally-parallel architecture, models feature multiple branches. guide focuses data parallelism, particular synchronous data parallelism, different replicas model stay sync batch process. Synchronicity keeps model convergence behavior identical see single-device training. Specifically, guide teaches use jax.sharding APIs train Keras models, minimal changes code, multiple GPUs TPUS (typically 2 16) installed single machine (single host, multi-device training). common setup researchers small-scale industry workflows.","code":""},{"path":"https://keras.posit.co/articles/guides/distributed_training_with_jax.html","id":"setup","dir":"Articles > Guides","previous_headings":"","what":"Setup","title":"Multi-GPU distributed training with JAX","text":"Let’s start defining function creates model train, function creates dataset train (MNIST case).","code":"import os  os.environ[\"KERAS_BACKEND\"] = \"jax\"  import jax import numpy as np import tensorflow as tf import keras as keras  from jax.experimental import mesh_utils from jax.sharding import Mesh from jax.sharding import NamedSharding from jax.sharding import PartitionSpec as P   def get_model():     # Make a simple convnet with batch normalization and dropout.     inputs = keras.Input(shape=(28, 28, 1))     x = keras.layers.Rescaling(1.0 / 255.0)(inputs)     x = keras.layers.Conv2D(         filters=12, kernel_size=3, padding=\"same\", use_bias=False     )(x)     x = keras.layers.BatchNormalization(scale=False, center=True)(x)     x = keras.layers.ReLU()(x)     x = keras.layers.Conv2D(         filters=24,         kernel_size=6,         use_bias=False,         strides=2,     )(x)     x = keras.layers.BatchNormalization(scale=False, center=True)(x)     x = keras.layers.ReLU()(x)     x = keras.layers.Conv2D(         filters=32,         kernel_size=6,         padding=\"same\",         strides=2,         name=\"large_k\",     )(x)     x = keras.layers.BatchNormalization(scale=False, center=True)(x)     x = keras.layers.ReLU()(x)     x = keras.layers.GlobalAveragePooling2D()(x)     x = keras.layers.Dense(256, activation=\"relu\")(x)     x = keras.layers.Dropout(0.5)(x)     outputs = keras.layers.Dense(10)(x)     model = keras.Model(inputs, outputs)     return model   def get_datasets():     # Load the data and split it between train and test sets     (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()      # Scale images to the [0, 1] range     x_train = x_train.astype(\"float32\")     x_test = x_test.astype(\"float32\")     # Make sure images have shape (28, 28, 1)     x_train = np.expand_dims(x_train, -1)     x_test = np.expand_dims(x_test, -1)     print(\"x_train shape:\", x_train.shape)     print(x_train.shape[0], \"train samples\")     print(x_test.shape[0], \"test samples\")      # Create TF Datasets     train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))     eval_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))     return train_data, eval_data"},{"path":"https://keras.posit.co/articles/guides/distributed_training_with_jax.html","id":"single-host-multi-device-synchronous-training","dir":"Articles > Guides","previous_headings":"","what":"Single-host, multi-device synchronous training","title":"Multi-GPU distributed training with JAX","text":"setup, one machine several GPUs TPUs (typically 2 16). device run copy model (called replica). simplicity, follows, ’ll assume ’re dealing 8 GPUs, loss generality. works step training: current batch data (called global batch) split 8 different sub-batches (called local batches). instance, global batch 512 samples, 8 local batches 64 samples. 8 replicas independently processes local batch: run forward pass, backward pass, outputting gradient weights respect loss model local batch. weight updates originating local gradients efficiently merged across 8 replicas. done end every step, replicas always stay sync. practice, process synchronously updating weights model replicas handled level individual weight variable. done using jax.sharding.NamedSharding configured replicate variables. use single-host, multi-device synchronous training Keras model, use jax.sharding features. ’s works: first create device mesh using mesh_utils.create_device_mesh. specify want replicate model optimizer variables across devices using spec axis. specify want shard data across devices using spec splits along batch dimension. use jax.device_put replicate model optimizer variables across devices. happens beginning. training loop, batch process, use jax.device_put split batch across devices invoking train step. ’s flow, step split utility function: ’s !","code":"# Config num_epochs = 2 batch_size = 64  train_data, eval_data = get_datasets() train_data = train_data.batch(batch_size, drop_remainder=True)  model = get_model() optimizer = keras.optimizers.Adam(1e-3) loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Initialize all state with .build() (one_batch, one_batch_labels) = next(iter(train_data)) model.build(one_batch) optimizer.build(model.trainable_variables)   # This is the loss function that will be differentiated. # Keras provides a pure functional forward pass: model.stateless_call def compute_loss(trainable_variables, non_trainable_variables, x, y):     y_pred, updated_non_trainable_variables = model.stateless_call(         trainable_variables, non_trainable_variables, x     )     loss_value = loss(y, y_pred)     return loss_value, updated_non_trainable_variables   # Function to compute gradients compute_gradients = jax.value_and_grad(compute_loss, has_aux=True)   # Training step, Keras provides a pure functional optimizer.stateless_apply @jax.jit def train_step(train_state, x, y):     (         trainable_variables,         non_trainable_variables,         optimizer_variables,     ) = train_state     (loss_value, non_trainable_variables), grads = compute_gradients(         trainable_variables, non_trainable_variables, x, y     )      trainable_variables, optimizer_variables = optimizer.stateless_apply(         optimizer_variables, grads, trainable_variables     )      return loss_value, (         trainable_variables,         non_trainable_variables,         optimizer_variables,     )   # Replicate the model and optimizer variable on all devices def get_replicated_train_state(devices):     # All variables will be replicated on all devices     var_mesh = Mesh(devices, axis_names=(\"_\"))     # In NamedSharding, axes not mentioned are replicated (all axes here)     var_replication = NamedSharding(var_mesh, P())      # Apply the distribution settings to the model variables     trainable_variables = jax.device_put(         model.trainable_variables, var_replication     )     non_trainable_variables = jax.device_put(         model.non_trainable_variables, var_replication     )     optimizer_variables = jax.device_put(optimizer.variables, var_replication)      # Combine all state in a tuple     return (trainable_variables, non_trainable_variables, optimizer_variables)   num_devices = len(jax.local_devices()) print(f\"Running on {num_devices} devices: {jax.local_devices()}\") devices = mesh_utils.create_device_mesh((num_devices,))  # Data will be split along the batch axis data_mesh = Mesh(devices, axis_names=(\"batch\",))  # naming axes of the mesh data_sharding = NamedSharding(     data_mesh,     P(         \"batch\",     ), )  # naming axes of the sharded partition  # Display data sharding x, y = next(iter(train_data)) sharded_x = jax.device_put(x.numpy(), data_sharding) print(\"Data sharding\") jax.debug.visualize_array_sharding(jax.numpy.reshape(sharded_x, [-1, 28 * 28]))  train_state = get_replicated_train_state(devices)  # Custom training loop for epoch in range(num_epochs):     data_iter = iter(train_data)     for data in data_iter:         x, y = data         sharded_x = jax.device_put(x.numpy(), data_sharding)         loss_value, train_state = train_step(train_state, sharded_x, y.numpy())     print(\"Epoch\", epoch, \"loss:\", loss_value)  # Post-processing model state update to write them back into the model trainable_variables, non_trainable_variables, optimizer_variables = train_state for variable, value in zip(model.trainable_variables, trainable_variables):     variable.assign(value) for variable, value in zip(     model.non_trainable_variables, non_trainable_variables ):     variable.assign(value)"},{"path":"https://keras.posit.co/articles/guides/distributed_training_with_tensorflow.html","id":"introduction","dir":"Articles > Guides","previous_headings":"","what":"Introduction","title":"Multi-GPU distributed training with TensorFlow","text":"generally two ways distribute computation across multiple devices: Data parallelism, single model gets replicated multiple devices multiple machines. processes different batches data, merge results. exist many variants setup, differ different model replicas merge results, whether stay sync every batch whether loosely coupled, etc. Model parallelism, different parts single model run different devices, processing single batch data together. works best models naturally-parallel architecture, models feature multiple branches. guide focuses data parallelism, particular synchronous data parallelism, different replicas model stay sync batch process. Synchronicity keeps model convergence behavior identical see single-device training. Specifically, guide teaches use tf.distribute API train Keras models multiple GPUs, minimal changes code, multiple GPUs (typically 2 16) installed single machine (single host, multi-device training). common setup researchers small-scale industry workflows.","code":""},{"path":"https://keras.posit.co/articles/guides/distributed_training_with_tensorflow.html","id":"setup","dir":"Articles > Guides","previous_headings":"","what":"Setup","title":"Multi-GPU distributed training with TensorFlow","text":"","code":"import os  os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  import tensorflow as tf import keras as keras"},{"path":"https://keras.posit.co/articles/guides/distributed_training_with_tensorflow.html","id":"single-host-multi-device-synchronous-training","dir":"Articles > Guides","previous_headings":"","what":"Single-host, multi-device synchronous training","title":"Multi-GPU distributed training with TensorFlow","text":"setup, one machine several GPUs (typically 2 16). device run copy model (called replica). simplicity, follows, ’ll assume ’re dealing 8 GPUs, loss generality. works step training: current batch data (called global batch) split 8 different sub-batches (called local batches). instance, global batch 512 samples, 8 local batches 64 samples. 8 replicas independently processes local batch: run forward pass, backward pass, outputting gradient weights respect loss model local batch. weight updates originating local gradients efficiently merged across 8 replicas. done end every step, replicas always stay sync. practice, process synchronously updating weights model replicas handled level individual weight variable. done mirrored variable object. use single-host, multi-device synchronous training Keras model, use tf.distribute.MirroredStrategy API. ’s works: Instantiate MirroredStrategy, optionally configuring specific devices want use (default strategy use GPUs available). Use strategy object open scope, within scope, create Keras objects need contain variables. Typically, means creating & compiling model inside distribution scope. cases, first call fit() may also create variables, ’s good idea put fit() call scope well. Train model via fit() usual. Importantly, recommend use tf.data.Dataset objects load data multi-device distributed workflow. Schematically, looks like : ’s simple end--end runnable example:","code":"# Create a MirroredStrategy. strategy = tf.distribute.MirroredStrategy() print('Number of devices: {}'.format(strategy.num_replicas_in_sync))  # Open a strategy scope. with strategy.scope():     # Everything that creates variables should be under the strategy scope.     # In general this is only model construction & `compile()`.     model = Model(...)     model.compile(...)      # Train the model on all available devices.     model.fit(train_dataset, validation_data=val_dataset, ...)      # Test the model on all available devices.     model.evaluate(test_dataset) def get_compiled_model():     # Make a simple 2-layer densely-connected neural network.     inputs = keras.Input(shape=(784,))     x = keras.layers.Dense(256, activation=\"relu\")(inputs)     x = keras.layers.Dense(256, activation=\"relu\")(x)     outputs = keras.layers.Dense(10)(x)     model = keras.Model(inputs, outputs)     model.compile(         optimizer=keras.optimizers.Adam(),         loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),         metrics=[keras.metrics.SparseCategoricalAccuracy()],     )     return model   def get_dataset():     batch_size = 32     num_val_samples = 10000      # Return the MNIST dataset in the form of a `tf.data.Dataset`.     (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()      # Preprocess the data (these are Numpy arrays)     x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255     x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255     y_train = y_train.astype(\"float32\")     y_test = y_test.astype(\"float32\")      # Reserve num_val_samples samples for validation     x_val = x_train[-num_val_samples:]     y_val = y_train[-num_val_samples:]     x_train = x_train[:-num_val_samples]     y_train = y_train[:-num_val_samples]     return (         tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(             batch_size         ),         tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size),         tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size),     )   # Create a MirroredStrategy. strategy = tf.distribute.MirroredStrategy() print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))  # Open a strategy scope. with strategy.scope():     # Everything that creates variables should be under the strategy scope.     # In general this is only model construction & `compile()`.     model = get_compiled_model()      # Train the model on all available devices.     train_dataset, val_dataset, test_dataset = get_dataset()     model.fit(train_dataset, epochs=2, validation_data=val_dataset)      # Test the model on all available devices.     model.evaluate(test_dataset)"},{"path":"https://keras.posit.co/articles/guides/distributed_training_with_tensorflow.html","id":"using-callbacks-to-ensure-fault-tolerance","dir":"Articles > Guides","previous_headings":"","what":"Using callbacks to ensure fault tolerance","title":"Multi-GPU distributed training with TensorFlow","text":"using distributed training, always make sure strategy recover failure (fault tolerance). simplest way handle pass ModelCheckpoint callback fit(), save model regular intervals (e.g. every 100 batches every epoch). can restart training saved model. ’s simple example:","code":"# Prepare a directory to store all the checkpoints. checkpoint_dir = \"./ckpt\" if not os.path.exists(checkpoint_dir):     os.makedirs(checkpoint_dir)   def make_or_restore_model():     # Either restore the latest model, or create a fresh one     # if there is no checkpoint available.     checkpoints = [         checkpoint_dir + \"/\" + name for name in os.listdir(checkpoint_dir)     ]     if checkpoints:         latest_checkpoint = max(checkpoints, key=os.path.getctime)         print(\"Restoring from\", latest_checkpoint)         return keras.models.load_model(latest_checkpoint)     print(\"Creating a new model\")     return get_compiled_model()   def run_training(epochs=1):     # Create a MirroredStrategy.     strategy = tf.distribute.MirroredStrategy()      # Open a strategy scope and create/restore the model     with strategy.scope():         model = make_or_restore_model()          callbacks = [             # This callback saves a SavedModel every epoch             # We include the current epoch in the folder name.             keras.callbacks.ModelCheckpoint(                 filepath=checkpoint_dir + \"/ckpt-{epoch}.keras\",                 save_freq=\"epoch\",             )         ]         model.fit(             train_dataset,             epochs=epochs,             callbacks=callbacks,             validation_data=val_dataset,             verbose=2,         )   # Running the first time creates the model run_training(epochs=1)  # Calling the same function again will resume from where we left off run_training(epochs=1)"},{"path":"https://keras.posit.co/articles/guides/distributed_training_with_tensorflow.html","id":"tf-data-performance-tips","dir":"Articles > Guides","previous_headings":"","what":"tf.data performance tips","title":"Multi-GPU distributed training with TensorFlow","text":"distributed training, efficiency load data can often become critical. tips make sure tf.data pipelines run fast possible. Note dataset batching creating dataset, make sure batched global batch size. instance, 8 GPUs capable running batch 64 samples, call use global batch size 512. Calling dataset.cache() call .cache() dataset, data cached running first iteration data. Every subsequent iteration use cached data. cache can memory (default) local file specify. can improve performance : data expected change iteration iteration reading data remote distributed filesystem reading data local disk, data fit memory workflow significantly IO-bound (e.g. reading & decoding image files). Calling dataset.prefetch(buffer_size) almost always call .prefetch(buffer_size) creating dataset. means data pipeline run asynchronously model, new samples preprocessed stored buffer current batch samples used train model. next batch prefetched GPU memory time current batch . ’s !","code":""},{"path":"https://keras.posit.co/articles/guides/distributed_training_with_torch.html","id":"introduction","dir":"Articles > Guides","previous_headings":"","what":"Introduction","title":"Multi-GPU distributed training with PyTorch","text":"generally two ways distribute computation across multiple devices: Data parallelism, single model gets replicated multiple devices multiple machines. processes different batches data, merge results. exist many variants setup, differ different model replicas merge results, whether stay sync every batch whether loosely coupled, etc. Model parallelism, different parts single model run different devices, processing single batch data together. works best models naturally-parallel architecture, models feature multiple branches. guide focuses data parallelism, particular synchronous data parallelism, different replicas model stay sync batch process. Synchronicity keeps model convergence behavior identical see single-device training. Specifically, guide teaches use PyTorch’s DistributedDataParallel module wrapper train Keras, minimal changes code, multiple GPUs (typically 2 16) installed single machine (single host, multi-device training). common setup researchers small-scale industry workflows.","code":""},{"path":"https://keras.posit.co/articles/guides/distributed_training_with_torch.html","id":"setup","dir":"Articles > Guides","previous_headings":"","what":"Setup","title":"Multi-GPU distributed training with PyTorch","text":"Let’s start defining function creates model train, function creates dataset train (MNIST case). Next, let’s define simple PyTorch training loop targets GPU (note calls .cuda()).","code":"import os  os.environ[\"KERAS_BACKEND\"] = \"torch\"  import torch import numpy as np import keras as keras   def get_model():     # Make a simple convnet with batch normalization and dropout.     inputs = keras.Input(shape=(28, 28, 1))     x = keras.layers.Rescaling(1.0 / 255.0)(inputs)     x = keras.layers.Conv2D(         filters=12, kernel_size=3, padding=\"same\", use_bias=False     )(x)     x = keras.layers.BatchNormalization(scale=False, center=True)(x)     x = keras.layers.ReLU()(x)     x = keras.layers.Conv2D(         filters=24,         kernel_size=6,         use_bias=False,         strides=2,     )(x)     x = keras.layers.BatchNormalization(scale=False, center=True)(x)     x = keras.layers.ReLU()(x)     x = keras.layers.Conv2D(         filters=32,         kernel_size=6,         padding=\"same\",         strides=2,         name=\"large_k\",     )(x)     x = keras.layers.BatchNormalization(scale=False, center=True)(x)     x = keras.layers.ReLU()(x)     x = keras.layers.GlobalAveragePooling2D()(x)     x = keras.layers.Dense(256, activation=\"relu\")(x)     x = keras.layers.Dropout(0.5)(x)     outputs = keras.layers.Dense(10)(x)     model = keras.Model(inputs, outputs)     return model   def get_dataset():     # Load the data and split it between train and test sets     (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()      # Scale images to the [0, 1] range     x_train = x_train.astype(\"float32\")     x_test = x_test.astype(\"float32\")     # Make sure images have shape (28, 28, 1)     x_train = np.expand_dims(x_train, -1)     x_test = np.expand_dims(x_test, -1)     print(\"x_train shape:\", x_train.shape)      # Create a TensorDataset     dataset = torch.utils.data.TensorDataset(         torch.from_numpy(x_train), torch.from_numpy(y_train)     )     return dataset def train_model(model, dataloader, num_epochs, optimizer, loss_fn):     for epoch in range(num_epochs):         running_loss = 0.0         running_loss_count = 0         for batch_idx, (inputs, targets) in enumerate(dataloader):             inputs = inputs.cuda(non_blocking=True)             targets = targets.cuda(non_blocking=True)              # Forward pass             outputs = model(inputs)             loss = loss_fn(outputs, targets)              # Backward and optimize             optimizer.zero_grad()             loss.backward()             optimizer.step()              running_loss += loss.item()             running_loss_count += 1          # Print loss statistics         print(             f\"Epoch {epoch + 1}/{num_epochs}, \"             f\"Loss: {running_loss / running_loss_count}\"         )"},{"path":"https://keras.posit.co/articles/guides/distributed_training_with_torch.html","id":"single-host-multi-device-synchronous-training","dir":"Articles > Guides","previous_headings":"","what":"Single-host, multi-device synchronous training","title":"Multi-GPU distributed training with PyTorch","text":"setup, one machine several GPUs (typically 2 16). device run copy model (called replica). simplicity, follows, ’ll assume ’re dealing 8 GPUs, loss generality. works step training: current batch data (called global batch) split 8 different sub-batches (called local batches). instance, global batch 512 samples, 8 local batches 64 samples. 8 replicas independently processes local batch: run forward pass, backward pass, outputting gradient weights respect loss model local batch. weight updates originating local gradients efficiently merged across 8 replicas. done end every step, replicas always stay sync. practice, process synchronously updating weights model replicas handled level individual weight variable. done mirrored variable object. use single-host, multi-device synchronous training Keras model, use torch.nn.parallel.DistributedDataParallel module wrapper. ’s works: use torch.multiprocessing.start_processes start multiple Python processes, one per device. process run per_device_launch_fn function. uses torch.distributed.init_process_group torch.cuda.set_device configure device used process. uses torch.utils.data.distributed.DistributedSampler torch.utils.data.DataLoader turn data distributed data loader. also uses torch.nn.parallel.DistributedDataParallel turn model distributed PyTorch module. calls train_model function. train_model function run process, model using separate device process. ’s flow, step split utility function: Time start multiple processes: ’s !","code":"# Config num_gpu = torch.cuda.device_count() num_epochs = 2 batch_size = 64 print(f\"Running on {num_gpu} GPUs\")   def setup_device(current_gpu_index, num_gpus):     # Device setup     os.environ[\"MASTER_ADDR\"] = \"localhost\"     os.environ[\"MASTER_PORT\"] = \"56492\"     device = torch.device(\"cuda:{}\".format(current_gpu_index))     torch.distributed.init_process_group(         backend=\"nccl\",         init_method=\"env://\",         world_size=num_gpus,         rank=current_gpu_index,     )     torch.cuda.set_device(device)   def cleanup():     torch.distributed.destroy_process_group()   def prepare_dataloader(dataset, current_gpu_index, num_gpus, batch_size):     sampler = torch.utils.data.distributed.DistributedSampler(         dataset,         num_replicas=num_gpus,         rank=current_gpu_index,         shuffle=False,     )     dataloader = torch.utils.data.DataLoader(         dataset,         sampler=sampler,         batch_size=batch_size,         shuffle=False,     )     return dataloader   def per_device_launch_fn(current_gpu_index, num_gpu):     # Setup the process groups     setup_device(current_gpu_index, num_gpu)      dataset = get_dataset()     model = get_model()      # prepare the dataloader     dataloader = prepare_dataloader(         dataset, current_gpu_index, num_gpu, batch_size     )      # Instantiate the torch optimizer     optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)      # Instantiate the torch loss function     loss_fn = torch.nn.CrossEntropyLoss()      # Put model on device     model = model.to(current_gpu_index)     ddp_model = torch.nn.parallel.DistributedDataParallel(         model, device_ids=[current_gpu_index], output_device=current_gpu_index     )      train_model(ddp_model, dataloader, num_epochs, optimizer, loss_fn)      cleanup() if __name__ == \"__main__\":     # We use the \"fork\" method rather than \"spawn\" to support notebooks     torch.multiprocessing.start_processes(         per_device_launch_fn,         args=(num_gpu,),         nprocs=num_gpu,         join=True,         start_method=\"fork\",     )"},{"path":"https://keras.posit.co/articles/guides/functional_api.html","id":"setup","dir":"Articles > Guides","previous_headings":"","what":"Setup","title":"The Functional API","text":"","code":"import numpy as np import keras as keras from keras import layers from keras import ops"},{"path":"https://keras.posit.co/articles/guides/functional_api.html","id":"introduction","dir":"Articles > Guides","previous_headings":"","what":"Introduction","title":"The Functional API","text":"Keras functional API way create models flexible keras.Sequential API. functional API can handle models non-linear topology, shared layers, even multiple inputs outputs. main idea deep learning model usually directed acyclic graph (DAG) layers. functional API way build graphs layers. Consider following model: basic graph three layers. build model using functional API, start creating input node: shape data set 784-dimensional vector. batch size always omitted since shape sample specified. , example, image input shape (32, 32, 3), use: inputs returned contains information shape dtype input data feed model. ’s shape: ’s dtype: create new node graph layers calling layer inputs object: “layer call” action like drawing arrow “inputs” layer created. ’re “passing” inputs dense layer, get x output. Let’s add layers graph layers: point, can create Model specifying inputs outputs graph layers: Let’s check model summary looks like: can also plot model graph: , optionally, display input output shapes layer plotted graph: figure code almost identical. code version, connection arrows replaced call operation. “graph layers” intuitive mental image deep learning model, functional API way create models closely mirrors .","code":"(input: 784-dimensional vectors)        ↧ [Dense (64 units, relu activation)]        ↧ [Dense (64 units, relu activation)]        ↧ [Dense (10 units, softmax activation)]        ↧ (output: logits of a probability distribution over 10 classes) inputs = keras.Input(shape=(784,)) # Just for demonstration purposes. img_inputs = keras.Input(shape=(32, 32, 3)) inputs.shape inputs.dtype dense = layers.Dense(64, activation=\"relu\") x = dense(inputs) x = layers.Dense(64, activation=\"relu\")(x) outputs = layers.Dense(10)(x) model = keras.Model(inputs=inputs, outputs=outputs, name=\"mnist_model\") model.summary() keras.utils.plot_model(model, \"my_first_model.png\") keras.utils.plot_model(     model, \"my_first_model_with_shape_info.png\", show_shapes=True )"},{"path":"https://keras.posit.co/articles/guides/functional_api.html","id":"training-evaluation-and-inference","dir":"Articles > Guides","previous_headings":"","what":"Training, evaluation, and inference","title":"The Functional API","text":"Training, evaluation, inference work exactly way models built using functional API Sequential models. Model class offers built-training loop (fit() method) built-evaluation loop (evaluate() method). Note can easily customize loops implement training routines beyond supervised learning (e.g. GANs). , load MNIST image data, reshape vectors, fit model data (monitoring performance validation split), evaluate model test data: reading, see training evaluation guide.","code":"(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()  x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255 x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255  model.compile(     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),     optimizer=keras.optimizers.RMSprop(),     metrics=[\"accuracy\"], )  history = model.fit(     x_train, y_train, batch_size=64, epochs=2, validation_split=0.2 )  test_scores = model.evaluate(x_test, y_test, verbose=2) print(\"Test loss:\", test_scores[0]) print(\"Test accuracy:\", test_scores[1])"},{"path":"https://keras.posit.co/articles/guides/functional_api.html","id":"save-and-serialize","dir":"Articles > Guides","previous_headings":"","what":"Save and serialize","title":"The Functional API","text":"Saving model serialization work way models built using functional API Sequential models. standard way save functional model call model.save() save entire model single file. can later recreate model file, even code built model longer available. saved file includes : - model architecture - model weight values (learned training) - model training config, (passed compile()) - optimizer state, (restart training left ) details, read model serialization & saving guide.","code":"model.save(\"my_model.keras\") del model # Recreate the exact same model purely from the file: model = keras.models.load_model(\"my_model.keras\")"},{"path":"https://keras.posit.co/articles/guides/functional_api.html","id":"use-the-same-graph-of-layers-to-define-multiple-models","dir":"Articles > Guides","previous_headings":"","what":"Use the same graph of layers to define multiple models","title":"The Functional API","text":"functional API, models created specifying inputs outputs graph layers. means single graph layers can used generate multiple models. example , use stack layers instantiate two models: encoder model turns image inputs 16-dimensional vectors, end--end autoencoder model training. , decoding architecture strictly symmetrical encoding architecture, output shape input shape (28, 28, 1). reverse Conv2D layer Conv2DTranspose layer, reverse MaxPooling2D layer UpSampling2D layer.","code":"encoder_input = keras.Input(shape=(28, 28, 1), name=\"img\") x = layers.Conv2D(16, 3, activation=\"relu\")(encoder_input) x = layers.Conv2D(32, 3, activation=\"relu\")(x) x = layers.MaxPooling2D(3)(x) x = layers.Conv2D(32, 3, activation=\"relu\")(x) x = layers.Conv2D(16, 3, activation=\"relu\")(x) encoder_output = layers.GlobalMaxPooling2D()(x)  encoder = keras.Model(encoder_input, encoder_output, name=\"encoder\") encoder.summary()  x = layers.Reshape((4, 4, 1))(encoder_output) x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x) x = layers.Conv2DTranspose(32, 3, activation=\"relu\")(x) x = layers.UpSampling2D(3)(x) x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x) decoder_output = layers.Conv2DTranspose(1, 3, activation=\"relu\")(x)  autoencoder = keras.Model(encoder_input, decoder_output, name=\"autoencoder\") autoencoder.summary()"},{"path":"https://keras.posit.co/articles/guides/functional_api.html","id":"all-models-are-callable-just-like-layers","dir":"Articles > Guides","previous_headings":"","what":"All models are callable, just like layers","title":"The Functional API","text":"can treat model layer invoking Input output another layer. calling model aren’t just reusing architecture model, ’re also reusing weights. see action, ’s different take autoencoder example creates encoder model, decoder model, chains two calls obtain autoencoder model: can see, model can nested: model can contain sub-models (since model just like layer). common use case model nesting ensembling. example, ’s ensemble set models single model averages predictions:","code":"encoder_input = keras.Input(shape=(28, 28, 1), name=\"original_img\") x = layers.Conv2D(16, 3, activation=\"relu\")(encoder_input) x = layers.Conv2D(32, 3, activation=\"relu\")(x) x = layers.MaxPooling2D(3)(x) x = layers.Conv2D(32, 3, activation=\"relu\")(x) x = layers.Conv2D(16, 3, activation=\"relu\")(x) encoder_output = layers.GlobalMaxPooling2D()(x)  encoder = keras.Model(encoder_input, encoder_output, name=\"encoder\") encoder.summary()  decoder_input = keras.Input(shape=(16,), name=\"encoded_img\") x = layers.Reshape((4, 4, 1))(decoder_input) x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x) x = layers.Conv2DTranspose(32, 3, activation=\"relu\")(x) x = layers.UpSampling2D(3)(x) x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x) decoder_output = layers.Conv2DTranspose(1, 3, activation=\"relu\")(x)  decoder = keras.Model(decoder_input, decoder_output, name=\"decoder\") decoder.summary()  autoencoder_input = keras.Input(shape=(28, 28, 1), name=\"img\") encoded_img = encoder(autoencoder_input) decoded_img = decoder(encoded_img) autoencoder = keras.Model(autoencoder_input, decoded_img, name=\"autoencoder\") autoencoder.summary() def get_model():     inputs = keras.Input(shape=(128,))     outputs = layers.Dense(1)(inputs)     return keras.Model(inputs, outputs)   model1 = get_model() model2 = get_model() model3 = get_model()  inputs = keras.Input(shape=(128,)) y1 = model1(inputs) y2 = model2(inputs) y3 = model3(inputs) outputs = layers.average([y1, y2, y3]) ensemble_model = keras.Model(inputs=inputs, outputs=outputs)"},{"path":[]},{"path":"https://keras.posit.co/articles/guides/functional_api.html","id":"models-with-multiple-inputs-and-outputs","dir":"Articles > Guides","previous_headings":"Manipulate complex graph topologies","what":"Models with multiple inputs and outputs","title":"The Functional API","text":"functional API makes easy manipulate multiple inputs outputs. handled Sequential API. example, ’re building system ranking customer issue tickets priority routing correct department, model three inputs: title ticket (text input), text body ticket (text input), tags added user (categorical input) model two outputs: priority score 0 1 (scalar sigmoid output), department handle ticket (softmax output set departments). can build model lines functional API: Now plot model: compiling model, can assign different losses output. can even assign different weights loss – modulate contribution total training loss. Since output layers different names, also specify losses loss weights corresponding layer names: Train model passing lists NumPy arrays inputs targets: calling fit Dataset object, yield either tuple lists like ([title_data, body_data, tags_data], [priority_targets, dept_targets]) tuple dictionaries like ({'title': title_data, 'body': body_data, 'tags': tags_data}, {'priority': priority_targets, 'department': dept_targets}). detailed explanation, refer training evaluation guide.","code":"num_tags = 12  # Number of unique issue tags num_words = 10000  # Size of vocabulary obtained when preprocessing text data num_departments = 4  # Number of departments for predictions  title_input = keras.Input(     shape=(None,), name=\"title\" )  # Variable-length sequence of ints body_input = keras.Input(     shape=(None,), name=\"body\" )  # Variable-length sequence of ints tags_input = keras.Input(     shape=(num_tags,), name=\"tags\" )  # Binary vectors of size `num_tags`  # Embed each word in the title into a 64-dimensional vector title_features = layers.Embedding(num_words, 64)(title_input) # Embed each word in the text into a 64-dimensional vector body_features = layers.Embedding(num_words, 64)(body_input)  # Reduce sequence of embedded words in the title into a single 128-dimensional vector title_features = layers.LSTM(128)(title_features) # Reduce sequence of embedded words in the body into a single 32-dimensional vector body_features = layers.LSTM(32)(body_features)  # Merge all available features into a single large vector via concatenation x = layers.concatenate([title_features, body_features, tags_input])  # Stick a logistic regression for priority prediction on top of the features priority_pred = layers.Dense(1, name=\"priority\")(x) # Stick a department classifier on top of the features department_pred = layers.Dense(num_departments, name=\"department\")(x)  # Instantiate an end-to-end model predicting both priority and department model = keras.Model(     inputs=[title_input, body_input, tags_input],     outputs={\"priority\": priority_pred, \"department\": department_pred}, ) keras.utils.plot_model(     model, \"multi_input_and_output_model.png\", show_shapes=True ) model.compile(     optimizer=keras.optimizers.RMSprop(1e-3),     loss=[         keras.losses.BinaryCrossentropy(from_logits=True),         keras.losses.CategoricalCrossentropy(from_logits=True),     ],     loss_weights=[1.0, 0.2], ) model.compile(     optimizer=keras.optimizers.RMSprop(1e-3),     loss={         \"priority\": keras.losses.BinaryCrossentropy(from_logits=True),         \"department\": keras.losses.CategoricalCrossentropy(from_logits=True),     },     loss_weights={\"priority\": 1.0, \"department\": 0.2}, ) # Dummy input data title_data = np.random.randint(num_words, size=(1280, 10)) body_data = np.random.randint(num_words, size=(1280, 100)) tags_data = np.random.randint(2, size=(1280, num_tags)).astype(\"float32\")  # Dummy target data priority_targets = np.random.random(size=(1280, 1)) dept_targets = np.random.randint(2, size=(1280, num_departments))  model.fit(     {\"title\": title_data, \"body\": body_data, \"tags\": tags_data},     {\"priority\": priority_targets, \"department\": dept_targets},     epochs=2,     batch_size=32, )"},{"path":"https://keras.posit.co/articles/guides/functional_api.html","id":"a-toy-resnet-model","dir":"Articles > Guides","previous_headings":"Manipulate complex graph topologies","what":"A toy ResNet model","title":"The Functional API","text":"addition models multiple inputs outputs, functional API makes easy manipulate non-linear connectivity topologies – models layers connected sequentially, Sequential API handle. common use case residual connections. Let’s build toy ResNet model CIFAR10 demonstrate : Plot model: Now train model:","code":"inputs = keras.Input(shape=(32, 32, 3), name=\"img\") x = layers.Conv2D(32, 3, activation=\"relu\")(inputs) x = layers.Conv2D(64, 3, activation=\"relu\")(x) block_1_output = layers.MaxPooling2D(3)(x)  x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(block_1_output) x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x) block_2_output = layers.add([x, block_1_output])  x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(block_2_output) x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x) block_3_output = layers.add([x, block_2_output])  x = layers.Conv2D(64, 3, activation=\"relu\")(block_3_output) x = layers.GlobalAveragePooling2D()(x) x = layers.Dense(256, activation=\"relu\")(x) x = layers.Dropout(0.5)(x) outputs = layers.Dense(10)(x)  model = keras.Model(inputs, outputs, name=\"toy_resnet\") model.summary() keras.utils.plot_model(model, \"mini_resnet.png\", show_shapes=True) (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()  x_train = x_train.astype(\"float32\") / 255.0 x_test = x_test.astype(\"float32\") / 255.0 y_train = keras.utils.to_categorical(y_train, 10) y_test = keras.utils.to_categorical(y_test, 10)  model.compile(     optimizer=keras.optimizers.RMSprop(1e-3),     loss=keras.losses.CategoricalCrossentropy(from_logits=True),     metrics=[\"acc\"], ) # We restrict the data to the first 1000 samples so as to limit execution time # on Colab. Try to train on the entire dataset until convergence! model.fit(     x_train[:1000],     y_train[:1000],     batch_size=64,     epochs=1,     validation_split=0.2, )"},{"path":"https://keras.posit.co/articles/guides/functional_api.html","id":"shared-layers","dir":"Articles > Guides","previous_headings":"","what":"Shared layers","title":"The Functional API","text":"Another good use functional API models use shared layers. Shared layers layer instances reused multiple times model – learn features correspond multiple paths graph--layers. Shared layers often used encode inputs similar spaces (say, two different pieces text feature similar vocabulary). enable sharing information across different inputs, make possible train model less data. given word seen one inputs, benefit processing inputs pass shared layer. share layer functional API, call layer instance multiple times. instance, ’s Embedding layer shared across two different text inputs:","code":"# Embedding for 1000 unique words mapped to 128-dimensional vectors shared_embedding = layers.Embedding(1000, 128)  # Variable-length sequence of integers text_input_a = keras.Input(shape=(None,), dtype=\"int32\")  # Variable-length sequence of integers text_input_b = keras.Input(shape=(None,), dtype=\"int32\")  # Reuse the same layer to encode both inputs encoded_input_a = shared_embedding(text_input_a) encoded_input_b = shared_embedding(text_input_b)"},{"path":"https://keras.posit.co/articles/guides/functional_api.html","id":"extract-and-reuse-nodes-in-the-graph-of-layers","dir":"Articles > Guides","previous_headings":"","what":"Extract and reuse nodes in the graph of layers","title":"The Functional API","text":"graph layers manipulating static data structure, can accessed inspected. able plot functional models images. also means can access activations intermediate layers (“nodes” graph) reuse elsewhere – useful something like feature extraction. Let’s look example. VGG19 model weights pretrained ImageNet: intermediate activations model, obtained querying graph data structure: Use features create new feature-extraction model returns values intermediate layer activations: comes handy tasks like neural style transfer, among things.","code":"vgg19 = keras.applications.VGG19() features_list = [layer.output for layer in vgg19.layers] feat_extraction_model = keras.Model(inputs=vgg19.input, outputs=features_list)  img = np.random.random((1, 224, 224, 3)).astype(\"float32\") extracted_features = feat_extraction_model(img)"},{"path":"https://keras.posit.co/articles/guides/functional_api.html","id":"extend-the-api-using-custom-layers","dir":"Articles > Guides","previous_headings":"","what":"Extend the API using custom layers","title":"The Functional API","text":"keras includes wide range built-layers, example: Convolutional layers: Conv1D, Conv2D, Conv3D, Conv2DTranspose Pooling layers: MaxPooling1D, MaxPooling2D, MaxPooling3D, AveragePooling1D RNN layers: GRU, LSTM, ConvLSTM2D BatchNormalization, Dropout, Embedding, etc. don’t find need, ’s easy extend API creating layers. layers subclass Layer class implement: call method, specifies computation done layer. build method, creates weights layer (just style convention since can create weights __init__, well). learn creating layers scratch, read custom layers models guide. following basic implementation keras.layers.Dense: serialization support custom layer, define get_config() method returns constructor arguments layer instance: Optionally, implement class method from_config(cls, config) used recreating layer instance given config dictionary. default implementation from_config :","code":"class CustomDense(layers.Layer):     def __init__(self, units=32):         super().__init__()         self.units = units      def build(self, input_shape):         self.w = self.add_weight(             shape=(input_shape[-1], self.units),             initializer=\"random_normal\",             trainable=True,         )         self.b = self.add_weight(             shape=(self.units,), initializer=\"random_normal\", trainable=True         )      def call(self, inputs):         return ops.matmul(inputs, self.w) + self.b   inputs = keras.Input((4,)) outputs = CustomDense(10)(inputs)  model = keras.Model(inputs, outputs) class CustomDense(layers.Layer):     def __init__(self, units=32):         super().__init__()         self.units = units      def build(self, input_shape):         self.w = self.add_weight(             shape=(input_shape[-1], self.units),             initializer=\"random_normal\",             trainable=True,         )         self.b = self.add_weight(             shape=(self.units,), initializer=\"random_normal\", trainable=True         )      def call(self, inputs):         return ops.matmul(inputs, self.w) + self.b      def get_config(self):         return {\"units\": self.units}   inputs = keras.Input((4,)) outputs = CustomDense(10)(inputs)  model = keras.Model(inputs, outputs) config = model.get_config()  new_model = keras.Model.from_config(     config, custom_objects={\"CustomDense\": CustomDense} ) def from_config(cls, config):   return cls(**config)"},{"path":"https://keras.posit.co/articles/guides/functional_api.html","id":"when-to-use-the-functional-api","dir":"Articles > Guides","previous_headings":"","what":"When to use the functional API","title":"The Functional API","text":"use Keras functional API create new model, just subclass Model class directly? general, functional API higher-level, easier safer, number features subclassed models support. However, model subclassing provides greater flexibility building models easily expressible directed acyclic graphs layers. example, implement Tree-RNN functional API subclass Model directly. -depth look differences functional API model subclassing, read Symbolic Imperative APIs TensorFlow 2.0?.","code":""},{"path":"https://keras.posit.co/articles/guides/functional_api.html","id":"functional-api-strengths","dir":"Articles > Guides","previous_headings":"When to use the functional API","what":"Functional API strengths:","title":"The Functional API","text":"following properties also true Sequential models (also data structures), true subclassed models (Python bytecode, data structures).","code":""},{"path":"https://keras.posit.co/articles/guides/functional_api.html","id":"less-verbose","dir":"Articles > Guides","previous_headings":"When to use the functional API > Functional API strengths:","what":"Less verbose","title":"The Functional API","text":"super().__init__(...), def call(self, ...):, etc. Compare: subclassed version:","code":"inputs = keras.Input(shape=(32,)) x = layers.Dense(64, activation='relu')(inputs) outputs = layers.Dense(10)(x) mlp = keras.Model(inputs, outputs) class MLP(keras.Model):    def __init__(self, **kwargs):     super().__init__(**kwargs)     self.dense_1 = layers.Dense(64, activation='relu')     self.dense_2 = layers.Dense(10)    def call(self, inputs):     x = self.dense_1(inputs)     return self.dense_2(x)  # Instantiate the model. mlp = MLP() # Necessary to create the model's state. # The model doesn't have a state until it's called at least once. _ = mlp(ops.zeros((1, 32)))"},{"path":"https://keras.posit.co/articles/guides/functional_api.html","id":"model-validation-while-defining-its-connectivity-graph","dir":"Articles > Guides","previous_headings":"When to use the functional API > Functional API strengths:","what":"Model validation while defining its connectivity graph","title":"The Functional API","text":"functional API, input specification (shape dtype) created advance (using Input). Every time call layer, layer checks specification passed matches assumptions, raise helpful error message . guarantees model can build functional API run. debugging – convergence-related debugging – happens statically model construction execution time. similar type checking compiler.","code":""},{"path":"https://keras.posit.co/articles/guides/functional_api.html","id":"a-functional-model-is-plottable-and-inspectable","dir":"Articles > Guides","previous_headings":"When to use the functional API > Functional API strengths:","what":"A functional model is plottable and inspectable","title":"The Functional API","text":"can plot model graph, can easily access intermediate nodes graph. example, extract reuse activations intermediate layers (seen previous example):","code":"features_list = [layer.output for layer in vgg19.layers] feat_extraction_model = keras.Model(inputs=vgg19.input, outputs=features_list)"},{"path":"https://keras.posit.co/articles/guides/functional_api.html","id":"a-functional-model-can-be-serialized-or-cloned","dir":"Articles > Guides","previous_headings":"When to use the functional API > Functional API strengths:","what":"A functional model can be serialized or cloned","title":"The Functional API","text":"functional model data structure rather piece code, safely serializable can saved single file allows recreate exact model without access original code. See serialization & saving guide. serialize subclassed model, necessary implementer specify get_config() from_config() method model level.","code":""},{"path":[]},{"path":"https://keras.posit.co/articles/guides/functional_api.html","id":"it-does-not-support-dynamic-architectures","dir":"Articles > Guides","previous_headings":"When to use the functional API > Functional API weakness:","what":"It does not support dynamic architectures","title":"The Functional API","text":"functional API treats models DAGs layers. true deep learning architectures, – example, recursive networks Tree RNNs follow assumption implemented functional API.","code":""},{"path":"https://keras.posit.co/articles/guides/functional_api.html","id":"mix-and-match-api-styles","dir":"Articles > Guides","previous_headings":"","what":"Mix-and-match API styles","title":"The Functional API","text":"Choosing functional API Model subclassing isn’t binary decision restricts one category models. models keras API can interact , whether ’re Sequential models, functional models, subclassed models written scratch. can always use functional model Sequential model part subclassed model layer: can use subclassed layer model functional API long implements call method follows one following patterns: call(self, inputs, **kwargs) – inputs tensor nested structure tensors (e.g. list tensors), **kwargs non-tensor arguments (non-inputs). call(self, inputs, training=None, **kwargs) – training boolean indicating whether layer behave training mode inference mode. call(self, inputs, mask=None, **kwargs) – mask boolean mask tensor (useful RNNs, instance). call(self, inputs, training=None, mask=None, **kwargs) – course, can masking training-specific behavior time. Additionally, implement get_config method custom Layer model, functional models create still serializable cloneable. ’s quick example custom RNN, written scratch, used functional model:","code":"units = 32 timesteps = 10 input_dim = 5  # Define a Functional model inputs = keras.Input((None, units)) x = layers.GlobalAveragePooling1D()(inputs) outputs = layers.Dense(1)(x) model = keras.Model(inputs, outputs)   class CustomRNN(layers.Layer):     def __init__(self):         super().__init__()         self.units = units         self.projection_1 = layers.Dense(units=units, activation=\"tanh\")         self.projection_2 = layers.Dense(units=units, activation=\"tanh\")         # Our previously-defined Functional model         self.classifier = model      def call(self, inputs):         outputs = []         state = ops.zeros(shape=(inputs.shape[0], self.units))         for t in range(inputs.shape[1]):             x = inputs[:, t, :]             h = self.projection_1(x)             y = h + self.projection_2(state)             state = y             outputs.append(y)         features = ops.stack(outputs, axis=1)         print(features.shape)         return self.classifier(features)   rnn_model = CustomRNN() _ = rnn_model(ops.zeros((1, timesteps, input_dim))) units = 32 timesteps = 10 input_dim = 5 batch_size = 16   class CustomRNN(layers.Layer):     def __init__(self):         super().__init__()         self.units = units         self.projection_1 = layers.Dense(units=units, activation=\"tanh\")         self.projection_2 = layers.Dense(units=units, activation=\"tanh\")         self.classifier = layers.Dense(1)      def call(self, inputs):         outputs = []         state = ops.zeros(shape=(inputs.shape[0], self.units))         for t in range(inputs.shape[1]):             x = inputs[:, t, :]             h = self.projection_1(x)             y = h + self.projection_2(state)             state = y             outputs.append(y)         features = ops.stack(outputs, axis=1)         return self.classifier(features)   # Note that you specify a static batch size for the inputs with the `batch_shape` # arg, because the inner computation of `CustomRNN` requires a static batch size # (when you create the `state` zeros tensor). inputs = keras.Input(batch_shape=(batch_size, timesteps, input_dim)) x = layers.Conv1D(32, 3)(inputs) outputs = CustomRNN()(x)  model = keras.Model(inputs, outputs)  rnn_model = CustomRNN() _ = rnn_model(ops.zeros((1, 10, 5)))"},{"path":"https://keras.posit.co/articles/guides/getting_started_with_keras_core.html","id":"introduction","dir":"Articles > Guides","previous_headings":"","what":"Introduction","title":"Getting started with Keras Core","text":"Keras Core full implementation Keras API works TensorFlow, JAX, PyTorch interchangeably. notebook walk key Keras Core workflows. First, let’s install Keras Core: pip install -q keras-core","code":""},{"path":"https://keras.posit.co/articles/guides/getting_started_with_keras_core.html","id":"setup","dir":"Articles > Guides","previous_headings":"","what":"Setup","title":"Getting started with Keras Core","text":"’re going using JAX backend – can edit string \"tensorflow\" \"torch\" hit “Restart runtime”, whole notebook run just ! entire guide backend-agnostic.","code":"import numpy as np import os  os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Note that keras_core should only be imported after the backend # has been configured. The backend cannot be changed once the # package is imported. import keras_core as keras"},{"path":"https://keras.posit.co/articles/guides/getting_started_with_keras_core.html","id":"a-first-example-a-mnist-convnet","dir":"Articles > Guides","previous_headings":"","what":"A first example: A MNIST convnet","title":"Getting started with Keras Core","text":"Let’s start Hello World ML: training convnet classify MNIST digits. ’s data: ’s model. Different model-building options Keras offers include: Sequential API (use ) Functional API (typical) Writing models via subclassing (advanced use cases) ’s model summary: use compile() method specify optimizer, loss function, metrics monitor. Note JAX TensorFlow backends, XLA compilation turned default. Let’s train evaluate model. ’ll set aside validation split 15% data training monitor generalization unseen data. training, saving model end epoch. can also save model latest state like : reload like : Next, can query predictions class probabilities predict(): ’s basics!","code":"# Load the data and split it between train and test sets (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()  # Scale images to the [0, 1] range x_train = x_train.astype(\"float32\") / 255 x_test = x_test.astype(\"float32\") / 255 # Make sure images have shape (28, 28, 1) x_train = np.expand_dims(x_train, -1) x_test = np.expand_dims(x_test, -1) print(\"x_train shape:\", x_train.shape) print(\"y_train shape:\", y_train.shape) print(x_train.shape[0], \"train samples\") print(x_test.shape[0], \"test samples\") # Model parameters num_classes = 10 input_shape = (28, 28, 1)  model = keras.Sequential(     [         keras.layers.Input(shape=input_shape),         keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),         keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),         keras.layers.MaxPooling2D(pool_size=(2, 2)),         keras.layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),         keras.layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),         keras.layers.GlobalAveragePooling2D(),         keras.layers.Dropout(0.5),         keras.layers.Dense(num_classes, activation=\"softmax\"),     ] ) model.summary() model.compile(     loss=keras.losses.SparseCategoricalCrossentropy(),     optimizer=keras.optimizers.Adam(learning_rate=1e-3),     metrics=[         keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),     ], ) batch_size = 128 epochs = 20  callbacks = [     keras.callbacks.ModelCheckpoint(filepath=\"model_at_epoch_{epoch}.keras\"),     keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2), ]  model.fit(     x_train,     y_train,     batch_size=batch_size,     epochs=epochs,     validation_split=0.15,     callbacks=callbacks, ) score = model.evaluate(x_test, y_test, verbose=0) model.save(\"final_model.keras\") model = keras.saving.load_model(\"final_model.keras\") predictions = model.predict(x_test)"},{"path":"https://keras.posit.co/articles/guides/getting_started_with_keras_core.html","id":"writing-cross-framework-custom-components","dir":"Articles > Guides","previous_headings":"","what":"Writing cross-framework custom components","title":"Getting started with Keras Core","text":"Keras Core enables write custom Layers, Models, Metrics, Losses, Optimizers work across TensorFlow, JAX, PyTorch codebase. Let’s take look custom layers first. ’re already familiar writing custom layers tf.keras – well, nothing changed. Except one thing: instead using functions tf namespace, use functions keras.ops.*. keras.ops namespace contains: implementation NumPy API, e.g. keras.ops.stack keras.ops.matmul. set neural network specific ops absent NumPy, keras.ops.conv keras.ops.binary_crossentropy. Let’s make custom Dense layer works backends: Next, let’s make custom Dropout layer relies keras.random namespace: Next, let’s write custom subclassed model uses two custom layers: Let’s compile fit :","code":"class MyDense(keras.layers.Layer):     def __init__(self, units, activation=None, name=None):         super().__init__(name=name)         self.units = units         self.activation = keras.activations.get(activation)      def build(self, input_shape):         input_dim = input_shape[-1]         self.w = self.add_weight(             shape=(input_dim, self.units),             initializer=keras.initializers.GlorotNormal(),             name=\"kernel\",             trainable=True,         )          self.b = self.add_weight(             shape=(self.units,),             initializer=keras.initializers.Zeros(),             name=\"bias\",             trainable=True,         )      def call(self, inputs):         # Use Keras ops to create backend-agnostic layers/metrics/etc.         x = keras.ops.matmul(inputs, self.w) + self.b         return self.activation(x) class MyDropout(keras.layers.Layer):     def __init__(self, rate, name=None):         super().__init__(name=name)         self.rate = rate         # Use seed_generator for managing RNG state.         # It is a state element and its seed variable is         # tracked as part of `layer.variables`.         self.seed_generator = keras.random.SeedGenerator(1337)      def call(self, inputs):         # Use `keras_core.random` for random ops.         return keras.random.dropout(inputs, self.rate, seed=self.seed_generator) class MyModel(keras.Model):     def __init__(self, num_classes):         super().__init__()         self.conv_base = keras.Sequential(             [                 keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),                 keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),                 keras.layers.MaxPooling2D(pool_size=(2, 2)),                 keras.layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),                 keras.layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),                 keras.layers.GlobalAveragePooling2D(),             ]         )         self.dp = MyDropout(0.5)         self.dense = MyDense(num_classes, activation=\"softmax\")      def call(self, x):         x = self.conv_base(x)         x = self.dp(x)         return self.dense(x) model = MyModel(num_classes=10) model.compile(     loss=keras.losses.SparseCategoricalCrossentropy(),     optimizer=keras.optimizers.Adam(learning_rate=1e-3),     metrics=[         keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),     ], )  model.fit(     x_train,     y_train,     batch_size=batch_size,     epochs=1,  # For speed     validation_split=0.15, )"},{"path":"https://keras.posit.co/articles/guides/getting_started_with_keras_core.html","id":"training-models-on-arbitrary-data-sources","dir":"Articles > Guides","previous_headings":"","what":"Training models on arbitrary data sources","title":"Getting started with Keras Core","text":"Keras models can trained evaluated wide variety data sources, independently backend ’re using. includes: NumPy arrays Pandas dataframes TensorFlowtf.data.Dataset objects PyTorch DataLoader objects Keras PyDataset objects work whether ’re using TensorFlow, JAX, PyTorch Keras backend. Let’s try PyTorch DataLoaders: Now let’s try tf.data:","code":"import torch  # Create a TensorDataset train_torch_dataset = torch.utils.data.TensorDataset(     torch.from_numpy(x_train), torch.from_numpy(y_train) ) val_torch_dataset = torch.utils.data.TensorDataset(     torch.from_numpy(x_test), torch.from_numpy(y_test) )  # Create a DataLoader train_dataloader = torch.utils.data.DataLoader(     train_torch_dataset, batch_size=batch_size, shuffle=True ) val_dataloader = torch.utils.data.DataLoader(     val_torch_dataset, batch_size=batch_size, shuffle=False )  model = MyModel(num_classes=10) model.compile(     loss=keras.losses.SparseCategoricalCrossentropy(),     optimizer=keras.optimizers.Adam(learning_rate=1e-3),     metrics=[         keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),     ], ) model.fit(train_dataloader, epochs=1, validation_data=val_dataloader) import tensorflow as tf  train_dataset = (     tf.data.Dataset.from_tensor_slices((x_train, y_train))     .batch(batch_size)     .prefetch(tf.data.AUTOTUNE) ) test_dataset = (     tf.data.Dataset.from_tensor_slices((x_test, y_test))     .batch(batch_size)     .prefetch(tf.data.AUTOTUNE) )  model = MyModel(num_classes=10) model.compile(     loss=keras.losses.SparseCategoricalCrossentropy(),     optimizer=keras.optimizers.Adam(learning_rate=1e-3),     metrics=[         keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),     ], ) model.fit(train_dataset, epochs=1, validation_data=test_dataset)"},{"path":"https://keras.posit.co/articles/guides/getting_started_with_keras_core.html","id":"further-reading","dir":"Articles > Guides","previous_headings":"","what":"Further reading","title":"Getting started with Keras Core","text":"concludes short overview new multi-backend capabilities Keras Core. Next, can learn :","code":""},{"path":"https://keras.posit.co/articles/guides/getting_started_with_keras_core.html","id":"how-to-customize-what-happens-in-fit","dir":"Articles > Guides","previous_headings":"Further reading","what":"How to customize what happens in fit()","title":"Getting started with Keras Core","text":"Want implement non-standard training algorithm (e.g. GAN training routine) still want benefit power usability fit()? ’s really easy customize fit() support arbitrary use cases. Customizing happens fit() TensorFlow Customizing happens fit() JAX Customizing happens fit() PyTorch","code":""},{"path":"https://keras.posit.co/articles/guides/getting_started_with_keras_core.html","id":"how-to-write-custom-training-loops","dir":"Articles > Guides","previous_headings":"","what":"How to write custom training loops","title":"Getting started with Keras Core","text":"Writing training loop scratch TensorFlow Writing training loop scratch JAX Writing training loop scratch PyTorch","code":""},{"path":"https://keras.posit.co/articles/guides/getting_started_with_keras_core.html","id":"how-to-distribute-training","dir":"Articles > Guides","previous_headings":"","what":"How to distribute training","title":"Getting started with Keras Core","text":"Guide distributed training TensorFlow JAX distributed training example PyTorch distributed training example Enjoy library! 🚀","code":""},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_engineers.html","id":"setup","dir":"Articles > Guides","previous_headings":"","what":"Setup","title":"Introduction to Keras for Engineers","text":"","code":"import numpy as np import tensorflow as tf import keras"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_engineers.html","id":"introduction","dir":"Articles > Guides","previous_headings":"","what":"Introduction","title":"Introduction to Keras for Engineers","text":"machine learning engineer looking use Keras ship deep-learning powered features real products? guide serve first introduction core Keras API concepts. guide, learn : Prepare data training model (turning either NumPy arrays tf.data.Dataset objects). data preprocessing, instance feature normalization vocabulary indexing. Build model turns data useful predictions, using Keras Functional API. Train model built-Keras fit() method, mindful checkpointing, metrics monitoring, fault tolerance. Evaluate model test data use inference new data. Customize fit() , instance build GAN. Speed training leveraging multiple GPUs. Refine model hyperparameter tuning. end guide, get pointers end--end examples solidify concepts: Image classification Text classification Credit card fraud detection","code":""},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_engineers.html","id":"data-loading-preprocessing","dir":"Articles > Guides","previous_headings":"","what":"Data loading & preprocessing","title":"Introduction to Keras for Engineers","text":"Neural networks don’t process raw data, like text files, encoded JPEG image files, CSV files. process vectorized & standardized representations. Text files need read string tensors, split words. Finally, words need indexed & turned integer tensors. Images need read decoded integer tensors, converted floating point normalized small values (usually 0 1). CSV data needs parsed, numerical features converted floating point tensors categorical features indexed converted integer tensors. feature typically needs normalized zero-mean unit-variance. Etc. Let’s start data loading.","code":""},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_engineers.html","id":"data-loading","dir":"Articles > Guides","previous_headings":"","what":"Data loading","title":"Introduction to Keras for Engineers","text":"Keras models accept three types inputs: NumPy arrays, just like Scikit-Learn many Python-based libraries. good option data fits memory. TensorFlow Dataset objects. high-performance option suitable datasets fit memory streamed disk distributed filesystem. Python generators yield batches data (custom subclasses keras.utils.Sequence class). start training model, need make data available one formats. large dataset training GPU(s), consider using Dataset objects, since take care performance-critical details, : Asynchronously preprocessing data CPU GPU busy, buffering queue. Prefetching data GPU memory ’s immediately available GPU finished processing previous batch, can reach full GPU utilization. Keras features range utilities help turn raw data disk Dataset: keras.utils.image_dataset_from_directory turns image files sorted class-specific folders labeled dataset image tensors. keras.utils.text_dataset_from_directory text files. addition, TensorFlow tf.data includes similar utilities, tf.data.experimental.make_csv_dataset load structured data CSV files. Example: obtaining labeled dataset image files disk Supposed image files sorted class different folders, like : can : label sample rank folder alphanumeric order. Naturally, can also configured explicitly passing, e.g. class_names=['class_a', 'class_b'], cases label 0 class_a 1 class_b. Example: obtaining labeled dataset text files disk Likewise text: .txt documents sorted class different folders, can :","code":"main_directory/ ...class_a/ ......a_image_1.jpg ......a_image_2.jpg ...class_b/ ......b_image_1.jpg ......b_image_2.jpg # Create a dataset. dataset = keras.utils.image_dataset_from_directory(   'path/to/main_directory', batch_size=64, image_size=(200, 200))  # For demonstration, iterate over the batches yielded by the dataset. for data, labels in dataset:    print(data.shape)  # (64, 200, 200, 3)    print(data.dtype)  # float32    print(labels.shape)  # (64,)    print(labels.dtype)  # int32 dataset = keras.utils.text_dataset_from_directory(   'path/to/main_directory', batch_size=64)  # For demonstration, iterate over the batches yielded by the dataset. for data, labels in dataset:    print(data.shape)  # (64,)    print(data.dtype)  # string    print(labels.shape)  # (64,)    print(labels.dtype)  # int32"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_engineers.html","id":"data-preprocessing-with-keras","dir":"Articles > Guides","previous_headings":"","what":"Data preprocessing with Keras","title":"Introduction to Keras for Engineers","text":"data form string/int/float NumPy arrays, Dataset object (Python generator) yields batches string/int/float tensors, time preprocess data. can mean: Tokenization string data, followed token indexing. Feature normalization. Rescaling data small values (general, input values neural network close zero – typically expect either data zero-mean unit-variance, data [0, 1] range.","code":""},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_engineers.html","id":"the-ideal-machine-learning-model-is-end-to-end","dir":"Articles > Guides","previous_headings":"Data preprocessing with Keras","what":"The ideal machine learning model is end-to-end","title":"Introduction to Keras for Engineers","text":"general, seek data preprocessing part model much possible, via external data preprocessing pipeline. ’s external data preprocessing makes models less portable ’s time use production. Consider model processes text: uses specific tokenization algorithm specific vocabulary index. want ship model mobile app JavaScript app, need recreate exact preprocessing setup target language. can get tricky: small discrepancy original pipeline one recreate potential completely invalidate model, least severely degrade performance. much easier able simply export end--end model already includes preprocessing. ideal model expect input something close possible raw data: image model expect RGB pixel values [0, 255] range, text model accept strings utf-8 characters. way, consumer exported model doesn’t know preprocessing pipeline.","code":""},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_engineers.html","id":"using-keras-preprocessing-layers","dir":"Articles > Guides","previous_headings":"Data preprocessing with Keras","what":"Using Keras preprocessing layers","title":"Introduction to Keras for Engineers","text":"Keras, -model data preprocessing via preprocessing layers. includes: Vectorizing raw strings text via TextVectorization layer Feature normalization via Normalization layer Image rescaling, cropping, image data augmentation key advantage using Keras preprocessing layers can included directly model, either training training, makes models portable. preprocessing layers state: TextVectorization holds index mapping words tokens integer indices Normalization holds mean variance features state preprocessing layer obtained calling layer.adapt(data) sample training data (). Example: turning strings sequences integer word indices Example: turning strings sequences one-hot encoded bigrams Example: normalizing features Example: rescaling & center-cropping images Rescaling layer CenterCrop layer stateless, isn’t necessary call adapt() case.","code":"from tensorflow.keras.layers import TextVectorization  # Example training data, of dtype `string`. training_data = np.array([[\"This is the 1st sample.\"], [\"And here's the 2nd sample.\"]])  # Create a TextVectorization layer instance. It can be configured to either # return integer token indices, or a dense token representation (e.g. multi-hot # or TF-IDF). The text standardization and text splitting algorithms are fully # configurable. vectorizer = TextVectorization(output_mode=\"int\")  # Calling `adapt` on an array or dataset makes the layer generate a vocabulary # index for the data, which can then be reused when seeing new data. vectorizer.adapt(training_data)  # After calling adapt, the layer is able to encode any n-gram it has seen before # in the `adapt()` data. Unknown n-grams are encoded via an \"out-of-vocabulary\" # token. integer_data = vectorizer(training_data) print(integer_data) from tensorflow.keras.layers import TextVectorization  # Example training data, of dtype `string`. training_data = np.array([[\"This is the 1st sample.\"], [\"And here's the 2nd sample.\"]])  # Create a TextVectorization layer instance. It can be configured to either # return integer token indices, or a dense token representation (e.g. multi-hot # or TF-IDF). The text standardization and text splitting algorithms are fully # configurable. vectorizer = TextVectorization(output_mode=\"binary\", ngrams=2)  # Calling `adapt` on an array or dataset makes the layer generate a vocabulary # index for the data, which can then be reused when seeing new data. vectorizer.adapt(training_data)  # After calling adapt, the layer is able to encode any n-gram it has seen before # in the `adapt()` data. Unknown n-grams are encoded via an \"out-of-vocabulary\" # token. integer_data = vectorizer(training_data) print(integer_data) from tensorflow.keras.layers import Normalization  # Example image data, with values in the [0, 255] range training_data = np.random.randint(0, 256, size=(64, 200, 200, 3)).astype(\"float32\")  normalizer = Normalization(axis=-1) normalizer.adapt(training_data)  normalized_data = normalizer(training_data) print(\"var: %.4f\" % np.var(normalized_data)) print(\"mean: %.4f\" % np.mean(normalized_data)) from tensorflow.keras.layers import CenterCrop from tensorflow.keras.layers import Rescaling  # Example image data, with values in the [0, 255] range training_data = np.random.randint(0, 256, size=(64, 200, 200, 3)).astype(\"float32\")  cropper = CenterCrop(height=150, width=150) scaler = Rescaling(scale=1.0 / 255)  output_data = scaler(cropper(training_data)) print(\"shape:\", output_data.shape) print(\"min:\", np.min(output_data)) print(\"max:\", np.max(output_data))"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_engineers.html","id":"building-models-with-the-keras-functional-api","dir":"Articles > Guides","previous_headings":"","what":"Building models with the Keras Functional API","title":"Introduction to Keras for Engineers","text":"“layer” simple input-output transformation (scaling & center-cropping transformations ). instance, ’s linear projection layer maps inputs 16-dimensional feature space: “model” directed acyclic graph layers. can think model “bigger layer” encompasses multiple sublayers can trained via exposure data. common powerful way build Keras models Functional API. build models Functional API, start specifying shape (optionally dtype) inputs. dimension input can vary, can specify None. instance, input 200x200 RGB image shape (200, 200, 3), input RGB images size shape (None,  None, 3). defining input(s), can chain layer transformations top inputs, final output: defined directed acyclic graph layers turns input(s) outputs, instantiate Model object: model behaves basically like bigger layer. can call batches data, like : can print summary data gets transformed stage model. useful debugging. Note output shape displayed layer includes batch size. batch size None, indicates model can process batches size. Functional API also makes easy build models multiple inputs (instance, image metadata) multiple outputs (instance, predicting class image likelihood user click ). deeper dive can , see guide Functional API.","code":"dense = keras.layers.Dense(units=16) # Let's say we expect our inputs to be RGB images of arbitrary size inputs = keras.Input(shape=(None, None, 3)) from tensorflow.keras import layers  # Center-crop images to 150x150 x = CenterCrop(height=150, width=150)(inputs) # Rescale images to [0, 1] x = Rescaling(scale=1.0 / 255)(x)  # Apply some convolution and pooling layers x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x) x = layers.MaxPooling2D(pool_size=(3, 3))(x) x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x) x = layers.MaxPooling2D(pool_size=(3, 3))(x) x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)  # Apply global average pooling to get flat feature vectors x = layers.GlobalAveragePooling2D()(x)  # Add a dense classifier on top num_classes = 10 outputs = layers.Dense(num_classes, activation=\"softmax\")(x) model = keras.Model(inputs=inputs, outputs=outputs) data = np.random.randint(0, 256, size=(64, 200, 200, 3)).astype(\"float32\") processed_data = model(data) print(processed_data.shape) model.summary()"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_engineers.html","id":"training-models-with-fit","dir":"Articles > Guides","previous_headings":"","what":"Training models with fit()","title":"Introduction to Keras for Engineers","text":"point, know: prepare data (e.g. NumPy array tf.data.Dataset object) build model process data next step train model data. Model class features built-training loop, fit() method. accepts Dataset objects, Python generators yield batches data, NumPy arrays. can call fit(), need specify optimizer loss function (assume already familiar concepts). compile() step: Loss optimizer can specified via string identifiers (case default constructor argument values used): model compiled, can start “fitting” model data. ’s fitting model looks like NumPy data: Besides data, specify two key parameters: batch_size number epochs (iterations data). data get sliced batches 32 samples, model iterate 10 times data training. ’s fitting model looks like dataset: Since data yielded dataset expected already batched, don’t need specify batch size . Let’s look practice toy example model learns classify MNIST digits: fit() call returns “history” object records happened course training. history.history dict contains per-epoch timeseries metrics values (one metric, loss, one epoch, get single scalar): detailed overview use fit(), see guide training & evaluation built-Keras methods.","code":"model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),               loss=keras.losses.CategoricalCrossentropy()) model.compile(optimizer='rmsprop', loss='categorical_crossentropy') model.fit(numpy_array_of_samples, numpy_array_of_labels,           batch_size=32, epochs=10) model.fit(dataset_of_samples_and_labels, epochs=10) # Get the data as Numpy arrays (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()  # Build a simple model inputs = keras.Input(shape=(28, 28)) x = layers.Rescaling(1.0 / 255)(inputs) x = layers.Flatten()(x) x = layers.Dense(128, activation=\"relu\")(x) x = layers.Dense(128, activation=\"relu\")(x) outputs = layers.Dense(10, activation=\"softmax\")(x) model = keras.Model(inputs, outputs) model.summary()  # Compile the model model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")  # Train the model for 1 epoch from Numpy data batch_size = 64 print(\"Fit on NumPy data\") history = model.fit(x_train, y_train, batch_size=batch_size, epochs=1)  # Train the model for 1 epoch using a dataset dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size) print(\"Fit on Dataset\") history = model.fit(dataset, epochs=1) print(history.history)"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_engineers.html","id":"keeping-track-of-performance-metrics","dir":"Articles > Guides","previous_headings":"Training models with fit()","what":"Keeping track of performance metrics","title":"Introduction to Keras for Engineers","text":"’re training model, want keep track metrics classification accuracy, precision, recall, AUC, etc. Besides, want monitor metrics training data, also validation set. Monitoring metrics can pass list metric objects compile(), like : Passing validation data fit() can pass validation data fit() monitor validation loss & validation metrics. Validation metrics get reported end epoch.","code":"model.compile(     optimizer=\"adam\",     loss=\"sparse_categorical_crossentropy\",     metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")], ) history = model.fit(dataset, epochs=1) val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size) history = model.fit(dataset, epochs=1, validation_data=val_dataset)"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_engineers.html","id":"using-callbacks-for-checkpointing-and-more","dir":"Articles > Guides","previous_headings":"Training models with fit()","what":"Using callbacks for checkpointing (and more)","title":"Introduction to Keras for Engineers","text":"training goes minutes, ’s important save model regular intervals training. can use saved models restart training case training process crashes (important multi-worker distributed training, since many workers least one bound fail point). important feature Keras callbacks, configured fit(). Callbacks objects get called model different point training, particular: beginning end batch beginning end epoch Callbacks way make model trainable entirely scriptable. can use callbacks periodically save model. ’s simple example: ModelCheckpoint callback configured save model end every epoch. filename include current epoch. can also use callbacks things like periodically changing learning optimizer, streaming metrics Slack bot, sending email notification training complete, etc. detailed overview callbacks available write , see callbacks API documentation guide writing custom callbacks.","code":"callbacks = [     keras.callbacks.ModelCheckpoint(         filepath='path/to/my/model_{epoch}',         save_freq='epoch') ] model.fit(dataset, epochs=2, callbacks=callbacks)"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_engineers.html","id":"monitoring-training-progress-with-tensorboard","dir":"Articles > Guides","previous_headings":"Training models with fit()","what":"Monitoring training progress with TensorBoard","title":"Introduction to Keras for Engineers","text":"Staring Keras progress bar isn’t ergonomic way monitor loss metrics evolving time. ’s better solution: TensorBoard, web application can display real-time graphs metrics (). use TensorBoard fit(), simply pass keras.callbacks.TensorBoard callback specifying directory store TensorBoard logs: can launch TensorBoard instance can open browser monitor logs getting written location: ’s , can launch -line TensorBoard tab training models Jupyter / Colab notebooks. ’s information.","code":"callbacks = [     keras.callbacks.TensorBoard(log_dir='./logs') ] model.fit(dataset, epochs=2, callbacks=callbacks) tensorboard --logdir=./logs"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_engineers.html","id":"after-fit-evaluating-test-performance-generating-predictions-on-new-data","dir":"Articles > Guides","previous_headings":"Training models with fit()","what":"After fit(): evaluating test performance & generating predictions on new data","title":"Introduction to Keras for Engineers","text":"trained model, can evaluate loss metrics new data via evaluate(): can also generate NumPy arrays predictions (activations output layer(s) model) via predict():","code":"loss, acc = model.evaluate(val_dataset)  # returns loss and metrics print(\"loss: %.2f\" % loss) print(\"acc: %.2f\" % acc) predictions = model.predict(val_dataset) print(predictions.shape)"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_engineers.html","id":"using-fit-with-a-custom-training-step","dir":"Articles > Guides","previous_headings":"","what":"Using fit() with a custom training step","title":"Introduction to Keras for Engineers","text":"default, fit() configured supervised learning. need different kind training loop (instance, GAN training loop), can provide implementation Model.train_step() method. method repeatedly called fit(). Metrics, callbacks, etc. work usual. ’s simple example reimplements fit() normally : detailed overview customize built-training & evaluation loops, see guide: “Customizing happens fit()”.","code":"class CustomModel(keras.Model):     def train_step(self, data):         # Unpack the data. Its structure depends on your model and         # on what you pass to `fit()`.         x, y = data         with tf.GradientTape() as tape:             y_pred = self(x, training=True)  # Forward pass             # Compute the loss value             # (the loss function is configured in `compile()`)             loss = self.compute_loss(y=y, y_pred=y_pred)         # Compute gradients         trainable_vars = self.trainable_variables         gradients = tape.gradient(loss, trainable_vars)         # Update weights         self.optimizer.apply_gradients(zip(gradients, trainable_vars))         # Update metrics (includes the metric that tracks the loss)         for metric in self.metrics:             if metric.name == \"loss\":                 metric.update_state(loss)             else:                 metric.update_state(y, y_pred)         # Return a dict mapping metric names to current value         return {m.name: m.result() for m in self.metrics}  # Construct and compile an instance of CustomModel inputs = keras.Input(shape=(32,)) outputs = keras.layers.Dense(1)(inputs) model = CustomModel(inputs, outputs) model.compile(optimizer='adam', loss='mse', metrics=[...])  # Just use `fit` as usual model.fit(dataset, epochs=3, callbacks=...)"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_engineers.html","id":"debugging-your-model-with-eager-execution","dir":"Articles > Guides","previous_headings":"","what":"Debugging your model with eager execution","title":"Introduction to Keras for Engineers","text":"write custom training steps custom layers, need debug . debugging experience integral part framework: Keras, debugging workflow designed user mind. default, Keras models compiled highly-optimized computation graphs deliver fast execution times. means Python code write (e.g. custom train_step) code actually executing. introduces layer indirection can make debugging hard. Debugging best done step step. want able sprinkle code print() statement see data looks like every operation, want able use pdb. can achieve running model eagerly. eager execution, Python code write code gets executed. Simply pass run_eagerly=True compile(): course, downside makes model significantly slower. Make sure switch back get benefits compiled computation graphs done debugging! general, use run_eagerly=True every time need debug ’s happening inside fit() call.","code":"model.compile(optimizer='adam', loss='mse', run_eagerly=True)"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_engineers.html","id":"speeding-up-training-with-multiple-gpus","dir":"Articles > Guides","previous_headings":"","what":"Speeding up training with multiple GPUs","title":"Introduction to Keras for Engineers","text":"Keras built-industry-strength support multi-GPU training distributed multi-worker training, via tf.distribute API. multiple GPUs machine, can train model : Creating tf.distribute.MirroredStrategy object Building & compiling model inside strategy’s scope Calling fit() evaluate() dataset usual detailed introduction multi-GPU & distributed training, see guide.","code":"# Create a MirroredStrategy. strategy = tf.distribute.MirroredStrategy()  # Open a strategy scope. with strategy.scope():     # Everything that creates variables should be under the strategy scope.     # In general this is only model construction & `compile()`.     model = Model(...)     model.compile(...)  # Train the model on all available devices. train_dataset, val_dataset, test_dataset = get_dataset() model.fit(train_dataset, epochs=2, validation_data=val_dataset)  # Test the model on all available devices. model.evaluate(test_dataset)"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_engineers.html","id":"doing-preprocessing-synchronously-on-device-vs--asynchronously-on-host-cpu","dir":"Articles > Guides","previous_headings":"","what":"Doing preprocessing synchronously on-device vs. asynchronously on host CPU","title":"Introduction to Keras for Engineers","text":"’ve learned preprocessing, ’ve seen example put image preprocessing layers (CenterCrop Rescaling) directly inside model. preprocessing happen part model training great want -device preprocessing, instance, GPU-accelerated feature normalization image augmentation. kinds preprocessing suited setup: particular, text preprocessing TextVectorization layer. Due sequential nature due fact can run CPU, ’s often good idea asynchronous preprocessing. asynchronous preprocessing, preprocessing operations run CPU, preprocessed samples buffered queue GPU busy previous batch data. next batch preprocessed samples fetched queue GPU memory right GPU becomes available (prefetching). ensures preprocessing blocking GPU can run full utilization. asynchronous preprocessing, simply use dataset.map inject preprocessing operation data pipeline: Compare text vectorization part model: training text models CPU, generally see performance difference two setups. training GPU, however, asynchronous buffered preprocessing host CPU GPU running model can result significant speedup. training, want export end--end model includes preprocessing layer(s), easy , since TextVectorization layer:","code":"# Example training data, of dtype `string`. samples = np.array([[\"This is the 1st sample.\"], [\"And here's the 2nd sample.\"]]) labels = [[0], [1]]  # Prepare a TextVectorization layer. vectorizer = TextVectorization(output_mode=\"int\") vectorizer.adapt(samples)  # Asynchronous preprocessing: the text vectorization is part of the tf.data pipeline. # First, create a dataset dataset = tf.data.Dataset.from_tensor_slices((samples, labels)).batch(2) # Apply text vectorization to the samples dataset = dataset.map(lambda x, y: (vectorizer(x), y)) # Prefetch with a buffer size of 2 batches dataset = dataset.prefetch(2)  # Our model should expect sequences of integers as inputs inputs = keras.Input(shape=(None,), dtype=\"int64\") x = layers.Embedding(input_dim=10, output_dim=32)(inputs) outputs = layers.Dense(1)(x) model = keras.Model(inputs, outputs)  model.compile(optimizer=\"adam\", loss=\"mse\", run_eagerly=True) model.fit(dataset) # Our dataset will yield samples that are strings dataset = tf.data.Dataset.from_tensor_slices((samples, labels)).batch(2)  # Our model should expect strings as inputs inputs = keras.Input(shape=(1,), dtype=\"string\") x = vectorizer(inputs) x = layers.Embedding(input_dim=10, output_dim=32)(x) outputs = layers.Dense(1)(x) model = keras.Model(inputs, outputs)  model.compile(optimizer=\"adam\", loss=\"mse\", run_eagerly=True) model.fit(dataset) inputs = keras.Input(shape=(1,), dtype='string') x = vectorizer(inputs) outputs = trained_model(x) end_to_end_model = keras.Model(inputs, outputs)"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_engineers.html","id":"finding-the-best-model-configuration-with-hyperparameter-tuning","dir":"Articles > Guides","previous_headings":"","what":"Finding the best model configuration with hyperparameter tuning","title":"Introduction to Keras for Engineers","text":"working model, ’re going want optimize configuration – architecture choices, layer sizes, etc. Human intuition can go far, ’ll want leverage systematic approach: hyperparameter search. can use KerasTuner find best hyperparameter Keras models. ’s easy calling fit(). works. First, place model definition function, takes single hp argument. Inside function, replace value want tune call hyperparameter sampling methods, e.g. hp.Int() hp.Choice(): function return compiled model. Next, instantiate tuner object specifying optimization objective search parameters: Finally, start search search() method, takes arguments Model.fit(): search , can retrieve best model(s): print summary results:","code":"def build_model(hp):     inputs = keras.Input(shape=(784,))     x = layers.Dense(         units=hp.Int('units', min_value=32, max_value=512, step=32),         activation='relu')(inputs)     outputs = layers.Dense(10, activation='softmax')(x)     model = keras.Model(inputs, outputs)     model.compile(         optimizer=keras.optimizers.Adam(             hp.Choice('learning_rate',                       values=[1e-2, 1e-3, 1e-4])),         loss='sparse_categorical_crossentropy',         metrics=['accuracy'])     return model import keras_tuner  tuner = keras_tuner.tuners.Hyperband(     build_model,     objective='val_loss',     max_epochs=100,     max_trials=200,     executions_per_trial=2,     directory='my_dir') tuner.search(dataset, validation_data=val_dataset) models = tuner.get_best_models(num_models=2) tuner.results_summary()"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_engineers.html","id":"end-to-end-examples","dir":"Articles > Guides","previous_headings":"","what":"End-to-end examples","title":"Introduction to Keras for Engineers","text":"familiarize concepts introduction, see following end--end examples: Text classification Image classification Credit card fraud detection","code":""},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_engineers.html","id":"what-to-learn-next","dir":"Articles > Guides","previous_headings":"","what":"What to learn next","title":"Introduction to Keras for Engineers","text":"Learn Functional API. Learn features fit() evaluate(). Learn callbacks. Learn creating custom training steps. Learn multi-GPU distributed training. Learn transfer learning.","code":""},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_researchers.html","id":"setup","dir":"Articles > Guides","previous_headings":"","what":"Setup","title":"Introduction to Keras for Researchers","text":"","code":"import tensorflow as tf import keras"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_researchers.html","id":"introduction","dir":"Articles > Guides","previous_headings":"","what":"Introduction","title":"Introduction to Keras for Researchers","text":"machine learning researcher? publish NeurIPS push state---art CV NLP? guide serve first introduction core Keras & TensorFlow API concepts. guide, learn : Tensors, variables, gradients TensorFlow Creating layers subclassing Layer class Writing low-level training loops Tracking losses created layers via add_loss() method Tracking metrics low-level training loop Speeding execution compiled tf.function Executing layers training inference mode Keras Functional API also see Keras API action two end--end research examples: Variational Autoencoder, Hypernetwork.","code":""},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_researchers.html","id":"tensors","dir":"Articles > Guides","previous_headings":"","what":"Tensors","title":"Introduction to Keras for Researchers","text":"TensorFlow infrastructure layer differentiable programming. heart, ’s framework manipulating N-dimensional arrays (tensors), much like NumPy. However, three key differences NumPy TensorFlow: TensorFlow can leverage hardware accelerators GPUs TPUs. TensorFlow can automatically compute gradient arbitrary differentiable tensor expressions. TensorFlow computation can distributed large numbers devices single machine, large number machines (potentially multiple devices ). Let’s take look object core TensorFlow: Tensor. ’s constant tensor: can get value NumPy array calling .numpy(): Much like NumPy array, features attributes dtype shape: common way create constant tensors via tf.ones tf.zeros (just like np.ones np.zeros): can also create random constant tensors:","code":"x = tf.constant([[5, 2], [1, 3]]) print(x) x.numpy() print(\"dtype:\", x.dtype) print(\"shape:\", x.shape) print(tf.ones(shape=(2, 1))) print(tf.zeros(shape=(2, 1))) x = tf.random.normal(shape=(2, 2), mean=0.0, stddev=1.0)  x = tf.random.uniform(shape=(2, 2), minval=0, maxval=10, dtype=\"int32\")"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_researchers.html","id":"variables","dir":"Articles > Guides","previous_headings":"","what":"Variables","title":"Introduction to Keras for Researchers","text":"Variables special tensors used store mutable state (weights neural network). create Variable using initial value: update value Variable using methods .assign(value), .assign_add(increment), .assign_sub(decrement):","code":"initial_value = tf.random.normal(shape=(2, 2)) a = tf.Variable(initial_value) print(a) new_value = tf.random.normal(shape=(2, 2)) a.assign(new_value) for i in range(2):     for j in range(2):         assert a[i, j] == new_value[i, j]  added_value = tf.random.normal(shape=(2, 2)) a.assign_add(added_value) for i in range(2):     for j in range(2):         assert a[i, j] == new_value[i, j] + added_value[i, j]"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_researchers.html","id":"doing-math-in-tensorflow","dir":"Articles > Guides","previous_headings":"","what":"Doing math in TensorFlow","title":"Introduction to Keras for Researchers","text":"’ve used NumPy, math TensorFlow look familiar. main difference TensorFlow code can run GPU TPU.","code":"a = tf.random.normal(shape=(2, 2)) b = tf.random.normal(shape=(2, 2))  c = a + b d = tf.square(c) e = tf.exp(d)"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_researchers.html","id":"gradients","dir":"Articles > Guides","previous_headings":"","what":"Gradients","title":"Introduction to Keras for Researchers","text":"’s another big difference NumPy: can automatically retrieve gradient differentiable expression. Just open GradientTape, start “watching” tensor via tape.watch(), compose differentiable expression using tensor input: default, variables watched automatically, don’t need manually watch : Note can compute higher-order derivatives nesting tapes:","code":"a = tf.random.normal(shape=(2, 2)) b = tf.random.normal(shape=(2, 2))  with tf.GradientTape() as tape:     tape.watch(a)  # Start recording the history of operations applied to `a`     c = tf.sqrt(tf.square(a) + tf.square(b))  # Do some math using `a`     # What's the gradient of `c` with respect to `a`?     dc_da = tape.gradient(c, a)     print(dc_da) a = tf.Variable(a)  with tf.GradientTape() as tape:     c = tf.sqrt(tf.square(a) + tf.square(b))     dc_da = tape.gradient(c, a)     print(dc_da) with tf.GradientTape() as outer_tape:     with tf.GradientTape() as tape:         c = tf.sqrt(tf.square(a) + tf.square(b))         dc_da = tape.gradient(c, a)     d2c_da2 = outer_tape.gradient(dc_da, a)     print(d2c_da2)"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_researchers.html","id":"keras-layers","dir":"Articles > Guides","previous_headings":"","what":"Keras layers","title":"Introduction to Keras for Researchers","text":"TensorFlow infrastructure layer differentiable programming, dealing tensors, variables, gradients, Keras user interface deep learning, dealing layers, models, optimizers, loss functions, metrics, . Keras serves high-level API TensorFlow: Keras makes TensorFlow simple productive. Layer class fundamental abstraction Keras. Layer encapsulates state (weights) computation (defined call method). simple layer looks like . self.add_weight() method gives shortcut creating weights: use Layer instance much like Python function: weight variables (created __init__) automatically tracked weights property: many built-layers available, Dense Conv2D LSTM fancier ones like Conv3DTranspose ConvLSTM2D. smart reusing built-functionality.","code":"class Linear(keras.layers.Layer):     \"\"\"y = w.x + b\"\"\"      def __init__(self, units=32, input_dim=32):         super().__init__()         self.w = self.add_weight(             shape=(input_dim, units), initializer=\"random_normal\", trainable=True         )         self.b = self.add_weight(shape=(units,), initializer=\"zeros\", trainable=True)      def call(self, inputs):         return tf.matmul(inputs, self.w) + self.b # Instantiate our layer. linear_layer = Linear(units=4, input_dim=2)  # The layer can be treated as a function. # Here we call it on some data. y = linear_layer(tf.ones((2, 2))) assert y.shape == (2, 4) assert linear_layer.weights == [linear_layer.w, linear_layer.b]"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_researchers.html","id":"layer-weight-creation-in-buildinput_shape","dir":"Articles > Guides","previous_headings":"","what":"Layer weight creation in build(input_shape)","title":"Introduction to Keras for Researchers","text":"’s often good idea defer weight creation build() method, don’t need specify input dim/shape layer construction time:","code":"class Linear(keras.layers.Layer):     \"\"\"y = w.x + b\"\"\"      def __init__(self, units=32):         super().__init__()         self.units = units      def build(self, input_shape):         self.w = self.add_weight(             shape=(input_shape[-1], self.units),             initializer=\"random_normal\",             trainable=True,         )         self.b = self.add_weight(             shape=(self.units,), initializer=\"random_normal\", trainable=True         )      def call(self, inputs):         return tf.matmul(inputs, self.w) + self.b   # Instantiate our layer. linear_layer = Linear(4)  # This will also call `build(input_shape)` and create the weights. y = linear_layer(tf.ones((2, 2)))"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_researchers.html","id":"layer-gradients","dir":"Articles > Guides","previous_headings":"","what":"Layer gradients","title":"Introduction to Keras for Researchers","text":"can automatically retrieve gradients weights layer calling inside GradientTape. Using gradients, can update weights layer, either manually, using optimizer object. course, can modify gradients using , need .","code":"# Prepare a dataset. (x_train, y_train), _ = keras.datasets.mnist.load_data() dataset = tf.data.Dataset.from_tensor_slices(     (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train) ) dataset = dataset.shuffle(buffer_size=1024).batch(64)  # Instantiate our linear layer (defined above) with 10 units. linear_layer = Linear(10)  # Instantiate a logistic loss function that expects integer targets. loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Instantiate an optimizer. optimizer = keras.optimizers.SGD(learning_rate=1e-3)  # Iterate over the batches of the dataset. for step, (x, y) in enumerate(dataset):     # Open a GradientTape.     with tf.GradientTape() as tape:         # Forward pass.         logits = linear_layer(x)          # Loss value for this batch.         loss = loss_fn(y, logits)      # Get gradients of the loss wrt the weights.     gradients = tape.gradient(loss, linear_layer.trainable_weights)      # Update the weights of our linear layer.     optimizer.apply_gradients(zip(gradients, linear_layer.trainable_weights))      # Logging.     if step % 100 == 0:         print(\"Step:\", step, \"Loss:\", float(loss))"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_researchers.html","id":"trainable-and-non-trainable-weights","dir":"Articles > Guides","previous_headings":"","what":"Trainable and non-trainable weights","title":"Introduction to Keras for Researchers","text":"Weights created layers can either trainable non-trainable. ’re exposed trainable_weights non_trainable_weights respectively. ’s layer non-trainable weight:","code":"class ComputeSum(keras.layers.Layer):     \"\"\"Returns the sum of the inputs.\"\"\"      def __init__(self, input_dim):         super().__init__()         # Create a non-trainable weight.         self.total = self.add_weight(             initializer=\"zeros\", shape=(input_dim,), trainable=False         )      def call(self, inputs):         self.total.assign_add(tf.reduce_sum(inputs, axis=0))         return self.total   my_sum = ComputeSum(2) x = tf.ones((2, 2))  y = my_sum(x) print(y.numpy())  # [2. 2.]  y = my_sum(x) print(y.numpy())  # [4. 4.]  assert my_sum.weights == [my_sum.total] assert my_sum.non_trainable_weights == [my_sum.total] assert my_sum.trainable_weights == []"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_researchers.html","id":"layers-that-own-layers","dir":"Articles > Guides","previous_headings":"","what":"Layers that own layers","title":"Introduction to Keras for Researchers","text":"Layers can recursively nested create bigger computation blocks. layer track weights sublayers (trainable non-trainable). Note manually-created MLP equivalent following built-option:","code":"# Let's reuse the Linear class # with a `build` method that we defined above.   class MLP(keras.layers.Layer):     \"\"\"Simple stack of Linear layers.\"\"\"      def __init__(self):         super().__init__()         self.linear_1 = Linear(32)         self.linear_2 = Linear(32)         self.linear_3 = Linear(10)      def call(self, inputs):         x = self.linear_1(inputs)         x = tf.nn.relu(x)         x = self.linear_2(x)         x = tf.nn.relu(x)         return self.linear_3(x)   mlp = MLP()  # The first call to the `mlp` object will create the weights. y = mlp(tf.ones(shape=(3, 64)))  # Weights are recursively tracked. assert len(mlp.weights) == 6 mlp = keras.Sequential(     [         keras.layers.Dense(32, activation=tf.nn.relu),         keras.layers.Dense(32, activation=tf.nn.relu),         keras.layers.Dense(10),     ] )"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_researchers.html","id":"tracking-losses-created-by-layers","dir":"Articles > Guides","previous_headings":"","what":"Tracking losses created by layers","title":"Introduction to Keras for Researchers","text":"Layers can create losses forward pass via add_loss() method. especially useful regularization losses. losses created sublayers recursively tracked parent layers. ’s layer creates activity regularization loss: model incorporating layer track regularization loss: losses cleared top-level layer start forward pass – don’t accumulate. layer.losses always contains losses created last forward pass. typically use losses summing computing gradients writing training loop.","code":"class ActivityRegularization(keras.layers.Layer):     \"\"\"Layer that creates an activity sparsity regularization loss.\"\"\"      def __init__(self, rate=0.1):         super().__init__()         self.rate = rate      def call(self, inputs):         # We use `add_loss` to create a regularization loss         # that depends on the inputs.         self.add_loss(self.rate * tf.reduce_mean(inputs))         return inputs # Let's use the loss layer in a MLP block.   class SparseMLP(keras.layers.Layer):     \"\"\"Stack of Linear layers with a sparsity regularization loss.\"\"\"      def __init__(self):         super().__init__()         self.linear_1 = Linear(32)         self.regularization = ActivityRegularization(0.1)         self.linear_3 = Linear(10)      def call(self, inputs):         x = self.linear_1(inputs)         x = tf.nn.relu(x)         x = self.regularization(x)         return self.linear_3(x)   mlp = SparseMLP() y = mlp(tf.ones((10, 10)))  print(mlp.losses)  # List containing one float32 scalar # Losses correspond to the *last* forward pass. mlp = SparseMLP() mlp(tf.ones((10, 10))) assert len(mlp.losses) == 1 mlp(tf.ones((10, 10))) assert len(mlp.losses) == 1  # No accumulation.  # Let's demonstrate how to use these losses in a training loop.  # Prepare a dataset. (x_train, y_train), _ = keras.datasets.mnist.load_data() dataset = tf.data.Dataset.from_tensor_slices(     (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train) ) dataset = dataset.shuffle(buffer_size=1024).batch(64)  # A new MLP. mlp = SparseMLP()  # Loss and optimizer. loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True) optimizer = keras.optimizers.SGD(learning_rate=1e-3)  for step, (x, y) in enumerate(dataset):     with tf.GradientTape() as tape:         # Forward pass.         logits = mlp(x)          # External loss value for this batch.         loss = loss_fn(y, logits)          # Add the losses created during the forward pass.         loss += sum(mlp.losses)          # Get gradients of the loss wrt the weights.         gradients = tape.gradient(loss, mlp.trainable_weights)      # Update the weights of our linear layer.     optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))      # Logging.     if step % 100 == 0:         print(\"Step:\", step, \"Loss:\", float(loss))"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_researchers.html","id":"keeping-track-of-training-metrics","dir":"Articles > Guides","previous_headings":"","what":"Keeping track of training metrics","title":"Introduction to Keras for Researchers","text":"Keras offers broad range built-metrics, like keras.metrics.AUC keras.metrics.PrecisionAtRecall. ’s also easy create metrics lines code. use metric custom training loop, : Instantiate metric object, e.g. metric = keras.metrics.AUC() Call metric.udpate_state(targets, predictions) method batch data Query result via metric.result() Reset metric’s state end epoch start evaluation via metric.reset_state() ’s simple example: can also define metrics subclassing keras.metrics.Metric. need override three functions called : Override update_state() update statistic values. Override result() return metric value. Override reset_state() reset metric initial state. example implement F1-score metric (support sample weighting). Let’s test-drive :","code":"# Instantiate a metric object accuracy = keras.metrics.SparseCategoricalAccuracy()  # Prepare our layer, loss, and optimizer. model = keras.Sequential(     [         keras.layers.Dense(32, activation=\"relu\"),         keras.layers.Dense(32, activation=\"relu\"),         keras.layers.Dense(10),     ] ) loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True) optimizer = keras.optimizers.Adam(learning_rate=1e-3)  for epoch in range(2):     # Iterate over the batches of a dataset.     for step, (x, y) in enumerate(dataset):         with tf.GradientTape() as tape:             logits = model(x)             # Compute the loss value for this batch.             loss_value = loss_fn(y, logits)          # Update the state of the `accuracy` metric.         accuracy.update_state(y, logits)          # Update the weights of the model to minimize the loss value.         gradients = tape.gradient(loss_value, model.trainable_weights)         optimizer.apply_gradients(zip(gradients, model.trainable_weights))          # Logging the current accuracy value so far.         if step % 200 == 0:             print(\"Epoch:\", epoch, \"Step:\", step)             print(\"Total running accuracy so far: %.3f\" % accuracy.result())      # Reset the metric's state at the end of an epoch     accuracy.reset_state() class F1Score(keras.metrics.Metric):     def __init__(self, name=\"f1_score\", dtype=\"float32\", threshold=0.5, **kwargs):         super().__init__(name=name, dtype=dtype, **kwargs)         self.threshold = 0.5         self.true_positives = self.add_weight(             name=\"tp\", dtype=dtype, initializer=\"zeros\"         )         self.false_positives = self.add_weight(             name=\"fp\", dtype=dtype, initializer=\"zeros\"         )         self.false_negatives = self.add_weight(             name=\"fn\", dtype=dtype, initializer=\"zeros\"         )      def update_state(self, y_true, y_pred, sample_weight=None):         y_pred = tf.math.greater_equal(y_pred, self.threshold)         y_true = tf.cast(y_true, tf.bool)         y_pred = tf.cast(y_pred, tf.bool)          true_positives = tf.cast(y_true & y_pred, self.dtype)         false_positives = tf.cast(~y_true & y_pred, self.dtype)         false_negatives = tf.cast(y_true & ~y_pred, self.dtype)          if sample_weight is not None:             sample_weight = tf.cast(sample_weight, self.dtype)             true_positives *= sample_weight             false_positives *= sample_weight             false_negatives *= sample_weight          self.true_positives.assign_add(tf.reduce_sum(true_positives))         self.false_positives.assign_add(tf.reduce_sum(false_positives))         self.false_negatives.assign_add(tf.reduce_sum(false_negatives))      def result(self):         precision = self.true_positives / (self.true_positives + self.false_positives)         recall = self.true_positives / (self.true_positives + self.false_negatives)         return precision * recall * 2.0 / (precision + recall)      def reset_state(self):         self.true_positives.assign(0)         self.false_positives.assign(0)         self.false_negatives.assign(0) m = F1Score() m.update_state([0, 1, 0, 0], [0.3, 0.5, 0.8, 0.9]) print(\"Intermediate result:\", float(m.result()))  m.update_state([1, 1, 1, 1], [0.1, 0.7, 0.6, 0.0]) print(\"Final result:\", float(m.result()))"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_researchers.html","id":"compiled-functions","dir":"Articles > Guides","previous_headings":"","what":"Compiled functions","title":"Introduction to Keras for Researchers","text":"Running eagerly great debugging, get better performance compiling computation static graphs. Static graphs researcher’s best friends. can compile function wrapping tf.function decorator.","code":"# Prepare our layer, loss, and optimizer. model = keras.Sequential(     [         keras.layers.Dense(32, activation=\"relu\"),         keras.layers.Dense(32, activation=\"relu\"),         keras.layers.Dense(10),     ] ) loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True) optimizer = keras.optimizers.Adam(learning_rate=1e-3)  # Create a training step function.   @tf.function  # Make it fast. def train_on_batch(x, y):     with tf.GradientTape() as tape:         logits = model(x)         loss = loss_fn(y, logits)         gradients = tape.gradient(loss, model.trainable_weights)     optimizer.apply_gradients(zip(gradients, model.trainable_weights))     return loss   # Prepare a dataset. (x_train, y_train), _ = keras.datasets.mnist.load_data() dataset = tf.data.Dataset.from_tensor_slices(     (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train) ) dataset = dataset.shuffle(buffer_size=1024).batch(64)  for step, (x, y) in enumerate(dataset):     loss = train_on_batch(x, y)     if step % 100 == 0:         print(\"Step:\", step, \"Loss:\", float(loss))"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_researchers.html","id":"training-mode-inference-mode","dir":"Articles > Guides","previous_headings":"","what":"Training mode & inference mode","title":"Introduction to Keras for Researchers","text":"layers, particular BatchNormalization layer Dropout layer, different behaviors training inference. layers, standard practice expose training (boolean) argument call method. exposing argument call, enable built-training evaluation loops (e.g. fit) correctly use layer training inference modes.","code":"class Dropout(keras.layers.Layer):     def __init__(self, rate):         super().__init__()         self.rate = rate      def call(self, inputs, training=None):         if training:             return tf.nn.dropout(inputs, rate=self.rate)         return inputs   class MLPWithDropout(keras.layers.Layer):     def __init__(self):         super().__init__()         self.linear_1 = Linear(32)         self.dropout = Dropout(0.5)         self.linear_3 = Linear(10)      def call(self, inputs, training=None):         x = self.linear_1(inputs)         x = tf.nn.relu(x)         x = self.dropout(x, training=training)         return self.linear_3(x)   mlp = MLPWithDropout() y_train = mlp(tf.ones((2, 2)), training=True) y_test = mlp(tf.ones((2, 2)), training=False)"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_researchers.html","id":"the-functional-api-for-model-building","dir":"Articles > Guides","previous_headings":"","what":"The Functional API for model-building","title":"Introduction to Keras for Researchers","text":"build deep learning models, don’t use object-oriented programming time. layers ’ve seen far can also composed functionally, like (call “Functional API”): Functional API tends concise subclassing, provides advantages (generally advantages functional, typed languages provide untyped OO development). However, can used define DAGs layers – recursive networks defined Layer subclasses instead. Learn Functional API . research workflows, may often find mix--matching OO models Functional models. Note Model class also features built-training & evaluation loops: fit(), predict() evaluate() (configured via compile() method). built-functions give access following built-training infrastructure features: Callbacks. can leverage built-callbacks early-stopping, model checkpointing, monitoring training TensorBoard. can also implement custom callbacks needed. Distributed training. can easily scale training multiple GPUs, TPU, even multiple machines tf.distribute API – changes code. Step fusing. steps_per_execution argument Model.compile(), can process multiple batches single tf.function call, greatly improves device utilization TPUs. won’t go details, provide simple code example . leverages built-training infrastructure implement MNIST example . can always subclass Model class (works exactly like subclassing Layer) want leverage built-training loops OO models. Just override Model.train_step() customize happens fit() retaining support built-infrastructure features outlined – callbacks, zero-code distribution support, step fusing support. may also override test_step() customize happens evaluate(), override predict_step() customize happens predict(). information, please refer guide.","code":"# We use an `Input` object to describe the shape and dtype of the inputs. # This is the deep learning equivalent of *declaring a type*. # The shape argument is per-sample; it does not include the batch size. # The functional API focused on defining per-sample transformations. # The model we create will automatically batch the per-sample transformations, # so that it can be called on batches of data. inputs = keras.Input(shape=(16,), dtype=\"float32\")  # We call layers on these \"type\" objects # and they return updated types (new shapes/dtypes). x = Linear(32)(inputs)  # We are reusing the Linear layer we defined earlier. x = Dropout(0.5)(x)  # We are reusing the Dropout layer we defined earlier. outputs = Linear(10)(x)  # A functional `Model` can be defined by specifying inputs and outputs. # A model is itself a layer like any other. model = keras.Model(inputs, outputs)  # A functional model already has weights, before being called on any data. # That's because we defined its input shape in advance (in `Input`). assert len(model.weights) == 4  # Let's call our model on some data, for fun. y = model(tf.ones((2, 16))) assert y.shape == (2, 10)  # You can pass a `training` argument in `__call__` # (it will get passed down to the Dropout layer). y = model(tf.ones((2, 16)), training=True) inputs = keras.Input(shape=(784,), dtype=\"float32\") x = keras.layers.Dense(32, activation=\"relu\")(inputs) x = keras.layers.Dense(32, activation=\"relu\")(x) outputs = keras.layers.Dense(10)(x) model = keras.Model(inputs, outputs)  # Specify the loss, optimizer, and metrics with `compile()`. model.compile(     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),     optimizer=keras.optimizers.Adam(learning_rate=1e-3),     metrics=[keras.metrics.SparseCategoricalAccuracy()], )  # Train the model with the dataset for 2 epochs. model.fit(dataset, epochs=2) model.predict(dataset) model.evaluate(dataset) class CustomModel(keras.Model):     def __init__(self, *args, **kwargs):         super().__init__(*args, **kwargs)         self.loss_tracker = keras.metrics.Mean(name=\"loss\")         self.accuracy = keras.metrics.SparseCategoricalAccuracy()         self.loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)         self.optimizer = keras.optimizers.Adam(learning_rate=1e-3)      def train_step(self, data):         # Unpack the data. Its structure depends on your model and         # on what you pass to `fit()`.         x, y = data         with tf.GradientTape() as tape:             y_pred = self(x, training=True)  # Forward pass             loss = self.loss_fn(y, y_pred)         gradients = tape.gradient(loss, self.trainable_weights)         self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))         # Update metrics (includes the metric that tracks the loss)         self.loss_tracker.update_state(loss)         self.accuracy.update_state(y, y_pred)         # Return a dict mapping metric names to current value         return {\"loss\": self.loss_tracker.result(), \"accuracy\": self.accuracy.result()}      @property     def metrics(self):         # We list our `Metric` objects here so that `reset_states()` can be         # called automatically at the start of each epoch.         return [self.loss_tracker, self.accuracy]   inputs = keras.Input(shape=(784,), dtype=\"float32\") x = keras.layers.Dense(32, activation=\"relu\")(inputs) x = keras.layers.Dense(32, activation=\"relu\")(x) outputs = keras.layers.Dense(10)(x) model = CustomModel(inputs, outputs) model.compile() model.fit(dataset, epochs=2)"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_researchers.html","id":"end-to-end-experiment-example-1-variational-autoencoders-","dir":"Articles > Guides","previous_headings":"","what":"End-to-end experiment example 1: variational autoencoders.","title":"Introduction to Keras for Researchers","text":"things ’ve learned far: Layer encapsulates state (created __init__ build) computation (defined call). Layers can recursively nested create new, bigger computation blocks. can easily write highly hackable training loops opening GradientTape, calling model inside tape’s scope, retrieving gradients applying via optimizer. can speed training loops using @tf.function decorator. Layers can create track losses (typically regularization losses) via self.add_loss(). Let’s put things together end--end example: ’re going implement Variational AutoEncoder (VAE). ’ll train MNIST digits. VAE subclass Layer, built nested composition layers subclass Layer. feature regularization loss (KL divergence). model definition. First, Encoder class, uses Sampling layer map MNIST digit latent-space triplet (z_mean, z_log_var, z). Next, Decoder class, maps probabilistic latent space coordinates back MNIST digit. Finally, VariationalAutoEncoder composes together encoder decoder, creates KL divergence regularization loss via add_loss(). Now, let’s write training loop. training step decorated @tf.function compile super fast graph function. can see, building training type model Keras quick painless.","code":"from tensorflow.keras import layers   class Sampling(layers.Layer):     \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"      def call(self, inputs):         z_mean, z_log_var = inputs         batch = tf.shape(z_mean)[0]         dim = tf.shape(z_mean)[1]         epsilon = keras.backend.random_normal(shape=(batch, dim))         return z_mean + tf.exp(0.5 * z_log_var) * epsilon   class Encoder(layers.Layer):     \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"      def __init__(self, latent_dim=32, intermediate_dim=64, **kwargs):         super().__init__(**kwargs)         self.dense_proj = layers.Dense(intermediate_dim, activation=tf.nn.relu)         self.dense_mean = layers.Dense(latent_dim)         self.dense_log_var = layers.Dense(latent_dim)         self.sampling = Sampling()      def call(self, inputs):         x = self.dense_proj(inputs)         z_mean = self.dense_mean(x)         z_log_var = self.dense_log_var(x)         z = self.sampling((z_mean, z_log_var))         return z_mean, z_log_var, z class Decoder(layers.Layer):     \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"      def __init__(self, original_dim, intermediate_dim=64, **kwargs):         super().__init__(**kwargs)         self.dense_proj = layers.Dense(intermediate_dim, activation=tf.nn.relu)         self.dense_output = layers.Dense(original_dim, activation=tf.nn.sigmoid)      def call(self, inputs):         x = self.dense_proj(inputs)         return self.dense_output(x) class VariationalAutoEncoder(layers.Layer):     \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"      def __init__(self, original_dim, intermediate_dim=64, latent_dim=32, **kwargs):         super().__init__(**kwargs)         self.original_dim = original_dim         self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)         self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)      def call(self, inputs):         z_mean, z_log_var, z = self.encoder(inputs)         reconstructed = self.decoder(z)         # Add KL divergence regularization loss.         kl_loss = -0.5 * tf.reduce_mean(             z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1         )         self.add_loss(kl_loss)         return reconstructed # Our model. vae = VariationalAutoEncoder(original_dim=784, intermediate_dim=64, latent_dim=32)  # Loss and optimizer. loss_fn = keras.losses.MeanSquaredError() optimizer = keras.optimizers.Adam(learning_rate=1e-3)  # Prepare a dataset. (x_train, _), _ = keras.datasets.mnist.load_data() dataset = tf.data.Dataset.from_tensor_slices(     x_train.reshape(60000, 784).astype(\"float32\") / 255 ) dataset = dataset.shuffle(buffer_size=1024).batch(32)   @tf.function def training_step(x):     with tf.GradientTape() as tape:         reconstructed = vae(x)  # Compute input reconstruction.         # Compute loss.         loss = loss_fn(x, reconstructed)         loss += sum(vae.losses)  # Add KLD term.     # Update the weights of the VAE.     grads = tape.gradient(loss, vae.trainable_weights)     optimizer.apply_gradients(zip(grads, vae.trainable_weights))     return loss   losses = []  # Keep track of the losses over time. for step, x in enumerate(dataset):     loss = training_step(x)     # Logging.     losses.append(float(loss))     if step % 100 == 0:         print(\"Step:\", step, \"Loss:\", sum(losses) / len(losses))      # Stop after 1000 steps.     # Training the model to convergence is left     # as an exercise to the reader.     if step >= 1000:         break"},{"path":"https://keras.posit.co/articles/guides/intro_to_keras_for_researchers.html","id":"end-to-end-experiment-example-2-hypernetworks-","dir":"Articles > Guides","previous_headings":"","what":"End-to-end experiment example 2: hypernetworks.","title":"Introduction to Keras for Researchers","text":"Let’s take look another kind research experiment: hypernetworks. idea use small deep neural network (hypernetwork) generate weights larger network (main network). Let’s implement really trivial hypernetwork: ’ll use small 2-layer network generate weights larger 3-layer network. training loop. batch data: use hypernetwork generate array weight coefficients, weights_pred reshape coefficients kernel & bias tensors main_network run forward pass main_network compute actual MNIST predictions run backprop weights hypernetwork minimize final classification loss Implementing arbitrary research ideas Keras straightforward highly productive. Imagine trying 25 ideas per day (20 minutes per experiment average)! Keras designed go idea results fast possible, believe key great research. hope enjoyed quick introduction. Let us know build Keras!","code":"import numpy as np  input_dim = 784 classes = 10  # This is the main network we'll actually use to predict labels. main_network = keras.Sequential(     [         keras.layers.Dense(64, activation=tf.nn.relu),         keras.layers.Dense(classes),     ] )  # It doesn't need to create its own weights, so let's mark its layers # as already built. That way, calling `main_network` won't create new variables. for layer in main_network.layers:     layer.built = True  # This is the number of weight coefficients to generate. Each layer in the # main network requires output_dim * input_dim + output_dim coefficients. num_weights_to_generate = (classes * 64 + classes) + (64 * input_dim + 64)  # This is the hypernetwork that generates the weights of the `main_network` above. hypernetwork = keras.Sequential(     [         keras.layers.Dense(16, activation=tf.nn.relu),         keras.layers.Dense(num_weights_to_generate, activation=tf.nn.sigmoid),     ] ) # Loss and optimizer. loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True) optimizer = keras.optimizers.Adam(learning_rate=1e-4)  # Prepare a dataset. (x_train, y_train), _ = keras.datasets.mnist.load_data() dataset = tf.data.Dataset.from_tensor_slices(     (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train) )  # We'll use a batch size of 1 for this experiment. dataset = dataset.shuffle(buffer_size=1024).batch(1)   @tf.function def train_step(x, y):     with tf.GradientTape() as tape:         # Predict weights for the outer model.         weights_pred = hypernetwork(x)          # Reshape them to the expected shapes for w and b for the outer model.         # Layer 0 kernel.         start_index = 0         w0_shape = (input_dim, 64)         w0_coeffs = weights_pred[:, start_index : start_index + np.prod(w0_shape)]         w0 = tf.reshape(w0_coeffs, w0_shape)         start_index += np.prod(w0_shape)         # Layer 0 bias.         b0_shape = (64,)         b0_coeffs = weights_pred[:, start_index : start_index + np.prod(b0_shape)]         b0 = tf.reshape(b0_coeffs, b0_shape)         start_index += np.prod(b0_shape)         # Layer 1 kernel.         w1_shape = (64, classes)         w1_coeffs = weights_pred[:, start_index : start_index + np.prod(w1_shape)]         w1 = tf.reshape(w1_coeffs, w1_shape)         start_index += np.prod(w1_shape)         # Layer 1 bias.         b1_shape = (classes,)         b1_coeffs = weights_pred[:, start_index : start_index + np.prod(b1_shape)]         b1 = tf.reshape(b1_coeffs, b1_shape)         start_index += np.prod(b1_shape)          # Set the weight predictions as the weight variables on the outer model.         main_network.layers[0].kernel = w0         main_network.layers[0].bias = b0         main_network.layers[1].kernel = w1         main_network.layers[1].bias = b1          # Inference on the outer model.         preds = main_network(x)         loss = loss_fn(y, preds)      # Train only inner model.     grads = tape.gradient(loss, hypernetwork.trainable_weights)     optimizer.apply_gradients(zip(grads, hypernetwork.trainable_weights))     return loss   losses = []  # Keep track of the losses over time. for step, (x, y) in enumerate(dataset):     loss = train_step(x, y)      # Logging.     losses.append(float(loss))     if step % 100 == 0:         print(\"Step:\", step, \"Loss:\", sum(losses) / len(losses))      # Stop after 1000 steps.     # Training the model to convergence is left     # as an exercise to the reader.     if step >= 1000:         break"},{"path":"https://keras.posit.co/articles/guides/making_new_layers_and_models_via_subclassing.html","id":"introduction","dir":"Articles > Guides","previous_headings":"","what":"Introduction","title":"Making new layers and models via subclassing","text":"guide cover everything need know build subclassed layers models. particular, ’ll learn following features: Layer class add_weight() method Trainable non-trainable weights build() method Making sure layers can used backend add_loss() method training argument call() mask argument call() Making sure layers can serialized Let’s dive .","code":""},{"path":"https://keras.posit.co/articles/guides/making_new_layers_and_models_via_subclassing.html","id":"setup","dir":"Articles > Guides","previous_headings":"","what":"Setup","title":"Making new layers and models via subclassing","text":"","code":"import numpy as np import keras as keras from keras import ops from keras import layers"},{"path":"https://keras.posit.co/articles/guides/making_new_layers_and_models_via_subclassing.html","id":"the-layer-class-the-combination-of-state-weights-and-some-computation","dir":"Articles > Guides","previous_headings":"","what":"The Layer class: the combination of state (weights) and some computation","title":"Making new layers and models via subclassing","text":"One central abstractions Keras Layer class. layer encapsulates state (layer’s “weights”) transformation inputs outputs (“call”, layer’s forward pass). ’s densely-connected layer. two state variables: variables w b. use layer calling tensor input(s), much like Python function. Note weights w b automatically tracked layer upon set layer attributes:","code":"class Linear(keras.layers.Layer):     def __init__(self, units=32, input_dim=32):         super().__init__()         self.w = self.add_weight(             shape=(input_dim, units),             initializer=\"random_normal\",             trainable=True,         )         self.b = self.add_weight(             shape=(units,), initializer=\"zeros\", trainable=True         )      def call(self, inputs):         return ops.matmul(inputs, self.w) + self.b x = ops.ones((2, 2)) linear_layer = Linear(4, 2) y = linear_layer(x) print(y) assert linear_layer.weights == [linear_layer.w, linear_layer.b]"},{"path":"https://keras.posit.co/articles/guides/making_new_layers_and_models_via_subclassing.html","id":"layers-can-have-non-trainable-weights","dir":"Articles > Guides","previous_headings":"","what":"Layers can have non-trainable weights","title":"Making new layers and models via subclassing","text":"Besides trainable weights, can add non-trainable weights layer well. weights meant taken account backpropagation, training layer. ’s add use non-trainable weight: ’s part layer.weights, gets categorized non-trainable weight:","code":"class ComputeSum(keras.layers.Layer):     def __init__(self, input_dim):         super().__init__()         self.total = self.add_weight(             initializer=\"zeros\", shape=(input_dim,), trainable=False         )      def call(self, inputs):         self.total.assign_add(ops.sum(inputs, axis=0))         return self.total   x = ops.ones((2, 2)) my_sum = ComputeSum(2) y = my_sum(x) print(y.numpy()) y = my_sum(x) print(y.numpy()) print(\"weights:\", len(my_sum.weights)) print(\"non-trainable weights:\", len(my_sum.non_trainable_weights))  # It's not included in the trainable weights: print(\"trainable_weights:\", my_sum.trainable_weights)"},{"path":"https://keras.posit.co/articles/guides/making_new_layers_and_models_via_subclassing.html","id":"best-practice-deferring-weight-creation-until-the-shape-of-the-inputs-is-known","dir":"Articles > Guides","previous_headings":"","what":"Best practice: deferring weight creation until the shape of the inputs is known","title":"Making new layers and models via subclassing","text":"Linear layer took input_dim argument used compute shape weights w b __init__(): many cases, may know advance size inputs, like lazily create weights value becomes known, time instantiating layer. Keras API, recommend creating layer weights build(self, inputs_shape) method layer. Like : __call__() method layer automatically run build first time called. now layer ’s lazy thus easier use: Implementing build() separately shown nicely separates creating weights using weights every call.","code":"class Linear(keras.layers.Layer):     def __init__(self, units=32, input_dim=32):         super().__init__()         self.w = self.add_weight(             shape=(input_dim, units),             initializer=\"random_normal\",             trainable=True,         )         self.b = self.add_weight(             shape=(units,), initializer=\"zeros\", trainable=True         )      def call(self, inputs):         return ops.matmul(inputs, self.w) + self.b class Linear(keras.layers.Layer):     def __init__(self, units=32):         super().__init__()         self.units = units      def build(self, input_shape):         self.w = self.add_weight(             shape=(input_shape[-1], self.units),             initializer=\"random_normal\",             trainable=True,         )         self.b = self.add_weight(             shape=(self.units,), initializer=\"random_normal\", trainable=True         )      def call(self, inputs):         return ops.matmul(inputs, self.w) + self.b # At instantiation, we don't know on what inputs this is going to get called linear_layer = Linear(32)  # The layer's weights are created dynamically the first time the layer is called y = linear_layer(x)"},{"path":"https://keras.posit.co/articles/guides/making_new_layers_and_models_via_subclassing.html","id":"layers-are-recursively-composable","dir":"Articles > Guides","previous_headings":"","what":"Layers are recursively composable","title":"Making new layers and models via subclassing","text":"assign Layer instance attribute another Layer, outer layer start tracking weights created inner layer. recommend creating sublayers __init__() method leave first __call__() trigger building weights.","code":"class MLPBlock(keras.layers.Layer):     def __init__(self):         super().__init__()         self.linear_1 = Linear(32)         self.linear_2 = Linear(32)         self.linear_3 = Linear(1)      def call(self, inputs):         x = self.linear_1(inputs)         x = keras.activations.relu(x)         x = self.linear_2(x)         x = keras.activations.relu(x)         return self.linear_3(x)   mlp = MLPBlock() y = mlp(     ops.ones(shape=(3, 64)) )  # The first call to the `mlp` will create the weights print(\"weights:\", len(mlp.weights)) print(\"trainable weights:\", len(mlp.trainable_weights))"},{"path":"https://keras.posit.co/articles/guides/making_new_layers_and_models_via_subclassing.html","id":"backend-agnostic-layers-and-backend-specific-layers","dir":"Articles > Guides","previous_headings":"","what":"Backend-agnostic layers and backend-specific layers","title":"Making new layers and models via subclassing","text":"long layer uses APIs keras.ops namespace (Keras namespaces keras.activations, keras.random, keras.layers), can used backend – TensorFlow, JAX, PyTorch. layers ’ve seen far guide work Keras backends. keras.ops namespace gives access : NumPy API, e.g. ops.matmul, ops.sum, ops.reshape, ops.stack, etc. Neural networks-specific APIs ops.softmax, ops.conv, ops.binary_crossentropy, ops.relu, etc. can also use backend-native APIs layers (tf.nn functions), , layer usable backend question. instance, write following JAX-specific layer using jax.numpy: equivalent TensorFlow-specific layer: equivalent PyTorch-specific layer: cross-backend compatibility tremendously useful property, strongly recommend seek always make layers backend-agnostic leveraging Keras APIs.","code":"import jax  class Linear(keras.layers.Layer):     ...      def call(self, inputs):         return jax.numpy.matmul(inputs, self.w) + self.b import tensorflow as tf  class Linear(keras.layers.Layer):     ...      def call(self, inputs):         return tf.matmul(inputs, self.w) + self.b import torch  class Linear(keras.layers.Layer):     ...      def call(self, inputs):         return torch.matmul(inputs, self.w) + self.b"},{"path":"https://keras.posit.co/articles/guides/making_new_layers_and_models_via_subclassing.html","id":"the-add_loss-method","dir":"Articles > Guides","previous_headings":"","what":"The add_loss() method","title":"Making new layers and models via subclassing","text":"writing call() method layer, can create loss tensors want use later, writing training loop. doable calling self.add_loss(value): losses (including created inner layer) can retrieved via layer.losses. property reset start every __call__() top-level layer, layer.losses always contains loss values created last forward pass. addition, loss property also contains regularization losses created weights inner layer: losses meant taken account writing custom training loops. also work seamlessly fit() (get automatically summed added main loss, ):","code":"# A layer that creates an activity regularization loss class ActivityRegularizationLayer(keras.layers.Layer):     def __init__(self, rate=1e-2):         super().__init__()         self.rate = rate      def call(self, inputs):         self.add_loss(self.rate * ops.mean(inputs))         return inputs class OuterLayer(keras.layers.Layer):     def __init__(self):         super().__init__()         self.activity_reg = ActivityRegularizationLayer(1e-2)      def call(self, inputs):         return self.activity_reg(inputs)   layer = OuterLayer() assert (     len(layer.losses) == 0 )  # No losses yet since the layer has never been called  _ = layer(ops.zeros((1, 1))) assert len(layer.losses) == 1  # We created one loss value  # `layer.losses` gets reset at the start of each __call__ _ = layer(ops.zeros((1, 1))) assert len(layer.losses) == 1  # This is the loss created during the call above class OuterLayerWithKernelRegularizer(keras.layers.Layer):     def __init__(self):         super().__init__()         self.dense = keras.layers.Dense(             32, kernel_regularizer=keras.regularizers.l2(1e-3)         )      def call(self, inputs):         return self.dense(inputs)   layer = OuterLayerWithKernelRegularizer() _ = layer(ops.zeros((1, 1)))  # This is `1e-3 * sum(layer.dense.kernel ** 2)`, # created by the `kernel_regularizer` above. print(layer.losses) inputs = keras.Input(shape=(3,)) outputs = ActivityRegularizationLayer()(inputs) model = keras.Model(inputs, outputs)  # If there is a loss passed in `compile`, the regularization # losses get added to it model.compile(optimizer=\"adam\", loss=\"mse\") model.fit(np.random.random((2, 3)), np.random.random((2, 3)))  # It's also possible not to pass any loss in `compile`, # since the model already has a loss to minimize, via the `add_loss` # call during the forward pass! model.compile(optimizer=\"adam\") model.fit(np.random.random((2, 3)), np.random.random((2, 3)))"},{"path":"https://keras.posit.co/articles/guides/making_new_layers_and_models_via_subclassing.html","id":"you-can-optionally-enable-serialization-on-your-layers","dir":"Articles > Guides","previous_headings":"","what":"You can optionally enable serialization on your layers","title":"Making new layers and models via subclassing","text":"need custom layers serializable part Functional model, can optionally implement get_config() method: Note __init__() method base Layer class takes keyword arguments, particular name dtype. ’s good practice pass arguments parent class __init__() include layer config: need flexibility deserializing layer config, can also override from_config() class method. base implementation from_config(): learn serialization saving, see complete guide saving serializing models.","code":"class Linear(keras.layers.Layer):     def __init__(self, units=32):         super().__init__()         self.units = units      def build(self, input_shape):         self.w = self.add_weight(             shape=(input_shape[-1], self.units),             initializer=\"random_normal\",             trainable=True,         )         self.b = self.add_weight(             shape=(self.units,), initializer=\"random_normal\", trainable=True         )      def call(self, inputs):         return ops.matmul(inputs, self.w) + self.b      def get_config(self):         return {\"units\": self.units}   # Now you can recreate the layer from its config: layer = Linear(64) config = layer.get_config() print(config) new_layer = Linear.from_config(config) class Linear(keras.layers.Layer):     def __init__(self, units=32, **kwargs):         super().__init__(**kwargs)         self.units = units      def build(self, input_shape):         self.w = self.add_weight(             shape=(input_shape[-1], self.units),             initializer=\"random_normal\",             trainable=True,         )         self.b = self.add_weight(             shape=(self.units,), initializer=\"random_normal\", trainable=True         )      def call(self, inputs):         return ops.matmul(inputs, self.w) + self.b      def get_config(self):         config = super().get_config()         config.update({\"units\": self.units})         return config   layer = Linear(64) config = layer.get_config() print(config) new_layer = Linear.from_config(config) def from_config(cls, config):     return cls(**config)"},{"path":"https://keras.posit.co/articles/guides/making_new_layers_and_models_via_subclassing.html","id":"privileged-training-argument-in-the-call-method","dir":"Articles > Guides","previous_headings":"","what":"Privileged training argument in the call() method","title":"Making new layers and models via subclassing","text":"layers, particular BatchNormalization layer Dropout layer, different behaviors training inference. layers, standard practice expose training (boolean) argument call() method. exposing argument call(), enable built-training evaluation loops (e.g. fit()) correctly use layer training inference.","code":"class CustomDropout(keras.layers.Layer):     def __init__(self, rate, **kwargs):         super().__init__(**kwargs)         self.rate = rate      def call(self, inputs, training=None):         if training:             return keras.random.dropout(inputs, rate=self.rate)         return inputs"},{"path":"https://keras.posit.co/articles/guides/making_new_layers_and_models_via_subclassing.html","id":"privileged-mask-argument-in-the-call-method","dir":"Articles > Guides","previous_headings":"","what":"Privileged mask argument in the call() method","title":"Making new layers and models via subclassing","text":"privileged argument supported call() mask argument. find Keras RNN layers. mask boolean tensor (one boolean value per timestep input) used skip certain input timesteps processing timeseries data. Keras automatically pass correct mask argument __call__() layers support , mask generated prior layer. Mask-generating layers Embedding layer configured mask_zero=True, Masking layer.","code":""},{"path":"https://keras.posit.co/articles/guides/making_new_layers_and_models_via_subclassing.html","id":"the-model-class","dir":"Articles > Guides","previous_headings":"","what":"The Model class","title":"Making new layers and models via subclassing","text":"general, use Layer class define inner computation blocks, use Model class define outer model – object train. instance, ResNet50 model, several ResNet blocks subclassing Layer, single Model encompassing entire ResNet50 network. Model class API Layer, following differences: exposes built-training, evaluation, prediction loops (model.fit(), model.evaluate(), model.predict()). exposes list inner layers, via model.layers property. exposes saving serialization APIs (save(), save_weights()…) Effectively, Layer class corresponds refer literature “layer” (“convolution layer” “recurrent layer”) “block” (“ResNet block” “Inception block”). Meanwhile, Model class corresponds referred literature “model” (“deep learning model”) “network” (“deep neural network”). ’re wondering, “use Layer class Model class?”, ask : need call fit() ? need call save() ? , go Model. (either class just block bigger system, writing training & saving code ), use Layer. instance, take mini-resnet example , use build Model train fit(), save save_weights():","code":"class ResNet(keras.Model):      def __init__(self, num_classes=1000):         super().__init__()         self.block_1 = ResNetBlock()         self.block_2 = ResNetBlock()         self.global_pool = layers.GlobalAveragePooling2D()         self.classifier = Dense(num_classes)      def call(self, inputs):         x = self.block_1(inputs)         x = self.block_2(x)         x = self.global_pool(x)         return self.classifier(x)  resnet = ResNet() dataset = ... resnet.fit(dataset, epochs=10) resnet.save(filepath.keras)"},{"path":"https://keras.posit.co/articles/guides/making_new_layers_and_models_via_subclassing.html","id":"putting-it-all-together-an-end-to-end-example","dir":"Articles > Guides","previous_headings":"","what":"Putting it all together: an end-to-end example","title":"Making new layers and models via subclassing","text":"’s ’ve learned far: Layer encapsulate state (created __init__() build()) computation (defined call()). Layers can recursively nested create new, bigger computation blocks. Layers backend-agnostic long use Keras APIs. can use backend-native APIs (jax.numpy, torch.nn tf.nn), layer usable specific backend. Layers can create track losses (typically regularization losses) via add_loss(). outer container, thing want train, Model. Model just like Layer, added training serialization utilities. Let’s put things together end--end example: ’re going implement Variational AutoEncoder (VAE) backend-agnostic fashion – runs TensorFlow, JAX, PyTorch. ’ll train MNIST digits. VAE subclass Model, built nested composition layers subclass Layer. feature regularization loss (KL divergence). Let’s train MNIST using fit() API:","code":"class Sampling(layers.Layer):     \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"      def call(self, inputs):         z_mean, z_log_var = inputs         batch = ops.shape(z_mean)[0]         dim = ops.shape(z_mean)[1]         epsilon = keras.random.normal(shape=(batch, dim))         return z_mean + ops.exp(0.5 * z_log_var) * epsilon   class Encoder(layers.Layer):     \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"      def __init__(         self, latent_dim=32, intermediate_dim=64, name=\"encoder\", **kwargs     ):         super().__init__(name=name, **kwargs)         self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")         self.dense_mean = layers.Dense(latent_dim)         self.dense_log_var = layers.Dense(latent_dim)         self.sampling = Sampling()      def call(self, inputs):         x = self.dense_proj(inputs)         z_mean = self.dense_mean(x)         z_log_var = self.dense_log_var(x)         z = self.sampling((z_mean, z_log_var))         return z_mean, z_log_var, z   class Decoder(layers.Layer):     \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"      def __init__(         self, original_dim, intermediate_dim=64, name=\"decoder\", **kwargs     ):         super().__init__(name=name, **kwargs)         self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")         self.dense_output = layers.Dense(original_dim, activation=\"sigmoid\")      def call(self, inputs):         x = self.dense_proj(inputs)         return self.dense_output(x)   class VariationalAutoEncoder(keras.Model):     \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"      def __init__(         self,         original_dim,         intermediate_dim=64,         latent_dim=32,         name=\"autoencoder\",         **kwargs     ):         super().__init__(name=name, **kwargs)         self.original_dim = original_dim         self.encoder = Encoder(             latent_dim=latent_dim, intermediate_dim=intermediate_dim         )         self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)      def call(self, inputs):         z_mean, z_log_var, z = self.encoder(inputs)         reconstructed = self.decoder(z)         # Add KL divergence regularization loss.         kl_loss = -0.5 * ops.mean(             z_log_var - ops.square(z_mean) - ops.exp(z_log_var) + 1         )         self.add_loss(kl_loss)         return reconstructed (x_train, _), _ = keras.datasets.mnist.load_data() x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255  original_dim = 784 vae = VariationalAutoEncoder(784, 64, 32)  optimizer = keras.optimizers.Adam(learning_rate=1e-3) vae.compile(optimizer, loss=keras.losses.MeanSquaredError())  vae.fit(x_train, x_train, epochs=2, batch_size=64)"},{"path":"https://keras.posit.co/articles/guides/preprocessing_layers.html","id":"keras-preprocessing","dir":"Articles > Guides","previous_headings":"","what":"Keras preprocessing","title":"Working with preprocessing layers","text":"Keras preprocessing layers API allows developers build Keras-native input processing pipelines. input processing pipelines can used independent preprocessing code non-Keras workflows, combined directly Keras models, exported part Keras SavedModel. Keras preprocessing layers, can build export models truly end--end: models accept raw images raw structured data input; models handle feature normalization feature value indexing .","code":""},{"path":[]},{"path":"https://keras.posit.co/articles/guides/preprocessing_layers.html","id":"text-preprocessing","dir":"Articles > Guides","previous_headings":"Available preprocessing","what":"Text preprocessing","title":"Working with preprocessing layers","text":"tf.keras.layers.TextVectorization: turns raw strings encoded representation can read Embedding layer Dense layer.","code":""},{"path":"https://keras.posit.co/articles/guides/preprocessing_layers.html","id":"numerical-features-preprocessing","dir":"Articles > Guides","previous_headings":"Available preprocessing","what":"Numerical features preprocessing","title":"Working with preprocessing layers","text":"tf.keras.layers.Normalization: performs feature-wise normalization input features. tf.keras.layers.Discretization: turns continuous numerical features integer categorical features.","code":""},{"path":"https://keras.posit.co/articles/guides/preprocessing_layers.html","id":"categorical-features-preprocessing","dir":"Articles > Guides","previous_headings":"Available preprocessing","what":"Categorical features preprocessing","title":"Working with preprocessing layers","text":"tf.keras.layers.CategoryEncoding: turns integer categorical features one-hot, multi-hot, count dense representations. tf.keras.layers.Hashing: performs categorical feature hashing, also known “hashing trick”. tf.keras.layers.StringLookup: turns string categorical values encoded representation can read Embedding layer Dense layer. tf.keras.layers.IntegerLookup: turns integer categorical values encoded representation can read Embedding layer Dense layer.","code":""},{"path":"https://keras.posit.co/articles/guides/preprocessing_layers.html","id":"image-preprocessing","dir":"Articles > Guides","previous_headings":"Available preprocessing","what":"Image preprocessing","title":"Working with preprocessing layers","text":"layers standardizing inputs image model. tf.keras.layers.Resizing: resizes batch images target size. tf.keras.layers.Rescaling: rescales offsets values batch images (e.g. go inputs [0, 255] range inputs [0, 1] range. tf.keras.layers.CenterCrop: returns center crop batch images.","code":""},{"path":"https://keras.posit.co/articles/guides/preprocessing_layers.html","id":"image-data-augmentation","dir":"Articles > Guides","previous_headings":"Available preprocessing","what":"Image data augmentation","title":"Working with preprocessing layers","text":"layers apply random augmentation transforms batch images. active training. tf.keras.layers.RandomCrop tf.keras.layers.RandomFlip tf.keras.layers.RandomTranslation tf.keras.layers.RandomRotation tf.keras.layers.RandomZoom tf.keras.layers.RandomContrast","code":""},{"path":"https://keras.posit.co/articles/guides/preprocessing_layers.html","id":"the-adapt-method","dir":"Articles > Guides","previous_headings":"","what":"The adapt() method","title":"Working with preprocessing layers","text":"preprocessing layers internal state can computed based sample training data. list stateful preprocessing layers : TextVectorization: holds mapping string tokens integer indices StringLookup IntegerLookup: hold mapping input values integer indices. Normalization: holds mean standard deviation features. Discretization: holds information value bucket boundaries. Crucially, layers non-trainable. state set training; must set training, either initializing precomputed constant, “adapting” data. set state preprocessing layer exposing training data, via adapt() method: adapt() method takes either Numpy array tf.data.Dataset object. case StringLookup TextVectorization, can also pass list strings: addition, adaptable layers always expose option directly set state via constructor arguments weight assignment. intended state values known layer construction time, calculated outside adapt() call, can set without relying layer’s internal computation. instance, external vocabulary files TextVectorization, StringLookup, IntegerLookup layers already exist, can loaded directly lookup tables passing path vocabulary file layer’s constructor arguments. ’s example instantiate StringLookup layer precomputed vocabulary:","code":"import numpy as np import tensorflow as tf import keras from keras import layers  data = np.array(     [         [0.1, 0.2, 0.3],         [0.8, 0.9, 1.0],         [1.5, 1.6, 1.7],     ] ) layer = layers.Normalization() layer.adapt(data) normalized_data = layer(data)  print(\"Features mean: %.2f\" % (normalized_data.numpy().mean())) print(\"Features std: %.2f\" % (normalized_data.numpy().std())) data = [     \"ξεῖν᾽, ἦ τοι μὲν ὄνειροι ἀμήχανοι ἀκριτόμυθοι\",     \"γίγνοντ᾽, οὐδέ τι πάντα τελείεται ἀνθρώποισι.\",     \"δοιαὶ γάρ τε πύλαι ἀμενηνῶν εἰσὶν ὀνείρων:\",     \"αἱ μὲν γὰρ κεράεσσι τετεύχαται, αἱ δ᾽ ἐλέφαντι:\",     \"τῶν οἳ μέν κ᾽ ἔλθωσι διὰ πριστοῦ ἐλέφαντος,\",     \"οἵ ῥ᾽ ἐλεφαίρονται, ἔπε᾽ ἀκράαντα φέροντες:\",     \"οἱ δὲ διὰ ξεστῶν κεράων ἔλθωσι θύραζε,\",     \"οἵ ῥ᾽ ἔτυμα κραίνουσι, βροτῶν ὅτε κέν τις ἴδηται.\", ] layer = layers.TextVectorization() layer.adapt(data) vectorized_text = layer(data) print(vectorized_text) vocab = [\"a\", \"b\", \"c\", \"d\"] data = tf.constant([[\"a\", \"c\", \"d\"], [\"d\", \"z\", \"b\"]]) layer = layers.StringLookup(vocabulary=vocab) vectorized_data = layer(data) print(vectorized_data)"},{"path":"https://keras.posit.co/articles/guides/preprocessing_layers.html","id":"preprocessing-data-before-the-model-or-inside-the-model","dir":"Articles > Guides","previous_headings":"","what":"Preprocessing data before the model or inside the model","title":"Working with preprocessing layers","text":"two ways using preprocessing layers: Option 1: Make part model, like : option, preprocessing happen device, synchronously rest model execution, meaning benefit GPU acceleration. ’re training GPU, best option Normalization layer, image preprocessing data augmentation layers. Option 2: apply tf.data.Dataset, obtain dataset yields batches preprocessed data, like : option, preprocessing happen CPU, asynchronously, buffered going model. addition, call dataset.prefetch(tf.data.AUTOTUNE) dataset, preprocessing happen efficiently parallel training: best option TextVectorization, structured data preprocessing layers. can also good option ’re training CPU use image preprocessing layers. Note TextVectorization layer can executed CPU, mostly dictionary lookup operation. Therefore, training model GPU TPU, put TextVectorization layer tf.data pipeline get best performance. running TPU, always place preprocessing layers tf.data pipeline (exception Normalization Rescaling, run fine TPU commonly used first layer image model).","code":"inputs = keras.Input(shape=input_shape) x = preprocessing_layer(inputs) outputs = rest_of_the_model(x) model = keras.Model(inputs, outputs) dataset = dataset.map(lambda x, y: (preprocessing_layer(x), y)) dataset = dataset.map(lambda x, y: (preprocessing_layer(x), y)) dataset = dataset.prefetch(tf.data.AUTOTUNE) model.fit(dataset, ...)"},{"path":"https://keras.posit.co/articles/guides/preprocessing_layers.html","id":"benefits-of-doing-preprocessing-inside-the-model-at-inference-time","dir":"Articles > Guides","previous_headings":"","what":"Benefits of doing preprocessing inside the model at inference time","title":"Working with preprocessing layers","text":"Even go option 2, may later want export inference-end--end model include preprocessing layers. key benefit makes model portable helps reduce training/serving skew. data preprocessing part model, people can load use model without aware feature expected encoded & normalized. inference model able process raw images raw structured data, require users model aware details e.g. tokenization scheme used text, indexing scheme used categorical features, whether image pixel values normalized [-1, +1] [0, 1], etc. especially powerful ’re exporting model another runtime, TensorFlow.js: won’t reimplement preprocessing pipeline JavaScript. initially put preprocessing layers tf.data pipeline, can export inference model packages preprocessing. Simply instantiate new model chains preprocessing layers training model:","code":"inputs = keras.Input(shape=input_shape) x = preprocessing_layer(inputs) outputs = training_model(x) inference_model = keras.Model(inputs, outputs)"},{"path":"https://keras.posit.co/articles/guides/preprocessing_layers.html","id":"preprocessing-during-multi-worker-training","dir":"Articles > Guides","previous_headings":"","what":"Preprocessing during multi-worker training","title":"Working with preprocessing layers","text":"Preprocessing layers compatible tf.distribute API running training across multiple machines. general, preprocessing layers placed inside tf.distribute.Strategy.scope() called either inside model discussed . details, refer Data preprocessing section Distributed input tutorial.","code":"with strategy.scope():     inputs = keras.Input(shape=input_shape)     preprocessing_layer = tf.keras.layers.Hashing(10)     dense_layer = tf.keras.layers.Dense(16)"},{"path":[]},{"path":"https://keras.posit.co/articles/guides/preprocessing_layers.html","id":"image-data-augmentation-1","dir":"Articles > Guides","previous_headings":"Quick recipes","what":"Image data augmentation","title":"Working with preprocessing layers","text":"Note image data augmentation layers active training (similarly Dropout layer). can see similar setup action example image classification scratch.","code":"from tensorflow import keras from tensorflow.keras import layers  # Create a data augmentation stage with horizontal flipping, rotations, zooms data_augmentation = keras.Sequential(     [         layers.RandomFlip(\"horizontal\"),         layers.RandomRotation(0.1),         layers.RandomZoom(0.1),     ] )  # Load some data (x_train, y_train), _ = keras.datasets.cifar10.load_data() input_shape = x_train.shape[1:] classes = 10  # Create a tf.data pipeline of augmented images (and their labels) train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)) train_dataset = train_dataset.batch(16).map(lambda x, y: (data_augmentation(x), y))   # Create a model and train it on the augmented image data inputs = keras.Input(shape=input_shape) x = layers.Rescaling(1.0 / 255)(inputs)  # Rescale inputs outputs = keras.applications.ResNet50(  # Add the rest of the model     weights=None, input_shape=input_shape, classes=classes )(x) model = keras.Model(inputs, outputs) model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\") model.fit(train_dataset, steps_per_epoch=5)"},{"path":"https://keras.posit.co/articles/guides/preprocessing_layers.html","id":"normalizing-numerical-features","dir":"Articles > Guides","previous_headings":"Quick recipes","what":"Normalizing numerical features","title":"Working with preprocessing layers","text":"","code":"# Load some data (x_train, y_train), _ = keras.datasets.cifar10.load_data() x_train = x_train.reshape((len(x_train), -1)) input_shape = x_train.shape[1:] classes = 10  # Create a Normalization layer and set its internal state using the training data normalizer = layers.Normalization() normalizer.adapt(x_train)  # Create a model that include the normalization layer inputs = keras.Input(shape=input_shape) x = normalizer(inputs) outputs = layers.Dense(classes, activation=\"softmax\")(x) model = keras.Model(inputs, outputs)  # Train the model model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\") model.fit(x_train, y_train)"},{"path":"https://keras.posit.co/articles/guides/preprocessing_layers.html","id":"encoding-string-categorical-features-via-one-hot-encoding","dir":"Articles > Guides","previous_headings":"Quick recipes","what":"Encoding string categorical features via one-hot encoding","title":"Working with preprocessing layers","text":"Note , , index 0 reserved --vocabulary values (values seen adapt()). can see StringLookup action Structured data classification scratch example.","code":"# Define some toy data data = tf.constant([[\"a\"], [\"b\"], [\"c\"], [\"b\"], [\"c\"], [\"a\"]])  # Use StringLookup to build an index of the feature values and encode output. lookup = layers.StringLookup(output_mode=\"one_hot\") lookup.adapt(data)  # Convert new test data (which includes unknown feature values) test_data = tf.constant([[\"a\"], [\"b\"], [\"c\"], [\"d\"], [\"e\"], [\"\"]]) encoded_data = lookup(test_data) print(encoded_data)"},{"path":"https://keras.posit.co/articles/guides/preprocessing_layers.html","id":"encoding-integer-categorical-features-via-one-hot-encoding","dir":"Articles > Guides","previous_headings":"Quick recipes","what":"Encoding integer categorical features via one-hot encoding","title":"Working with preprocessing layers","text":"Note index 0 reserved missing values (specify value 0), index 1 reserved --vocabulary values (values seen adapt()). can configure using mask_token oov_token constructor arguments IntegerLookup. can see IntegerLookup action example structured data classification scratch.","code":"# Define some toy data data = tf.constant([[10], [20], [20], [10], [30], [0]])  # Use IntegerLookup to build an index of the feature values and encode output. lookup = layers.IntegerLookup(output_mode=\"one_hot\") lookup.adapt(data)  # Convert new test data (which includes unknown feature values) test_data = tf.constant([[10], [10], [20], [50], [60], [0]]) encoded_data = lookup(test_data) print(encoded_data)"},{"path":"https://keras.posit.co/articles/guides/preprocessing_layers.html","id":"applying-the-hashing-trick-to-an-integer-categorical-feature","dir":"Articles > Guides","previous_headings":"Quick recipes","what":"Applying the hashing trick to an integer categorical feature","title":"Working with preprocessing layers","text":"categorical feature can take many different values (order 10e3 higher), value appears times data, becomes impractical ineffective index one-hot encode feature values. Instead, can good idea apply “hashing trick”: hash values vector fixed size. keeps size feature space manageable, removes need explicit indexing.","code":"# Sample data: 10,000 random integers with values between 0 and 100,000 data = np.random.randint(0, 100000, size=(10000, 1))  # Use the Hashing layer to hash the values to the range [0, 64] hasher = layers.Hashing(num_bins=64, salt=1337)  # Use the CategoryEncoding layer to multi-hot encode the hashed values encoder = layers.CategoryEncoding(num_tokens=64, output_mode=\"multi_hot\") encoded_data = encoder(hasher(data)) print(encoded_data.shape)"},{"path":"https://keras.posit.co/articles/guides/preprocessing_layers.html","id":"encoding-text-as-a-sequence-of-token-indices","dir":"Articles > Guides","previous_headings":"Quick recipes","what":"Encoding text as a sequence of token indices","title":"Working with preprocessing layers","text":"preprocess text passed Embedding layer. can see TextVectorization layer action, combined Embedding mode, example text classification scratch. Note training model, best performance, always use TextVectorization layer part input pipeline.","code":"# Define some text data to adapt the layer adapt_data = tf.constant(     [         \"The Brain is wider than the Sky\",         \"For put them side by side\",         \"The one the other will contain\",         \"With ease and You beside\",     ] )  # Create a TextVectorization layer text_vectorizer = layers.TextVectorization(output_mode=\"int\") # Index the vocabulary via `adapt()` text_vectorizer.adapt(adapt_data)  # Try out the layer print(     \"Encoded text:\\n\",     text_vectorizer([\"The Brain is deeper than the sea\"]).numpy(), )  # Create a simple model inputs = keras.Input(shape=(None,), dtype=\"int64\") x = layers.Embedding(input_dim=text_vectorizer.vocabulary_size(), output_dim=16)(inputs) x = layers.GRU(8)(x) outputs = layers.Dense(1)(x) model = keras.Model(inputs, outputs)  # Create a labeled dataset (which includes unknown tokens) train_dataset = tf.data.Dataset.from_tensor_slices(     ([\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"], [1, 0]) )  # Preprocess the string inputs, turning them into int sequences train_dataset = train_dataset.batch(2).map(lambda x, y: (text_vectorizer(x), y)) # Train the model on the int sequences print(\"\\nTraining model...\") model.compile(optimizer=\"rmsprop\", loss=\"mse\") model.fit(train_dataset)  # For inference, you can export a model that accepts strings as input inputs = keras.Input(shape=(1,), dtype=\"string\") x = text_vectorizer(inputs) outputs = model(x) end_to_end_model = keras.Model(inputs, outputs)  # Call the end-to-end model on test data (which includes unknown tokens) print(\"\\nCalling end-to-end model on test string...\") test_data = tf.constant([\"The one the other will absorb\"]) test_output = end_to_end_model(test_data) print(\"Model output:\", test_output)"},{"path":"https://keras.posit.co/articles/guides/preprocessing_layers.html","id":"encoding-text-as-a-dense-matrix-of-n-grams-with-multi-hot-encoding","dir":"Articles > Guides","previous_headings":"Quick recipes","what":"Encoding text as a dense matrix of N-grams with multi-hot encoding","title":"Working with preprocessing layers","text":"preprocess text passed Dense layer.","code":"# Define some text data to adapt the layer adapt_data = tf.constant(     [         \"The Brain is wider than the Sky\",         \"For put them side by side\",         \"The one the other will contain\",         \"With ease and You beside\",     ] ) # Instantiate TextVectorization with \"multi_hot\" output_mode # and ngrams=2 (index all bigrams) text_vectorizer = layers.TextVectorization(output_mode=\"multi_hot\", ngrams=2) # Index the bigrams via `adapt()` text_vectorizer.adapt(adapt_data)  # Try out the layer print(     \"Encoded text:\\n\",     text_vectorizer([\"The Brain is deeper than the sea\"]).numpy(), )  # Create a simple model inputs = keras.Input(shape=(text_vectorizer.vocabulary_size(),)) outputs = layers.Dense(1)(inputs) model = keras.Model(inputs, outputs)  # Create a labeled dataset (which includes unknown tokens) train_dataset = tf.data.Dataset.from_tensor_slices(     ([\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"], [1, 0]) )  # Preprocess the string inputs, turning them into int sequences train_dataset = train_dataset.batch(2).map(lambda x, y: (text_vectorizer(x), y)) # Train the model on the int sequences print(\"\\nTraining model...\") model.compile(optimizer=\"rmsprop\", loss=\"mse\") model.fit(train_dataset)  # For inference, you can export a model that accepts strings as input inputs = keras.Input(shape=(1,), dtype=\"string\") x = text_vectorizer(inputs) outputs = model(x) end_to_end_model = keras.Model(inputs, outputs)  # Call the end-to-end model on test data (which includes unknown tokens) print(\"\\nCalling end-to-end model on test string...\") test_data = tf.constant([\"The one the other will absorb\"]) test_output = end_to_end_model(test_data) print(\"Model output:\", test_output)"},{"path":"https://keras.posit.co/articles/guides/preprocessing_layers.html","id":"encoding-text-as-a-dense-matrix-of-n-grams-with-tf-idf-weighting","dir":"Articles > Guides","previous_headings":"Quick recipes","what":"Encoding text as a dense matrix of N-grams with TF-IDF weighting","title":"Working with preprocessing layers","text":"alternative way preprocessing text passing Dense layer.","code":"# Define some text data to adapt the layer adapt_data = tf.constant(     [         \"The Brain is wider than the Sky\",         \"For put them side by side\",         \"The one the other will contain\",         \"With ease and You beside\",     ] ) # Instantiate TextVectorization with \"tf-idf\" output_mode # (multi-hot with TF-IDF weighting) and ngrams=2 (index all bigrams) text_vectorizer = layers.TextVectorization(output_mode=\"tf-idf\", ngrams=2) # Index the bigrams and learn the TF-IDF weights via `adapt()` text_vectorizer.adapt(adapt_data)  # Try out the layer print(     \"Encoded text:\\n\",     text_vectorizer([\"The Brain is deeper than the sea\"]).numpy(), )  # Create a simple model inputs = keras.Input(shape=(text_vectorizer.vocabulary_size(),)) outputs = layers.Dense(1)(inputs) model = keras.Model(inputs, outputs)  # Create a labeled dataset (which includes unknown tokens) train_dataset = tf.data.Dataset.from_tensor_slices(     ([\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"], [1, 0]) )  # Preprocess the string inputs, turning them into int sequences train_dataset = train_dataset.batch(2).map(lambda x, y: (text_vectorizer(x), y)) # Train the model on the int sequences print(\"\\nTraining model...\") model.compile(optimizer=\"rmsprop\", loss=\"mse\") model.fit(train_dataset)  # For inference, you can export a model that accepts strings as input inputs = keras.Input(shape=(1,), dtype=\"string\") x = text_vectorizer(inputs) outputs = model(x) end_to_end_model = keras.Model(inputs, outputs)  # Call the end-to-end model on test data (which includes unknown tokens) print(\"\\nCalling end-to-end model on test string...\") test_data = tf.constant([\"The one the other will absorb\"]) test_output = end_to_end_model(test_data) print(\"Model output:\", test_output)"},{"path":[]},{"path":"https://keras.posit.co/articles/guides/preprocessing_layers.html","id":"working-with-lookup-layers-with-very-large-vocabularies","dir":"Articles > Guides","previous_headings":"Important gotchas","what":"Working with lookup layers with very large vocabularies","title":"Working with preprocessing layers","text":"may find working large vocabulary TextVectorization, StringLookup layer, IntegerLookup layer. Typically, vocabulary larger 500MB considered “large”. case, best performance, avoid using adapt(). Instead, pre-compute vocabulary advance (use Apache Beam TF Transform ) store file. load vocabulary layer construction time passing file path vocabulary argument.","code":""},{"path":"https://keras.posit.co/articles/guides/preprocessing_layers.html","id":"using-lookup-layers-on-a-tpu-pod-or-with-parameterserverstrategy-","dir":"Articles > Guides","previous_headings":"Important gotchas","what":"Using lookup layers on a TPU pod or with ParameterServerStrategy.","title":"Working with preprocessing layers","text":"outstanding issue causes performance degrade using TextVectorization, StringLookup, IntegerLookup layer training TPU pod multiple machines via ParameterServerStrategy. slated fixed TensorFlow 2.7.","code":""},{"path":"https://keras.posit.co/articles/guides/sequential_model.html","id":"setup","dir":"Articles > Guides","previous_headings":"","what":"Setup","title":"The Sequential model","text":"","code":"import keras as keras from keras import layers from keras import ops"},{"path":"https://keras.posit.co/articles/guides/sequential_model.html","id":"when-to-use-a-sequential-model","dir":"Articles > Guides","previous_headings":"","what":"When to use a Sequential model","title":"The Sequential model","text":"Sequential model appropriate plain stack layers layer exactly one input tensor one output tensor. Schematically, following Sequential model: equivalent function: Sequential model appropriate : model multiple inputs multiple outputs layers multiple inputs multiple outputs need layer sharing want non-linear topology (e.g. residual connection, multi-branch model)","code":"# Define Sequential model with 3 layers model = keras.Sequential(     [         layers.Dense(2, activation=\"relu\", name=\"layer1\"),         layers.Dense(3, activation=\"relu\", name=\"layer2\"),         layers.Dense(4, name=\"layer3\"),     ] ) # Call model on a test input x = ops.ones((3, 3)) y = model(x) # Create 3 layers layer1 = layers.Dense(2, activation=\"relu\", name=\"layer1\") layer2 = layers.Dense(3, activation=\"relu\", name=\"layer2\") layer3 = layers.Dense(4, name=\"layer3\")  # Call layers on a test input x = ops.ones((3, 3)) y = layer3(layer2(layer1(x)))"},{"path":"https://keras.posit.co/articles/guides/sequential_model.html","id":"creating-a-sequential-model","dir":"Articles > Guides","previous_headings":"","what":"Creating a Sequential model","title":"The Sequential model","text":"can create Sequential model passing list layers Sequential constructor: layers accessible via layers attribute: can also create Sequential model incrementally via add() method: Note ’s also corresponding pop() method remove layers: Sequential model behaves much like list layers. Also note Sequential constructor accepts name argument, just like layer model Keras. useful annotate TensorBoard graphs semantically meaningful names.","code":"model = keras.Sequential(     [         layers.Dense(2, activation=\"relu\"),         layers.Dense(3, activation=\"relu\"),         layers.Dense(4),     ] ) model.layers model = keras.Sequential() model.add(layers.Dense(2, activation=\"relu\")) model.add(layers.Dense(3, activation=\"relu\")) model.add(layers.Dense(4)) model.pop() print(len(model.layers))  # 2 model = keras.Sequential(name=\"my_sequential\") model.add(layers.Dense(2, activation=\"relu\", name=\"layer1\")) model.add(layers.Dense(3, activation=\"relu\", name=\"layer2\")) model.add(layers.Dense(4, name=\"layer3\"))"},{"path":"https://keras.posit.co/articles/guides/sequential_model.html","id":"specifying-the-input-shape-in-advance","dir":"Articles > Guides","previous_headings":"","what":"Specifying the input shape in advance","title":"The Sequential model","text":"Generally, layers Keras need know shape inputs order able create weights. create layer like , initially, weights: creates weights first time called input, since shape weights depends shape inputs: Naturally, also applies Sequential models. instantiate Sequential model without input shape, isn’t “built”: weights (calling model.weights results error stating just ). weights created model first sees input data: model “built”, can call summary() method display contents: However, can useful building Sequential model incrementally able display summary model far, including current output shape. case, start model passing Input object model, knows input shape start: Note Input object displayed part model.layers, since isn’t layer: Models built predefined input shape like always weights (even seeing data) always defined output shape. general, ’s recommended best practice always specify input shape Sequential model advance know .","code":"layer = layers.Dense(3) layer.weights  # Empty # Call layer on a test input x = ops.ones((1, 4)) y = layer(x) layer.weights  # Now it has weights, of shape (4, 3) and (3,) model = keras.Sequential(     [         layers.Dense(2, activation=\"relu\"),         layers.Dense(3, activation=\"relu\"),         layers.Dense(4),     ] )  # No weights at this stage!  # At this point, you can't do this: # model.weights  # You also can't do this: # model.summary()  # Call the model on a test input x = ops.ones((1, 4)) y = model(x) print(\"Number of weights after calling the model:\", len(model.weights))  # 6 model.summary() model = keras.Sequential() model.add(keras.Input(shape=(4,))) model.add(layers.Dense(2, activation=\"relu\"))  model.summary() model.layers"},{"path":"https://keras.posit.co/articles/guides/sequential_model.html","id":"a-common-debugging-workflow-add-summary","dir":"Articles > Guides","previous_headings":"","what":"A common debugging workflow: add() + summary()","title":"The Sequential model","text":"building new Sequential architecture, ’s useful incrementally stack layers add() frequently print model summaries. instance, enables monitor stack Conv2D MaxPooling2D layers downsampling image feature maps: practical, right?","code":"model = keras.Sequential() model.add(keras.Input(shape=(250, 250, 3)))  # 250x250 RGB images model.add(layers.Conv2D(32, 5, strides=2, activation=\"relu\")) model.add(layers.Conv2D(32, 3, activation=\"relu\")) model.add(layers.MaxPooling2D(3))  # Can you guess what the current output shape is at this point? Probably not. # Let's just print it: model.summary()  # The answer was: (40, 40, 32), so we can keep downsampling...  model.add(layers.Conv2D(32, 3, activation=\"relu\")) model.add(layers.Conv2D(32, 3, activation=\"relu\")) model.add(layers.MaxPooling2D(3)) model.add(layers.Conv2D(32, 3, activation=\"relu\")) model.add(layers.Conv2D(32, 3, activation=\"relu\")) model.add(layers.MaxPooling2D(2))  # And now? model.summary()  # Now that we have 4x4 feature maps, time to apply global max pooling. model.add(layers.GlobalMaxPooling2D())  # Finally, we add a classification layer. model.add(layers.Dense(10))"},{"path":"https://keras.posit.co/articles/guides/sequential_model.html","id":"what-to-do-once-you-have-a-model","dir":"Articles > Guides","previous_headings":"","what":"What to do once you have a model","title":"The Sequential model","text":"model architecture ready, want : Train model, evaluate , run inference. See guide training & evaluation built-loops Save model disk restore . See guide serialization & saving. Speed model training leveraging multiple GPUs. See guide multi-GPU distributed training.","code":""},{"path":"https://keras.posit.co/articles/guides/sequential_model.html","id":"feature-extraction-with-a-sequential-model","dir":"Articles > Guides","previous_headings":"","what":"Feature extraction with a Sequential model","title":"The Sequential model","text":"Sequential model built, behaves like Functional API model. means every layer input output attribute. attributes can used neat things, like quickly creating model extracts outputs intermediate layers Sequential model: ’s similar example extract features one layer:","code":"initial_model = keras.Sequential(     [         keras.Input(shape=(250, 250, 3)),         layers.Conv2D(32, 5, strides=2, activation=\"relu\"),         layers.Conv2D(32, 3, activation=\"relu\"),         layers.Conv2D(32, 3, activation=\"relu\"),     ] ) feature_extractor = keras.Model(     inputs=initial_model.inputs,     outputs=[layer.output for layer in initial_model.layers], )  # Call feature extractor on test input. x = ops.ones((1, 250, 250, 3)) features = feature_extractor(x) initial_model = keras.Sequential(     [         keras.Input(shape=(250, 250, 3)),         layers.Conv2D(32, 5, strides=2, activation=\"relu\"),         layers.Conv2D(32, 3, activation=\"relu\", name=\"my_intermediate_layer\"),         layers.Conv2D(32, 3, activation=\"relu\"),     ] ) feature_extractor = keras.Model(     inputs=initial_model.inputs,     outputs=initial_model.get_layer(name=\"my_intermediate_layer\").output, ) # Call feature extractor on test input. x = ops.ones((1, 250, 250, 3)) features = feature_extractor(x)"},{"path":"https://keras.posit.co/articles/guides/sequential_model.html","id":"transfer-learning-with-a-sequential-model","dir":"Articles > Guides","previous_headings":"","what":"Transfer learning with a Sequential model","title":"The Sequential model","text":"Transfer learning consists freezing bottom layers model training top layers. aren’t familiar , make sure read guide transfer learning. two common transfer learning blueprint involving Sequential models. First, let’s say Sequential model, want freeze layers except last one. case, simply iterate model.layers set layer.trainable = False layer, except last one. Like : Another common blueprint use Sequential model stack pre-trained model freshly initialized classification layers. Like : transfer learning, probably find frequently using two patterns. ’s need know Sequential models! find building models Keras, see: Guide Functional API Guide making new Layers & Models via subclassing","code":"model = keras.Sequential([     keras.Input(shape=(784)),     layers.Dense(32, activation='relu'),     layers.Dense(32, activation='relu'),     layers.Dense(32, activation='relu'),     layers.Dense(10), ])  # Presumably you would want to first load pre-trained weights. model.load_weights(...)  # Freeze all layers except the last one. for layer in model.layers[:-1]:   layer.trainable = False  # Recompile and train (this will only update the weights of the last layer). model.compile(...) model.fit(...) # Load a convolutional base with pre-trained weights base_model = keras.applications.Xception(     weights='imagenet',     include_top=False,     pooling='avg')  # Freeze the base model base_model.trainable = False  # Use a Sequential model to add a trainable classifier on top model = keras.Sequential([     base_model,     layers.Dense(1000), ])  # Compile & train model.compile(...) model.fit(...)"},{"path":"https://keras.posit.co/articles/guides/serialization_and_saving.html","id":"introduction","dir":"Articles > Guides","previous_headings":"","what":"Introduction","title":"Save, serialize, and export models","text":"Keras model consists multiple components: architecture, configuration, specifies layers model contain, ’re connected. set weights values (“state model”). optimizer (defined compiling model). set losses metrics (defined compiling model). Keras API saves pieces together unified format, marked .keras extension. zip archive consisting following: JSON-based configuration file (config.json): Records model, layer, trackables’ configuration. H5-based state file, model.weights.h5 (whole model), directory keys layers weights. metadata file JSON, storing things current Keras version. Let’s take look works.","code":""},{"path":"https://keras.posit.co/articles/guides/serialization_and_saving.html","id":"how-to-save-and-load-a-model","dir":"Articles > Guides","previous_headings":"","what":"How to save and load a model","title":"Save, serialize, and export models","text":"10 seconds read guide, ’s need know. Saving Keras model: Loading model back: Now, let’s look details.","code":"model = ...  # Get model (Sequential, Functional Model, or Model subclass) model.save('path/to/location.keras')  # The file needs to end with the .keras extension model = keras.models.load_model('path/to/location.keras')"},{"path":"https://keras.posit.co/articles/guides/serialization_and_saving.html","id":"setup","dir":"Articles > Guides","previous_headings":"","what":"Setup","title":"Save, serialize, and export models","text":"","code":"import numpy as np import keras_core as keras from keras_core import ops"},{"path":"https://keras.posit.co/articles/guides/serialization_and_saving.html","id":"saving","dir":"Articles > Guides","previous_headings":"","what":"Saving","title":"Save, serialize, and export models","text":"section saving entire model single file. file include: model’s architecture/config model’s weight values (learned training) model’s compilation information (compile() called) optimizer state, (enables restart training left)","code":""},{"path":"https://keras.posit.co/articles/guides/serialization_and_saving.html","id":"apis","dir":"Articles > Guides","previous_headings":"Saving","what":"APIs","title":"Save, serialize, and export models","text":"can save model model.save() keras.models.save_model() (equivalent). can load back keras.models.load_model(). supported format Keras Core “Keras v3” format, uses .keras extension. Example:","code":"def get_model():     # Create a simple model.     inputs = keras.Input(shape=(32,))     outputs = keras.layers.Dense(1)(inputs)     model = keras.Model(inputs, outputs)     model.compile(optimizer=keras.optimizers.Adam(), loss=\"mean_squared_error\")     return model   model = get_model()  # Train the model. test_input = np.random.random((128, 32)) test_target = np.random.random((128, 1)) model.fit(test_input, test_target)  # Calling `save('my_model.keras')` creates a zip archive `my_model.keras`. model.save(\"my_model.keras\")  # It can be used to reconstruct the model identically. reconstructed_model = keras.models.load_model(\"my_model.keras\")  # Let's check: np.testing.assert_allclose(     model.predict(test_input), reconstructed_model.predict(test_input) )"},{"path":"https://keras.posit.co/articles/guides/serialization_and_saving.html","id":"custom-objects","dir":"Articles > Guides","previous_headings":"Saving","what":"Custom objects","title":"Save, serialize, and export models","text":"section covers basic workflows handling custom layers, functions, models Keras saving reloading. saving model includes custom objects, subclassed Layer, must define get_config() method object class. arguments passed constructor (__init__() method) custom object aren’t Python objects (anything base types like ints, strings, etc.), must also explicitly deserialize arguments from_config() class method. Like : Please see Defining config methods section details examples. saved .keras file lightweight store Python code custom objects. Therefore, reload model, load_model requires access definition custom objects used one following methods: Registering custom objects (preferred), Passing custom objects directly loading, Using custom object scope examples workflow:","code":"class CustomLayer(keras.layers.Layer):     def __init__(self, sublayer, **kwargs):         super().__init__(**kwargs)         self.sublayer = layer      def call(self, x):         return self.sublayer(x)      def get_config(self):         base_config = super().get_config()         config = {             \"sublayer\": keras.saving.serialize_keras_object(self.sublayer),         }         return {**base_config, **config}      @classmethod     def from_config(cls, config):         sublayer_config = config.pop(\"sublayer\")         sublayer = keras.saving.deserialize_keras_object(sublayer_config)         return cls(sublayer, **config)"},{"path":"https://keras.posit.co/articles/guides/serialization_and_saving.html","id":"registering-custom-objects-preferred","dir":"Articles > Guides","previous_headings":"Saving > Custom objects","what":"Registering custom objects (preferred)","title":"Save, serialize, and export models","text":"preferred method, custom object registration greatly simplifies saving loading code. Adding @keras.saving.register_keras_serializable decorator class definition custom object registers object globally master list, allowing Keras recognize object loading model. Let’s create custom model involving custom layer custom activation function demonstrate . Example:","code":"# Clear all previously registered custom objects keras.saving.get_custom_objects().clear()   # Upon registration, you can optionally specify a package or a name. # If left blank, the package defaults to `Custom` and the name defaults to # the class name. @keras.saving.register_keras_serializable(package=\"MyLayers\") class CustomLayer(keras.layers.Layer):     def __init__(self, factor):         super().__init__()         self.factor = factor      def call(self, x):         return x * self.factor      def get_config(self):         return {\"factor\": self.factor}   @keras.saving.register_keras_serializable(package=\"my_package\", name=\"custom_fn\") def custom_fn(x):     return x**2   # Create the model. def get_model():     inputs = keras.Input(shape=(4,))     mid = CustomLayer(0.5)(inputs)     outputs = keras.layers.Dense(1, activation=custom_fn)(mid)     model = keras.Model(inputs, outputs)     model.compile(optimizer=\"rmsprop\", loss=\"mean_squared_error\")     return model   # Train the model. def train_model(model):     input = np.random.random((4, 4))     target = np.random.random((4, 1))     model.fit(input, target)     return model   test_input = np.random.random((4, 4)) test_target = np.random.random((4, 1))  model = get_model() model = train_model(model) model.save(\"custom_model.keras\")  # Now, we can simply load without worrying about our custom objects. reconstructed_model = keras.models.load_model(\"custom_model.keras\")  # Let's check: np.testing.assert_allclose(     model.predict(test_input), reconstructed_model.predict(test_input) )"},{"path":"https://keras.posit.co/articles/guides/serialization_and_saving.html","id":"passing-custom-objects-to-load_model","dir":"Articles > Guides","previous_headings":"Saving > Custom objects","what":"Passing custom objects to load_model()","title":"Save, serialize, and export models","text":"","code":"model = get_model() model = train_model(model)  # Calling `save('my_model.keras')` creates a zip archive `my_model.keras`. model.save(\"custom_model.keras\")  # Upon loading, pass a dict containing the custom objects used in the # `custom_objects` argument of `keras.models.load_model()`. reconstructed_model = keras.models.load_model(     \"custom_model.keras\",     custom_objects={\"CustomLayer\": CustomLayer, \"custom_fn\": custom_fn}, )  # Let's check: np.testing.assert_allclose(     model.predict(test_input), reconstructed_model.predict(test_input) )"},{"path":"https://keras.posit.co/articles/guides/serialization_and_saving.html","id":"using-a-custom-object-scope","dir":"Articles > Guides","previous_headings":"Saving > Custom objects","what":"Using a custom object scope","title":"Save, serialize, and export models","text":"code within custom object scope able recognize custom objects passed scope argument. Therefore, loading model within scope allow loading custom objects. Example:","code":"model = get_model() model = train_model(model) model.save(\"custom_model.keras\")  # Pass the custom objects dictionary to a custom object scope and place # the `keras.models.load_model()` call within the scope. custom_objects = {\"CustomLayer\": CustomLayer, \"custom_fn\": custom_fn}  with keras.saving.custom_object_scope(custom_objects):     reconstructed_model = keras.models.load_model(\"custom_model.keras\")  # Let's check: np.testing.assert_allclose(     model.predict(test_input), reconstructed_model.predict(test_input) )"},{"path":"https://keras.posit.co/articles/guides/serialization_and_saving.html","id":"model-serialization","dir":"Articles > Guides","previous_headings":"Saving","what":"Model serialization","title":"Save, serialize, and export models","text":"section saving model’s configuration, without state. model’s configuration (architecture) specifies layers model contains, layers connected. configuration model, model can created freshly initialized state (weights compilation information).","code":""},{"path":"https://keras.posit.co/articles/guides/serialization_and_saving.html","id":"apis-1","dir":"Articles > Guides","previous_headings":"Saving > Model serialization","what":"APIs","title":"Save, serialize, and export models","text":"following serialization APIs available: keras.models.clone_model(model): make (randomly initialized) copy model. get_config() cls.from_config(): retrieve configuration layer model, recreate model instance config, respectively. keras.models.model_to_json() keras.models.model_from_json(): similar, JSON strings. keras.saving.serialize_keras_object(): retrieve configuration arbitrary Keras object. keras.saving.deserialize_keras_object(): recreate object instance configuration.","code":""},{"path":"https://keras.posit.co/articles/guides/serialization_and_saving.html","id":"in-memory-model-cloning","dir":"Articles > Guides","previous_headings":"Saving > Model serialization","what":"In-memory model cloning","title":"Save, serialize, and export models","text":"can -memory cloning model via keras.models.clone_model(). equivalent getting config recreating model config (preserve compilation information layer weights values). Example:","code":"new_model = keras.models.clone_model(model)"},{"path":"https://keras.posit.co/articles/guides/serialization_and_saving.html","id":"get_config-and-from_config","dir":"Articles > Guides","previous_headings":"Saving > Model serialization","what":"get_config() and from_config()","title":"Save, serialize, and export models","text":"Calling model.get_config() layer.get_config() return Python dict containing configuration model layer, respectively. define get_config() contain arguments needed __init__() method model layer. loading time, from_config(config) method call __init__() arguments reconstruct model layer. Layer example: Now let’s reconstruct layer using from_config() method: Sequential model example: Functional model example:","code":"layer = keras.layers.Dense(3, activation=\"relu\") layer_config = layer.get_config() print(layer_config) new_layer = keras.layers.Dense.from_config(layer_config) model = keras.Sequential([keras.Input((32,)), keras.layers.Dense(1)]) config = model.get_config() new_model = keras.Sequential.from_config(config) inputs = keras.Input((32,)) outputs = keras.layers.Dense(1)(inputs) model = keras.Model(inputs, outputs) config = model.get_config() new_model = keras.Model.from_config(config)"},{"path":"https://keras.posit.co/articles/guides/serialization_and_saving.html","id":"to_json-and-keras-models-model_from_json","dir":"Articles > Guides","previous_headings":"Saving > Model serialization","what":"to_json() and keras.models.model_from_json()","title":"Save, serialize, and export models","text":"similar get_config / from_config, except turns model JSON string, can loaded without original model class. also specific models, isn’t meant layers. Example:","code":"model = keras.Sequential([keras.Input((32,)), keras.layers.Dense(1)]) json_config = model.to_json() new_model = keras.models.model_from_json(json_config)"},{"path":"https://keras.posit.co/articles/guides/serialization_and_saving.html","id":"arbitrary-object-serialization-and-deserialization","dir":"Articles > Guides","previous_headings":"Saving > Model serialization","what":"Arbitrary object serialization and deserialization","title":"Save, serialize, and export models","text":"keras.saving.serialize_keras_object() keras.saving.deserialize_keras_object() APIs general-purpose APIs can used serialize deserialize Keras object custom object. foundation saving model architecture behind serialize()/deserialize() calls keras. Example: Note serialization format containing necessary information proper reconstruction: module containing name Keras module identifying module object comes class_name containing name object’s class. config information needed reconstruct object registered_name custom objects. See . Now can reconstruct regularizer.","code":"my_reg = keras.regularizers.L1(0.005) config = keras.saving.serialize_keras_object(my_reg) print(config) new_reg = keras.saving.deserialize_keras_object(config)"},{"path":"https://keras.posit.co/articles/guides/serialization_and_saving.html","id":"model-weights-saving","dir":"Articles > Guides","previous_headings":"Saving","what":"Model weights saving","title":"Save, serialize, and export models","text":"can choose save & load model’s weights. can useful : need model inference: case won’t need restart training, don’t need compilation information optimizer state. transfer learning: case training new model reusing state prior model, don’t need compilation information prior model.","code":""},{"path":"https://keras.posit.co/articles/guides/serialization_and_saving.html","id":"apis-for-in-memory-weight-transfer","dir":"Articles > Guides","previous_headings":"Saving > Model weights saving","what":"APIs for in-memory weight transfer","title":"Save, serialize, and export models","text":"Weights can copied different objects using get_weights() set_weights(): keras.layers.Layer.get_weights(): Returns list NumPy arrays weight values. keras.layers.Layer.set_weights(weights): Sets model weights values provided (NumPy arrays). Examples: Transfering weights one layer another, memory Transfering weights one model another model compatible architecture, memory case stateless layers stateless layers change order number weights, models can compatible architectures even extra/missing stateless layers.","code":"def create_layer():     layer = keras.layers.Dense(64, activation=\"relu\", name=\"dense_2\")     layer.build((None, 784))     return layer   layer_1 = create_layer() layer_2 = create_layer()  # Copy weights from layer 1 to layer 2 layer_2.set_weights(layer_1.get_weights()) # Create a simple functional model inputs = keras.Input(shape=(784,), name=\"digits\") x = keras.layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs) x = keras.layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x) outputs = keras.layers.Dense(10, name=\"predictions\")(x) functional_model = keras.Model(inputs=inputs, outputs=outputs, name=\"3_layer_mlp\")   # Define a subclassed model with the same architecture class SubclassedModel(keras.Model):     def __init__(self, output_dim, name=None):         super().__init__(name=name)         self.output_dim = output_dim         self.dense_1 = keras.layers.Dense(64, activation=\"relu\", name=\"dense_1\")         self.dense_2 = keras.layers.Dense(64, activation=\"relu\", name=\"dense_2\")         self.dense_3 = keras.layers.Dense(output_dim, name=\"predictions\")      def call(self, inputs):         x = self.dense_1(inputs)         x = self.dense_2(x)         x = self.dense_3(x)         return x      def get_config(self):         return {\"output_dim\": self.output_dim, \"name\": self.name}   subclassed_model = SubclassedModel(10) # Call the subclassed model once to create the weights. subclassed_model(np.ones((1, 784)))  # Copy weights from functional_model to subclassed_model. subclassed_model.set_weights(functional_model.get_weights())  assert len(functional_model.weights) == len(subclassed_model.weights) for a, b in zip(functional_model.weights, subclassed_model.weights):     np.testing.assert_allclose(a.numpy(), b.numpy()) inputs = keras.Input(shape=(784,), name=\"digits\") x = keras.layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs) x = keras.layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x) outputs = keras.layers.Dense(10, name=\"predictions\")(x) functional_model = keras.Model(inputs=inputs, outputs=outputs, name=\"3_layer_mlp\")  inputs = keras.Input(shape=(784,), name=\"digits\") x = keras.layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs) x = keras.layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)  # Add a dropout layer, which does not contain any weights. x = keras.layers.Dropout(0.5)(x) outputs = keras.layers.Dense(10, name=\"predictions\")(x) functional_model_with_dropout = keras.Model(     inputs=inputs, outputs=outputs, name=\"3_layer_mlp\" )  functional_model_with_dropout.set_weights(functional_model.get_weights())"},{"path":"https://keras.posit.co/articles/guides/serialization_and_saving.html","id":"apis-for-saving-weights-to-disk-loading-them-back","dir":"Articles > Guides","previous_headings":"Saving > Model weights saving","what":"APIs for saving weights to disk & loading them back","title":"Save, serialize, and export models","text":"Weights can saved disk calling model.save_weights(filepath). filename end .weights.h5. Example: Note changing layer.trainable may result different layer.weights ordering model contains nested layers.","code":"# Runnable example sequential_model = keras.Sequential(     [         keras.Input(shape=(784,), name=\"digits\"),         keras.layers.Dense(64, activation=\"relu\", name=\"dense_1\"),         keras.layers.Dense(64, activation=\"relu\", name=\"dense_2\"),         keras.layers.Dense(10, name=\"predictions\"),     ] ) sequential_model.save_weights(\"my_model.weights.h5\") sequential_model.load_weights(\"my_model.weights.h5\") class NestedDenseLayer(keras.layers.Layer):     def __init__(self, units, name=None):         super().__init__(name=name)         self.dense_1 = keras.layers.Dense(units, name=\"dense_1\")         self.dense_2 = keras.layers.Dense(units, name=\"dense_2\")      def call(self, inputs):         return self.dense_2(self.dense_1(inputs))   nested_model = keras.Sequential([keras.Input((784,)), NestedDenseLayer(10, \"nested\")]) variable_names = [v.name for v in nested_model.weights] print(\"variables: {}\".format(variable_names))  print(\"\\nChanging trainable status of one of the nested layers...\") nested_model.get_layer(\"nested\").dense_1.trainable = False  variable_names_2 = [v.name for v in nested_model.weights] print(\"\\nvariables: {}\".format(variable_names_2)) print(\"variable ordering changed:\", variable_names != variable_names_2)"},{"path":"https://keras.posit.co/articles/guides/serialization_and_saving.html","id":"transfer-learning-example","dir":"Articles > Guides","previous_headings":"Saving > Model weights saving > APIs for saving weights to disk & loading them back","what":"Transfer learning example","title":"Save, serialize, and export models","text":"loading pretrained weights weights file, recommended load weights original checkpointed model, extract desired weights/layers new model. Example:","code":"def create_functional_model():     inputs = keras.Input(shape=(784,), name=\"digits\")     x = keras.layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)     x = keras.layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)     outputs = keras.layers.Dense(10, name=\"predictions\")(x)     return keras.Model(inputs=inputs, outputs=outputs, name=\"3_layer_mlp\")   functional_model = create_functional_model() functional_model.save_weights(\"pretrained.weights.h5\")  # In a separate program: pretrained_model = create_functional_model() pretrained_model.load_weights(\"pretrained.weights.h5\")  # Create a new model by extracting layers from the original model: extracted_layers = pretrained_model.layers[:-1] extracted_layers.append(keras.layers.Dense(5, name=\"dense_3\")) model = keras.Sequential(extracted_layers) model.summary()"},{"path":"https://keras.posit.co/articles/guides/serialization_and_saving.html","id":"appendix-handling-custom-objects","dir":"Articles > Guides","previous_headings":"Saving","what":"Appendix: Handling custom objects","title":"Save, serialize, and export models","text":"#### Defining config methods Specifications: get_config() return JSON-serializable dictionary order compatible Keras architecture- model-saving APIs. from_config(config) (classmethod) return new layer model object created config. default implementation returns cls(**config). NOTE: constructor arguments already serializable, e.g. strings ints, non-custom Keras objects, overriding from_config necessary. However, complex objects layers models passed __init__, deserialization must handled explicitly either __init__ overriding from_config() method. Example: Note overriding from_config unnecessary MyDense hidden_units, kernel_initializer, kernel_regularizer ints, strings, built-Keras object, respectively. means default from_config implementation cls(**config) work intended. complex objects, layers models passed __init__, example, must explicitly deserialize objects. Let’s take look example model from_config override necessary. Example: #### custom objects serialized serialization format special key custom objects registered via @keras.saving.register_keras_serializable. registered_name key allows easy retrieval loading/deserialization time also allowing users add custom naming. Let’s take look config serializing custom layer MyDense defined . Example: shown, registered_name key contains lookup information Keras master list, including package MyLayers custom name KernelMult gave @keras.saving.register_keras_serializable decorator. Take look custom class definition/registration . Note class_name key contains original name class, allowing proper re-initialization from_config. Additionally, note module key None since custom object.","code":"@keras.saving.register_keras_serializable(package=\"MyLayers\", name=\"KernelMult\") class MyDense(keras.layers.Layer):     def __init__(         self,         units,         *,         kernel_regularizer=None,         kernel_initializer=None,         nested_model=None,         **kwargs     ):         super().__init__(**kwargs)         self.hidden_units = units         self.kernel_regularizer = kernel_regularizer         self.kernel_initializer = kernel_initializer         self.nested_model = nested_model      def get_config(self):         config = super().get_config()         # Update the config with the custom layer's parameters         config.update(             {                 \"units\": self.hidden_units,                 \"kernel_regularizer\": self.kernel_regularizer,                 \"kernel_initializer\": self.kernel_initializer,                 \"nested_model\": self.nested_model,             }         )         return config      def build(self, input_shape):         input_units = input_shape[-1]         self.kernel = self.add_weight(             name=\"kernel\",             shape=(input_units, self.hidden_units),             regularizer=self.kernel_regularizer,             initializer=self.kernel_initializer,         )      def call(self, inputs):         return ops.matmul(inputs, self.kernel)   layer = MyDense(units=16, kernel_regularizer=\"l1\", kernel_initializer=\"ones\") layer3 = MyDense(units=64, nested_model=layer)  config = keras.layers.serialize(layer3)  print(config)  new_layer = keras.layers.deserialize(config)  print(new_layer) @keras.saving.register_keras_serializable(package=\"ComplexModels\") class CustomModel(keras.layers.Layer):     def __init__(self, first_layer, second_layer=None, **kwargs):         super().__init__(**kwargs)         self.first_layer = first_layer         if second_layer is not None:             self.second_layer = second_layer         else:             self.second_layer = keras.layers.Dense(8)      def get_config(self):         config = super().get_config()         config.update(             {                 \"first_layer\": self.first_layer,                 \"second_layer\": self.second_layer,             }         )         return config      @classmethod     def from_config(cls, config):         # Note that you can also use `keras.saving.deserialize_keras_object` here         config[\"first_layer\"] = keras.layers.deserialize(config[\"first_layer\"])         config[\"second_layer\"] = keras.layers.deserialize(config[\"second_layer\"])         return cls(**config)      def call(self, inputs):         return self.first_layer(self.second_layer(inputs))   # Let's make our first layer the custom layer from the previous example (MyDense) inputs = keras.Input((32,)) outputs = CustomModel(first_layer=layer)(inputs) model = keras.Model(inputs, outputs)  config = model.get_config() new_model = keras.Model.from_config(config) layer = MyDense(     units=16,     kernel_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-4),     kernel_initializer=\"ones\", ) config = keras.layers.serialize(layer) print(config)"},{"path":"https://keras.posit.co/articles/guides/training_with_built_in_methods.html","id":"setup","dir":"Articles > Guides","previous_headings":"","what":"Setup","title":"Training & evaluation with the built-in methods","text":"","code":"# We import torch & TF so as to use torch Dataloaders & tf.data.Datasets. import torch import tensorflow as tf  import os import numpy as np import keras as keras from keras import layers from keras import ops"},{"path":"https://keras.posit.co/articles/guides/training_with_built_in_methods.html","id":"introduction","dir":"Articles > Guides","previous_headings":"","what":"Introduction","title":"Training & evaluation with the built-in methods","text":"guide covers training, evaluation, prediction (inference) models using built-APIs training & validation (Model.fit(), Model.evaluate() Model.predict()). interested leveraging fit() specifying training step function, see Customizing happens fit() guide. interested writing training & evaluation loops scratch, see guide “writing training loop scratch”. general, whether using built-loops writing , model training & evaluation works strictly way across every kind Keras model – Sequential models, models built Functional API, models written scratch via model subclassing. guide doesn’t cover distributed training, covered guide multi-GPU & distributed training.","code":""},{"path":"https://keras.posit.co/articles/guides/training_with_built_in_methods.html","id":"api-overview-a-first-end-to-end-example","dir":"Articles > Guides","previous_headings":"","what":"API overview: a first end-to-end example","title":"Training & evaluation with the built-in methods","text":"passing data built-training loops model, either use: NumPy arrays (data small fits memory) Subclasses keras.utils.PyDataset tf.data.Dataset objects PyTorch DataLoader instances next paragraphs, ’ll use MNIST dataset NumPy arrays, order demonstrate use optimizers, losses, metrics. Afterwards, ’ll take close look options. Let’s consider following model (, build Functional API, Sequential model subclassed model well): ’s typical end--end workflow looks like, consisting : Training Validation holdout set generated original training data Evaluation test data ’ll use MNIST data example. specify training configuration (optimizer, loss, metrics): call fit(), train model slicing data “batches” size batch_size, repeatedly iterating entire dataset given number epochs. returned history object holds record loss values metric values training: evaluate model test data via evaluate(): Now, let’s review piece workflow detail.","code":"inputs = keras.Input(shape=(784,), name=\"digits\") x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs) x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x) outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)  model = keras.Model(inputs=inputs, outputs=outputs) (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()  # Preprocess the data (these are NumPy arrays) x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255 x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255  y_train = y_train.astype(\"float32\") y_test = y_test.astype(\"float32\")  # Reserve 10,000 samples for validation x_val = x_train[-10000:] y_val = y_train[-10000:] x_train = x_train[:-10000] y_train = y_train[:-10000] model.compile(     optimizer=keras.optimizers.RMSprop(),  # Optimizer     # Loss function to minimize     loss=keras.losses.SparseCategoricalCrossentropy(),     # List of metrics to monitor     metrics=[keras.metrics.SparseCategoricalAccuracy()], ) print(\"Fit model on training data\") history = model.fit(     x_train,     y_train,     batch_size=64,     epochs=2,     # We pass some validation for     # monitoring validation loss and metrics     # at the end of each epoch     validation_data=(x_val, y_val), ) history.history # Evaluate the model on the test data using `evaluate` print(\"Evaluate on test data\") results = model.evaluate(x_test, y_test, batch_size=128) print(\"test loss, test acc:\", results)  # Generate predictions (probabilities -- the output of the last layer) # on new data using `predict` print(\"Generate predictions for 3 samples\") predictions = model.predict(x_test[:3]) print(\"predictions shape:\", predictions.shape)"},{"path":"https://keras.posit.co/articles/guides/training_with_built_in_methods.html","id":"the-compile-method-specifying-a-loss-metrics-and-an-optimizer","dir":"Articles > Guides","previous_headings":"","what":"The compile() method: specifying a loss, metrics, and an optimizer","title":"Training & evaluation with the built-in methods","text":"train model fit(), need specify loss function, optimizer, optionally, metrics monitor. pass model arguments compile() method: metrics argument list – model can number metrics. model multiple outputs, can specify different losses metrics output, can modulate contribution output total loss model. find details Passing data multi-input, multi-output models section. Note ’re satisfied default settings, many cases optimizer, loss, metrics can specified via string identifiers shortcut: later reuse, let’s put model definition compile step functions; call several times across different examples guide.","code":"model.compile(     optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),     loss=keras.losses.SparseCategoricalCrossentropy(),     metrics=[keras.metrics.SparseCategoricalAccuracy()], ) model.compile(     optimizer=\"rmsprop\",     loss=\"sparse_categorical_crossentropy\",     metrics=[\"sparse_categorical_accuracy\"], ) def get_uncompiled_model():     inputs = keras.Input(shape=(784,), name=\"digits\")     x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)     x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)     outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)     model = keras.Model(inputs=inputs, outputs=outputs)     return model   def get_compiled_model():     model = get_uncompiled_model()     model.compile(         optimizer=\"rmsprop\",         loss=\"sparse_categorical_crossentropy\",         metrics=[\"sparse_categorical_accuracy\"],     )     return model"},{"path":"https://keras.posit.co/articles/guides/training_with_built_in_methods.html","id":"many-built-in-optimizers-losses-and-metrics-are-available","dir":"Articles > Guides","previous_headings":"The compile() method: specifying a loss, metrics, and an optimizer","what":"Many built-in optimizers, losses, and metrics are available","title":"Training & evaluation with the built-in methods","text":"general, won’t create losses, metrics, optimizers scratch, need likely already part Keras API: Optimizers: SGD() (without momentum) RMSprop() Adam() etc. Losses: MeanSquaredError() KLDivergence() CosineSimilarity() etc. Metrics: AUC() Precision() Recall() etc.","code":""},{"path":"https://keras.posit.co/articles/guides/training_with_built_in_methods.html","id":"custom-losses","dir":"Articles > Guides","previous_headings":"The compile() method: specifying a loss, metrics, and an optimizer","what":"Custom losses","title":"Training & evaluation with the built-in methods","text":"need create custom loss, Keras provides three ways . first method involves creating function accepts inputs y_true y_pred. following example shows loss function computes mean squared error real data predictions: need loss function takes parameters beside y_true y_pred, can subclass keras.losses.Loss class implement following two methods: __init__(self): accept parameters pass call loss function call(self, y_true, y_pred): use targets (y_true) model predictions (y_pred) compute model’s loss Let’s say want use mean squared error, added term de-incentivize prediction values far 0.5 (assume categorical targets one-hot encoded take values 0 1). creates incentive model confident, may help reduce overfitting (won’t know works try!). ’s :","code":"def custom_mean_squared_error(y_true, y_pred):     return ops.mean(ops.square(y_true - y_pred), axis=-1)   model = get_uncompiled_model() model.compile(optimizer=keras.optimizers.Adam(), loss=custom_mean_squared_error)  # We need to one-hot encode the labels to use MSE y_train_one_hot = ops.one_hot(y_train, num_classes=10) model.fit(x_train, y_train_one_hot, batch_size=64, epochs=1) class CustomMSE(keras.losses.Loss):     def __init__(self, regularization_factor=0.1, name=\"custom_mse\"):         super().__init__(name=name)         self.regularization_factor = regularization_factor      def call(self, y_true, y_pred):         mse = ops.mean(ops.square(y_true - y_pred), axis=-1)         reg = ops.mean(ops.square(0.5 - y_pred), axis=-1)         return mse + reg * self.regularization_factor   model = get_uncompiled_model() model.compile(optimizer=keras.optimizers.Adam(), loss=CustomMSE())  y_train_one_hot = ops.one_hot(y_train, num_classes=10) model.fit(x_train, y_train_one_hot, batch_size=64, epochs=1)"},{"path":"https://keras.posit.co/articles/guides/training_with_built_in_methods.html","id":"custom-metrics","dir":"Articles > Guides","previous_headings":"The compile() method: specifying a loss, metrics, and an optimizer","what":"Custom metrics","title":"Training & evaluation with the built-in methods","text":"need metric isn’t part API, can easily create custom metrics subclassing keras.metrics.Metric class. need implement 4 methods: __init__(self), create state variables metric. update_state(self, y_true, y_pred, sample_weight=None), uses targets y_true model predictions y_pred update state variables. result(self), uses state variables compute final results. reset_state(self), reinitializes state metric. State update results computation kept separate (update_state() result(), respectively) cases, results computation might expensive done periodically. ’s simple example showing implement CategoricalTruePositives metric counts many samples correctly classified belonging given class:","code":"class CategoricalTruePositives(keras.metrics.Metric):     def __init__(self, name=\"categorical_true_positives\", **kwargs):         super().__init__(name=name, **kwargs)         self.true_positives = self.add_variable(             shape=(), name=\"ctp\", initializer=\"zeros\"         )      def update_state(self, y_true, y_pred, sample_weight=None):         y_pred = ops.reshape(ops.argmax(y_pred, axis=1), (-1, 1))         values = ops.cast(y_true, \"int32\") == ops.cast(y_pred, \"int32\")         values = ops.cast(values, \"float32\")         if sample_weight is not None:             sample_weight = ops.cast(sample_weight, \"float32\")             values = ops.multiply(values, sample_weight)         self.true_positives.assign_add(ops.sum(values))      def result(self):         return self.true_positives      def reset_state(self):         # The state of the metric will be reset at the start of each epoch.         self.true_positives.assign(0.0)   model = get_uncompiled_model() model.compile(     optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),     loss=keras.losses.SparseCategoricalCrossentropy(),     metrics=[CategoricalTruePositives()], ) model.fit(x_train, y_train, batch_size=64, epochs=3)"},{"path":"https://keras.posit.co/articles/guides/training_with_built_in_methods.html","id":"handling-losses-and-metrics-that-dont-fit-the-standard-signature","dir":"Articles > Guides","previous_headings":"The compile() method: specifying a loss, metrics, and an optimizer","what":"Handling losses and metrics that don’t fit the standard signature","title":"Training & evaluation with the built-in methods","text":"overwhelming majority losses metrics can computed y_true y_pred, y_pred output model – . instance, regularization loss may require activation layer (targets case), activation may model output. cases, can call self.add_loss(loss_value) inside call method custom layer. Losses added way get added “main” loss training (one passed compile()). ’s simple example adds activity regularization (note activity regularization built-Keras layers – layer just sake providing concrete example): Note pass losses via add_loss(), becomes possible call compile() without loss function, since model already loss minimize. Consider following LogisticEndpoint layer: takes inputs targets & logits, tracks crossentropy loss via add_loss(). can use model two inputs (input data & targets), compiled without loss argument, like : information training multi-input models, see section Passing data multi-input, multi-output models.","code":"class ActivityRegularizationLayer(layers.Layer):     def call(self, inputs):         self.add_loss(ops.sum(inputs) * 0.1)         return inputs  # Pass-through layer.   inputs = keras.Input(shape=(784,), name=\"digits\") x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)  # Insert activity regularization as a layer x = ActivityRegularizationLayer()(x)  x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x) outputs = layers.Dense(10, name=\"predictions\")(x)  model = keras.Model(inputs=inputs, outputs=outputs) model.compile(     optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), )  # The displayed loss will be much higher than before # due to the regularization component. model.fit(x_train, y_train, batch_size=64, epochs=1) class LogisticEndpoint(keras.layers.Layer):     def __init__(self, name=None):         super().__init__(name=name)         self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)      def call(self, targets, logits, sample_weights=None):         # Compute the training-time loss value and add it         # to the layer using `self.add_loss()`.         loss = self.loss_fn(targets, logits, sample_weights)         self.add_loss(loss)          # Return the inference-time prediction tensor (for `.predict()`).         return ops.softmax(logits) inputs = keras.Input(shape=(3,), name=\"inputs\") targets = keras.Input(shape=(10,), name=\"targets\") logits = keras.layers.Dense(10)(inputs) predictions = LogisticEndpoint(name=\"predictions\")(targets, logits)  model = keras.Model(inputs=[inputs, targets], outputs=predictions) model.compile(optimizer=\"adam\")  # No loss argument!  data = {     \"inputs\": np.random.random((3, 3)),     \"targets\": np.random.random((3, 10)), } model.fit(data)"},{"path":"https://keras.posit.co/articles/guides/training_with_built_in_methods.html","id":"automatically-setting-apart-a-validation-holdout-set","dir":"Articles > Guides","previous_headings":"The compile() method: specifying a loss, metrics, and an optimizer","what":"Automatically setting apart a validation holdout set","title":"Training & evaluation with the built-in methods","text":"first end--end example saw, used validation_data argument pass tuple NumPy arrays (x_val, y_val) model evaluating validation loss validation metrics end epoch. ’s another option: argument validation_split allows automatically reserve part training data validation. argument value represents fraction data reserved validation, set number higher 0 lower 1. instance, validation_split=0.2 means “use 20% data validation”, validation_split=0.6 means “use 60% data validation”. way validation computed taking last x% samples arrays received fit() call, shuffling. Note can use validation_split training NumPy data.","code":"model = get_compiled_model() model.fit(x_train, y_train, batch_size=64, validation_split=0.2, epochs=1)"},{"path":"https://keras.posit.co/articles/guides/training_with_built_in_methods.html","id":"training-evaluation-using-tf-data-datasets","dir":"Articles > Guides","previous_headings":"","what":"Training & evaluation using tf.data Datasets","title":"Training & evaluation with the built-in methods","text":"past paragraphs, ’ve seen handle losses, metrics, optimizers, ’ve seen use validation_data validation_split arguments fit(), data passed NumPy arrays. Another option use iterator-like, tf.data.Dataset, PyTorch DataLoader, Keras PyDataset. Let’s take look former. tf.data API set utilities TensorFlow 2.0 loading preprocessing data way ’s fast scalable. complete guide creating Datasets, see tf.data documentation. can use tf.data train Keras models regardless backend ’re using – whether ’s JAX, PyTorch, TensorFlow. can pass Dataset instance directly methods fit(), evaluate(), predict(): Note Dataset reset end epoch, can reused next epoch. want run training specific number batches Dataset, can pass steps_per_epoch argument, specifies many training steps model run using Dataset moving next epoch. can also pass Dataset instance validation_data argument fit(): end epoch, model iterate validation dataset compute validation loss validation metrics. want run validation specific number batches dataset, can pass validation_steps argument, specifies many validation steps model run validation dataset interrupting validation moving next epoch: Note validation dataset reset use (always evaluating samples epoch epoch). argument validation_split (generating holdout set training data) supported training Dataset objects, since feature requires ability index samples datasets, possible general Dataset API.","code":"model = get_compiled_model()  # First, let's create a training Dataset instance. # For the sake of our example, we'll use the same MNIST data as before. train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)) # Shuffle and slice the dataset. train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)  # Now we get a test dataset. test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)) test_dataset = test_dataset.batch(64)  # Since the dataset already takes care of batching, # we don't pass a `batch_size` argument. model.fit(train_dataset, epochs=3)  # You can also evaluate or predict on a dataset. print(\"Evaluate\") result = model.evaluate(test_dataset) dict(zip(model.metrics_names, result)) model = get_compiled_model()  # Prepare the training dataset train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)) train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)  # Only use the 100 batches per epoch (that's 64 * 100 samples) model.fit(train_dataset, epochs=3, steps_per_epoch=100) model = get_compiled_model()  # Prepare the training dataset train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)) train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)  # Prepare the validation dataset val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)) val_dataset = val_dataset.batch(64)  model.fit(train_dataset, epochs=1, validation_data=val_dataset) model = get_compiled_model()  # Prepare the training dataset train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)) train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)  # Prepare the validation dataset val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)) val_dataset = val_dataset.batch(64)  model.fit(     train_dataset,     epochs=1,     # Only run validation using the first 10 batches of the dataset     # using the `validation_steps` argument     validation_data=val_dataset,     validation_steps=10, )"},{"path":"https://keras.posit.co/articles/guides/training_with_built_in_methods.html","id":"training-evaluation-using-pydataset-instances","dir":"Articles > Guides","previous_headings":"","what":"Training & evaluation using PyDataset instances","title":"Training & evaluation with the built-in methods","text":"keras.utils.PyDataset utility can subclass obtain Python generator two important properties: works well multiprocessing. can shuffled (e.g. passing shuffle=True fit()). PyDataset must implement two methods: __getitem__ __len__ method __getitem__ return complete batch. want modify dataset epochs, may implement on_epoch_end. ’s quick example: fit model, pass dataset instead x argument (need y argument since dataset includes targets), pass validation dataset validation_data argument. need batch_size argument, since dataset already batched! Evaluating model just easy: Importantly, PyDataset objects support three common constructor arguments handle parallel processing configuration: workers: Number workers use multithreading multiprocessing. Typically, ’d set number cores CPU. use_multiprocessing: Whether use Python multiprocessing parallelism. Setting True means dataset replicated multiple forked processes. necessary gain compute-level (rather /O level) benefits parallelism. However can set True dataset can safely pickled. max_queue_size: Maximum number batches keep queue iterating dataset multithreaded multipricessed setting. can reduce value reduce CPU memory consumption dataset. defaults 10. default, multiprocessing disabled (use_multiprocessing=False) one thread used. make sure turn use_multiprocessing code running inside Python __name__ == \"__main__\": block order avoid issues. ’s 4-thread, non-multiprocessed example:","code":"class ExamplePyDataset(keras.utils.PyDataset):     def __init__(self, x, y, batch_size, **kwargs):         super().__init__(**kwargs)         self.x = x         self.y = y         self.batch_size = batch_size      def __len__(self):         return int(np.ceil(len(self.x) / float(self.batch_size)))      def __getitem__(self, idx):         batch_x = self.x[idx * self.batch_size : (idx + 1) * self.batch_size]         batch_y = self.y[idx * self.batch_size : (idx + 1) * self.batch_size]         return batch_x, batch_y   train_py_dataset = ExamplePyDataset(x_train, y_train, batch_size=32) val_py_dataset = ExamplePyDataset(x_val, y_val, batch_size=32) model = get_compiled_model() model.fit(     train_py_dataset, batch_size=64, validation_data=val_py_dataset, epochs=1 ) model.evaluate(val_py_dataset) train_py_dataset = ExamplePyDataset(x_train, y_train, batch_size=32, workers=4) val_py_dataset = ExamplePyDataset(x_val, y_val, batch_size=32, workers=4)  model = get_compiled_model() model.fit(     train_py_dataset, batch_size=64, validation_data=val_py_dataset, epochs=1 )"},{"path":"https://keras.posit.co/articles/guides/training_with_built_in_methods.html","id":"training-evaluation-using-pytorch-dataloader-objects","dir":"Articles > Guides","previous_headings":"","what":"Training & evaluation using PyTorch DataLoader objects","title":"Training & evaluation with the built-in methods","text":"built-training evaluation APIs also compatible torch.utils.data.Dataset torch.utils.data.DataLoader objects – regardless whether ’re using PyTorch backend, JAX TensorFlow backends. Let’s take look simple example. Unlike PyDataset batch-centric, PyTorch Dataset objects sample-centric: __len__ method returns number samples, __getitem__ method returns specific sample. use PyTorch Dataset, need wrap Dataloader takes care batching shuffling: Now can use Keras API just like iterator:","code":"class ExampleTorchDataset(torch.utils.data.Dataset):     def __init__(self, x, y):         self.x = x         self.y = y      def __len__(self):         return len(self.x)      def __getitem__(self, idx):         return self.x[idx], self.y[idx]   train_torch_dataset = ExampleTorchDataset(x_train, y_train) val_torch_dataset = ExampleTorchDataset(x_val, y_val) train_dataloader = torch.utils.data.DataLoader(     train_torch_dataset, batch_size=32, shuffle=True ) val_dataloader = torch.utils.data.DataLoader(     val_torch_dataset, batch_size=32, shuffle=True ) model = get_compiled_model() model.fit(     train_dataloader, batch_size=64, validation_data=val_dataloader, epochs=1 ) model.evaluate(val_dataloader)"},{"path":"https://keras.posit.co/articles/guides/training_with_built_in_methods.html","id":"using-sample-weighting-and-class-weighting","dir":"Articles > Guides","previous_headings":"","what":"Using sample weighting and class weighting","title":"Training & evaluation with the built-in methods","text":"default settings weight sample decided frequency dataset. two methods weight data, independent sample frequency: Class weights Sample weights","code":""},{"path":"https://keras.posit.co/articles/guides/training_with_built_in_methods.html","id":"class-weights","dir":"Articles > Guides","previous_headings":"Using sample weighting and class weighting","what":"Class weights","title":"Training & evaluation with the built-in methods","text":"set passing dictionary class_weight argument Model.fit(). dictionary maps class indices weight used samples belonging class. can used balance classes without resampling, train model gives importance particular class. instance, class “0” half represented class “1” data, use Model.fit(..., class_weight={0: 1., 1: 0.5}). ’s NumPy example use class weights sample weights give importance correct classification class #5 (digit “5” MNIST dataset).","code":"class_weight = {     0: 1.0,     1: 1.0,     2: 1.0,     3: 1.0,     4: 1.0,     # Set weight \"2\" for class \"5\",     # making this class 2x more important     5: 2.0,     6: 1.0,     7: 1.0,     8: 1.0,     9: 1.0, }  print(\"Fit with class weight\") model = get_compiled_model() model.fit(x_train, y_train, class_weight=class_weight, batch_size=64, epochs=1)"},{"path":"https://keras.posit.co/articles/guides/training_with_built_in_methods.html","id":"sample-weights","dir":"Articles > Guides","previous_headings":"Using sample weighting and class weighting","what":"Sample weights","title":"Training & evaluation with the built-in methods","text":"fine grained control, building classifier, can use “sample weights”. training NumPy data: Pass sample_weight argument Model.fit(). training tf.data sort iterator: Yield (input_batch, label_batch, sample_weight_batch) tuples. “sample weights” array array numbers specify much weight sample batch computing total loss. commonly used imbalanced classification problems (idea give weight rarely-seen classes). weights used ones zeros, array can used mask loss function (entirely discarding contribution certain samples total loss). ’s matching Dataset example:","code":"sample_weight = np.ones(shape=(len(y_train),)) sample_weight[y_train == 5] = 2.0  print(\"Fit with sample weight\") model = get_compiled_model() model.fit(     x_train, y_train, sample_weight=sample_weight, batch_size=64, epochs=1 ) sample_weight = np.ones(shape=(len(y_train),)) sample_weight[y_train == 5] = 2.0  # Create a Dataset that includes sample weights # (3rd element in the return tuple). train_dataset = tf.data.Dataset.from_tensor_slices(     (x_train, y_train, sample_weight) )  # Shuffle and slice the dataset. train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)  model = get_compiled_model() model.fit(train_dataset, epochs=1)"},{"path":"https://keras.posit.co/articles/guides/training_with_built_in_methods.html","id":"passing-data-to-multi-input-multi-output-models","dir":"Articles > Guides","previous_headings":"","what":"Passing data to multi-input, multi-output models","title":"Training & evaluation with the built-in methods","text":"previous examples, considering model single input (tensor shape (764,)) single output (prediction tensor shape (10,)). models multiple inputs outputs? Consider following model, image input shape (32, 32, 3) (’s (height, width, channels)) time series input shape (None, 10) (’s (timesteps, features)). model two outputs computed combination inputs: “score” (shape (1,)) probability distribution five classes (shape (5,)). Let’s plot model, can clearly see ’re (note shapes shown plot batch shapes, rather per-sample shapes). compilation time, can specify different losses different outputs, passing loss functions list: passed single loss function model, loss function applied every output (appropriate ). Likewise metrics: Since gave names output layers, also specify per-output losses metrics via dict: recommend use explicit names dicts 2 outputs. ’s possible give different weights different output-specific losses (instance, one might wish privilege “score” loss example, giving 2x importance class loss), using loss_weights argument: also choose compute loss certain outputs, outputs meant prediction training: Passing data multi-input multi-output model fit() works similar way specifying loss function compile: can pass lists NumPy arrays (1:1 mapping outputs received loss function) dicts mapping output names NumPy arrays. ’s Dataset use case: similarly NumPy arrays, Dataset return tuple dicts.","code":"image_input = keras.Input(shape=(32, 32, 3), name=\"img_input\") timeseries_input = keras.Input(shape=(None, 10), name=\"ts_input\")  x1 = layers.Conv2D(3, 3)(image_input) x1 = layers.GlobalMaxPooling2D()(x1)  x2 = layers.Conv1D(3, 3)(timeseries_input) x2 = layers.GlobalMaxPooling1D()(x2)  x = layers.concatenate([x1, x2])  score_output = layers.Dense(1, name=\"score_output\")(x) class_output = layers.Dense(5, name=\"class_output\")(x)  model = keras.Model(     inputs=[image_input, timeseries_input], outputs=[score_output, class_output] ) keras.utils.plot_model(     model, \"multi_input_and_output_model.png\", show_shapes=True ) model.compile(     optimizer=keras.optimizers.RMSprop(1e-3),     loss=[         keras.losses.MeanSquaredError(),         keras.losses.CategoricalCrossentropy(),     ], ) model.compile(     optimizer=keras.optimizers.RMSprop(1e-3),     loss=[         keras.losses.MeanSquaredError(),         keras.losses.CategoricalCrossentropy(),     ],     metrics=[         [             keras.metrics.MeanAbsolutePercentageError(),             keras.metrics.MeanAbsoluteError(),         ],         [keras.metrics.CategoricalAccuracy()],     ], ) model.compile(     optimizer=keras.optimizers.RMSprop(1e-3),     loss={         \"score_output\": keras.losses.MeanSquaredError(),         \"class_output\": keras.losses.CategoricalCrossentropy(),     },     metrics={         \"score_output\": [             keras.metrics.MeanAbsolutePercentageError(),             keras.metrics.MeanAbsoluteError(),         ],         \"class_output\": [keras.metrics.CategoricalAccuracy()],     }, ) model.compile(     optimizer=keras.optimizers.RMSprop(1e-3),     loss={         \"score_output\": keras.losses.MeanSquaredError(),         \"class_output\": keras.losses.CategoricalCrossentropy(),     },     metrics={         \"score_output\": [             keras.metrics.MeanAbsolutePercentageError(),             keras.metrics.MeanAbsoluteError(),         ],         \"class_output\": [keras.metrics.CategoricalAccuracy()],     },     loss_weights={\"score_output\": 2.0, \"class_output\": 1.0}, ) # List loss version model.compile(     optimizer=keras.optimizers.RMSprop(1e-3),     loss=[None, keras.losses.CategoricalCrossentropy()], )  # Or dict loss version model.compile(     optimizer=keras.optimizers.RMSprop(1e-3),     loss={\"class_output\": keras.losses.CategoricalCrossentropy()}, ) model.compile(     optimizer=keras.optimizers.RMSprop(1e-3),     loss=[         keras.losses.MeanSquaredError(),         keras.losses.CategoricalCrossentropy(),     ], )  # Generate dummy NumPy data img_data = np.random.random_sample(size=(100, 32, 32, 3)) ts_data = np.random.random_sample(size=(100, 20, 10)) score_targets = np.random.random_sample(size=(100, 1)) class_targets = np.random.random_sample(size=(100, 5))  # Fit on lists model.fit(     [img_data, ts_data], [score_targets, class_targets], batch_size=32, epochs=1 )  # Alternatively, fit on dicts model.fit(     {\"img_input\": img_data, \"ts_input\": ts_data},     {\"score_output\": score_targets, \"class_output\": class_targets},     batch_size=32,     epochs=1, ) train_dataset = tf.data.Dataset.from_tensor_slices(     (         {\"img_input\": img_data, \"ts_input\": ts_data},         {\"score_output\": score_targets, \"class_output\": class_targets},     ) ) train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)  model.fit(train_dataset, epochs=1)"},{"path":"https://keras.posit.co/articles/guides/training_with_built_in_methods.html","id":"using-callbacks","dir":"Articles > Guides","previous_headings":"","what":"Using callbacks","title":"Training & evaluation with the built-in methods","text":"Callbacks Keras objects called different points training (start epoch, end batch, end epoch, etc.). can used implement certain behaviors, : validation different points training (beyond built-per-epoch validation) Checkpointing model regular intervals exceeds certain accuracy threshold Changing learning rate model training seems plateauing fine-tuning top layers training seems plateauing Sending email instant message notifications training ends certain performance threshold exceeded Etc. Callbacks can passed list call fit():","code":"model = get_compiled_model()  callbacks = [     keras.callbacks.EarlyStopping(         # Stop training when `val_loss` is no longer improving         monitor=\"val_loss\",         # \"no longer improving\" being defined as \"no better than 1e-2 less\"         min_delta=1e-2,         # \"no longer improving\" being further defined as \"for at least 2 epochs\"         patience=2,         verbose=1,     ) ] model.fit(     x_train,     y_train,     epochs=20,     batch_size=64,     callbacks=callbacks,     validation_split=0.2, )"},{"path":"https://keras.posit.co/articles/guides/training_with_built_in_methods.html","id":"many-built-in-callbacks-are-available","dir":"Articles > Guides","previous_headings":"Using callbacks","what":"Many built-in callbacks are available","title":"Training & evaluation with the built-in methods","text":"many built-callbacks already available Keras, : ModelCheckpoint: Periodically save model. EarlyStopping: Stop training training longer improving validation metrics. TensorBoard: periodically write model logs can visualized TensorBoard (details section “Visualization”). CSVLogger: streams loss metrics data CSV file. etc. See callbacks documentation complete list.","code":""},{"path":"https://keras.posit.co/articles/guides/training_with_built_in_methods.html","id":"writing-your-own-callback","dir":"Articles > Guides","previous_headings":"Using callbacks","what":"Writing your own callback","title":"Training & evaluation with the built-in methods","text":"can create custom callback extending base class keras.callbacks.Callback. callback access associated model class property self.model. Make sure read complete guide writing custom callbacks. ’s simple example saving list per-batch loss values training:","code":"class LossHistory(keras.callbacks.Callback):     def on_train_begin(self, logs):         self.per_batch_losses = []      def on_batch_end(self, batch, logs):         self.per_batch_losses.append(logs.get(\"loss\"))"},{"path":"https://keras.posit.co/articles/guides/training_with_built_in_methods.html","id":"checkpointing-models","dir":"Articles > Guides","previous_headings":"","what":"Checkpointing models","title":"Training & evaluation with the built-in methods","text":"’re training model relatively large datasets, ’s crucial save checkpoints model frequent intervals. easiest way achieve ModelCheckpoint callback: ModelCheckpoint callback can used implement fault-tolerance: ability restart training last saved state model case training gets randomly interrupted. ’s basic example: call also write callback saving restoring models. complete guide serialization saving, see guide saving serializing Models.","code":"model = get_compiled_model()  callbacks = [     keras.callbacks.ModelCheckpoint(         # Path where to save the model         # The two parameters below mean that we will overwrite         # the current checkpoint if and only if         # the `val_loss` score has improved.         # The saved model name will include the current epoch.         filepath=\"mymodel_{epoch}.keras\",         save_best_only=True,  # Only save a model if `val_loss` has improved.         monitor=\"val_loss\",         verbose=1,     ) ] model.fit(     x_train,     y_train,     epochs=2,     batch_size=64,     callbacks=callbacks,     validation_split=0.2, ) # Prepare a directory to store all the checkpoints. checkpoint_dir = \"./ckpt\" if not os.path.exists(checkpoint_dir):     os.makedirs(checkpoint_dir)   def make_or_restore_model():     # Either restore the latest model, or create a fresh one     # if there is no checkpoint available.     checkpoints = [         checkpoint_dir + \"/\" + name for name in os.listdir(checkpoint_dir)     ]     if checkpoints:         latest_checkpoint = max(checkpoints, key=os.path.getctime)         print(\"Restoring from\", latest_checkpoint)         return keras.models.load_model(latest_checkpoint)     print(\"Creating a new model\")     return get_compiled_model()   model = make_or_restore_model() callbacks = [     # This callback saves the model every 100 batches.     # We include the training loss in the saved model name.     keras.callbacks.ModelCheckpoint(         filepath=checkpoint_dir + \"/model-loss={loss:.2f}.keras\", save_freq=100     ) ] model.fit(x_train, y_train, epochs=1, callbacks=callbacks)"},{"path":"https://keras.posit.co/articles/guides/training_with_built_in_methods.html","id":"using-learning-rate-schedules","dir":"Articles > Guides","previous_headings":"","what":"Using learning rate schedules","title":"Training & evaluation with the built-in methods","text":"common pattern training deep learning models gradually reduce learning training progresses. generally known “learning rate decay”. learning decay schedule static (fixed advance, function current epoch current batch index), dynamic (responding current behavior model, particular validation loss).","code":""},{"path":"https://keras.posit.co/articles/guides/training_with_built_in_methods.html","id":"passing-a-schedule-to-an-optimizer","dir":"Articles > Guides","previous_headings":"Using learning rate schedules","what":"Passing a schedule to an optimizer","title":"Training & evaluation with the built-in methods","text":"can easily use static learning rate decay schedule passing schedule object learning_rate argument optimizer: Several built-schedules available: ExponentialDecay, PiecewiseConstantDecay, PolynomialDecay, InverseTimeDecay.","code":"initial_learning_rate = 0.1 lr_schedule = keras.optimizers.schedules.ExponentialDecay(     initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True )  optimizer = keras.optimizers.RMSprop(learning_rate=lr_schedule)"},{"path":"https://keras.posit.co/articles/guides/training_with_built_in_methods.html","id":"using-callbacks-to-implement-a-dynamic-learning-rate-schedule","dir":"Articles > Guides","previous_headings":"Using learning rate schedules","what":"Using callbacks to implement a dynamic learning rate schedule","title":"Training & evaluation with the built-in methods","text":"dynamic learning rate schedule (instance, decreasing learning rate validation loss longer improving) achieved schedule objects, since optimizer access validation metrics. However, callbacks access metrics, including validation metrics! can thus achieve pattern using callback modifies current learning rate optimizer. fact, even built-ReduceLROnPlateau callback.","code":""},{"path":"https://keras.posit.co/articles/guides/training_with_built_in_methods.html","id":"visualizing-loss-and-metrics-during-training-with-tensorboard","dir":"Articles > Guides","previous_headings":"","what":"Visualizing loss and metrics during training with TensorBoard","title":"Training & evaluation with the built-in methods","text":"best way keep eye model training use TensorBoard – browser-based application can run locally provides : Live plots loss metrics training evaluation (optionally) Visualizations histograms layer activations (optionally) 3D visualizations embedding spaces learned Embedding layers installed TensorFlow pip, able launch TensorBoard command line:","code":"tensorboard --logdir=/full_path_to_your_logs"},{"path":"https://keras.posit.co/articles/guides/training_with_built_in_methods.html","id":"using-the-tensorboard-callback","dir":"Articles > Guides","previous_headings":"Visualizing loss and metrics during training with TensorBoard","what":"Using the TensorBoard callback","title":"Training & evaluation with the built-in methods","text":"easiest way use TensorBoard Keras model fit() method TensorBoard callback. simplest case, just specify want callback write logs, ’re good go: information, see documentation TensorBoard callback.","code":"keras.callbacks.TensorBoard(     log_dir=\"/full_path_to_your_logs\",     histogram_freq=0,  # How often to log histogram visualizations     embeddings_freq=0,  # How often to log embedding visualizations     update_freq=\"epoch\", )  # How often to write logs (default: once per epoch)"},{"path":"https://keras.posit.co/articles/guides/transfer_learning.html","id":"setup","dir":"Articles > Guides","previous_headings":"","what":"Setup","title":"Transfer learning & fine-tuning","text":"","code":"import numpy as np import keras as keras from keras import layers import tensorflow_datasets as tfds import matplotlib.pyplot as plt"},{"path":"https://keras.posit.co/articles/guides/transfer_learning.html","id":"introduction","dir":"Articles > Guides","previous_headings":"","what":"Introduction","title":"Transfer learning & fine-tuning","text":"Transfer learning consists taking features learned one problem, leveraging new, similar problem. instance, features model learned identify racoons may useful kick-start model meant identify tanukis. Transfer learning usually done tasks dataset little data train full-scale model scratch. common incarnation transfer learning context deep learning following workflow: Take layers previously trained model. Freeze , avoid destroying information contain future training rounds. Add new, trainable layers top frozen layers. learn turn old features predictions new dataset. Train new layers dataset. last, optional step, fine-tuning, consists unfreezing entire model obtained (part ), re-training new data low learning rate. can potentially achieve meaningful improvements, incrementally adapting pretrained features new data. First, go Keras trainable API detail, underlies transfer learning & fine-tuning workflows. , ’ll demonstrate typical workflow taking model pretrained ImageNet dataset, retraining Kaggle “cats vs dogs” classification dataset. adapted Deep Learning Python 2016 blog post “building powerful image classification models using little data”.","code":""},{"path":"https://keras.posit.co/articles/guides/transfer_learning.html","id":"freezing-layers-understanding-the-trainable-attribute","dir":"Articles > Guides","previous_headings":"","what":"Freezing layers: understanding the trainable attribute","title":"Transfer learning & fine-tuning","text":"Layers & models three weight attributes: weights list weights variables layer. trainable_weights list meant updated (via gradient descent) minimize loss training. non_trainable_weights list aren’t meant trained. Typically updated model forward pass. Example: Dense layer 2 trainable weights (kernel & bias) general, weights trainable weights. built-layer non-trainable weights BatchNormalization layer. uses non-trainable weights keep track mean variance inputs training. learn use non-trainable weights custom layers, see guide writing new layers scratch. Example: BatchNormalization layer 2 trainable weights 2 non-trainable weights Layers & models also feature boolean attribute trainable. value can changed. Setting layer.trainable False moves layer’s weights trainable non-trainable. called “freezing” layer: state frozen layer won’t updated training (either training fit() training custom loop relies trainable_weights apply gradient updates). Example: setting trainable False trainable weight becomes non-trainable, value longer updated training. confuse layer.trainable attribute argument training layer.__call__() (controls whether layer run forward pass inference mode training mode). information, see Keras FAQ.","code":"layer = keras.layers.Dense(3) layer.build((None, 4))  # Create the weights  print(\"weights:\", len(layer.weights)) print(\"trainable_weights:\", len(layer.trainable_weights)) print(\"non_trainable_weights:\", len(layer.non_trainable_weights)) layer = keras.layers.BatchNormalization() layer.build((None, 4))  # Create the weights  print(\"weights:\", len(layer.weights)) print(\"trainable_weights:\", len(layer.trainable_weights)) print(\"non_trainable_weights:\", len(layer.non_trainable_weights)) layer = keras.layers.Dense(3) layer.build((None, 4))  # Create the weights layer.trainable = False  # Freeze the layer  print(\"weights:\", len(layer.weights)) print(\"trainable_weights:\", len(layer.trainable_weights)) print(\"non_trainable_weights:\", len(layer.non_trainable_weights)) # Make a model with 2 layers layer1 = keras.layers.Dense(3, activation=\"relu\") layer2 = keras.layers.Dense(3, activation=\"sigmoid\") model = keras.Sequential([keras.Input(shape=(3,)), layer1, layer2])  # Freeze the first layer layer1.trainable = False  # Keep a copy of the weights of layer1 for later reference initial_layer1_weights_values = layer1.get_weights()  # Train the model model.compile(optimizer=\"adam\", loss=\"mse\") model.fit(np.random.random((2, 3)), np.random.random((2, 3)))  # Check that the weights of layer1 have not changed during training final_layer1_weights_values = layer1.get_weights() np.testing.assert_allclose(     initial_layer1_weights_values[0], final_layer1_weights_values[0] ) np.testing.assert_allclose(     initial_layer1_weights_values[1], final_layer1_weights_values[1] )"},{"path":"https://keras.posit.co/articles/guides/transfer_learning.html","id":"recursive-setting-of-the-trainable-attribute","dir":"Articles > Guides","previous_headings":"","what":"Recursive setting of the trainable attribute","title":"Transfer learning & fine-tuning","text":"set trainable = False model layer sublayers, children layers become non-trainable well. Example:","code":"inner_model = keras.Sequential(     [         keras.Input(shape=(3,)),         keras.layers.Dense(3, activation=\"relu\"),         keras.layers.Dense(3, activation=\"relu\"),     ] )  model = keras.Sequential(     [         keras.Input(shape=(3,)),         inner_model,         keras.layers.Dense(3, activation=\"sigmoid\"),     ] )  model.trainable = False  # Freeze the outer model  assert inner_model.trainable == False  # All layers in `model` are now frozen assert (     inner_model.layers[0].trainable == False )  # `trainable` is propagated recursively"},{"path":"https://keras.posit.co/articles/guides/transfer_learning.html","id":"the-typical-transfer-learning-workflow","dir":"Articles > Guides","previous_headings":"","what":"The typical transfer-learning workflow","title":"Transfer learning & fine-tuning","text":"leads us typical transfer learning workflow can implemented Keras: Instantiate base model load pre-trained weights . Freeze layers base model setting trainable = False. Create new model top output one (several) layers base model. Train new model new dataset. Note alternative, lightweight workflow also : Instantiate base model load pre-trained weights . Run new dataset record output one (several) layers base model. called feature extraction. Use output input data new, smaller model. key advantage second workflow run base model data, rather per epoch training. ’s lot faster & cheaper. issue second workflow, though, doesn’t allow dynamically modify input data new model training, required data augmentation, instance. Transfer learning typically used tasks new dataset little data train full-scale model scratch, scenarios data augmentation important. follows, focus first workflow. ’s first workflow looks like Keras: First, instantiate base model pre-trained weights. , freeze base model. Create new model top. Train model new data.","code":"base_model = keras.applications.Xception(     weights='imagenet',  # Load weights pre-trained on ImageNet.     input_shape=(150, 150, 3),     include_top=False)  # Do not include the ImageNet classifier at the top. base_model.trainable = False inputs = keras.Input(shape=(150, 150, 3)) # We make sure that the base_model is running in inference mode here, # by passing `training=False`. This is important for fine-tuning, as you will # learn in a few paragraphs. x = base_model(inputs, training=False) # Convert features of shape `base_model.output_shape[1:]` to vectors x = keras.layers.GlobalAveragePooling2D()(x) # A Dense classifier with a single unit (binary classification) outputs = keras.layers.Dense(1)(x) model = keras.Model(inputs, outputs) model.compile(optimizer=keras.optimizers.Adam(),               loss=keras.losses.BinaryCrossentropy(from_logits=True),               metrics=[keras.metrics.BinaryAccuracy()]) model.fit(new_dataset, epochs=20, callbacks=..., validation_data=...)"},{"path":"https://keras.posit.co/articles/guides/transfer_learning.html","id":"fine-tuning","dir":"Articles > Guides","previous_headings":"","what":"Fine-tuning","title":"Transfer learning & fine-tuning","text":"model converged new data, can try unfreeze part base model retrain whole model end--end low learning rate. optional last step can potentially give incremental improvements. also potentially lead quick overfitting – keep mind. critical step model frozen layers trained convergence. mix randomly-initialized trainable layers trainable layers hold pre-trained features, randomly-initialized layers cause large gradient updates training, destroy pre-trained features. ’s also critical use low learning rate stage, training much larger model first round training, dataset typically small. result, risk overfitting quickly apply large weight updates. , want readapt pretrained weights incremental way. implement fine-tuning whole base model: Important note compile() trainable Calling compile() model meant “freeze” behavior model. implies trainable attribute values time model compiled preserved throughout lifetime model, compile called . Hence, change trainable value, make sure call compile() model changes taken account. Important notes BatchNormalization layer Many image models contain BatchNormalization layers. layer special case every imaginable count. things keep mind. BatchNormalization contains 2 non-trainable weights get updated training. variables tracking mean variance inputs. set bn_layer.trainable = False, BatchNormalization layer run inference mode, update mean & variance statistics. case layers general, weight trainability & inference/training modes two orthogonal concepts. two tied case BatchNormalization layer. unfreeze model contains BatchNormalization layers order fine-tuning, keep BatchNormalization layers inference mode passing training=False calling base model. Otherwise updates applied non-trainable weights suddenly destroy model learned. ’ll see pattern action end--end example end guide.","code":"# Unfreeze the base model base_model.trainable = True  # It's important to recompile your model after you make any changes # to the `trainable` attribute of any inner layer, so that your changes # are take into account model.compile(optimizer=keras.optimizers.Adam(1e-5),  # Very low learning rate               loss=keras.losses.BinaryCrossentropy(from_logits=True),               metrics=[keras.metrics.BinaryAccuracy()])  # Train end-to-end. Be careful to stop before you overfit! model.fit(new_dataset, epochs=10, callbacks=..., validation_data=...)"},{"path":"https://keras.posit.co/articles/guides/transfer_learning.html","id":"an-end-to-end-example-fine-tuning-an-image-classification-model-on-a-cats-vs--dogs-dataset","dir":"Articles > Guides","previous_headings":"","what":"An end-to-end example: fine-tuning an image classification model on a cats vs. dogs dataset","title":"Transfer learning & fine-tuning","text":"solidify concepts, let’s walk concrete end--end transfer learning & fine-tuning example. load Xception model, pre-trained ImageNet, use Kaggle “cats vs. dogs” classification dataset.","code":""},{"path":"https://keras.posit.co/articles/guides/transfer_learning.html","id":"getting-the-data","dir":"Articles > Guides","previous_headings":"An end-to-end example: fine-tuning an image classification model on a cats vs. dogs dataset","what":"Getting the data","title":"Transfer learning & fine-tuning","text":"First, let’s fetch cats vs. dogs dataset using TFDS. dataset, ’ll probably want use utility keras.utils.image_dataset_from_directory generate similar labeled dataset objects set images disk filed class-specific folders. Transfer learning useful working small datasets. keep dataset small, use 40% original training data (25,000 images) training, 10% validation, 10% testing. first 9 images training dataset – can see, ’re different sizes. can also see label 1 “dog” label 0 “cat”.","code":"tfds.disable_progress_bar()  train_ds, validation_ds, test_ds = tfds.load(     \"cats_vs_dogs\",     # Reserve 10% for validation and 10% for test     split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],     as_supervised=True,  # Include labels )  print(f\"Number of training samples: {train_ds.cardinality()}\") print(f\"Number of validation samples: {validation_ds.cardinality()}\") print(f\"Number of test samples: {test_ds.cardinality()}\") plt.figure(figsize=(10, 10)) for i, (image, label) in enumerate(train_ds.take(9)):     ax = plt.subplot(3, 3, i + 1)     plt.imshow(image)     plt.title(int(label))     plt.axis(\"off\")"},{"path":"https://keras.posit.co/articles/guides/transfer_learning.html","id":"standardizing-the-data","dir":"Articles > Guides","previous_headings":"An end-to-end example: fine-tuning an image classification model on a cats vs. dogs dataset","what":"Standardizing the data","title":"Transfer learning & fine-tuning","text":"raw images variety sizes. addition, pixel consists 3 integer values 0 255 (RGB level values). isn’t great fit feeding neural network. need 2 things: Standardize fixed image size. pick 150x150. Normalize pixel values -1 1. ’ll using Normalization layer part model . general, ’s good practice develop models take raw data input, opposed models take already-preprocessed data. reason , model expects preprocessed data, time export model use elsewhere (web browser, mobile app), ’ll need reimplement exact preprocessing pipeline. gets tricky quickly. least possible amount preprocessing hitting model. , ’ll image resizing data pipeline (deep neural network can process contiguous batches data), ’ll input value scaling part model, create . Let’s resize images 150x150:","code":"resize_fn = keras.layers.Resizing(150, 150)  train_ds = train_ds.map(lambda x, y: (resize_fn(x), y)) validation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y)) test_ds = test_ds.map(lambda x, y: (resize_fn(x), y))"},{"path":"https://keras.posit.co/articles/guides/transfer_learning.html","id":"using-random-data-augmentation","dir":"Articles > Guides","previous_headings":"An end-to-end example: fine-tuning an image classification model on a cats vs. dogs dataset","what":"Using random data augmentation","title":"Transfer learning & fine-tuning","text":"don’t large image dataset, ’s good practice artificially introduce sample diversity applying random yet realistic transformations training images, random horizontal flipping small random rotations. helps expose model different aspects training data slowing overfitting. Let’s batch data use prefetching optimize loading speed. Let’s visualize first image first batch looks like various random transformations:","code":"augmentation_layers = [     layers.RandomFlip(\"horizontal\"),     layers.RandomRotation(0.1), ]   def data_augmentation(x):     for layer in augmentation_layers:         x = layer(x)     return x   train_ds = train_ds.map(lambda x, y: (data_augmentation(x), y)) from tensorflow import data as tf_data  batch_size = 64  train_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache() validation_ds = (     validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache() ) test_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache() for images, labels in train_ds.take(1):     plt.figure(figsize=(10, 10))     first_image = images[0]     for i in range(9):         ax = plt.subplot(3, 3, i + 1)         augmented_image = data_augmentation(np.expand_dims(first_image, 0))         plt.imshow(np.array(augmented_image[0]).astype(\"int32\"))         plt.title(int(labels[0]))         plt.axis(\"off\")"},{"path":"https://keras.posit.co/articles/guides/transfer_learning.html","id":"build-a-model","dir":"Articles > Guides","previous_headings":"","what":"Build a model","title":"Transfer learning & fine-tuning","text":"Now let’s built model follows blueprint ’ve explained earlier. Note : add Rescaling layer scale input values (initially [0, 255] range) [-1, 1] range. add Dropout layer classification layer, regularization. make sure pass training=False calling base model, runs inference mode, batchnorm statistics don’t get updated even unfreeze base model fine-tuning.","code":"base_model = keras.applications.Xception(     weights=\"imagenet\",  # Load weights pre-trained on ImageNet.     input_shape=(150, 150, 3),     include_top=False, )  # Do not include the ImageNet classifier at the top.  # Freeze the base_model base_model.trainable = False  # Create new model on top inputs = keras.Input(shape=(150, 150, 3))  # Pre-trained Xception weights requires that input be scaled # from (0, 255) to a range of (-1., +1.), the rescaling layer # outputs: `(inputs * scale) + offset` scale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1) x = scale_layer(inputs)  # The base model contains batchnorm layers. We want to keep them in inference mode # when we unfreeze the base model for fine-tuning, so we make sure that the # base_model is running in inference mode here. x = base_model(x, training=False) x = keras.layers.GlobalAveragePooling2D()(x) x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout outputs = keras.layers.Dense(1)(x) model = keras.Model(inputs, outputs)  model.summary(show_trainable=True)"},{"path":"https://keras.posit.co/articles/guides/transfer_learning.html","id":"train-the-top-layer","dir":"Articles > Guides","previous_headings":"","what":"Train the top layer","title":"Transfer learning & fine-tuning","text":"","code":"model.compile(     optimizer=keras.optimizers.Adam(),     loss=keras.losses.BinaryCrossentropy(from_logits=True),     metrics=[keras.metrics.BinaryAccuracy()], )  epochs = 2 print(\"Fitting the top layer of the model\") model.fit(train_ds, epochs=epochs, validation_data=validation_ds)"},{"path":"https://keras.posit.co/articles/guides/transfer_learning.html","id":"do-a-round-of-fine-tuning-of-the-entire-model","dir":"Articles > Guides","previous_headings":"","what":"Do a round of fine-tuning of the entire model","title":"Transfer learning & fine-tuning","text":"Finally, let’s unfreeze base model train entire model end--end low learning rate. Importantly, although base model becomes trainable, still running inference mode since passed training=False calling built model. means batch normalization layers inside won’t update batch statistics. , wreck havoc representations learned model far. 10 epochs, fine-tuning gains us nice improvement . Let’s evaluate model test dataset:","code":"# Unfreeze the base_model. Note that it keeps running in inference mode # since we passed `training=False` when calling it. This means that # the batchnorm layers will not update their batch statistics. # This prevents the batchnorm layers from undoing all the training # we've done so far. base_model.trainable = True model.summary(show_trainable=True)  model.compile(     optimizer=keras.optimizers.Adam(1e-5),  # Low learning rate     loss=keras.losses.BinaryCrossentropy(from_logits=True),     metrics=[keras.metrics.BinaryAccuracy()], )  epochs = 1 print(\"Fitting the end-to-end model\") model.fit(train_ds, epochs=epochs, validation_data=validation_ds) print(\"Test dataset evaluation\") model.evaluate(test_ds)"},{"path":"https://keras.posit.co/articles/guides/understanding_masking_and_padding.html","id":"setup","dir":"Articles > Guides","previous_headings":"","what":"Setup","title":"Understanding masking & padding","text":"","code":"import numpy as np import keras as keras from keras import ops from keras import layers"},{"path":"https://keras.posit.co/articles/guides/understanding_masking_and_padding.html","id":"introduction","dir":"Articles > Guides","previous_headings":"","what":"Introduction","title":"Understanding masking & padding","text":"Masking way tell sequence-processing layers certain timesteps input missing, thus skipped processing data. Padding special form masking masked steps start end sequence. Padding comes need encode sequence data contiguous batches: order make sequences batch fit given standard length, necessary pad truncate sequences. Let’s take close look.","code":""},{"path":"https://keras.posit.co/articles/guides/understanding_masking_and_padding.html","id":"padding-sequence-data","dir":"Articles > Guides","previous_headings":"","what":"Padding sequence data","title":"Understanding masking & padding","text":"processing sequence data, common individual samples different lengths. Consider following example (text tokenized words): vocabulary lookup, data might vectorized integers, e.g.: data nested list individual samples length 3, 5, 6, respectively. Since input data deep learning model must single tensor (shape e.g. (batch_size, 6, vocab_size) case), samples shorter longest item need padded placeholder value (alternatively, one might also truncate long samples padding short samples). Keras provides utility function truncate pad Python lists common length: keras.utils.pad_sequences.","code":"[   [\"Hello\", \"world\", \"!\"],   [\"How\", \"are\", \"you\", \"doing\", \"today\"],   [\"The\", \"weather\", \"will\", \"be\", \"nice\", \"tomorrow\"], ] [   [71, 1331, 4231]   [73, 8, 3215, 55, 927],   [83, 91, 1, 645, 1253, 927], ] raw_inputs = [     [711, 632, 71],     [73, 8, 3215, 55, 927],     [83, 91, 1, 645, 1253, 927], ]  # By default, this will pad using 0s; it is configurable via the # \"value\" parameter. # Note that you could use \"pre\" padding (at the beginning) or # \"post\" padding (at the end). # We recommend using \"post\" padding when working with RNN layers # (in order to be able to use the # CuDNN implementation of the layers). padded_inputs = keras.utils.pad_sequences(raw_inputs, padding=\"post\") print(padded_inputs)"},{"path":"https://keras.posit.co/articles/guides/understanding_masking_and_padding.html","id":"masking","dir":"Articles > Guides","previous_headings":"","what":"Masking","title":"Understanding masking & padding","text":"Now samples uniform length, model must informed part data actually padding ignored. mechanism masking. three ways introduce input masks Keras models: Add keras.layers.Masking layer. Configure keras.layers.Embedding layer mask_zero=True. Pass mask argument manually calling layers support argument (e.g. RNN layers).","code":""},{"path":"https://keras.posit.co/articles/guides/understanding_masking_and_padding.html","id":"mask-generating-layers-embedding-and-masking","dir":"Articles > Guides","previous_headings":"","what":"Mask-generating layers: Embedding and Masking","title":"Understanding masking & padding","text":"hood, layers create mask tensor (2D tensor shape (batch, sequence_length)), attach tensor output returned Masking Embedding layer. can see printed result, mask 2D boolean tensor shape (batch_size, sequence_length), individual False entry indicates corresponding timestep ignored processing.","code":"embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True) masked_output = embedding(padded_inputs)  print(masked_output._keras_mask)  masking_layer = layers.Masking() # Simulate the embedding lookup by expanding the 2D input to 3D, # with embedding dimension of 10. unmasked_embedding = ops.cast(     ops.tile(ops.expand_dims(padded_inputs, axis=-1), [1, 1, 10]),     dtype=\"float32\", )  masked_embedding = masking_layer(unmasked_embedding) print(masked_embedding._keras_mask)"},{"path":"https://keras.posit.co/articles/guides/understanding_masking_and_padding.html","id":"mask-propagation-in-the-functional-api-and-sequential-api","dir":"Articles > Guides","previous_headings":"","what":"Mask propagation in the Functional API and Sequential API","title":"Understanding masking & padding","text":"using Functional API Sequential API, mask generated Embedding Masking layer propagated network layer capable using (example, RNN layers). Keras automatically fetch mask corresponding input pass layer knows use . instance, following Sequential model, LSTM layer automatically receive mask, means ignore padded values: also case following Functional API model:","code":"model = keras.Sequential(     [         layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True),         layers.LSTM(32),     ] ) inputs = keras.Input(shape=(None,), dtype=\"int32\") x = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)(inputs) outputs = layers.LSTM(32)(x)  model = keras.Model(inputs, outputs)"},{"path":"https://keras.posit.co/articles/guides/understanding_masking_and_padding.html","id":"passing-mask-tensors-directly-to-layers","dir":"Articles > Guides","previous_headings":"","what":"Passing mask tensors directly to layers","title":"Understanding masking & padding","text":"Layers can handle masks (LSTM layer) mask argument __call__ method. Meanwhile, layers produce mask (e.g. Embedding) expose compute_mask(input, previous_mask) method can call. Thus, can pass output compute_mask() method mask-producing layer __call__ method mask-consuming layer, like :","code":"class MyLayer(layers.Layer):     def __init__(self, **kwargs):         super().__init__(**kwargs)         self.embedding = layers.Embedding(             input_dim=5000, output_dim=16, mask_zero=True         )         self.lstm = layers.LSTM(32)      def call(self, inputs):         x = self.embedding(inputs)         # Note that you could also prepare a `mask` tensor manually.         # It only needs to be a boolean tensor         # with the right shape, i.e. (batch_size, timesteps).         mask = self.embedding.compute_mask(inputs)         output = self.lstm(             x, mask=mask         )  # The layer will ignore the masked values         return output   layer = MyLayer() x = np.random.random((32, 10)) * 100 x = x.astype(\"int32\") layer(x)"},{"path":"https://keras.posit.co/articles/guides/understanding_masking_and_padding.html","id":"supporting-masking-in-your-custom-layers","dir":"Articles > Guides","previous_headings":"","what":"Supporting masking in your custom layers","title":"Understanding masking & padding","text":"Sometimes, may need write layers generate mask (like Embedding), layers need modify current mask. instance, layer produces tensor different time dimension input, Concatenate layer concatenates time dimension, need modify current mask downstream layers able properly take masked timesteps account. , layer implement layer.compute_mask() method, produces new mask given input current mask. example TemporalSplit layer needs modify current mask. another example CustomEmbedding layer capable generating mask input values: Note: details format limitations related masking, see serialization guide.","code":"class TemporalSplit(keras.layers.Layer):     \"\"\"Split the input tensor into 2 tensors along the time dimension.\"\"\"      def call(self, inputs):         # Expect the input to be 3D and mask to be 2D, split the input tensor into 2         # subtensors along the time axis (axis 1).         return ops.split(inputs, 2, axis=1)      def compute_mask(self, inputs, mask=None):         # Also split the mask into 2 if it presents.         if mask is None:             return None         return ops.split(mask, 2, axis=1)   first_half, second_half = TemporalSplit()(masked_embedding) print(first_half._keras_mask) print(second_half._keras_mask) class CustomEmbedding(keras.layers.Layer):     def __init__(self, input_dim, output_dim, mask_zero=False, **kwargs):         super().__init__(**kwargs)         self.input_dim = input_dim         self.output_dim = output_dim         self.mask_zero = mask_zero      def build(self, input_shape):         self.embeddings = self.add_weight(             shape=(self.input_dim, self.output_dim),             initializer=\"random_normal\",             dtype=\"float32\",         )      def call(self, inputs):         inputs = ops.cast(inputs, \"int32\")         return ops.take(self.embeddings, inputs)      def compute_mask(self, inputs, mask=None):         if not self.mask_zero:             return None         return ops.not_equal(inputs, 0)   layer = CustomEmbedding(10, 32, mask_zero=True) x = np.random.random((3, 10)) * 9 x = x.astype(\"int32\")  y = layer(x) mask = layer.compute_mask(x)  print(mask)"},{"path":"https://keras.posit.co/articles/guides/understanding_masking_and_padding.html","id":"opting-in-to-mask-propagation-on-compatible-layers","dir":"Articles > Guides","previous_headings":"","what":"Opting-in to mask propagation on compatible layers","title":"Understanding masking & padding","text":"layers don’t modify time dimension, don’t need modify current mask. However, may still want able propagate current mask, unchanged, next layer. opt-behavior. default, custom layer destroy current mask (since framework way tell whether propagating mask safe ). custom layer modify time dimension, want able propagate current input mask, set self.supports_masking = True layer constructor. case, default behavior compute_mask() just pass current mask . ’s example layer whitelisted mask propagation: can now use custom layer -mask-generating layer (like Embedding) mask-consuming layer (like LSTM), pass mask along reaches mask-consuming layer.","code":"class MyActivation(keras.layers.Layer):     def __init__(self, **kwargs):         super().__init__(**kwargs)         # Signal that the layer is safe for mask propagation         self.supports_masking = True      def call(self, inputs):         return ops.relu(inputs) inputs = keras.Input(shape=(None,), dtype=\"int32\") x = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)(inputs) x = MyActivation()(x)  # Will pass the mask along print(\"Mask found:\", x._keras_mask) outputs = layers.LSTM(32)(x)  # Will receive the mask  model = keras.Model(inputs, outputs) y = model(np.random.randint(0, 5000, size=(32, 100)))"},{"path":"https://keras.posit.co/articles/guides/understanding_masking_and_padding.html","id":"writing-layers-that-need-mask-information","dir":"Articles > Guides","previous_headings":"","what":"Writing layers that need mask information","title":"Understanding masking & padding","text":"layers mask consumers: accept mask argument call use determine whether skip certain time steps. write layer, can simply add mask=None argument call signature. mask associated inputs passed layer whenever available. ’s simple example : layer computes softmax time dimension (axis 1) input sequence, discarding masked timesteps.","code":"class TemporalSoftmax(keras.layers.Layer):     def __init__(self, **kwargs):         super().__init__(**kwargs)         self.supports_masking = True      def call(self, inputs, mask=None):         assert mask is not None         broadcast_float_mask = ops.expand_dims(ops.cast(mask, \"float32\"), -1)         inputs_exp = ops.exp(inputs) * broadcast_float_mask         inputs_sum = ops.sum(             inputs_exp * broadcast_float_mask, axis=-1, keepdims=True         )         return inputs_exp / inputs_sum   inputs = keras.Input(shape=(None,), dtype=\"int32\") x = layers.Embedding(input_dim=10, output_dim=32, mask_zero=True)(inputs) x = layers.Dense(1)(x) outputs = TemporalSoftmax()(x)  model = keras.Model(inputs, outputs) y = model(np.random.randint(0, 10, size=(32, 100)))"},{"path":"https://keras.posit.co/articles/guides/understanding_masking_and_padding.html","id":"summary","dir":"Articles > Guides","previous_headings":"","what":"Summary","title":"Understanding masking & padding","text":"need know padding & masking Keras. recap: “Masking” layers able know skip / ignore certain timesteps sequence inputs. layers mask-generators: Embedding can generate mask input values (mask_zero=True), can Masking layer. layers mask-consumers: expose mask argument __call__ method. case RNN layers. Functional API Sequential API, mask information propagated automatically. using layers standalone way, can pass mask arguments layers manually. can easily write layers modify current mask, generate new mask, consume mask associated inputs.","code":""},{"path":"https://keras.posit.co/articles/guides/working_with_rnns.html","id":"introduction","dir":"Articles > Guides","previous_headings":"","what":"Introduction","title":"Working with RNNs","text":"Recurrent neural networks (RNN) class neural networks powerful modeling sequence data time series natural language. Schematically, RNN layer uses loop iterate timesteps sequence, maintaining internal state encodes information timesteps seen far. Keras RNN API designed focus : Ease use: built-keras.layers.RNN, keras.layers.LSTM, keras.layers.GRU layers enable quickly build recurrent models without make difficult configuration choices. Ease customization: can also define RNN cell layer (inner part loop) custom behavior, use generic keras.layers.RNN layer (loop ). allows quickly prototype different research ideas flexible way minimal code.","code":""},{"path":"https://keras.posit.co/articles/guides/working_with_rnns.html","id":"setup","dir":"Articles > Guides","previous_headings":"","what":"Setup","title":"Working with RNNs","text":"","code":"import numpy as np import tensorflow as tf import keras from keras import layers"},{"path":"https://keras.posit.co/articles/guides/working_with_rnns.html","id":"built-in-rnn-layers-a-simple-example","dir":"Articles > Guides","previous_headings":"","what":"Built-in RNN layers: a simple example","title":"Working with RNNs","text":"three built-RNN layers Keras: keras.layers.SimpleRNN, fully-connected RNN output previous timestep fed next timestep. keras.layers.GRU, first proposed Cho et al., 2014. keras.layers.LSTM, first proposed Hochreiter & Schmidhuber, 1997. early 2015, Keras first reusable open-source Python implementations LSTM GRU. simple example Sequential model processes sequences integers, embeds integer 64-dimensional vector, processes sequence vectors using LSTM layer. Built-RNNs support number useful features: Recurrent dropout, via dropout recurrent_dropout arguments Ability process input sequence reverse, via go_backwards argument Loop unrolling (can lead large speedup processing short sequences CPU), via unroll argument …. information, see RNN API documentation.","code":"model = keras.Sequential() # Add an Embedding layer expecting input vocab of size 1000, and # output embedding dimension of size 64. model.add(layers.Embedding(input_dim=1000, output_dim=64))  # Add a LSTM layer with 128 internal units. model.add(layers.LSTM(128))  # Add a Dense layer with 10 units. model.add(layers.Dense(10))  model.summary()"},{"path":"https://keras.posit.co/articles/guides/working_with_rnns.html","id":"outputs-and-states","dir":"Articles > Guides","previous_headings":"","what":"Outputs and states","title":"Working with RNNs","text":"default, output RNN layer contains single vector per sample. vector RNN cell output corresponding last timestep, containing information entire input sequence. shape output (batch_size, units) units corresponds units argument passed layer’s constructor. RNN layer can also return entire sequence outputs sample (one vector per timestep per sample), set return_sequences=True. shape output (batch_size, timesteps, units). addition, RNN layer can return final internal state(s). returned states can used resume RNN execution later, initialize another RNN. setting commonly used encoder-decoder sequence--sequence model, encoder final state used initial state decoder. configure RNN layer return internal state, set return_state parameter True creating layer. Note LSTM 2 state tensors, GRU one. configure initial state layer, just call layer additional keyword argument initial_state. Note shape state needs match unit size layer, like example .","code":"model = keras.Sequential() model.add(layers.Embedding(input_dim=1000, output_dim=64))  # The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256) model.add(layers.GRU(256, return_sequences=True))  # The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128) model.add(layers.SimpleRNN(128))  model.add(layers.Dense(10))  model.summary() encoder_vocab = 1000 decoder_vocab = 2000  encoder_input = layers.Input(shape=(None,)) encoder_embedded = layers.Embedding(input_dim=encoder_vocab, output_dim=64)(     encoder_input )  # Return states in addition to output output, state_h, state_c = layers.LSTM(64, return_state=True, name=\"encoder\")(     encoder_embedded ) encoder_state = [state_h, state_c]  decoder_input = layers.Input(shape=(None,)) decoder_embedded = layers.Embedding(input_dim=decoder_vocab, output_dim=64)(     decoder_input )  # Pass the 2 states to a new LSTM layer, as initial state decoder_output = layers.LSTM(64, name=\"decoder\")(     decoder_embedded, initial_state=encoder_state ) output = layers.Dense(10)(decoder_output)  model = keras.Model([encoder_input, decoder_input], output) model.summary()"},{"path":"https://keras.posit.co/articles/guides/working_with_rnns.html","id":"rnn-layers-and-rnn-cells","dir":"Articles > Guides","previous_headings":"","what":"RNN layers and RNN cells","title":"Working with RNNs","text":"addition built-RNN layers, RNN API also provides cell-level APIs. Unlike RNN layers, processes whole batches input sequences, RNN cell processes single timestep. cell inside loop RNN layer. Wrapping cell inside keras.layers.RNN layer gives layer capable processing batches sequences, e.g. RNN(LSTMCell(10)). Mathematically, RNN(LSTMCell(10)) produces result LSTM(10). fact, implementation layer TF v1.x just creating corresponding RNN cell wrapping RNN layer. However using built-GRU LSTM layers enable use CuDNN may see better performance. three built-RNN cells, corresponding matching RNN layer. keras.layers.SimpleRNNCell corresponds SimpleRNN layer. keras.layers.GRUCell corresponds GRU layer. keras.layers.LSTMCell corresponds LSTM layer. cell abstraction, together generic keras.layers.RNN class, make easy implement custom RNN architectures research.","code":""},{"path":"https://keras.posit.co/articles/guides/working_with_rnns.html","id":"cross-batch-statefulness","dir":"Articles > Guides","previous_headings":"","what":"Cross-batch statefulness","title":"Working with RNNs","text":"processing long sequences (possibly infinite), may want use pattern cross-batch statefulness. Normally, internal state RNN layer reset every time sees new batch (.e. every sample seen layer assumed independent past). layer maintain state processing given sample. long sequences though, useful break shorter sequences, feed shorter sequences sequentially RNN layer without resetting layer’s state. way, layer can retain information entirety sequence, even though ’s seeing one sub-sequence time. can setting stateful=True constructor. sequence s = [t0, t1, ... t1546, t1547], split e.g. process via: want clear state, can use layer.reset_states(). Note: setup, sample given batch assumed continuation sample previous batch. means batches contain number samples (batch size). E.g. batch contains [sequence_A_from_t0_to_t100, sequence_B_from_t0_to_t100], next batch contain [sequence_A_from_t101_to_t200,  sequence_B_from_t101_to_t200]. complete example:","code":"s1 = [t0, t1, ... t100] s2 = [t101, ... t201] ... s16 = [t1501, ... t1547] lstm_layer = layers.LSTM(64, stateful=True) for s in sub_sequences:   output = lstm_layer(s) paragraph1 = np.random.random((20, 10, 50)).astype(np.float32) paragraph2 = np.random.random((20, 10, 50)).astype(np.float32) paragraph3 = np.random.random((20, 10, 50)).astype(np.float32)  lstm_layer = layers.LSTM(64, stateful=True) output = lstm_layer(paragraph1) output = lstm_layer(paragraph2) output = lstm_layer(paragraph3)  # reset_states() will reset the cached state to the original initial_state. # If no initial_state was provided, zero-states will be used by default. lstm_layer.reset_states()"},{"path":"https://keras.posit.co/articles/guides/working_with_rnns.html","id":"rnn-state-reuse","dir":"Articles > Guides","previous_headings":"Cross-batch statefulness","what":"RNN State Reuse","title":"Working with RNNs","text":"recorded states RNN layer included layer.weights(). like reuse state RNN layer, can retrieve states value layer.states use initial state new layer via Keras functional API like new_layer(inputs, initial_state=layer.states), model subclassing. Please also note sequential model might used case since supports layers single input output, extra input initial state makes impossible use .","code":"paragraph1 = np.random.random((20, 10, 50)).astype(np.float32) paragraph2 = np.random.random((20, 10, 50)).astype(np.float32) paragraph3 = np.random.random((20, 10, 50)).astype(np.float32)  lstm_layer = layers.LSTM(64, stateful=True) output = lstm_layer(paragraph1) output = lstm_layer(paragraph2)  existing_state = lstm_layer.states  new_lstm_layer = layers.LSTM(64) new_output = new_lstm_layer(paragraph3, initial_state=existing_state)"},{"path":"https://keras.posit.co/articles/guides/working_with_rnns.html","id":"bidirectional-rnns","dir":"Articles > Guides","previous_headings":"","what":"Bidirectional RNNs","title":"Working with RNNs","text":"sequences time series (e.g. text), often case RNN model can perform better processes sequence start end, also backwards. example, predict next word sentence, often useful context around word, just words come . Keras provides easy API build bidirectional RNNs: keras.layers.Bidirectional wrapper. hood, Bidirectional copy RNN layer passed , flip go_backwards field newly copied layer, process inputs reverse order. output Bidirectional RNN , default, concatenation forward layer output backward layer output. need different merging behavior, e.g. concatenation, change merge_mode parameter Bidirectional wrapper constructor. details Bidirectional, please check API docs.","code":"model = keras.Sequential()  model.add(     layers.Bidirectional(layers.LSTM(64, return_sequences=True), input_shape=(5, 10)) ) model.add(layers.Bidirectional(layers.LSTM(32))) model.add(layers.Dense(10))  model.summary()"},{"path":"https://keras.posit.co/articles/guides/working_with_rnns.html","id":"performance-optimization-and-cudnn-kernels","dir":"Articles > Guides","previous_headings":"","what":"Performance optimization and CuDNN kernels","title":"Working with RNNs","text":"TensorFlow 2.0, built-LSTM GRU layers updated leverage CuDNN kernels default GPU available. change, prior keras.layers.CuDNNLSTM/CuDNNGRU layers deprecated, can build model without worrying hardware run . Since CuDNN kernel built certain assumptions, means layer able use CuDNN kernel change defaults built-LSTM GRU layers. E.g.: Changing activation function tanh something else. Changing recurrent_activation function sigmoid something else. Using recurrent_dropout > 0. Setting unroll True, forces LSTM/GRU decompose inner tf.while_loop unrolled loop. Setting use_bias False. Using masking input data strictly right padded (mask corresponds strictly right padded data, CuDNN can still used. common case). detailed list constraints, please see documentation LSTM GRU layers.","code":""},{"path":"https://keras.posit.co/articles/guides/working_with_rnns.html","id":"using-cudnn-kernels-when-available","dir":"Articles > Guides","previous_headings":"Performance optimization and CuDNN kernels","what":"Using CuDNN kernels when available","title":"Working with RNNs","text":"Let’s build simple LSTM model demonstrate performance difference. ’ll use input sequences sequence rows MNIST digits (treating row pixels timestep), ’ll predict digit’s label. Let’s load MNIST dataset: Let’s create model instance train . choose sparse_categorical_crossentropy loss function model. output model shape [batch_size, 10]. target model integer vector, integer range 0 9. Now, let’s compare model use CuDNN kernel: running machine NVIDIA GPU CuDNN installed, model built CuDNN much faster train compared model uses regular TensorFlow kernel. CuDNN-enabled model can also used run inference CPU-environment. tf.device annotation just forcing device placement. model run CPU default GPU available. simply don’t worry hardware ’re running anymore. Isn’t pretty cool?","code":"batch_size = 64 # Each MNIST image batch is a tensor of shape (batch_size, 28, 28). # Each input sequence will be of size (28, 28) (height is treated like time). input_dim = 28  units = 64 output_size = 10  # labels are from 0 to 9   # Build the RNN model def build_model(allow_cudnn_kernel=True):     # CuDNN is only available at the layer level, and not at the cell level.     # This means `LSTM(units)` will use the CuDNN kernel,     # while RNN(LSTMCell(units)) will run on non-CuDNN kernel.     if allow_cudnn_kernel:         # The LSTM layer with default options uses CuDNN.         lstm_layer = keras.layers.LSTM(units, input_shape=(None, input_dim))     else:         # Wrapping a LSTMCell in a RNN layer will not use CuDNN.         lstm_layer = keras.layers.RNN(             keras.layers.LSTMCell(units), input_shape=(None, input_dim)         )     model = keras.models.Sequential(         [             lstm_layer,             keras.layers.BatchNormalization(),             keras.layers.Dense(output_size),         ]     )     return model mnist = keras.datasets.mnist  (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 sample, sample_label = x_train[0], y_train[0] model = build_model(allow_cudnn_kernel=True)  model.compile(     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),     optimizer=\"sgd\",     metrics=[\"accuracy\"], )   model.fit(     x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=1 ) noncudnn_model = build_model(allow_cudnn_kernel=False) noncudnn_model.set_weights(model.get_weights()) noncudnn_model.compile(     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),     optimizer=\"sgd\",     metrics=[\"accuracy\"], ) noncudnn_model.fit(     x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=1 ) import matplotlib.pyplot as plt  with tf.device(\"CPU:0\"):     cpu_model = build_model(allow_cudnn_kernel=True)     cpu_model.set_weights(model.get_weights())     result = tf.argmax(cpu_model.predict_on_batch(tf.expand_dims(sample, 0)), axis=1)     print(         \"Predicted result is: %s, target result is: %s\" % (result.numpy(), sample_label)     )     plt.imshow(sample, cmap=plt.get_cmap(\"gray\"))"},{"path":"https://keras.posit.co/articles/guides/working_with_rnns.html","id":"rnns-with-listdict-inputs-or-nested-inputs","dir":"Articles > Guides","previous_headings":"","what":"RNNs with list/dict inputs, or nested inputs","title":"Working with RNNs","text":"Nested structures allow implementers include information within single timestep. example, video frame audio video input time. data shape case : [batch, timestep, {\"video\": [height, width, channel], \"audio\": [frequency]}] another example, handwriting data coordinates x y current position pen, well pressure information. data representation : [batch, timestep, {\"location\": [x, y], \"pressure\": [force]}] following code provides example build custom RNN cell accepts structured inputs.","code":""},{"path":"https://keras.posit.co/articles/guides/working_with_rnns.html","id":"define-a-custom-cell-that-supports-nested-inputoutput","dir":"Articles > Guides","previous_headings":"RNNs with list/dict inputs, or nested inputs","what":"Define a custom cell that supports nested input/output","title":"Working with RNNs","text":"See Making new Layers & Models via subclassing details writing layers.","code":"@keras.saving.register_keras_serializable() class NestedCell(keras.layers.Layer):     def __init__(self, unit_1, unit_2, unit_3, **kwargs):         self.unit_1 = unit_1         self.unit_2 = unit_2         self.unit_3 = unit_3         self.state_size = [tf.TensorShape([unit_1]), tf.TensorShape([unit_2, unit_3])]         self.output_size = [tf.TensorShape([unit_1]), tf.TensorShape([unit_2, unit_3])]         super().__init__(**kwargs)      def build(self, input_shapes):         # expect input_shape to contain 2 items, [(batch, i1), (batch, i2, i3)]         i1 = input_shapes[0][1]         i2 = input_shapes[1][1]         i3 = input_shapes[1][2]          self.kernel_1 = self.add_weight(             shape=(i1, self.unit_1), initializer=\"uniform\", name=\"kernel_1\"         )         self.kernel_2_3 = self.add_weight(             shape=(i2, i3, self.unit_2, self.unit_3),             initializer=\"uniform\",             name=\"kernel_2_3\",         )      def call(self, inputs, states):         # inputs should be in [(batch, input_1), (batch, input_2, input_3)]         # state should be in shape [(batch, unit_1), (batch, unit_2, unit_3)]         input_1, input_2 = tf.nest.flatten(inputs)         s1, s2 = states          output_1 = tf.matmul(input_1, self.kernel_1)         output_2_3 = tf.einsum(\"bij,ijkl->bkl\", input_2, self.kernel_2_3)         state_1 = s1 + output_1         state_2_3 = s2 + output_2_3          output = (output_1, output_2_3)         new_states = (state_1, state_2_3)          return output, new_states      def get_config(self):         return {\"unit_1\": self.unit_1, \"unit_2\": unit_2, \"unit_3\": self.unit_3}"},{"path":"https://keras.posit.co/articles/guides/working_with_rnns.html","id":"build-a-rnn-model-with-nested-inputoutput","dir":"Articles > Guides","previous_headings":"RNNs with list/dict inputs, or nested inputs","what":"Build a RNN model with nested input/output","title":"Working with RNNs","text":"Let’s build Keras model uses keras.layers.RNN layer custom cell just defined.","code":"unit_1 = 10 unit_2 = 20 unit_3 = 30  i1 = 32 i2 = 64 i3 = 32 batch_size = 64 num_batches = 10 timestep = 50  cell = NestedCell(unit_1, unit_2, unit_3) rnn = keras.layers.RNN(cell)  input_1 = keras.Input((None, i1)) input_2 = keras.Input((None, i2, i3))  outputs = rnn((input_1, input_2))  model = keras.models.Model([input_1, input_2], outputs)  model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])"},{"path":"https://keras.posit.co/articles/guides/working_with_rnns.html","id":"train-the-model-with-randomly-generated-data","dir":"Articles > Guides","previous_headings":"RNNs with list/dict inputs, or nested inputs","what":"Train the model with randomly generated data","title":"Working with RNNs","text":"Since isn’t good candidate dataset model, use random Numpy data demonstration. Keras keras.layers.RNN layer, expected define math logic individual step within sequence, keras.layers.RNN layer handle sequence iteration . ’s incredibly powerful way quickly prototype new kinds RNNs (e.g. LSTM variant). details, please visit API docs.","code":"input_1_data = np.random.random((batch_size * num_batches, timestep, i1)) input_2_data = np.random.random((batch_size * num_batches, timestep, i2, i3)) target_1_data = np.random.random((batch_size * num_batches, unit_1)) target_2_data = np.random.random((batch_size * num_batches, unit_2, unit_3)) input_data = [input_1_data, input_2_data] target_data = [target_1_data, target_2_data]  model.fit(input_data, target_data, batch_size=batch_size)"},{"path":"https://keras.posit.co/articles/guides/writing_a_custom_training_loop_in_jax.html","id":"setup","dir":"Articles > Guides","previous_headings":"","what":"Setup","title":"Writing a training loop from scratch in JAX","text":"","code":"import os  # This guide can only be run with the jax backend. os.environ[\"KERAS_BACKEND\"] = \"jax\"  import jax  # We import TF so we can use tf.data. import tensorflow as tf import keras as keras import numpy as np"},{"path":"https://keras.posit.co/articles/guides/writing_a_custom_training_loop_in_jax.html","id":"introduction","dir":"Articles > Guides","previous_headings":"","what":"Introduction","title":"Writing a training loop from scratch in JAX","text":"Keras provides default training evaluation loops, fit() evaluate(). usage covered guide Training & evaluation built-methods. want customize learning algorithm model still leveraging convenience fit() (instance, train GAN using fit()), can subclass Model class implement train_step() method, called repeatedly fit(). Now, want low-level control training & evaluation, write training & evaluation loops scratch. guide .","code":""},{"path":"https://keras.posit.co/articles/guides/writing_a_custom_training_loop_in_jax.html","id":"a-first-end-to-end-example","dir":"Articles > Guides","previous_headings":"","what":"A first end-to-end example","title":"Writing a training loop from scratch in JAX","text":"write custom training loop, need following ingredients: model train, course. optimizer. either use optimizer keras.optimizers, one optax package. loss function. dataset. standard JAX ecosystem load data via tf.data, ’s ’ll use. Let’s line . First, let’s get model MNIST dataset: Next, ’s loss function optimizer. ’ll use Keras optimizer case.","code":"def get_model():     inputs = keras.Input(shape=(784,), name=\"digits\")     x1 = keras.layers.Dense(64, activation=\"relu\")(inputs)     x2 = keras.layers.Dense(64, activation=\"relu\")(x1)     outputs = keras.layers.Dense(10, name=\"predictions\")(x2)     model = keras.Model(inputs=inputs, outputs=outputs)     return model   model = get_model()  # Prepare the training dataset. batch_size = 32 (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data() x_train = np.reshape(x_train, (-1, 784)).astype(\"float32\") x_test = np.reshape(x_test, (-1, 784)).astype(\"float32\") y_train = keras.utils.to_categorical(y_train) y_test = keras.utils.to_categorical(y_test)  # Reserve 10,000 samples for validation. x_val = x_train[-10000:] y_val = y_train[-10000:] x_train = x_train[:-10000] y_train = y_train[:-10000]  # Prepare the training dataset. train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)) train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)  # Prepare the validation dataset. val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)) val_dataset = val_dataset.batch(batch_size) # Instantiate a loss function. loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)  # Instantiate an optimizer. optimizer = keras.optimizers.Adam(learning_rate=1e-3)"},{"path":"https://keras.posit.co/articles/guides/writing_a_custom_training_loop_in_jax.html","id":"getting-gradients-in-jax","dir":"Articles > Guides","previous_headings":"A first end-to-end example","what":"Getting gradients in JAX","title":"Writing a training loop from scratch in JAX","text":"Let’s train model using mini-batch gradient custom training loop. JAX, gradients computed via metaprogramming: call jax.grad (jax.value_and_grad function order create gradient-computing function first function. first thing need function returns loss value. ’s function ’ll use generate gradient function. Something like : function, can compute gradients via metaprogramming : Typically, don’t just want get gradient values, also want get loss value. can using jax.value_and_grad instead jax.grad:","code":"def compute_loss(x, y):     ...     return loss grad_fn = jax.grad(compute_loss) grads = grad_fn(x, y) grad_fn = jax.value_and_grad(compute_loss) loss, grads = grad_fn(x, y)"},{"path":"https://keras.posit.co/articles/guides/writing_a_custom_training_loop_in_jax.html","id":"jax-computation-is-purely-stateless","dir":"Articles > Guides","previous_headings":"A first end-to-end example","what":"JAX computation is purely stateless","title":"Writing a training loop from scratch in JAX","text":"JAX, everything must stateless function – loss computation function must stateless well. means Keras variables (e.g. weight tensors) must passed function inputs, variable updated forward pass must returned function output. function side effect. forward pass, non-trainable variables Keras model might get updated. variables , instance, RNG seed state variables BatchNormalization statistics. ’re going need return . need something like : function, can get gradient function specifying hax_aux value_and_grad: tells JAX loss computation function returns outputs just loss. Note loss always first output. Now established basics, let’s implement compute_loss_and_updates function. Keras models stateless_call method come handy . works just like model.__call__, requires explicitly pass value variables model, returns just __call__ outputs also (potentially updated) non-trainable variables. Let’s get gradient function:","code":"def compute_loss_and_updates(trainable_variables, non_trainable_variables, x, y):     ...     return loss, non_trainable_variables grad_fn = jax.value_and_grad(compute_loss_and_updates, has_aux=True) (loss, non_trainable_variables), grads = grad_fn(     trainable_variables, non_trainable_variables, x, y ) def compute_loss_and_updates(     trainable_variables, non_trainable_variables, x, y ):     y_pred, non_trainable_variables = model.stateless_call(         trainable_variables, non_trainable_variables, x     )     loss = loss_fn(y, y_pred)     return loss, non_trainable_variables grad_fn = jax.value_and_grad(compute_loss_and_updates, has_aux=True)"},{"path":"https://keras.posit.co/articles/guides/writing_a_custom_training_loop_in_jax.html","id":"the-training-step-function","dir":"Articles > Guides","previous_headings":"A first end-to-end example","what":"The training step function","title":"Writing a training loop from scratch in JAX","text":"Next, let’s implement end--end training step, function run forward pass, compute loss, compute gradients, also use optimizer update trainable variables. function also needs stateless, get input state tuple includes every state element ’re going use: trainable_variables non_trainable_variables: model’s variables. optimizer_variables: optimizer’s state variables, momentum accumulators. update trainable variables, use optimizer’s stateless method stateless_apply. ’s equivalent optimizer.apply(), requires always passing trainable_variables optimizer_variables. returns updated trainable variables updated optimizer_variables.","code":"def train_step(state, data):     trainable_variables, non_trainable_variables, optimizer_variables = state     x, y = data     (loss, non_trainable_variables), grads = grad_fn(         trainable_variables, non_trainable_variables, x, y     )     trainable_variables, optimizer_variables = optimizer.stateless_apply(         optimizer_variables, grads, trainable_variables     )     # Return updated state     return loss, (         trainable_variables,         non_trainable_variables,         optimizer_variables,     )"},{"path":"https://keras.posit.co/articles/guides/writing_a_custom_training_loop_in_jax.html","id":"make-it-fast-with-jax-jit","dir":"Articles > Guides","previous_headings":"A first end-to-end example","what":"Make it fast with jax.jit","title":"Writing a training loop from scratch in JAX","text":"default, JAX operations run eagerly, just like TensorFlow eager mode PyTorch eager mode. just like TensorFlow eager mode PyTorch eager mode, ’s pretty slow – eager mode better used debugging environment, way actual work. let’s make train_step fast compiling . stateless JAX function, can compile XLA via @jax.jit decorator. get traced first execution, subsequent executions executing traced graph (just like @tf.function(jit_compile=True). Let’s try : ’re now ready train model. training loop trivial: just repeatedly call loss, state = train_step(state, data). Note: convert TF tensors yielded tf.data.Dataset NumPy passing JAX function. variables must built beforehand: model must built optimizer must built. Since ’re using Functional API model, ’s already built, subclassed model ’d need call batch data build . key thing notice loop entirely stateless – variables attached model (model.weights) never getting updated loop. new values stored state tuple. means point, saving model, attaching new variable values back model. Just call variable.assign(new_value) model variable want update:","code":"@jax.jit def train_step(state, data):     trainable_variables, non_trainable_variables, optimizer_variables = state     x, y = data     (loss, non_trainable_variables), grads = grad_fn(         trainable_variables, non_trainable_variables, x, y     )     trainable_variables, optimizer_variables = optimizer.stateless_apply(         optimizer_variables, grads, trainable_variables     )     # Return updated state     return loss, (         trainable_variables,         non_trainable_variables,         optimizer_variables,     ) # Build optimizer variables. optimizer.build(model.trainable_variables)  trainable_variables = model.trainable_variables non_trainable_variables = model.non_trainable_variables optimizer_variables = optimizer.variables state = trainable_variables, non_trainable_variables, optimizer_variables  # Training loop for step, data in enumerate(train_dataset):     data = (data[0].numpy(), data[1].numpy())     loss, state = train_step(state, data)     # Log every 100 batches.     if step % 100 == 0:         print(f\"Training loss (for 1 batch) at step {step}: {float(loss):.4f}\")         print(f\"Seen so far: {(step + 1) * batch_size} samples\") trainable_variables, non_trainable_variables, optimizer_variables = state for variable, value in zip(model.trainable_variables, trainable_variables):     variable.assign(value) for variable, value in zip(     model.non_trainable_variables, non_trainable_variables ):     variable.assign(value)"},{"path":"https://keras.posit.co/articles/guides/writing_a_custom_training_loop_in_jax.html","id":"low-level-handling-of-metrics","dir":"Articles > Guides","previous_headings":"","what":"Low-level handling of metrics","title":"Writing a training loop from scratch in JAX","text":"Let’s add metrics monitoring basic training loop. can readily reuse built-Keras metrics (custom ones wrote) training loops written scratch. ’s flow: Instantiate metric start loop Include metric_variables train_step arguments compute_loss_and_updates arguments. Call metric.stateless_update_state() compute_loss_and_updates function. ’s equivalent update_state() – stateless. need display current value metric, outside train_step (eager scope), attach new metric variable values metric object vall metric.result(). Call metric.reset_state() need clear state metric (typically end epoch) Let’s use knowledge compute CategoricalAccuracy training validation data end training: ’ll also prepare evaluation step function: loops:","code":"# Get a fresh model model = get_model()  # Instantiate an optimizer to train the model. optimizer = keras.optimizers.Adam(learning_rate=1e-3) # Instantiate a loss function. loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)  # Prepare the metrics. train_acc_metric = keras.metrics.CategoricalAccuracy() val_acc_metric = keras.metrics.CategoricalAccuracy()   def compute_loss_and_updates(     trainable_variables, non_trainable_variables, metric_variables, x, y ):     y_pred, non_trainable_variables = model.stateless_call(         trainable_variables, non_trainable_variables, x     )     loss = loss_fn(y, y_pred)     metric_variables = train_acc_metric.stateless_update_state(         metric_variables, y, y_pred     )     return loss, (non_trainable_variables, metric_variables)   grad_fn = jax.value_and_grad(compute_loss_and_updates, has_aux=True)   @jax.jit def train_step(state, data):     (         trainable_variables,         non_trainable_variables,         optimizer_variables,         metric_variables,     ) = state     x, y = data     (loss, (non_trainable_variables, metric_variables)), grads = grad_fn(         trainable_variables, non_trainable_variables, metric_variables, x, y     )     trainable_variables, optimizer_variables = optimizer.stateless_apply(         optimizer_variables, grads, trainable_variables     )     # Return updated state     return loss, (         trainable_variables,         non_trainable_variables,         optimizer_variables,         metric_variables,     ) @jax.jit def eval_step(state, data):     trainable_variables, non_trainable_variables, metric_variables = state     x, y = data     y_pred, non_trainable_variables = model.stateless_call(         trainable_variables, non_trainable_variables, x     )     loss = loss_fn(y, y_pred)     metric_variables = val_acc_metric.stateless_update_state(         metric_variables, y, y_pred     )     return loss, (         trainable_variables,         non_trainable_variables,         metric_variables,     ) # Build optimizer variables. optimizer.build(model.trainable_variables)  trainable_variables = model.trainable_variables non_trainable_variables = model.non_trainable_variables optimizer_variables = optimizer.variables metric_variables = train_acc_metric.variables state = (     trainable_variables,     non_trainable_variables,     optimizer_variables,     metric_variables, )  # Training loop for step, data in enumerate(train_dataset):     data = (data[0].numpy(), data[1].numpy())     loss, state = train_step(state, data)     # Log every 100 batches.     if step % 100 == 0:         print(f\"Training loss (for 1 batch) at step {step}: {float(loss):.4f}\")         _, _, _, metric_variables = state         for variable, value in zip(             train_acc_metric.variables, metric_variables         ):             variable.assign(value)         print(f\"Training accuracy: {train_acc_metric.result()}\")         print(f\"Seen so far: {(step + 1) * batch_size} samples\")  metric_variables = val_acc_metric.variables (     trainable_variables,     non_trainable_variables,     optimizer_variables,     metric_variables, ) = state state = trainable_variables, non_trainable_variables, metric_variables  # Eval loop for step, data in enumerate(val_dataset):     data = (data[0].numpy(), data[1].numpy())     loss, state = eval_step(state, data)     # Log every 100 batches.     if step % 100 == 0:         print(             f\"Validation loss (for 1 batch) at step {step}: {float(loss):.4f}\"         )         _, _, metric_variables = state         for variable, value in zip(val_acc_metric.variables, metric_variables):             variable.assign(value)         print(f\"Validation accuracy: {val_acc_metric.result()}\")         print(f\"Seen so far: {(step + 1) * batch_size} samples\")"},{"path":"https://keras.posit.co/articles/guides/writing_a_custom_training_loop_in_jax.html","id":"low-level-handling-of-losses-tracked-by-the-model","dir":"Articles > Guides","previous_headings":"","what":"Low-level handling of losses tracked by the model","title":"Writing a training loop from scratch in JAX","text":"Layers & models recursively track losses created forward pass layers call self.add_loss(value). resulting list scalar loss values available via property model.losses end forward pass. want using loss components, sum add main loss training step. Consider layer, creates activity regularization loss: Let’s build really simple model uses : ’s compute_loss_and_updates function look like now: Pass return_losses=True model.stateless_call(). Sum resulting losses add main loss. ’s !","code":"class ActivityRegularizationLayer(keras.layers.Layer):     def call(self, inputs):         self.add_loss(1e-2 * jax.numpy.sum(inputs))         return inputs inputs = keras.Input(shape=(784,), name=\"digits\") x = keras.layers.Dense(64, activation=\"relu\")(inputs) # Insert activity regularization as a layer x = ActivityRegularizationLayer()(x) x = keras.layers.Dense(64, activation=\"relu\")(x) outputs = keras.layers.Dense(10, name=\"predictions\")(x)  model = keras.Model(inputs=inputs, outputs=outputs) def compute_loss_and_updates(     trainable_variables, non_trainable_variables, metric_variables, x, y ):     y_pred, non_trainable_variables, losses = model.stateless_call(         trainable_variables, non_trainable_variables, x, return_losses=True     )     loss = loss_fn(y, y_pred)     if losses:         loss += jax.numpy.sum(losses)     metric_variables = train_acc_metric.stateless_update_state(         metric_variables, y, y_pred     )     return loss, non_trainable_variables, metric_variables"},{"path":"https://keras.posit.co/articles/guides/writing_a_custom_training_loop_in_tensorflow.html","id":"setup","dir":"Articles > Guides","previous_headings":"","what":"Setup","title":"Writing a training loop from scratch in TensorFlow","text":"","code":"import time import os  # This guide can only be run with the TensorFlow backend. os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  import tensorflow as tf import keras as keras import numpy as np"},{"path":"https://keras.posit.co/articles/guides/writing_a_custom_training_loop_in_tensorflow.html","id":"introduction","dir":"Articles > Guides","previous_headings":"","what":"Introduction","title":"Writing a training loop from scratch in TensorFlow","text":"Keras provides default training evaluation loops, fit() evaluate(). usage covered guide Training & evaluation built-methods. want customize learning algorithm model still leveraging convenience fit() (instance, train GAN using fit()), can subclass Model class implement train_step() method, called repeatedly fit(). Now, want low-level control training & evaluation, write training & evaluation loops scratch. guide .","code":""},{"path":"https://keras.posit.co/articles/guides/writing_a_custom_training_loop_in_tensorflow.html","id":"a-first-end-to-end-example","dir":"Articles > Guides","previous_headings":"","what":"A first end-to-end example","title":"Writing a training loop from scratch in TensorFlow","text":"Let’s consider simple MNIST model: Let’s train using mini-batch gradient custom training loop. First, ’re going need optimizer, loss function, dataset: Calling model inside GradientTape scope enables retrieve gradients trainable weights layer respect loss value. Using optimizer instance, can use gradients update variables (can retrieve using model.trainable_weights). ’s training loop, step step: open loop iterates epochs epoch, open loop iterates dataset, batches batch, open GradientTape() scope Inside scope, call model (forward pass) compute loss Outside scope, retrieve gradients weights model regard loss Finally, use optimizer update weights model based gradients","code":"def get_model():     inputs = keras.Input(shape=(784,), name=\"digits\")     x1 = keras.layers.Dense(64, activation=\"relu\")(inputs)     x2 = keras.layers.Dense(64, activation=\"relu\")(x1)     outputs = keras.layers.Dense(10, name=\"predictions\")(x2)     model = keras.Model(inputs=inputs, outputs=outputs)     return model   model = get_model() # Instantiate an optimizer. optimizer = keras.optimizers.Adam(learning_rate=1e-3) # Instantiate a loss function. loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Prepare the training dataset. batch_size = 32 (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data() x_train = np.reshape(x_train, (-1, 784)) x_test = np.reshape(x_test, (-1, 784))  # Reserve 10,000 samples for validation. x_val = x_train[-10000:] y_val = y_train[-10000:] x_train = x_train[:-10000] y_train = y_train[:-10000]  # Prepare the training dataset. train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)) train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)  # Prepare the validation dataset. val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)) val_dataset = val_dataset.batch(batch_size) epochs = 3 for epoch in range(epochs):     print(f\"\\nStart of epoch {epoch}\")      # Iterate over the batches of the dataset.     for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):         # Open a GradientTape to record the operations run         # during the forward pass, which enables auto-differentiation.         with tf.GradientTape() as tape:             # Run the forward pass of the layer.             # The operations that the layer applies             # to its inputs are going to be recorded             # on the GradientTape.             logits = model(                 x_batch_train, training=True             )  # Logits for this minibatch              # Compute the loss value for this minibatch.             loss_value = loss_fn(y_batch_train, logits)          # Use the gradient tape to automatically retrieve         # the gradients of the trainable variables with respect to the loss.         grads = tape.gradient(loss_value, model.trainable_weights)          # Run one step of gradient descent by updating         # the value of the variables to minimize the loss.         optimizer.apply(grads, model.trainable_weights)          # Log every 100 batches.         if step % 100 == 0:             print(                 f\"Training loss (for 1 batch) at step {step}: {float(loss_value):.4f}\"             )             print(f\"Seen so far: {(step + 1) * batch_size} samples\")"},{"path":"https://keras.posit.co/articles/guides/writing_a_custom_training_loop_in_tensorflow.html","id":"low-level-handling-of-metrics","dir":"Articles > Guides","previous_headings":"","what":"Low-level handling of metrics","title":"Writing a training loop from scratch in TensorFlow","text":"Let’s add metrics monitoring basic loop. can readily reuse built-metrics (custom ones wrote) training loops written scratch. ’s flow: Instantiate metric start loop Call metric.update_state() batch Call metric.result() need display current value metric Call metric.reset_state() need clear state metric (typically end epoch) Let’s use knowledge compute SparseCategoricalAccuracy training validation data end epoch: ’s training & evaluation loop:","code":"# Get a fresh model model = get_model()  # Instantiate an optimizer to train the model. optimizer = keras.optimizers.Adam(learning_rate=1e-3) # Instantiate a loss function. loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Prepare the metrics. train_acc_metric = keras.metrics.SparseCategoricalAccuracy() val_acc_metric = keras.metrics.SparseCategoricalAccuracy() epochs = 2 for epoch in range(epochs):     print(f\"\\nStart of epoch {epoch}\")     start_time = time.time()      # Iterate over the batches of the dataset.     for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):         with tf.GradientTape() as tape:             logits = model(x_batch_train, training=True)             loss_value = loss_fn(y_batch_train, logits)         grads = tape.gradient(loss_value, model.trainable_weights)         optimizer.apply(grads, model.trainable_weights)          # Update training metric.         train_acc_metric.update_state(y_batch_train, logits)          # Log every 100 batches.         if step % 100 == 0:             print(                 f\"Training loss (for 1 batch) at step {step}: {float(loss_value):.4f}\"             )             print(f\"Seen so far: {(step + 1) * batch_size} samples\")      # Display metrics at the end of each epoch.     train_acc = train_acc_metric.result()     print(f\"Training acc over epoch: {float(train_acc):.4f}\")      # Reset training metrics at the end of each epoch     train_acc_metric.reset_state()      # Run a validation loop at the end of each epoch.     for x_batch_val, y_batch_val in val_dataset:         val_logits = model(x_batch_val, training=False)         # Update val metrics         val_acc_metric.update_state(y_batch_val, val_logits)     val_acc = val_acc_metric.result()     val_acc_metric.reset_state()     print(f\"Validation acc: {float(val_acc):.4f}\")     print(f\"Time taken: {time.time() - start_time:.2f}s\")"},{"path":"https://keras.posit.co/articles/guides/writing_a_custom_training_loop_in_tensorflow.html","id":"speeding-up-your-training-step-with-tf-function","dir":"Articles > Guides","previous_headings":"","what":"Speeding-up your training step with tf.function","title":"Writing a training loop from scratch in TensorFlow","text":"default runtime TensorFlow eager execution. , training loop executes eagerly. great debugging, graph compilation definite performance advantage. Describing computation static graph enables framework apply global performance optimizations. impossible framework constrained greedily execute one operation another, knowledge comes next. can compile static graph function takes tensors input. Just add @tf.function decorator , like : Let’s evaluation step: Now, let’s re-run training loop compiled training step: Much faster, isn’t ?","code":"@tf.function def train_step(x, y):     with tf.GradientTape() as tape:         logits = model(x, training=True)         loss_value = loss_fn(y, logits)     grads = tape.gradient(loss_value, model.trainable_weights)     optimizer.apply(grads, model.trainable_weights)     train_acc_metric.update_state(y, logits)     return loss_value @tf.function def test_step(x, y):     val_logits = model(x, training=False)     val_acc_metric.update_state(y, val_logits) epochs = 2 for epoch in range(epochs):     print(f\"\\nStart of epoch {epoch}\")     start_time = time.time()      # Iterate over the batches of the dataset.     for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):         loss_value = train_step(x_batch_train, y_batch_train)          # Log every 100 batches.         if step % 100 == 0:             print(                 f\"Training loss (for 1 batch) at step {step}: {float(loss_value):.4f}\"             )             print(f\"Seen so far: {(step + 1) * batch_size} samples\")      # Display metrics at the end of each epoch.     train_acc = train_acc_metric.result()     print(f\"Training acc over epoch: {float(train_acc):.4f}\")      # Reset training metrics at the end of each epoch     train_acc_metric.reset_state()      # Run a validation loop at the end of each epoch.     for x_batch_val, y_batch_val in val_dataset:         test_step(x_batch_val, y_batch_val)      val_acc = val_acc_metric.result()     val_acc_metric.reset_state()     print(f\"Validation acc: {float(val_acc):.4f}\")     print(f\"Time taken: {time.time() - start_time:.2f}s\")"},{"path":"https://keras.posit.co/articles/guides/writing_a_custom_training_loop_in_tensorflow.html","id":"low-level-handling-of-losses-tracked-by-the-model","dir":"Articles > Guides","previous_headings":"","what":"Low-level handling of losses tracked by the model","title":"Writing a training loop from scratch in TensorFlow","text":"Layers & models recursively track losses created forward pass layers call self.add_loss(value). resulting list scalar loss values available via property model.losses end forward pass. want using loss components, sum add main loss training step. Consider layer, creates activity regularization loss: Let’s build really simple model uses : ’s training step look like now:","code":"class ActivityRegularizationLayer(keras.layers.Layer):     def call(self, inputs):         self.add_loss(1e-2 * tf.reduce_sum(inputs))         return inputs inputs = keras.Input(shape=(784,), name=\"digits\") x = keras.layers.Dense(64, activation=\"relu\")(inputs) # Insert activity regularization as a layer x = ActivityRegularizationLayer()(x) x = keras.layers.Dense(64, activation=\"relu\")(x) outputs = keras.layers.Dense(10, name=\"predictions\")(x)  model = keras.Model(inputs=inputs, outputs=outputs) @tf.function def train_step(x, y):     with tf.GradientTape() as tape:         logits = model(x, training=True)         loss_value = loss_fn(y, logits)         # Add any extra losses created during the forward pass.         loss_value += sum(model.losses)     grads = tape.gradient(loss_value, model.trainable_weights)     optimizer.apply(grads, model.trainable_weights)     train_acc_metric.update_state(y, logits)     return loss_value"},{"path":"https://keras.posit.co/articles/guides/writing_a_custom_training_loop_in_tensorflow.html","id":"summary","dir":"Articles > Guides","previous_headings":"","what":"Summary","title":"Writing a training loop from scratch in TensorFlow","text":"Now know everything know using built-training loops writing scratch. conclude, ’s simple end--end example ties together everything ’ve learned guide: DCGAN trained MNIST digits.","code":""},{"path":"https://keras.posit.co/articles/guides/writing_a_custom_training_loop_in_tensorflow.html","id":"end-to-end-example-a-gan-training-loop-from-scratch","dir":"Articles > Guides","previous_headings":"","what":"End-to-end example: a GAN training loop from scratch","title":"Writing a training loop from scratch in TensorFlow","text":"may familiar Generative Adversarial Networks (GANs). GANs can generate new images look almost real, learning latent distribution training dataset images (“latent space” images). GAN made two parts: “generator” model maps points latent space points image space, “discriminator” model, classifier can tell difference real images (training dataset) fake images (output generator network). GAN training loop looks like : Train discriminator. Sample batch random points latent space. Turn points fake images via “generator” model. Get batch real images combine generated images. Train “discriminator” model classify generated vs. real images. Train generator. Sample random points latent space. Turn points fake images via “generator” network. Get batch real images combine generated images. Train “generator” model “fool” discriminator classify fake images real. much detailed overview GANs works, see Deep Learning Python. Let’s implement training loop. First, create discriminator meant classify fake vs real digits: let’s create generator network, turns latent vectors outputs shape (28, 28, 1) (representing MNIST digits): ’s key bit: training loop. can see quite straightforward. training step function takes 17 lines. Let’s train GAN, repeatedly calling train_step batches images. Since discriminator generator convnets, ’re going want run code GPU. ’s ! ’ll get nice-looking fake MNIST digits just ~30s training Colab GPU.","code":"discriminator = keras.Sequential(     [         keras.Input(shape=(28, 28, 1)),         keras.layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),         keras.layers.LeakyReLU(negative_slope=0.2),         keras.layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),         keras.layers.LeakyReLU(negative_slope=0.2),         keras.layers.GlobalMaxPooling2D(),         keras.layers.Dense(1),     ],     name=\"discriminator\", ) discriminator.summary() latent_dim = 128  generator = keras.Sequential(     [         keras.Input(shape=(latent_dim,)),         # We want to generate 128 coefficients to reshape into a 7x7x128 map         keras.layers.Dense(7 * 7 * 128),         keras.layers.LeakyReLU(negative_slope=0.2),         keras.layers.Reshape((7, 7, 128)),         keras.layers.Conv2DTranspose(             128, (4, 4), strides=(2, 2), padding=\"same\"         ),         keras.layers.LeakyReLU(negative_slope=0.2),         keras.layers.Conv2DTranspose(             128, (4, 4), strides=(2, 2), padding=\"same\"         ),         keras.layers.LeakyReLU(negative_slope=0.2),         keras.layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),     ],     name=\"generator\", ) # Instantiate one optimizer for the discriminator and another for the generator. d_optimizer = keras.optimizers.Adam(learning_rate=0.0003) g_optimizer = keras.optimizers.Adam(learning_rate=0.0004)  # Instantiate a loss function. loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)   @tf.function def train_step(real_images):     # Sample random points in the latent space     random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))     # Decode them to fake images     generated_images = generator(random_latent_vectors)     # Combine them with real images     combined_images = tf.concat([generated_images, real_images], axis=0)      # Assemble labels discriminating real from fake images     labels = tf.concat(         [tf.ones((batch_size, 1)), tf.zeros((real_images.shape[0], 1))], axis=0     )     # Add random noise to the labels - important trick!     labels += 0.05 * tf.random.uniform(labels.shape)      # Train the discriminator     with tf.GradientTape() as tape:         predictions = discriminator(combined_images)         d_loss = loss_fn(labels, predictions)     grads = tape.gradient(d_loss, discriminator.trainable_weights)     d_optimizer.apply(grads, discriminator.trainable_weights)      # Sample random points in the latent space     random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))     # Assemble labels that say \"all real images\"     misleading_labels = tf.zeros((batch_size, 1))      # Train the generator (note that we should *not* update the weights     # of the discriminator)!     with tf.GradientTape() as tape:         predictions = discriminator(generator(random_latent_vectors))         g_loss = loss_fn(misleading_labels, predictions)     grads = tape.gradient(g_loss, generator.trainable_weights)     g_optimizer.apply(grads, generator.trainable_weights)     return d_loss, g_loss, generated_images # Prepare the dataset. We use both the training & test MNIST digits. batch_size = 64 (x_train, _), (x_test, _) = keras.datasets.mnist.load_data() all_digits = np.concatenate([x_train, x_test]) all_digits = all_digits.astype(\"float32\") / 255.0 all_digits = np.reshape(all_digits, (-1, 28, 28, 1)) dataset = tf.data.Dataset.from_tensor_slices(all_digits) dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)  epochs = 1  # In practice you need at least 20 epochs to generate nice digits. save_dir = \"./\"  for epoch in range(epochs):     print(f\"\\nStart epoch {epoch}\")      for step, real_images in enumerate(dataset):         # Train the discriminator & generator on one batch of real images.         d_loss, g_loss, generated_images = train_step(real_images)          # Logging.         if step % 100 == 0:             # Print metrics             print(f\"discriminator loss at step {step}: {d_loss:.2f}\")             print(f\"adversarial loss at step {step}: {g_loss:.2f}\")              # Save one generated image             img = keras.utils.array_to_img(                 generated_images[0] * 255.0, scale=False             )             img.save(os.path.join(save_dir, f\"generated_img_{step}.png\"))          # To limit execution time we stop after 10 steps.         # Remove the lines below to actually train the model!         if step > 10:             break"},{"path":"https://keras.posit.co/articles/guides/writing_a_custom_training_loop_in_torch.html","id":"setup","dir":"Articles > Guides","previous_headings":"","what":"Setup","title":"Writing a training loop from scratch in PyTorch","text":"","code":"import os  # This guide can only be run with the torch backend. os.environ[\"KERAS_BACKEND\"] = \"torch\"  import torch import keras as keras import numpy as np"},{"path":"https://keras.posit.co/articles/guides/writing_a_custom_training_loop_in_torch.html","id":"introduction","dir":"Articles > Guides","previous_headings":"","what":"Introduction","title":"Writing a training loop from scratch in PyTorch","text":"Keras provides default training evaluation loops, fit() evaluate(). usage covered guide Training & evaluation built-methods. want customize learning algorithm model still leveraging convenience fit() (instance, train GAN using fit()), can subclass Model class implement train_step() method, called repeatedly fit(). Now, want low-level control training & evaluation, write training & evaluation loops scratch. guide .","code":""},{"path":"https://keras.posit.co/articles/guides/writing_a_custom_training_loop_in_torch.html","id":"a-first-end-to-end-example","dir":"Articles > Guides","previous_headings":"","what":"A first end-to-end example","title":"Writing a training loop from scratch in PyTorch","text":"write custom training loop, need following ingredients: model train, course. optimizer. either use keras.optimizers optimizer, native PyTorch optimizer torch.optim. loss function. either use keras.losses loss, native PyTorch loss torch.nn. dataset. use format: tf.data.Dataset, PyTorch DataLoader, Python generator, etc. Let’s line . ’ll use torch-native objects case – except, course, Keras model. First, let’s get model MNIST dataset: Next, ’s PyTorch optimizer PyTorch loss function: Let’s train model using mini-batch gradient custom training loop. Calling loss.backward() loss tensor triggers backpropagation. ’s done, optimizer magically aware gradients variable can update variables, done via optimizer.step(). Tensors, variables, optimizers interconnected one another via hidden global state. Also, don’t forget call model.zero_grad() loss.backward(), won’t get right gradients variables. ’s training loop, step step: open loop iterates epochs epoch, open loop iterates dataset, batches batch, call model input data retrive predictions, use compute loss value call loss.backward() Outside scope, retrieve gradients weights model regard loss Finally, use optimizer update weights model based gradients alternative, let’s look loop looks like using Keras optimizer Keras loss function. Important differences: retrieve gradients variables via v.value.grad, called trainable variable. update variables via optimizer.apply(), must called torch.no_grad() scope. Also, big gotcha: NumPy/TensorFlow/JAX/Keras APIs well Python unittest APIs use argument order convention fn(y_true, y_pred) (reference values first, predicted values second), PyTorch actually uses fn(y_pred, y_true) losses. make sure invert order logits targets.","code":"# Let's consider a simple MNIST model def get_model():     inputs = keras.Input(shape=(784,), name=\"digits\")     x1 = keras.layers.Dense(64, activation=\"relu\")(inputs)     x2 = keras.layers.Dense(64, activation=\"relu\")(x1)     outputs = keras.layers.Dense(10, name=\"predictions\")(x2)     model = keras.Model(inputs=inputs, outputs=outputs)     return model   # Create load up the MNIST dataset and put it in a torch DataLoader # Prepare the training dataset. batch_size = 32 (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data() x_train = np.reshape(x_train, (-1, 784)).astype(\"float32\") x_test = np.reshape(x_test, (-1, 784)).astype(\"float32\") y_train = keras.utils.to_categorical(y_train) y_test = keras.utils.to_categorical(y_test)  # Reserve 10,000 samples for validation. x_val = x_train[-10000:] y_val = y_train[-10000:] x_train = x_train[:-10000] y_train = y_train[:-10000]  # Create torch Datasets train_dataset = torch.utils.data.TensorDataset(     torch.from_numpy(x_train), torch.from_numpy(y_train) ) val_dataset = torch.utils.data.TensorDataset(     torch.from_numpy(x_val), torch.from_numpy(y_val) )  # Create DataLoaders for the Datasets train_dataloader = torch.utils.data.DataLoader(     train_dataset, batch_size=batch_size, shuffle=True ) val_dataloader = torch.utils.data.DataLoader(     val_dataset, batch_size=batch_size, shuffle=False ) # Instantiate a torch optimizer model = get_model() optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # Instantiate a torch loss function loss_fn = torch.nn.CrossEntropyLoss() epochs = 3 for epoch in range(epochs):     for step, (inputs, targets) in enumerate(train_dataloader):         # Forward pass         logits = model(inputs)         loss = loss_fn(logits, targets)          # Backward pass         model.zero_grad()         loss.backward()          # Optimizer variable updates         optimizer.step()          # Log every 100 batches.         if step % 100 == 0:             print(                 f\"Training loss (for 1 batch) at step {step}: {loss.detach().numpy():.4f}\"             )             print(f\"Seen so far: {(step + 1) * batch_size} samples\") model = get_model() optimizer = keras.optimizers.Adam(learning_rate=1e-3) loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)  for epoch in range(epochs):     print(f\"\\nStart of epoch {epoch}\")     for step, (inputs, targets) in enumerate(train_dataloader):         # Forward pass         logits = model(inputs)         loss = loss_fn(targets, logits)          # Backward pass         model.zero_grad()         trainable_weights = [v for v in model.trainable_weights]          # Call torch.Tensor.backward() on the loss to compute gradients         # for the weights.         loss.backward()         gradients = [v.value.grad for v in trainable_weights]          # Update weights         with torch.no_grad():             optimizer.apply(gradients, trainable_weights)          # Log every 100 batches.         if step % 100 == 0:             print(                 f\"Training loss (for 1 batch) at step {step}: {loss.detach().numpy():.4f}\"             )             print(f\"Seen so far: {(step + 1) * batch_size} samples\")"},{"path":"https://keras.posit.co/articles/guides/writing_a_custom_training_loop_in_torch.html","id":"low-level-handling-of-metrics","dir":"Articles > Guides","previous_headings":"","what":"Low-level handling of metrics","title":"Writing a training loop from scratch in PyTorch","text":"Let’s add metrics monitoring basic training loop. can readily reuse built-Keras metrics (custom ones wrote) training loops written scratch. ’s flow: Instantiate metric start loop Call metric.update_state() batch Call metric.result() need display current value metric Call metric.reset_state() need clear state metric (typically end epoch) Let’s use knowledge compute CategoricalAccuracy training validation data end epoch: ’s training & evaluation loop:","code":"# Get a fresh model model = get_model()  # Instantiate an optimizer to train the model. optimizer = keras.optimizers.Adam(learning_rate=1e-3) # Instantiate a loss function. loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)  # Prepare the metrics. train_acc_metric = keras.metrics.CategoricalAccuracy() val_acc_metric = keras.metrics.CategoricalAccuracy() for epoch in range(epochs):     print(f\"\\nStart of epoch {epoch}\")     for step, (inputs, targets) in enumerate(train_dataloader):         # Forward pass         logits = model(inputs)         loss = loss_fn(targets, logits)          # Backward pass         model.zero_grad()         trainable_weights = [v for v in model.trainable_weights]          # Call torch.Tensor.backward() on the loss to compute gradients         # for the weights.         loss.backward()         gradients = [v.value.grad for v in trainable_weights]          # Update weights         with torch.no_grad():             optimizer.apply(gradients, trainable_weights)          # Update training metric.         train_acc_metric.update_state(targets, logits)          # Log every 100 batches.         if step % 100 == 0:             print(                 f\"Training loss (for 1 batch) at step {step}: {loss.detach().numpy():.4f}\"             )             print(f\"Seen so far: {(step + 1) * batch_size} samples\")      # Display metrics at the end of each epoch.     train_acc = train_acc_metric.result()     print(f\"Training acc over epoch: {float(train_acc):.4f}\")      # Reset training metrics at the end of each epoch     train_acc_metric.reset_state()      # Run a validation loop at the end of each epoch.     for x_batch_val, y_batch_val in val_dataloader:         val_logits = model(x_batch_val, training=False)         # Update val metrics         val_acc_metric.update_state(y_batch_val, val_logits)     val_acc = val_acc_metric.result()     val_acc_metric.reset_state()     print(f\"Validation acc: {float(val_acc):.4f}\")"},{"path":"https://keras.posit.co/articles/guides/writing_a_custom_training_loop_in_torch.html","id":"low-level-handling-of-losses-tracked-by-the-model","dir":"Articles > Guides","previous_headings":"","what":"Low-level handling of losses tracked by the model","title":"Writing a training loop from scratch in PyTorch","text":"Layers & models recursively track losses created forward pass layers call self.add_loss(value). resulting list scalar loss values available via property model.losses end forward pass. want using loss components, sum add main loss training step. Consider layer, creates activity regularization loss: Let’s build really simple model uses : ’s training loop look like now: ’s !","code":"class ActivityRegularizationLayer(keras.layers.Layer):     def call(self, inputs):         self.add_loss(1e-2 * torch.sum(inputs))         return inputs inputs = keras.Input(shape=(784,), name=\"digits\") x = keras.layers.Dense(64, activation=\"relu\")(inputs) # Insert activity regularization as a layer x = ActivityRegularizationLayer()(x) x = keras.layers.Dense(64, activation=\"relu\")(x) outputs = keras.layers.Dense(10, name=\"predictions\")(x)  model = keras.Model(inputs=inputs, outputs=outputs) # Get a fresh model model = get_model()  # Instantiate an optimizer to train the model. optimizer = keras.optimizers.Adam(learning_rate=1e-3) # Instantiate a loss function. loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)  # Prepare the metrics. train_acc_metric = keras.metrics.CategoricalAccuracy() val_acc_metric = keras.metrics.CategoricalAccuracy()  for epoch in range(epochs):     print(f\"\\nStart of epoch {epoch}\")     for step, (inputs, targets) in enumerate(train_dataloader):         # Forward pass         logits = model(inputs)         loss = loss_fn(targets, logits)         if model.losses:             loss = loss + torch.sum(*model.losses)          # Backward pass         model.zero_grad()         trainable_weights = [v for v in model.trainable_weights]          # Call torch.Tensor.backward() on the loss to compute gradients         # for the weights.         loss.backward()         gradients = [v.value.grad for v in trainable_weights]          # Update weights         with torch.no_grad():             optimizer.apply(gradients, trainable_weights)          # Update training metric.         train_acc_metric.update_state(targets, logits)          # Log every 100 batches.         if step % 100 == 0:             print(                 f\"Training loss (for 1 batch) at step {step}: {loss.detach().numpy():.4f}\"             )             print(f\"Seen so far: {(step + 1) * batch_size} samples\")      # Display metrics at the end of each epoch.     train_acc = train_acc_metric.result()     print(f\"Training acc over epoch: {float(train_acc):.4f}\")      # Reset training metrics at the end of each epoch     train_acc_metric.reset_state()      # Run a validation loop at the end of each epoch.     for x_batch_val, y_batch_val in val_dataloader:         val_logits = model(x_batch_val, training=False)         # Update val metrics         val_acc_metric.update_state(y_batch_val, val_logits)     val_acc = val_acc_metric.result()     val_acc_metric.reset_state()     print(f\"Validation acc: {float(val_acc):.4f}\")"},{"path":"https://keras.posit.co/articles/guides/writing_a_training_loop_from_scratch.html","id":"setup","dir":"Articles > Guides","previous_headings":"","what":"Setup","title":"Writing a training loop from scratch","text":"","code":"import tensorflow as tf import keras from keras import layers import numpy as np"},{"path":"https://keras.posit.co/articles/guides/writing_a_training_loop_from_scratch.html","id":"introduction","dir":"Articles > Guides","previous_headings":"","what":"Introduction","title":"Writing a training loop from scratch","text":"Keras provides default training evaluation loops, fit() evaluate(). usage covered guide Training & evaluation built-methods. want customize learning algorithm model still leveraging convenience fit() (instance, train GAN using fit()), can subclass Model class implement train_step() method, called repeatedly fit(). covered guide Customizing happens fit(). Now, want low-level control training & evaluation, write training & evaluation loops scratch. guide .","code":""},{"path":"https://keras.posit.co/articles/guides/writing_a_training_loop_from_scratch.html","id":"using-the-gradienttape-a-first-end-to-end-example","dir":"Articles > Guides","previous_headings":"","what":"Using the GradientTape: a first end-to-end example","title":"Writing a training loop from scratch","text":"Calling model inside GradientTape scope enables retrieve gradients trainable weights layer respect loss value. Using optimizer instance, can use gradients update variables (can retrieve using model.trainable_weights). Let’s consider simple MNIST model: Let’s train using mini-batch gradient custom training loop. First, ’re going need optimizer, loss function, dataset: ’s training loop: open loop iterates epochs epoch, open loop iterates dataset, batches batch, open GradientTape() scope Inside scope, call model (forward pass) compute loss Outside scope, retrieve gradients weights model regard loss Finally, use optimizer update weights model based gradients","code":"inputs = keras.Input(shape=(784,), name=\"digits\") x1 = layers.Dense(64, activation=\"relu\")(inputs) x2 = layers.Dense(64, activation=\"relu\")(x1) outputs = layers.Dense(10, name=\"predictions\")(x2) model = keras.Model(inputs=inputs, outputs=outputs) # Instantiate an optimizer. optimizer = keras.optimizers.SGD(learning_rate=1e-3) # Instantiate a loss function. loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Prepare the training dataset. batch_size = 64 (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data() x_train = np.reshape(x_train, (-1, 784)) x_test = np.reshape(x_test, (-1, 784))  # Reserve 10,000 samples for validation. x_val = x_train[-10000:] y_val = y_train[-10000:] x_train = x_train[:-10000] y_train = y_train[:-10000]  # Prepare the training dataset. train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)) train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)  # Prepare the validation dataset. val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)) val_dataset = val_dataset.batch(batch_size) epochs = 2 for epoch in range(epochs):     print(\"\\nStart of epoch %d\" % (epoch,))      # Iterate over the batches of the dataset.     for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):         # Open a GradientTape to record the operations run         # during the forward pass, which enables auto-differentiation.         with tf.GradientTape() as tape:             # Run the forward pass of the layer.             # The operations that the layer applies             # to its inputs are going to be recorded             # on the GradientTape.             logits = model(x_batch_train, training=True)  # Logits for this minibatch              # Compute the loss value for this minibatch.             loss_value = loss_fn(y_batch_train, logits)          # Use the gradient tape to automatically retrieve         # the gradients of the trainable variables with respect to the loss.         grads = tape.gradient(loss_value, model.trainable_weights)          # Run one step of gradient descent by updating         # the value of the variables to minimize the loss.         optimizer.apply_gradients(zip(grads, model.trainable_weights))          # Log every 200 batches.         if step % 200 == 0:             print(                 \"Training loss (for one batch) at step %d: %.4f\"                 % (step, float(loss_value))             )             print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))"},{"path":"https://keras.posit.co/articles/guides/writing_a_training_loop_from_scratch.html","id":"low-level-handling-of-metrics","dir":"Articles > Guides","previous_headings":"","what":"Low-level handling of metrics","title":"Writing a training loop from scratch","text":"Let’s add metrics monitoring basic loop. can readily reuse built-metrics (custom ones wrote) training loops written scratch. ’s flow: Instantiate metric start loop Call metric.update_state() batch Call metric.result() need display current value metric Call metric.reset_states() need clear state metric (typically end epoch) Let’s use knowledge compute SparseCategoricalAccuracy validation data end epoch: ’s training & evaluation loop:","code":"# Get model inputs = keras.Input(shape=(784,), name=\"digits\") x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs) x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x) outputs = layers.Dense(10, name=\"predictions\")(x) model = keras.Model(inputs=inputs, outputs=outputs)  # Instantiate an optimizer to train the model. optimizer = keras.optimizers.SGD(learning_rate=1e-3) # Instantiate a loss function. loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Prepare the metrics. train_acc_metric = keras.metrics.SparseCategoricalAccuracy() val_acc_metric = keras.metrics.SparseCategoricalAccuracy() import time  epochs = 2 for epoch in range(epochs):     print(\"\\nStart of epoch %d\" % (epoch,))     start_time = time.time()      # Iterate over the batches of the dataset.     for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):         with tf.GradientTape() as tape:             logits = model(x_batch_train, training=True)             loss_value = loss_fn(y_batch_train, logits)         grads = tape.gradient(loss_value, model.trainable_weights)         optimizer.apply_gradients(zip(grads, model.trainable_weights))          # Update training metric.         train_acc_metric.update_state(y_batch_train, logits)          # Log every 200 batches.         if step % 200 == 0:             print(                 \"Training loss (for one batch) at step %d: %.4f\"                 % (step, float(loss_value))             )             print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))      # Display metrics at the end of each epoch.     train_acc = train_acc_metric.result()     print(\"Training acc over epoch: %.4f\" % (float(train_acc),))      # Reset training metrics at the end of each epoch     train_acc_metric.reset_states()      # Run a validation loop at the end of each epoch.     for x_batch_val, y_batch_val in val_dataset:         val_logits = model(x_batch_val, training=False)         # Update val metrics         val_acc_metric.update_state(y_batch_val, val_logits)     val_acc = val_acc_metric.result()     val_acc_metric.reset_states()     print(\"Validation acc: %.4f\" % (float(val_acc),))     print(\"Time taken: %.2fs\" % (time.time() - start_time))"},{"path":"https://keras.posit.co/articles/guides/writing_a_training_loop_from_scratch.html","id":"speeding-up-your-training-step-with-tf-function","dir":"Articles > Guides","previous_headings":"","what":"Speeding-up your training step with tf.function","title":"Writing a training loop from scratch","text":"default runtime TensorFlow 2 eager execution. , training loop executes eagerly. great debugging, graph compilation definite performance advantage. Describing computation static graph enables framework apply global performance optimizations. impossible framework constrained greedily execute one operation another, knowledge comes next. can compile static graph function takes tensors input. Just add @tf.function decorator , like : Let’s evaluation step: Now, let’s re-run training loop compiled training step: Much faster, isn’t ?","code":"@tf.function def train_step(x, y):     with tf.GradientTape() as tape:         logits = model(x, training=True)         loss_value = loss_fn(y, logits)     grads = tape.gradient(loss_value, model.trainable_weights)     optimizer.apply_gradients(zip(grads, model.trainable_weights))     train_acc_metric.update_state(y, logits)     return loss_value @tf.function def test_step(x, y):     val_logits = model(x, training=False)     val_acc_metric.update_state(y, val_logits) import time  epochs = 2 for epoch in range(epochs):     print(\"\\nStart of epoch %d\" % (epoch,))     start_time = time.time()      # Iterate over the batches of the dataset.     for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):         loss_value = train_step(x_batch_train, y_batch_train)          # Log every 200 batches.         if step % 200 == 0:             print(                 \"Training loss (for one batch) at step %d: %.4f\"                 % (step, float(loss_value))             )             print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))      # Display metrics at the end of each epoch.     train_acc = train_acc_metric.result()     print(\"Training acc over epoch: %.4f\" % (float(train_acc),))      # Reset training metrics at the end of each epoch     train_acc_metric.reset_states()      # Run a validation loop at the end of each epoch.     for x_batch_val, y_batch_val in val_dataset:         test_step(x_batch_val, y_batch_val)      val_acc = val_acc_metric.result()     val_acc_metric.reset_states()     print(\"Validation acc: %.4f\" % (float(val_acc),))     print(\"Time taken: %.2fs\" % (time.time() - start_time))"},{"path":"https://keras.posit.co/articles/guides/writing_a_training_loop_from_scratch.html","id":"low-level-handling-of-losses-tracked-by-the-model","dir":"Articles > Guides","previous_headings":"","what":"Low-level handling of losses tracked by the model","title":"Writing a training loop from scratch","text":"Layers & models recursively track losses created forward pass layers call self.add_loss(value). resulting list scalar loss values available via property model.losses end forward pass. want using loss components, sum add main loss training step. Consider layer, creates activity regularization loss: Let’s build really simple model uses : ’s training step look like now:","code":"@keras.saving.register_keras_serializable() class ActivityRegularizationLayer(layers.Layer):     def call(self, inputs):         self.add_loss(0.1 * tf.reduce_mean(inputs))         return inputs inputs = keras.Input(shape=(784,), name=\"digits\") x = layers.Dense(64, activation=\"relu\")(inputs) # Insert activity regularization as a layer x = ActivityRegularizationLayer()(x) x = layers.Dense(64, activation=\"relu\")(x) outputs = layers.Dense(10, name=\"predictions\")(x)  model = keras.Model(inputs=inputs, outputs=outputs) @tf.function def train_step(x, y):     with tf.GradientTape() as tape:         logits = model(x, training=True)         loss_value = loss_fn(y, logits)         # Add any extra losses created during the forward pass.         loss_value += sum(model.losses)     grads = tape.gradient(loss_value, model.trainable_weights)     optimizer.apply_gradients(zip(grads, model.trainable_weights))     train_acc_metric.update_state(y, logits)     return loss_value"},{"path":"https://keras.posit.co/articles/guides/writing_a_training_loop_from_scratch.html","id":"summary","dir":"Articles > Guides","previous_headings":"","what":"Summary","title":"Writing a training loop from scratch","text":"Now know everything know using built-training loops writing scratch. conclude, ’s simple end--end example ties together everything ’ve learned guide: DCGAN trained MNIST digits.","code":""},{"path":"https://keras.posit.co/articles/guides/writing_a_training_loop_from_scratch.html","id":"end-to-end-example-a-gan-training-loop-from-scratch","dir":"Articles > Guides","previous_headings":"","what":"End-to-end example: a GAN training loop from scratch","title":"Writing a training loop from scratch","text":"may familiar Generative Adversarial Networks (GANs). GANs can generate new images look almost real, learning latent distribution training dataset images (“latent space” images). GAN made two parts: “generator” model maps points latent space points image space, “discriminator” model, classifier can tell difference real images (training dataset) fake images (output generator network). GAN training loop looks like : Train discriminator. Sample batch random points latent space. Turn points fake images via “generator” model. Get batch real images combine generated images. Train “discriminator” model classify generated vs. real images. Train generator. Sample random points latent space. Turn points fake images via “generator” network. Get batch real images combine generated images. Train “generator” model “fool” discriminator classify fake images real. much detailed overview GANs works, see Deep Learning Python. Let’s implement training loop. First, create discriminator meant classify fake vs real digits: let’s create generator network, turns latent vectors outputs shape (28, 28, 1) (representing MNIST digits): ’s key bit: training loop. can see quite straightforward. training step function takes 17 lines. Let’s train GAN, repeatedly calling train_step batches images. Since discriminator generator convnets, ’re going want run code GPU. ’s ! ’ll get nice-looking fake MNIST digits just ~30s training Colab GPU.","code":"discriminator = keras.Sequential(     [         keras.Input(shape=(28, 28, 1)),         layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),         layers.LeakyReLU(alpha=0.2),         layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),         layers.LeakyReLU(alpha=0.2),         layers.GlobalMaxPooling2D(),         layers.Dense(1),     ],     name=\"discriminator\", ) discriminator.summary() latent_dim = 128  generator = keras.Sequential(     [         keras.Input(shape=(latent_dim,)),         # We want to generate 128 coefficients to reshape into a 7x7x128 map         layers.Dense(7 * 7 * 128),         layers.LeakyReLU(alpha=0.2),         layers.Reshape((7, 7, 128)),         layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),         layers.LeakyReLU(alpha=0.2),         layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),         layers.LeakyReLU(alpha=0.2),         layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),     ],     name=\"generator\", ) # Instantiate one optimizer for the discriminator and another for the generator. d_optimizer = keras.optimizers.Adam(learning_rate=0.0003) g_optimizer = keras.optimizers.Adam(learning_rate=0.0004)  # Instantiate a loss function. loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)   @tf.function def train_step(real_images):     # Sample random points in the latent space     random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))     # Decode them to fake images     generated_images = generator(random_latent_vectors)     # Combine them with real images     combined_images = tf.concat([generated_images, real_images], axis=0)      # Assemble labels discriminating real from fake images     labels = tf.concat(         [tf.ones((batch_size, 1)), tf.zeros((real_images.shape[0], 1))], axis=0     )     # Add random noise to the labels - important trick!     labels += 0.05 * tf.random.uniform(labels.shape)      # Train the discriminator     with tf.GradientTape() as tape:         predictions = discriminator(combined_images)         d_loss = loss_fn(labels, predictions)     grads = tape.gradient(d_loss, discriminator.trainable_weights)     d_optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))      # Sample random points in the latent space     random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))     # Assemble labels that say \"all real images\"     misleading_labels = tf.zeros((batch_size, 1))      # Train the generator (note that we should *not* update the weights     # of the discriminator)!     with tf.GradientTape() as tape:         predictions = discriminator(generator(random_latent_vectors))         g_loss = loss_fn(misleading_labels, predictions)     grads = tape.gradient(g_loss, generator.trainable_weights)     g_optimizer.apply_gradients(zip(grads, generator.trainable_weights))     return d_loss, g_loss, generated_images import os  # Prepare the dataset. We use both the training & test MNIST digits. batch_size = 64 (x_train, _), (x_test, _) = keras.datasets.mnist.load_data() all_digits = np.concatenate([x_train, x_test]) all_digits = all_digits.astype(\"float32\") / 255.0 all_digits = np.reshape(all_digits, (-1, 28, 28, 1)) dataset = tf.data.Dataset.from_tensor_slices(all_digits) dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)  epochs = 1  # In practice you need at least 20 epochs to generate nice digits. save_dir = \"./\"  for epoch in range(epochs):     print(\"\\nStart epoch\", epoch)      for step, real_images in enumerate(dataset):         # Train the discriminator & generator on one batch of real images.         d_loss, g_loss, generated_images = train_step(real_images)          # Logging.         if step % 200 == 0:             # Print metrics             print(\"discriminator loss at step %d: %.2f\" % (step, d_loss))             print(\"adversarial loss at step %d: %.2f\" % (step, g_loss))              # Save one generated image             img = keras.utils.array_to_img(generated_images[0] * 255.0, scale=False)             img.save(os.path.join(save_dir, \"generated_img\" + str(step) + \".png\"))          # To limit execution time we stop after 10 steps.         # Remove the lines below to actually train the model!         if step > 10:             break"},{"path":"https://keras.posit.co/articles/guides/writing_your_own_callbacks.html","id":"introduction","dir":"Articles > Guides","previous_headings":"","what":"Introduction","title":"Writing your own callbacks","text":"callback powerful tool customize behavior Keras model training, evaluation, inference. Examples include keras.callbacks.TensorBoard visualize training progress results TensorBoard, keras.callbacks.ModelCheckpoint periodically save model training. guide, learn Keras callback , can , can build . provide demos simple callback applications get started.","code":""},{"path":"https://keras.posit.co/articles/guides/writing_your_own_callbacks.html","id":"setup","dir":"Articles > Guides","previous_headings":"","what":"Setup","title":"Writing your own callbacks","text":"","code":"import numpy as np import keras as keras"},{"path":"https://keras.posit.co/articles/guides/writing_your_own_callbacks.html","id":"keras-callbacks-overview","dir":"Articles > Guides","previous_headings":"","what":"Keras callbacks overview","title":"Writing your own callbacks","text":"callbacks subclass keras.callbacks.Callback class, override set methods called various stages training, testing, predicting. Callbacks useful get view internal states statistics model training. can pass list callbacks (keyword argument callbacks) following model methods: keras.Model.fit() keras.Model.evaluate() keras.Model.predict()","code":""},{"path":[]},{"path":[]},{"path":"https://keras.posit.co/articles/guides/writing_your_own_callbacks.html","id":"on_traintestpredict_beginself-logsnone","dir":"Articles > Guides","previous_headings":"An overview of callback methods > Global methods","what":"on_(train|test|predict)_begin(self, logs=None)","title":"Writing your own callbacks","text":"Called beginning fit/evaluate/predict.","code":""},{"path":"https://keras.posit.co/articles/guides/writing_your_own_callbacks.html","id":"on_traintestpredict_endself-logsnone","dir":"Articles > Guides","previous_headings":"An overview of callback methods > Global methods","what":"on_(train|test|predict)_end(self, logs=None)","title":"Writing your own callbacks","text":"Called end fit/evaluate/predict.","code":""},{"path":[]},{"path":"https://keras.posit.co/articles/guides/writing_your_own_callbacks.html","id":"on_traintestpredict_batch_beginself-batch-logsnone","dir":"Articles > Guides","previous_headings":"An overview of callback methods > Batch-level methods for training/testing/predicting","what":"on_(train|test|predict)_batch_begin(self, batch, logs=None)","title":"Writing your own callbacks","text":"Called right processing batch training/testing/predicting.","code":""},{"path":"https://keras.posit.co/articles/guides/writing_your_own_callbacks.html","id":"on_traintestpredict_batch_endself-batch-logsnone","dir":"Articles > Guides","previous_headings":"An overview of callback methods > Batch-level methods for training/testing/predicting","what":"on_(train|test|predict)_batch_end(self, batch, logs=None)","title":"Writing your own callbacks","text":"Called end training/testing/predicting batch. Within method, logs dict containing metrics results.","code":""},{"path":[]},{"path":"https://keras.posit.co/articles/guides/writing_your_own_callbacks.html","id":"on_epoch_beginself-epoch-logsnone","dir":"Articles > Guides","previous_headings":"An overview of callback methods > Epoch-level methods (training only)","what":"on_epoch_begin(self, epoch, logs=None)","title":"Writing your own callbacks","text":"Called beginning epoch training.","code":""},{"path":"https://keras.posit.co/articles/guides/writing_your_own_callbacks.html","id":"on_epoch_endself-epoch-logsnone","dir":"Articles > Guides","previous_headings":"An overview of callback methods > Epoch-level methods (training only)","what":"on_epoch_end(self, epoch, logs=None)","title":"Writing your own callbacks","text":"Called end epoch training.","code":""},{"path":"https://keras.posit.co/articles/guides/writing_your_own_callbacks.html","id":"a-basic-example","dir":"Articles > Guides","previous_headings":"","what":"A basic example","title":"Writing your own callbacks","text":"Let’s take look concrete example. get started, let’s import tensorflow define simple Sequential Keras model: , load MNIST data training testing Keras datasets API: Now, define simple custom callback logs: fit/evaluate/predict starts & ends epoch starts & ends training batch starts & ends evaluation (test) batch starts & ends inference (prediction) batch starts & ends Let’s try :","code":"# Define the Keras model to add callbacks to def get_model():     model = keras.Sequential()     model.add(keras.layers.Dense(1))     model.compile(         optimizer=keras.optimizers.RMSprop(learning_rate=0.1),         loss=\"mean_squared_error\",         metrics=[\"mean_absolute_error\"],     )     return model # Load example MNIST data and pre-process it (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data() x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255.0 x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255.0  # Limit the data to 1000 samples x_train = x_train[:1000] y_train = y_train[:1000] x_test = x_test[:1000] y_test = y_test[:1000] class CustomCallback(keras.callbacks.Callback):     def on_train_begin(self, logs=None):         keys = list(logs.keys())         print(\"Starting training; got log keys: {}\".format(keys))      def on_train_end(self, logs=None):         keys = list(logs.keys())         print(\"Stop training; got log keys: {}\".format(keys))      def on_epoch_begin(self, epoch, logs=None):         keys = list(logs.keys())         print(             \"Start epoch {} of training; got log keys: {}\".format(epoch, keys)         )      def on_epoch_end(self, epoch, logs=None):         keys = list(logs.keys())         print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))      def on_test_begin(self, logs=None):         keys = list(logs.keys())         print(\"Start testing; got log keys: {}\".format(keys))      def on_test_end(self, logs=None):         keys = list(logs.keys())         print(\"Stop testing; got log keys: {}\".format(keys))      def on_predict_begin(self, logs=None):         keys = list(logs.keys())         print(\"Start predicting; got log keys: {}\".format(keys))      def on_predict_end(self, logs=None):         keys = list(logs.keys())         print(\"Stop predicting; got log keys: {}\".format(keys))      def on_train_batch_begin(self, batch, logs=None):         keys = list(logs.keys())         print(             \"...Training: start of batch {}; got log keys: {}\".format(                 batch, keys             )         )      def on_train_batch_end(self, batch, logs=None):         keys = list(logs.keys())         print(             \"...Training: end of batch {}; got log keys: {}\".format(batch, keys)         )      def on_test_batch_begin(self, batch, logs=None):         keys = list(logs.keys())         print(             \"...Evaluating: start of batch {}; got log keys: {}\".format(                 batch, keys             )         )      def on_test_batch_end(self, batch, logs=None):         keys = list(logs.keys())         print(             \"...Evaluating: end of batch {}; got log keys: {}\".format(                 batch, keys             )         )      def on_predict_batch_begin(self, batch, logs=None):         keys = list(logs.keys())         print(             \"...Predicting: start of batch {}; got log keys: {}\".format(                 batch, keys             )         )      def on_predict_batch_end(self, batch, logs=None):         keys = list(logs.keys())         print(             \"...Predicting: end of batch {}; got log keys: {}\".format(                 batch, keys             )         ) model = get_model() model.fit(     x_train,     y_train,     batch_size=128,     epochs=1,     verbose=0,     validation_split=0.5,     callbacks=[CustomCallback()], )  res = model.evaluate(     x_test, y_test, batch_size=128, verbose=0, callbacks=[CustomCallback()] )  res = model.predict(x_test, batch_size=128, callbacks=[CustomCallback()])"},{"path":"https://keras.posit.co/articles/guides/writing_your_own_callbacks.html","id":"usage-of-logs-dict","dir":"Articles > Guides","previous_headings":"A basic example","what":"Usage of logs dict","title":"Writing your own callbacks","text":"logs dict contains loss value, metrics end batch epoch. Example includes loss mean absolute error.","code":"class LossAndErrorPrintingCallback(keras.callbacks.Callback):     def on_train_batch_end(self, batch, logs=None):         print(             \"Up to batch {}, the average loss is {:7.2f}.\".format(                 batch, logs[\"loss\"]             )         )      def on_test_batch_end(self, batch, logs=None):         print(             \"Up to batch {}, the average loss is {:7.2f}.\".format(                 batch, logs[\"loss\"]             )         )      def on_epoch_end(self, epoch, logs=None):         print(             \"The average loss for epoch {} is {:7.2f} \"             \"and mean absolute error is {:7.2f}.\".format(                 epoch, logs[\"loss\"], logs[\"mean_absolute_error\"]             )         )   model = get_model() model.fit(     x_train,     y_train,     batch_size=128,     epochs=2,     verbose=0,     callbacks=[LossAndErrorPrintingCallback()], )  res = model.evaluate(     x_test,     y_test,     batch_size=128,     verbose=0,     callbacks=[LossAndErrorPrintingCallback()], )"},{"path":"https://keras.posit.co/articles/guides/writing_your_own_callbacks.html","id":"usage-of-self-model-attribute","dir":"Articles > Guides","previous_headings":"","what":"Usage of self.model attribute","title":"Writing your own callbacks","text":"addition receiving log information one methods called, callbacks access model associated current round training/evaluation/inference: self.model. things can self.model callback: Set self.model.stop_training = True immediately interrupt training. Mutate hyperparameters optimizer (available self.model.optimizer), self.model.optimizer.learning_rate. Save model period intervals. Record output model.predict() test samples end epoch, use sanity check training. Extract visualizations intermediate features end epoch, monitor model learning time. etc. Let’s see action couple examples.","code":""},{"path":[]},{"path":"https://keras.posit.co/articles/guides/writing_your_own_callbacks.html","id":"early-stopping-at-minimum-loss","dir":"Articles > Guides","previous_headings":"Examples of Keras callback applications","what":"Early stopping at minimum loss","title":"Writing your own callbacks","text":"first example shows creation Callback stops training minimum loss reached, setting attribute self.model.stop_training (boolean). Optionally, can provide argument patience specify many epochs wait stopping reached local minimum. keras.callbacks.EarlyStopping provides complete general implementation.","code":"class EarlyStoppingAtMinLoss(keras.callbacks.Callback):     \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.      Arguments:         patience: Number of epochs to wait after min has been hit. After this         number of no improvement, training stops.     \"\"\"      def __init__(self, patience=0):         super().__init__()         self.patience = patience         # best_weights to store the weights at which the minimum loss occurs.         self.best_weights = None      def on_train_begin(self, logs=None):         # The number of epoch it has waited when loss is no longer minimum.         self.wait = 0         # The epoch the training stops at.         self.stopped_epoch = 0         # Initialize the best as infinity.         self.best = np.Inf      def on_epoch_end(self, epoch, logs=None):         current = logs.get(\"loss\")         if np.less(current, self.best):             self.best = current             self.wait = 0             # Record the best weights if current results is better (less).             self.best_weights = self.model.get_weights()         else:             self.wait += 1             if self.wait >= self.patience:                 self.stopped_epoch = epoch                 self.model.stop_training = True                 print(\"Restoring model weights from the end of the best epoch.\")                 self.model.set_weights(self.best_weights)      def on_train_end(self, logs=None):         if self.stopped_epoch > 0:             print(f\"Epoch {self.stopped_epoch + 1}: early stopping\")   model = get_model() model.fit(     x_train,     y_train,     batch_size=64,     epochs=30,     verbose=0,     callbacks=[LossAndErrorPrintingCallback(), EarlyStoppingAtMinLoss()], )"},{"path":"https://keras.posit.co/articles/guides/writing_your_own_callbacks.html","id":"learning-rate-scheduling","dir":"Articles > Guides","previous_headings":"Examples of Keras callback applications","what":"Learning rate scheduling","title":"Writing your own callbacks","text":"example, show custom Callback can used dynamically change learning rate optimizer course training. See callbacks.LearningRateScheduler general implementations.","code":"class CustomLearningRateScheduler(keras.callbacks.Callback):     \"\"\"Learning rate scheduler which sets the learning rate according to schedule.      Arguments:         schedule: a function that takes an epoch index             (integer, indexed from 0) and current learning rate             as inputs and returns a new learning rate as output (float).     \"\"\"      def __init__(self, schedule):         super().__init__()         self.schedule = schedule      def on_epoch_begin(self, epoch, logs=None):         if not hasattr(self.model.optimizer, \"learning_rate\"):             raise ValueError('Optimizer must have a \"learning_rate\" attribute.')         # Get the current learning rate from model's optimizer.         lr = self.model.optimizer.learning_rate         # Call schedule function to get the scheduled learning rate.         scheduled_lr = self.schedule(epoch, lr)         # Set the value back to the optimizer before this epoch starts         self.model.optimizer.learning_rate = scheduled_lr         print(             f\"\\nEpoch {epoch}: Learning rate is {float(np.array(scheduled_lr))}.\"         )   LR_SCHEDULE = [     # (epoch to start, learning rate) tuples     (3, 0.05),     (6, 0.01),     (9, 0.005),     (12, 0.001), ]   def lr_schedule(epoch, lr):     \"\"\"Helper function to retrieve the scheduled learning rate based on epoch.\"\"\"     if epoch < LR_SCHEDULE[0][0] or epoch > LR_SCHEDULE[-1][0]:         return lr     for i in range(len(LR_SCHEDULE)):         if epoch == LR_SCHEDULE[i][0]:             return LR_SCHEDULE[i][1]     return lr   model = get_model() model.fit(     x_train,     y_train,     batch_size=64,     epochs=15,     verbose=0,     callbacks=[         LossAndErrorPrintingCallback(),         CustomLearningRateScheduler(lr_schedule),     ], )"},{"path":"https://keras.posit.co/articles/guides/writing_your_own_callbacks.html","id":"built-in-keras-callbacks","dir":"Articles > Guides","previous_headings":"Examples of Keras callback applications","what":"Built-in Keras callbacks","title":"Writing your own callbacks","text":"sure check existing Keras callbacks reading API docs. Applications include logging CSV, saving model, visualizing metrics TensorBoard, lot !","code":""},{"path":"https://keras.posit.co/articles/intro_to_keras_for_engineers.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Introduction to Keras for engineers","text":"Keras 3 deep learning framework works TensorFlow, JAX, PyTorch interchangeably. notebook walk key Keras 3 workflows. Let’s start installing Keras 3: pip install keras –upgrade –quiet","code":""},{"path":"https://keras.posit.co/articles/intro_to_keras_for_engineers.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Introduction to Keras for engineers","text":"’re going using tensorflow backend – can edit string \"jax\" \"torch\" hit “Restart runtime”, whole notebook run just ! entire guide backend-agnostic.","code":"Sys.setenv(KERAS_BACKEND = \"tensorflow\")  # Note that Keras should only be loaded after the backend # has been configured. The backend cannot be changed once the # package is imported. library(tensorflow) ## ## Attaching package: 'tensorflow' ## The following objects are masked from 'package:keras3': ## ##     set_random_seed, shape library(keras3)"},{"path":"https://keras.posit.co/articles/intro_to_keras_for_engineers.html","id":"a-first-example-a-mnist-convnet","dir":"Articles","previous_headings":"","what":"A first example: A MNIST convnet","title":"Introduction to Keras for engineers","text":"Let’s start Hello World ML: training convnet classify MNIST digits. ’s data: ’s model. Different model-building options Keras offers include: Sequential API (use ) Functional API (typical) Writing models via subclassing (advanced use cases) ’s model summary: use compile() method specify optimizer, loss function, metrics monitor. Note JAX TensorFlow backends, XLA compilation turned default. Let’s train evaluate model. ’ll set aside validation split 15% data training monitor generalization unseen data. training, saving model end epoch. can also save model latest state like : reload like : Next, can query predictions class probabilities predict(): ’s basics!","code":"# Load the data and split it between train and test sets c(c(x_train, y_train), c(x_test, y_test)) %<-% keras3::dataset_mnist()  # Scale images to the [0, 1] range x_train <- x_train / 255 x_test <- x_test / 255 # Make sure images have shape (28, 28, 1) x_train <- op_expand_dims(x_train, -1) x_test <- op_expand_dims(x_test, -1)  dim(x_train) ## [1] 60000    28    28     1 dim(x_test) ## [1] 10000    28    28     1 # Model parameters num_classes <- 10 input_shape <- c(28, 28, 1)  model <- keras_model_sequential(input_shape = input_shape) model %>%   layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = \"relu\") %>%   layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = \"relu\") %>%   layer_max_pooling_2d(pool_size = c(2, 2)) %>%   layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = \"relu\") %>%   layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = \"relu\") %>%   layer_global_average_pooling_2d() %>%   layer_dropout(rate = 0.5) %>%   layer_dense(units = num_classes, activation = \"softmax\") summary(model) ## Model: \"sequential\" ## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓ ## ┃ Layer (type)                    ┃ Output Shape              ┃    Param # ┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩ ## │ conv2d_3 (Conv2D)               │ (None, 26, 26, 64)        │        640 │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ conv2d_2 (Conv2D)               │ (None, 24, 24, 64)        │     36,928 │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ max_pooling2d (MaxPooling2D)    │ (None, 12, 12, 64)        │          0 │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ conv2d_1 (Conv2D)               │ (None, 10, 10, 128)       │     73,856 │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ conv2d (Conv2D)                 │ (None, 8, 8, 128)         │    147,584 │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ global_average_pooling2d        │ (None, 128)               │          0 │ ## │ (GlobalAveragePooling2D)        │                           │            │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ dropout (Dropout)               │ (None, 128)               │          0 │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ dense (Dense)                   │ (None, 10)                │      1,290 │ ## └─────────────────────────────────┴───────────────────────────┴────────────┘ ##  Total params: 260,298 (1016.79 KB) ##  Trainable params: 260,298 (1016.79 KB) ##  Non-trainable params: 0 (0.00 B) model %>% compile(   optimizer = \"adam\",   loss = \"sparse_categorical_crossentropy\",   metrics = list(     metric_sparse_categorical_accuracy(name = \"acc\")   ) ) batch_size <- 128 epochs <- 1  callbacks <- list(   callback_model_checkpoint(filepath=\"model_at_epoch_{epoch}.keras\"),   callback_early_stopping(monitor=\"val_loss\", patience=2) )  model %>% fit(     x_train,     y_train,     batch_size=batch_size,     epochs=epochs,     validation_split=0.15,     callbacks=callbacks ) ## 399/399 - 36s - 91ms/step - acc: 0.7463 - loss: 0.7500 - val_acc: 0.9627 - val_loss: 0.1317 score <- model %>% evaluate(x_test, y_test, verbose=0) save_model(model, \"final_model.keras\", overwrite=TRUE) model <- load_model(\"final_model.keras\") predictions <- model %>% predict(x_test) ## 313/313 - 3s - 9ms/step dim(predictions) ## [1] 10000    10"},{"path":"https://keras.posit.co/articles/intro_to_keras_for_engineers.html","id":"writing-cross-framework-custom-components","dir":"Articles","previous_headings":"","what":"Writing cross-framework custom components","title":"Introduction to Keras for engineers","text":"Keras enables write custom Layers, Models, Metrics, Losses, Optimizers work across TensorFlow, JAX, PyTorch codebase. Let’s take look custom layers first. op_ namespace contains: implementation NumPy API, e.g. op_stack op_matmul. set neural network specific ops absent NumPy, op_conv op_binary_crossentropy. Let’s make custom Dense layer works backends: Next, let’s make custom Dropout layer relies keras.random namespace: Next, let’s write custom subclassed model uses two custom layers: Let’s compile fit :","code":"MyDense <- new_layer_class(   \"MyDense\",   initialize = function(units, activation = NULL, name = NULL) {     super$initialize(name = name)     self$units <- units     self$activation <- activation   },   build = function(input_shape) {     input_dim <- tail(input_shape, 1)     self$w <- self$add_weight(       shape = shape(input_dim, self$units),       initializer = initializer_glorot_normal(),       name = \"kernel\",       trainable = TRUE     )     self$b <- self$add_weight(       shape = shape(self$units),       initializer = initializer_zeros(),       name = \"bias\",       trainable = TRUE     )   },   call = function(inputs) {     # Use Keras ops to create backend-agnostic layers/metrics/etc.     x <- op_matmul(inputs, self$w) + self$b     if (is.null(self$activation)) return(x)     self$activation(x)   } ) MyDropout <- new_layer_class(   \"MyDropout\",   initialize = function(rate, name = NULL) {     super$initialize(name = name)     self$rate <- rate     # Use seed_generator for managing RNG state.     # It is a state element and its seed variable is     # tracked as part of `layer$variables`.     self$seed_generator <- random_seed_generator(1337)   },   call = function(inputs) {     # Use `keras.random` for random ops.     random_dropout(inputs, self$rate, seed = self$seed_generator)   } ) MyModel <- new_model_class(   \"MyModel\",   initialize = function(num_classes) {     super$initialize()     self$conv_base <- keras_model_sequential() %>%       layer_conv_2d(64, kernel_size = c(3, 3), activation = \"relu\") %>%       layer_conv_2d(64, kernel_size = c(3, 3), activation = \"relu\") %>%       layer_max_pooling_2d(pool_size = c(2, 2)) %>%       layer_conv_2d(128, kernel_size = c(3, 3), activation = \"relu\") %>%       layer_conv_2d(128, kernel_size = c(3, 3), activation = \"relu\") %>%       layer_global_average_pooling_2d()     self$dp <- MyDropout(rate = 0.5)     self$dense <- MyDense(units = num_classes, activation = activation_softmax)   },   call = function(inputs) {     x <- self$conv_base(inputs)     x <- self$dp(x)     self$dense(x)   } ) model <- MyModel(num_classes=10) model %>% compile(   loss=loss_sparse_categorical_crossentropy(),   optimizer=optimizer_adam(learning_rate=1e-3),   metrics=list(     metric_sparse_categorical_accuracy(name=\"acc\")   ) )  model %>% fit(     x_train,     y_train,     batch_size=batch_size,     epochs=1,  # For speed     validation_split=0.15, ) ## 399/399 - 37s - 92ms/step - acc: 0.7325 - loss: 0.7821 - val_acc: 0.9253 - val_loss: 0.2463"},{"path":"https://keras.posit.co/articles/intro_to_keras_for_engineers.html","id":"training-models-on-arbitrary-data-sources","dir":"Articles","previous_headings":"","what":"Training models on arbitrary data sources","title":"Introduction to Keras for engineers","text":"Keras models can trained evaluated wide variety data sources, independently backend ’re using. includes: Arrays Dataframes TensorFlow tf_dataset objects PyTorch DataLoader objects Keras PyDataset objects work whether ’re using TensorFlow, JAX, PyTorch Keras backend. Let’s try tf_dataset:","code":"library(tfdatasets) ## ## Attaching package: 'tfdatasets' ## The following object is masked from 'package:keras3': ## ##     shape train_dataset <- list(x_train, y_train) %>%   tensor_slices_dataset() %>%   dataset_batch(batch_size) %>%   dataset_prefetch(buffer_size=tf$data$AUTOTUNE)  test_dataset <- list(x_test, y_test) %>%   tensor_slices_dataset() %>%   dataset_batch(batch_size) %>%   dataset_prefetch(buffer_size=tf$data$AUTOTUNE)  model <- MyModel(num_classes=10) model %>% compile(   loss=loss_sparse_categorical_crossentropy(),   optimizer=optimizer_adam(learning_rate=1e-3),   metrics=list(     metric_sparse_categorical_accuracy(name=\"acc\")   ) ) model %>% fit(train_dataset, epochs=1, validation_data=test_dataset) ## 469/469 - 44s - 94ms/step - acc: 0.7552 - loss: 0.7242 - val_acc: 0.9199 - val_loss: 0.2651"},{"path":"https://keras.posit.co/articles/intro_to_keras_for_engineers.html","id":"further-reading","dir":"Articles","previous_headings":"","what":"Further reading","title":"Introduction to Keras for engineers","text":"concludes short overview new multi-backend capabilities Keras 3. Next, can learn :","code":""},{"path":"https://keras.posit.co/articles/intro_to_keras_for_engineers.html","id":"how-to-customize-what-happens-in-fit","dir":"Articles","previous_headings":"Further reading","what":"How to customize what happens in fit()","title":"Introduction to Keras for engineers","text":"Want implement non-standard training algorithm still want benefit power usability fit()? ’s easy customize fit() support arbitrary use cases: Customizing happens fit() TensorFlow Customizing happens fit() JAX Customizing happens fit() PyTorch","code":""},{"path":"https://keras.posit.co/articles/intro_to_keras_for_engineers.html","id":"how-to-write-custom-training-loops","dir":"Articles","previous_headings":"","what":"How to write custom training loops","title":"Introduction to Keras for engineers","text":"Writing training loop scratch TensorFlow Writing training loop scratch JAX Writing training loop scratch PyTorch","code":""},{"path":"https://keras.posit.co/articles/intro_to_keras_for_engineers.html","id":"how-to-distribute-training","dir":"Articles","previous_headings":"","what":"How to distribute training","title":"Introduction to Keras for engineers","text":"Guide distributed training TensorFlow JAX distributed training example PyTorch distributed training example Enjoy library! 🚀","code":""},{"path":"https://keras.posit.co/articles/intro_to_keras_for_researchers.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Introduction to Keras for Researchers","text":"","code":"library(keras3) library(tensorflow) ## ## Attaching package: 'tensorflow' ## The following objects are masked from 'package:keras': ## ##     set_random_seed, shape"},{"path":"https://keras.posit.co/articles/intro_to_keras_for_researchers.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Introduction to Keras for Researchers","text":"machine learning researcher? publish NeurIPS push state---art CV NLP? guide serve first introduction core Keras & TensorFlow API concepts. guide, learn : Tensors, variables, gradients TensorFlow Creating layers subclassing Layer class Writing low-level training loops Tracking losses created layers via add_loss() method Tracking metrics low-level training loop Speeding execution compiled tf.function Executing layers training inference mode Keras Functional API also see Keras API action two end--end research examples: Variational Autoencoder, Hypernetwork.","code":""},{"path":"https://keras.posit.co/articles/intro_to_keras_for_researchers.html","id":"tensors","dir":"Articles","previous_headings":"","what":"Tensors","title":"Introduction to Keras for Researchers","text":"TensorFlow infrastructure layer differentiable programming. heart, ’s framework manipulating N-dimensional arrays (tensors), much like NumPy. However, three key differences NumPy TensorFlow: TensorFlow can leverage hardware accelerators GPUs TPUs. TensorFlow can automatically compute gradient arbitrary differentiable tensor expressions. TensorFlow computation can distributed large numbers devices single machine, large number machines (potentially multiple devices ). Let’s take look object core TensorFlow: Tensor. ’s constant tensor: can get value R array calling .array(): features attributes dtype shape: common way create constant tensors via tf$ones tf$zeros: can also create random constant tensors:","code":"x <- tf$constant(rbind(c(5, 2), c(1, 3))) print(x) ## tf.Tensor( ## [[5. 2.] ##  [1. 3.]], shape=(2, 2), dtype=float64) as.array(x) ##      [,1] [,2] ## [1,]    5    2 ## [2,]    1    3 x$dtype ## tf.float64 x$shape ## TensorShape([2, 2]) tf$ones(shape=shape(2, 1)) ## tf.Tensor( ## [[1.] ##  [1.]], shape=(2, 1), dtype=float32) tf$zeros(shape=shape(2, 1)) ## tf.Tensor( ## [[0.] ##  [0.]], shape=(2, 1), dtype=float32) x <- random_normal(shape=c(2, 2), mean=0.0, stddev=1.0)  x <- random_uniform(shape=c(2, 2), minval=0, maxval=10)"},{"path":"https://keras.posit.co/articles/intro_to_keras_for_researchers.html","id":"variables","dir":"Articles","previous_headings":"","what":"Variables","title":"Introduction to Keras for Researchers","text":"Variables special tensors used store mutable state (weights neural network). create Variable using initial value: update value Variable using methods $assign(value), $assign_add(increment), $assign_sub(decrement):","code":"initial_value <- random_normal(shape=c(2, 2)) a <- tf$Variable(initial_value) print(a) ## <tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy= ## array([[ 0.90574193,  0.7916686 ], ##        [ 0.28754202, -0.5408821 ]], dtype=float32)> new_value <- random_normal(shape=c(2, 2)) a$assign(new_value) ## <tf.Variable 'UnreadVariable' shape=(2, 2) dtype=float32, numpy= ## array([[-0.3405368 , -2.1463926 ], ##        [ 1.2602988 ,  0.12241419]], dtype=float32)> added_value <- random_normal(shape=c(2, 2)) a$assign_add(added_value) ## <tf.Variable 'UnreadVariable' shape=(2, 2) dtype=float32, numpy= ## array([[ 0.04820395, -2.6854615 ], ##        [ 0.23246336,  1.4535258 ]], dtype=float32)>"},{"path":"https://keras.posit.co/articles/intro_to_keras_for_researchers.html","id":"doing-math-in-tensorflow","dir":"Articles","previous_headings":"","what":"Doing math in TensorFlow","title":"Introduction to Keras for Researchers","text":"’ve used NumPy, math TensorFlow look familiar. main difference TensorFlow code can run GPU TPU.","code":"a <- random_normal(shape=c(2, 2)) b <- random_normal(shape=c(2, 2))  c <- a + b d <- tf$square(c) e <- tf$exp(d)"},{"path":"https://keras.posit.co/articles/intro_to_keras_for_researchers.html","id":"gradients","dir":"Articles","previous_headings":"","what":"Gradients","title":"Introduction to Keras for Researchers","text":"’s another big difference R: can automatically retrieve gradient differentiable expression. Just open GradientTape, start “watching” tensor via tape$watch(), compose differentiable expression using tensor input: default, variables watched automatically, don’t need manually watch : Note can compute higher-order derivatives nesting tapes:","code":"a <- random_normal(shape=c(2, 2)) b <- random_normal(shape=c(2, 2))  with(tf$GradientTape() %as% tape, {   tape$watch(a)  # Start recording the history of operations applied to `a`   c <- tf$sqrt(tf$square(a) + tf$square(b))  # Do some math using `a`   # What's the gradient of `c` with respect to `a`?   dc_da <- tape$gradient(c, a)   print(dc_da) }) ## tf.Tensor( ## [[ 0.9969011  -0.77071446] ##  [ 0.23378514  0.9625516 ]], shape=(2, 2), dtype=float32) a <- tf$Variable(a)  with(tf$GradientTape() %as% tape, {   c <- tf$sqrt(tf$square(a) + tf$square(b))   dc_da <- tape$gradient(c, a)   print(dc_da) }) ## tf.Tensor( ## [[ 0.9969011  -0.77071446] ##  [ 0.23378514  0.9625516 ]], shape=(2, 2), dtype=float32) with(tf$GradientTape() %as% outer_tape, {   with(tf$GradientTape() %as% tape, {     c <- tf$sqrt(tf$square(a) + tf$square(b))     dc_da <- tape$gradient(c, a)   })   d2c_da2 <- outer_tape$gradient(dc_da, a)   print(d2c_da2) }) ## tf.Tensor( ## [[3.3447742e-03 7.1282017e-01] ##  [5.7464113e+00 5.5013239e-02]], shape=(2, 2), dtype=float32)"},{"path":"https://keras.posit.co/articles/intro_to_keras_for_researchers.html","id":"keras-layers","dir":"Articles","previous_headings":"","what":"Keras layers","title":"Introduction to Keras for Researchers","text":"TensorFlow infrastructure layer differentiable programming, dealing tensors, variables, gradients, Keras user interface deep learning, dealing layers, models, optimizers, loss functions, metrics, . Keras serves high-level API TensorFlow: Keras makes TensorFlow simple productive. Layer class fundamental abstraction Keras. Layer encapsulates state (weights) computation (defined call method). simple layer looks like . self$add_weight() method gives shortcut creating weights: use Layer instance much like R function: weight variables (created initialize) automatically tracked weights property: many built-layers available, Dense Conv2D LSTM fancier ones like Conv3DTranspose ConvLSTM2D. smart reusing built-functionality.","code":"Linear <- new_layer_class(   \"Linear\",   initialize = function(units = 32, input_dim = 32) {     super$initialize()     self$w <- self$add_weight(       shape = shape(input_dim, units),       initializer = \"random_normal\",       trainable = TRUE     )     self$b <- self$add_weight(       shape = shape(units),       initializer = \"zeros\",       trainable = TRUE     )   },   call = function(inputs) {     tf$matmul(inputs, self$w) + self$b   } ) # Instantiate our layer. linear_layer <- Linear(units=4, input_dim=2)  # The layer can be treated as a function. # Here we call it on some data. y <- linear_layer(tf$ones(shape(2, 2))) linear_layer$weights ## [[1]] ## <KerasVariable shape=(2, 4), dtype=float32, path=linear/variable> ## ## [[2]] ## <KerasVariable shape=(4), dtype=float32, path=linear/variable_1>"},{"path":"https://keras.posit.co/articles/intro_to_keras_for_researchers.html","id":"layer-weight-creation-in-buildinput_shape","dir":"Articles","previous_headings":"","what":"Layer weight creation in build(input_shape)","title":"Introduction to Keras for Researchers","text":"’s often good idea defer weight creation build() method, don’t need specify input dim/shape layer construction time:","code":"Linear <- new_layer_class(   \"Linear\",   initialize = function(units = 32) {     super$initialize()     self$units <- units   },   build = function(input_shape) {     self$w <- self$add_weight(       shape = shape(input_shape[-1], self$units),       initializer = \"random_normal\",       trainable = TRUE     )     self$b <- self$add_weight(       shape = shape(self$units),       initializer = \"zeros\",       trainable = TRUE     )   },   call = function(inputs) {     tf$matmul(inputs, self$w) + self$b   } )  # Instantiate our layer. linear_layer <- Linear(units = 4)  # This will also call `build(input_shape)` and create the weights. y <- linear_layer(tf$ones(shape(2, 2)))"},{"path":"https://keras.posit.co/articles/intro_to_keras_for_researchers.html","id":"layer-gradients","dir":"Articles","previous_headings":"","what":"Layer gradients","title":"Introduction to Keras for Researchers","text":"can automatically retrieve gradients weights layer calling inside GradientTape. Using gradients, can update weights layer, either manually, using optimizer object. course, can modify gradients using , need .","code":"# Prepare a dataset. c(c(x_train, y_train), .) %<-% dataset_mnist()  x_train <- array_reshape(x_train, c(60000, 784)) / 255  dataset <- tfdatasets::tensor_slices_dataset(list(x_train, y_train)) %>%   tfdatasets::dataset_shuffle(buffer_size=1024) %>%   tfdatasets::dataset_batch(64)  # Instantiate our linear layer (defined above) with 10 units. linear_layer <- Linear(units = 10)  # Instantiate a logistic loss function that expects integer targets. loss_fn <- loss_sparse_categorical_crossentropy(from_logits=TRUE)  # Instantiate an optimizer. optimizer <- optimizer_sgd(learning_rate=1e-3)  # Iterate over the batches of the dataset. coro::loop(for(data in dataset) {   # Open a GradientTape.   with(tf$GradientTape() %as% tape, {     # Forward pass.     logits <- linear_layer(data[[1]])      # Loss value for this batch.     loss_value <- loss_fn(data[[2]], logits)   })    # Get gradients of the loss wrt the weights.   gradients <- tape$gradient(loss_value, linear_layer$trainable_weights)    # Update the weights of our linear layer.   optimizer$apply_gradients(zip_lists(gradients, linear_layer$trainable_weights)) }) loss_value ## tf.Tensor(1.281973, shape=(), dtype=float32)"},{"path":"https://keras.posit.co/articles/intro_to_keras_for_researchers.html","id":"trainable-and-non-trainable-weights","dir":"Articles","previous_headings":"","what":"Trainable and non-trainable weights","title":"Introduction to Keras for Researchers","text":"Weights created layers can either trainable non-trainable. ’re exposed trainable_weights non_trainable_weights respectively. ’s layer non-trainable weight:","code":"ComputeSum <- new_layer_class(   \"ComputeSum\",   initialize = function(input_dim) {     super$initialize()     # Create a non-trainable weight.     self$total <- self$add_weight(       initializer = \"zeros\",       shape = shape(input_dim),       trainable = FALSE     )   },   call = function(inputs) {     self$total$assign_add(tf$reduce_sum(inputs, axis=0L))     self$total   } )  my_sum <- ComputeSum(input_dim = 2) x <- tf$ones(shape(2, 2))  as.array(my_sum(x)) ## [1] 2 2 as.array(my_sum(x)) ## [1] 4 4 my_sum$trainable_weights ## list()"},{"path":"https://keras.posit.co/articles/intro_to_keras_for_researchers.html","id":"layers-that-own-layers","dir":"Articles","previous_headings":"","what":"Layers that own layers","title":"Introduction to Keras for Researchers","text":"Layers can recursively nested create bigger computation blocks. layer track weights sublayers (trainable non-trainable). Note manually-created MLP equivalent following built-option:","code":"# Let's reuse the Linear class # with a `build` method that we defined above.  MLP <- new_layer_class(   \"MLP\",   initialize = function() {     super$initialize()     self$linear_1 <- Linear(units = 32)     self$linear_2 <- Linear(units = 32)     self$linear_3 <- Linear(units = 10)   },   call = function(inputs) {     x <- self$linear_1(inputs)     x <- tf$nn$relu(x)     x <- self$linear_2(x)     x <- tf$nn$relu(x)     return(self$linear_3(x))   } )  mlp <- MLP()  # The first call to the `mlp` object will create the weights. y <- mlp(tf$ones(shape=shape(3, 64)))  # Weights are recursively tracked. length(mlp$weights) ## [1] 6 mlp <- keras_model_sequential() %>%   layer_dense(units = 32, activation = \"relu\") %>%   layer_dense(units = 32, activation = \"relu\") %>%   layer_dense(units = 10)"},{"path":"https://keras.posit.co/articles/intro_to_keras_for_researchers.html","id":"tracking-losses-created-by-layers","dir":"Articles","previous_headings":"","what":"Tracking losses created by layers","title":"Introduction to Keras for Researchers","text":"Layers can create losses forward pass via add_loss() method. especially useful regularization losses. losses created sublayers recursively tracked parent layers. ’s layer creates activity regularization loss: model incorporating layer track regularization loss: losses cleared top-level layer start forward pass – don’t accumulate. layer.losses always contains losses created last forward pass. typically use losses summing computing gradients writing training loop.","code":"# A layer that creates an activity sparsity regularization loss ActivityRegularization <- new_layer_class(   \"ActivityRegularization\",   initialize = function(rate=1e-2) {     super$initialize()     self$rate <- rate   },   call = function(inputs) {     self$add_loss(self$rate * tf$reduce_sum(tf$abs(inputs)))     inputs   } ) # Let's use the loss layer in a MLP block. SparseMLP <- new_layer_class(   \"SparseMLP\",   initialize = function() {     super$initialize()     self$linear_1 <- Linear(units = 32)     self$reg <- ActivityRegularization(rate = 1e-2)     self$linear_3 <- Linear(units = 10)   },   call = function(inputs) {     x <- self$linear_1(inputs)     x <- tf$nn$relu(x)     x <- self$reg(x)     return(self$linear_3(x))   } )  mlp <- SparseMLP() y <- mlp(tf$ones(shape(10, 10)))  mlp$losses  # List containing one float32 scalar ## [[1]] ## tf.Tensor(0.18065463, shape=(), dtype=float32) # Losses correspond to the *last* forward pass. mlp <- SparseMLP() mlp(tf$ones(shape(10, 10))) ## tf.Tensor( ## [[ 0.0388482  -0.03920118  0.01624807 -0.01361975 -0.013549    0.07107338 ##   -0.01077365  0.05688906 -0.02838149 -0.0408462 ] ##  [ 0.0388482  -0.03920118  0.01624807 -0.01361975 -0.013549    0.07107338 ##   -0.01077365  0.05688906 -0.02838149 -0.0408462 ] ##  [ 0.0388482  -0.03920118  0.01624807 -0.01361975 -0.013549    0.07107338 ##   -0.01077365  0.05688906 -0.02838149 -0.0408462 ] ##  [ 0.0388482  -0.03920118  0.01624807 -0.01361975 -0.013549    0.07107338 ##   -0.01077365  0.05688906 -0.02838149 -0.0408462 ] ##  [ 0.0388482  -0.03920118  0.01624807 -0.01361975 -0.013549    0.07107338 ##   -0.01077365  0.05688906 -0.02838149 -0.0408462 ] ##  [ 0.0388482  -0.03920118  0.01624807 -0.01361975 -0.013549    0.07107338 ##   -0.01077365  0.05688906 -0.02838149 -0.0408462 ] ##  [ 0.0388482  -0.03920118  0.01624807 -0.01361975 -0.013549    0.07107338 ##   -0.01077365  0.05688906 -0.02838149 -0.0408462 ] ##  [ 0.0388482  -0.03920118  0.01624807 -0.01361975 -0.013549    0.07107338 ##   -0.01077365  0.05688906 -0.02838149 -0.0408462 ] ##  [ 0.0388482  -0.03920118  0.01624807 -0.01361975 -0.013549    0.07107338 ##   -0.01077365  0.05688906 -0.02838149 -0.0408462 ] ##  [ 0.0388482  -0.03920118  0.01624807 -0.01361975 -0.013549    0.07107338 ##   -0.01077365  0.05688906 -0.02838149 -0.0408462 ]], shape=(10, 10), dtype=float32) length(mlp$losses) ## [1] 1 mlp(tf$ones(shape(10, 10))) ## tf.Tensor( ## [[ 0.0388482  -0.03920118  0.01624807 -0.01361975 -0.013549    0.07107338 ##   -0.01077365  0.05688906 -0.02838149 -0.0408462 ] ##  [ 0.0388482  -0.03920118  0.01624807 -0.01361975 -0.013549    0.07107338 ##   -0.01077365  0.05688906 -0.02838149 -0.0408462 ] ##  [ 0.0388482  -0.03920118  0.01624807 -0.01361975 -0.013549    0.07107338 ##   -0.01077365  0.05688906 -0.02838149 -0.0408462 ] ##  [ 0.0388482  -0.03920118  0.01624807 -0.01361975 -0.013549    0.07107338 ##   -0.01077365  0.05688906 -0.02838149 -0.0408462 ] ##  [ 0.0388482  -0.03920118  0.01624807 -0.01361975 -0.013549    0.07107338 ##   -0.01077365  0.05688906 -0.02838149 -0.0408462 ] ##  [ 0.0388482  -0.03920118  0.01624807 -0.01361975 -0.013549    0.07107338 ##   -0.01077365  0.05688906 -0.02838149 -0.0408462 ] ##  [ 0.0388482  -0.03920118  0.01624807 -0.01361975 -0.013549    0.07107338 ##   -0.01077365  0.05688906 -0.02838149 -0.0408462 ] ##  [ 0.0388482  -0.03920118  0.01624807 -0.01361975 -0.013549    0.07107338 ##   -0.01077365  0.05688906 -0.02838149 -0.0408462 ] ##  [ 0.0388482  -0.03920118  0.01624807 -0.01361975 -0.013549    0.07107338 ##   -0.01077365  0.05688906 -0.02838149 -0.0408462 ] ##  [ 0.0388482  -0.03920118  0.01624807 -0.01361975 -0.013549    0.07107338 ##   -0.01077365  0.05688906 -0.02838149 -0.0408462 ]], shape=(10, 10), dtype=float32) length(mlp$losses)  # No accumulation. ## [1] 1 # Let's demonstrate how to use these losses in a training loop.  # Prepare a dataset. c(c(x_train, y_train), .) %<-% dataset_mnist() x_train <- array_reshape(x_train, c(60000, 784)) / 255  dataset <- tfdatasets::tensor_slices_dataset(list(x_train, y_train)) %>%   tfdatasets::dataset_shuffle(buffer_size=1024) %>%   tfdatasets::dataset_batch(64)  # A new MLP. mlp <- SparseMLP()  # Loss and optimizer. loss_fn <- loss_sparse_categorical_crossentropy(from_logits=TRUE) optimizer <- optimizer_sgd(learning_rate=1e-3)  coro::loop(for(data in dataset) {   x <- data[[1]]   y <- data[[2]]   with(tf$GradientTape() %as% tape, {     # Forward pass.     logits <- mlp(x)      # External loss value for this batch.     loss <- loss_fn(y, logits)      # Add the losses created during the forward pass.     loss <- loss + Reduce(`+`, mlp$losses)      # Get gradients of the loss wrt the weights.     gradients <- tape$gradient(loss, mlp$trainable_weights)      # Update the weights of our linear layer.     optimizer$apply_gradients(zip_lists(gradients, mlp$trainable_weights))   }) })"},{"path":"https://keras.posit.co/articles/intro_to_keras_for_researchers.html","id":"keeping-track-of-training-metrics","dir":"Articles","previous_headings":"","what":"Keeping track of training metrics","title":"Introduction to Keras for Researchers","text":"Keras offers broad range built-metrics, like metric_auc metric_precision_at_recall. ’s also easy create metrics lines code. use metric custom training loop, : Instantiate metric object, e.g. metric = metric_auc() Call metric$udpate_state(targets, predictions) method batch data Query result via metric$result() Reset metric’s state end epoch start evaluation via metric$reset_state() ’s simple example: can also define metrics subclassing keras.metrics.Metric. need override three functions called : Override update_state() update statistic values. Override result() return metric value. Override reset_state() reset metric initial state. example implement F1-score metric (support sample weighting). Let’s test-drive :","code":"# Instantiate a metric object accuracy <- metric_sparse_categorical_accuracy()  # Prepare our layer, loss, and optimizer. model <- keras_model_sequential() %>%   layer_dense(units = 32, activation = \"relu\") %>%   layer_dense(units = 32, activation = \"relu\") %>%   layer_dense(units = 10) loss_fn <- loss_sparse_categorical_crossentropy(from_logits = TRUE) optimizer <- optimizer_adam(learning_rate=1e-3)  for (epoch in seq_len(2)) {   coro::loop(for (data in dataset) {     x <- data[[1]]     y <- data[[2]]     with(tf$GradientTape() %as% tape, {       # Forward pass.       logits <- model(x)        # External loss value for this batch.       loss_value <- loss_fn(y, logits)     })      # Update the state of the `accuracy` metric.     accuracy$update_state(y, logits)      # Update the weights of the model to minimize the loss value.     gradients <- tape$gradient(loss_value, model$trainable_weights)     optimizer$apply_gradients(zip_lists(gradients, model$trainable_weights))    })   cat(\"Epoch:\", epoch, \"Accuracy:\", as.numeric(accuracy$result()), \"\\n\")   accuracy$reset_state() } ## Epoch: 1 Accuracy: 0.8757833 ## Epoch: 2 Accuracy: 0.93915 F1Score <- new_metric_class(   \"F1Score\",   initialize = function(self, name=\"f1_score\", dtype=\"float32\", threshold=0.5, ...) {     super$initialize(name=name, dtype=dtype, ...)     self$threshold <- threshold     self$true_positives <- self$add_weight(       name=\"tp\", dtype=dtype, initializer=\"zeros\"     )     self$false_positives <- self$add_weight(       name=\"fp\", dtype=dtype, initializer=\"zeros\"     )     self$false_negatives <- self$add_weight(       name=\"fn\", dtype=dtype, initializer=\"zeros\"     )   },   update_state = function(y_true, y_pred, sample_weight=NULL) {     y_pred <- tf$math$greater_equal(y_pred, self$threshold)     y_true <- tf$cast(y_true, tf$bool)     y_pred <- tf$cast(y_pred, tf$bool)      true_positives <- tf$cast(y_true & y_pred, self$dtype)     false_positives <- tf$cast((!y_true) & y_pred, self$dtype)     false_negatives <- tf$cast(y_true & (!y_pred), self$dtype)      if (!is.null(sample_weight)) {       sample_weight <- tf$cast(sample_weight, self$dtype)       true_positives <- true_positives * sample_weight       false_positives <- false_positives * sample_weight       false_negatives <- false_negatives * sample_weight     }      self$true_positives$assign_add(tf$reduce_sum(true_positives))     self$false_positives$assign_add(tf$reduce_sum(false_positives))     self$false_negatives$assign_add(tf$reduce_sum(false_negatives))   },    result = function() {     precision <- self$true_positives / (self$true_positives + self$false_positives)     recall <- self$true_positives / (self$true_positives + self$false_negatives)     f1_score <- 2 * precision * recall / (precision + recall)     f1_score   },    reset_state = function() {     self$true_positives$assign(0)     self$false_positives$assign(0)     self$false_negatives$assign(0)   } ) m <- F1Score() m$update_state(c(0, 1, 0, 0), c(0.3, 0.5, 0.8, 0.9)) cat(\"Intermediate result:\", as.numeric(m$result()), \"\\n\") ## Intermediate result: 0.5 m$update_state(c(1, 1, 1, 1), c(0.1, 0.7, 0.6, 0.0)) cat(\"Final result:\", as.numeric(m$result()), \"\\n\") ## Final result: 0.6"},{"path":"https://keras.posit.co/articles/intro_to_keras_for_researchers.html","id":"compiled-functions","dir":"Articles","previous_headings":"","what":"Compiled functions","title":"Introduction to Keras for Researchers","text":"Running eagerly great debugging, get better performance compiling computation static graphs. Static graphs researcher’s best friends. can compile function wrapping tf.function decorator.","code":"# Prepare our layer, loss, and optimizer. model <- keras_model_sequential() %>%   layer_dense(units = 32, activation = \"relu\") %>%   layer_dense(units = 32, activation = \"relu\") %>%   layer_dense(units = 10) loss_fn <- loss_sparse_categorical_crossentropy(from_logits = TRUE) optimizer <- optimizer_adam(learning_rate=1e-3)  # Create a training step function. train_on_batch <- tf_function(function(x, y) {   with(tf$GradientTape() %as% tape, {     # Forward pass.     logits <- model(x)     # External loss value for this batch.     loss_value <- loss_fn(y, logits)   })   # Update the weights of the model to minimize the loss value.   gradients <- tape$gradient(loss_value, model$trainable_weights)   optimizer$apply_gradients(zip_lists(gradients, model$trainable_weights))   loss_value })   # Prepare a dataset. c(c(x_train, y_train), .) %<-% dataset_mnist() x_train <- array_reshape(x_train, c(60000, 784)) / 255  dataset <- tfdatasets::tensor_slices_dataset(list(x_train, y_train)) %>%   tfdatasets::dataset_shuffle(buffer_size=1024) %>%   tfdatasets::dataset_batch(64)  i <- 0 coro::loop(for (data in dataset) {   i <- i + 1   x <- data[[1]]   y <- data[[2]]   loss <- train_on_batch(x, y)   if (i %% 100 == 0)     cat(\"Loss:\", as.numeric(loss), \"\\n\") }) ## Loss: 0.551749 ## Loss: 0.2131136 ## Loss: 0.2765952 ## Loss: 0.1296219 ## Loss: 0.2657076 ## Loss: 0.2683381 ## Loss: 0.1570166 ## Loss: 0.3139241 ## Loss: 0.0898185"},{"path":"https://keras.posit.co/articles/intro_to_keras_for_researchers.html","id":"training-mode-inference-mode","dir":"Articles","previous_headings":"","what":"Training mode & inference mode","title":"Introduction to Keras for Researchers","text":"layers, particular BatchNormalization layer Dropout layer, different behaviors training inference. layers, standard practice expose training (boolean) argument call method. exposing argument call, enable built-training evaluation loops (e.g. fit) correctly use layer training inference modes.","code":"Dropout <- new_layer_class(   \"Dropout\",   initialize = function(rate) {     super$initialize()     self$rate <- rate   },   call = function(inputs, training = NULL) {     if (!is.null(training) && training) {       return(tf$nn$dropout(inputs, rate = self$rate))     }     inputs   } )  MLPWithDropout <- new_layer_class(   \"MLPWithDropout\",   initialize = function() {     super$initialize()     self$linear_1 <- Linear(units = 32)     self$dropout <- Dropout(rate = 0.5)     self$linear_3 <- Linear(units = 10)   },   call = function(inputs, training = NULL) {     x <- self$linear_1(inputs)     x <- tf$nn$relu(x)     x <- self$dropout(x, training = training)     self$linear_3(x)   } )  mlp <- MLPWithDropout() y_train <- mlp(tf$ones(shape(2, 2)), training=TRUE) y_test <- mlp(tf$ones(shape(2, 2)), training=FALSE)"},{"path":"https://keras.posit.co/articles/intro_to_keras_for_researchers.html","id":"the-functional-api-for-model-building","dir":"Articles","previous_headings":"","what":"The Functional API for model-building","title":"Introduction to Keras for Researchers","text":"build deep learning models, don’t use object-oriented programming time. layers ’ve seen far can also composed functionally, like (call “Functional API”): Functional API tends concise subclassing, provides advantages (generally advantages functional, typed languages provide untyped OO development). However, can used define DAGs layers – recursive networks defined Layer subclasses instead. Learn Functional API . research workflows, may often find mix--matching OO models Functional models. Note Model class also features built-training & evaluation loops: fit(), predict() evaluate() (configured via compile() method). built-functions give access following built-training infrastructure features: Callbacks. can leverage built-callbacks early-stopping, model checkpointing, monitoring training TensorBoard. can also implement custom callbacks needed. Distributed training. can easily scale training multiple GPUs, TPU, even multiple machines tf.distribute API – changes code. Step fusing. steps_per_execution argument Model.compile(), can process multiple batches single tf.function call, greatly improves device utilization TPUs. won’t go details, provide simple code example . leverages built-training infrastructure implement MNIST example . can always subclass Model class (works exactly like subclassing Layer) want leverage built-training loops OO models. Just override Model$train_step() customize happens fit() retaining support built-infrastructure features outlined – callbacks, zero-code distribution support, step fusing support. may also override test_step() customize happens evaluate(), override predict_step() customize happens predict(). information, please refer guide.","code":"# We use an `Input` object to describe the shape and dtype of the inputs. # This is the deep learning equivalent of *declaring a type*. # The shape argument is per-sample; it does not include the batch size. # The functional API focused on defining per-sample transformations. # The model we create will automatically batch the per-sample transformations, # so that it can be called on batches of data. inputs <- layer_input(shape = 16, dtype = \"float32\")  # We call layers on these \"type\" objects # and they return updated types (new shapes/dtypes). outputs <- inputs %>%   Linear(units = 32) %>% # We are reusing the Linear layer we defined earlier.   Dropout(rate = 0.5) %>% # We are reusing the Dropout layer we defined earlier.   Linear(units = 10)  # A functional `Model` can be defined by specifying inputs and outputs. # A model is itself a layer like any other. model <- keras_model(inputs, outputs)  # A functional model already has weights, before being called on any data. # That's because we defined its input shape in advance (in `Input`). length(model$weights) ## [1] 4 # Let's call our model on some data, for fun. y <- model(tf$ones(shape(2, 16))) y$shape ## TensorShape([2, 10]) # You can pass a `training` argument in `__call__` # (it will get passed down to the Dropout layer). y <- model(tf$ones(shape(2, 16)), training=TRUE) inputs <- layer_input(shape = 784, dtype=\"float32\") outputs <- inputs %>%   layer_dense(units = 32, activation = \"relu\") %>%   layer_dense(units = 32, activation = \"relu\") %>%   layer_dense(units = 10) model <- keras_model(inputs, outputs)  # Specify the loss, optimizer, and metrics with `compile()`. model %>% compile(     loss = loss_sparse_categorical_crossentropy(from_logits=TRUE),     optimizer=optimizer_adam(learning_rate=1e-3),     metrics=list(metric_sparse_categorical_accuracy()), )  # Train the model with the dataset for 2 epochs. model %>% fit(dataset, epochs=2) ## Epoch 1/2 ## 938/938 - 1s - 846us/step - loss: 0.3958 - sparse_categorical_accuracy: 0.8866 ## Epoch 2/2 ## 938/938 - 1s - 602us/step - loss: 0.1888 - sparse_categorical_accuracy: 0.9443 predictions <- model %>% predict(dataset) ## 938/938 - 0s - 378us/step model %>% evaluate(dataset) ## 938/938 - 0s - 367us/step - loss: 0.1763 - sparse_categorical_accuracy: 0.9454 ##            loss compile_metrics ##       0.1763445       0.9454167 CustomModel <- new_model_class(   \"CustomModel\",   initialize = function(...) {     super$initialize(...)     self$loss_tracker <- metric_mean(name=\"loss\")     self$accuracy <- metric_sparse_categorical_accuracy()     self$loss_fn <- loss_sparse_categorical_crossentropy(from_logits=TRUE)     self$optimizer <- optimizer_adam(learning_rate=1e-3)   },   train_step = function(data) {     c(x, y, sample_weight) %<-% unpack_x_y_sample_weight(data)     with(tf$GradientTape() %as% tape, {       y_pred <- self(x, training=TRUE)       loss <- self$loss_fn(y = y, y_pred = y_pred, sample_weight=sample_weight)     })     gradients <- tape$gradient(loss, self$trainable_variables)     self$optimizer$apply_gradients(       zip_lists(gradients, self$trainable_variables)     )      # Update metrics (includes the metric that tracks the loss)     self$loss_tracker$update_state(loss)     self$accuracy$update_state(y, y_pred, sample_weight=sample_weight)     # Return a list mapping metric names to current value     list(       loss = self$loss_tracker$result(),       accuracy = self$accuracy$result()     )   },   metrics = mark_active(function() {     list(self$loss_tracker, self$accuracy)   }) )  inputs <- layer_input(shape = 784, dtype=\"float32\") outputs <- inputs %>%   layer_dense(units = 32, activation = \"relu\") %>%   layer_dense(units = 32, activation = \"relu\") %>%   layer_dense(units = 10) model <- CustomModel(inputs, outputs) model %>% compile() model %>% fit(dataset, epochs=2) ## Epoch 1/2 ## 938/938 - 1s - 780us/step - loss: 0.3869 - sparse_categorical_accuracy: 0.8924 ## Epoch 2/2 ## 938/938 - 1s - 603us/step - loss: 0.2163 - sparse_categorical_accuracy: 0.9370"},{"path":"https://keras.posit.co/articles/intro_to_keras_for_researchers.html","id":"end-to-end-experiment-example-1-variational-autoencoders-","dir":"Articles","previous_headings":"","what":"End-to-end experiment example 1: variational autoencoders.","title":"Introduction to Keras for Researchers","text":"things ’ve learned far: Layer encapsulates state (created __init__ build) computation (defined call). Layers can recursively nested create new, bigger computation blocks. can easily write highly hackable training loops opening GradientTape, calling model inside tape’s scope, retrieving gradients applying via optimizer. can speed training loops using @tf.function decorator. Layers can create track losses (typically regularization losses) via self.add_loss(). Let’s put things together end--end example: ’re going implement Variational AutoEncoder (VAE). ’ll train MNIST digits. VAE subclass Layer, built nested composition layers subclass Layer. feature regularization loss (KL divergence). model definition. First, Encoder class, uses Sampling layer map MNIST digit latent-space triplet (z_mean, z_log_var, z). Next, Decoder class, maps probabilistic latent space coordinates back MNIST digit. Finally, VariationalAutoEncoder composes together encoder decoder, creates KL divergence regularization loss via add_loss(). Now, let’s write training loop. training step decorated @tf.function compile super fast graph function. can see, building training type model Keras quick painless.","code":"Sampling <- new_layer_class(   \"Sampling\",   call = function(inputs) {     c(z_mean, z_log_var) %<-% inputs     batch <- op_shape(z_mean)[[1]]     dim <- op_shape(z_mean)[[2]]     epsilon <- random_normal(shape = c(batch, dim))     z_mean + op_exp(0.5 * z_log_var) * epsilon   } )  Encoder <- new_layer_class(   \"Encoder\",   initialize = function(latent_dim = 32, intermediate_dim = 64, ...) {     super$initialize(...)     self$dense_proj <- layer_dense(units = intermediate_dim, activation = \"relu\")     self$dense_mean <- layer_dense(units = latent_dim)     self$dense_log_var <- layer_dense(units = latent_dim)     self$sampling <- Sampling()   },   call = function(inputs) {     x <- self$dense_proj(inputs)     z_mean <- self$dense_mean(x)     z_log_var <- self$dense_log_var(x)     z <- self$sampling(list(z_mean, z_log_var))     list(z_mean, z_log_var, z)   } ) Decoder <- new_layer_class(   \"Decoder\",   initialize = function(original_dim, intermediate_dim = 64, ...) {     super$initialize(...)     self$dense_proj <- layer_dense(units = intermediate_dim, activation = \"relu\")     self$dense_output <- layer_dense(units = original_dim, activation = \"sigmoid\")   },   call = function(inputs) {     x <- self$dense_proj(inputs)     self$dense_output(x)   } ) VariationalAutoEncoder <- new_model_class(   \"VariationalAutoEncoder\",   initialize = function(original_dim,         intermediate_dim=64,         latent_dim=32,         name=\"autoencoder\", ...) {     super$initialize(name = name, ...)     self$original_dim <- original_dim     self$encoder <- Encoder(       latent_dim = latent_dim,       intermediate_dim = intermediate_dim     )     self$decoder <- Decoder(       original_dim = original_dim,       intermediate_dim = intermediate_dim     )   },   call = function(inputs) {     c(z_mean, z_log_var, z) %<-% self$encoder(inputs)     reconstructed <- self$decoder(z)     # Add KL divergence regularization loss.     kl_loss <- -0.5 * op_mean(       z_log_var - op_square(z_mean) - op_exp(z_log_var) + 1     )     self$add_loss(kl_loss)     reconstructed   } ) # Our model. vae <- VariationalAutoEncoder(   original_dim = 784,   intermediate_dim = 64,   latent_dim = 32 )  # Loss and optimizer. loss_fn <- loss_mean_squared_error() optimizer = optimizer_adam(learning_rate=1e-3)  # Prepare a dataset. c(c(x_train, .), .) %<-% dataset_mnist() x_train <- array_reshape(x_train, c(60000, 784)) / 255  dataset <- tfdatasets::tensor_slices_dataset(x_train) %>%   tfdatasets::dataset_shuffle(buffer_size=1024) %>%   tfdatasets::dataset_batch(32)   training_step <- tf_function(function(x) {   with(tf$GradientTape() %as% tape, {     reconstructed <- vae(x)  # Compute input reconstruction.     # Compute loss.     loss <- loss_fn(x, reconstructed)     loss <- loss + op_sum(vae$losses)  # Add KLD term.   })   # Update the weights of the VAE.   grads <- tape$gradient(loss, vae$trainable_weights)   optimizer$apply_gradients(zip_lists(grads, vae$trainable_weights))   loss })  losses <- c()  # Keep track of the losses over time. coro::loop(for(data in dataset) {   loss <- training_step(data)    # Logging.   losses[length(losses) + 1] <- as.numeric(loss)   if (length(losses) %% 100 == 0) {     cat(\"Step:\", length(losses), \"Loss:\", mean(losses), \"\\n\")   }   # Stop after 1000 steps.   # Training the model to convergence is left   # as an exercise to the reader.   if (length(losses) >= 1000) {     break   } }) ## Step: 100 Loss: 0.1270978 ## Step: 200 Loss: 0.1003238 ## Step: 300 Loss: 0.09001128 ## Step: 400 Loss: 0.08493649 ## Step: 500 Loss: 0.08171404 ## Step: 600 Loss: 0.07926706 ## Step: 700 Loss: 0.07790598 ## Step: 800 Loss: 0.07670418 ## Step: 900 Loss: 0.07570736 ## Step: 1000 Loss: 0.07476592"},{"path":"https://keras.posit.co/articles/intro_to_keras_for_researchers.html","id":"end-to-end-experiment-example-2-hypernetworks-","dir":"Articles","previous_headings":"","what":"End-to-end experiment example 2: hypernetworks.","title":"Introduction to Keras for Researchers","text":"Let’s take look another kind research experiment: hypernetworks. idea use small deep neural network (hypernetwork) generate weights larger network (main network). Let’s implement really trivial hypernetwork: ’ll use small 2-layer network generate weights larger 3-layer network. training loop. batch data: use hypernetwork generate array weight coefficients, weights_pred reshape coefficients kernel & bias tensors main_network run forward pass main_network compute actual MNIST predictions run backprop weights hypernetwork minimize final classification loss Implementing arbitrary research ideas Keras straightforward highly productive. Imagine trying 25 ideas per day (20 minutes per experiment average)! Keras designed go idea results fast possible, believe key great research. hope enjoyed quick introduction. Let us know build Keras!","code":"input_dim <- 784 classes <- 10  # This is the main network we'll actually use to predict labels. inputs <- layer_input(shape = input_dim) dense1 <- layer_dense(units = 64, activation = \"relu\") dense1$built <- TRUE  dense2 <- layer_dense(units = classes) dense2$built <- TRUE  outputs <- inputs %>% dense1() %>% dense2() main_network <- keras_model(inputs, outputs)  # This is the number of weight coefficients to generate. Each layer in the # main network requires output_dim * input_dim + output_dim coefficients. num_weights_to_generate <- (classes * 64 + classes) + (64 * input_dim + 64)  # This is the hypernetwork that generates the weights of the `main_network` above. hypernetwork <- keras_model_sequential() %>%   layer_dense(units=16, activation=\"relu\") %>%   layer_dense(units=num_weights_to_generate, activation=\"sigmoid\") # Loss and optimizer. loss_fn <- loss_sparse_categorical_crossentropy(from_logits = TRUE) optimizer <- optimizer_adam(learning_rate=1e-4)  # Prepare a dataset. c(c(x_train, y_train), .) %<-% dataset_mnist() x_train <- array_reshape(x_train, c(60000, 784)) / 255  dataset <- tfdatasets::tensor_slices_dataset(list(x_train, y_train)) %>%   tfdatasets::dataset_shuffle(buffer_size=1024) %>%   # We'll use a batch size of 1 for this experiment.   tfdatasets::dataset_batch(1)  train_step <- function(x, y) {   with(tf$GradientTape() %as% tape, {     weights_pred <- hypernetwork(x)      # Reshape them to the expected shapes for w and b for the outer model.     # Layer 1 kernel.     start_index <- 1     w1_shape <- c(input_dim, 64)     w1_coeffs <- weights_pred[, start_index:(start_index + prod(w1_shape) - 1)]     w1 <- tf$reshape(w1_coeffs, as.integer(w1_shape))     start_index <- start_index + prod(w1_shape)      # Layer 1 bias.     b1_shape <- c(64)     b1_coeffs <- weights_pred[, start_index:(start_index + prod(b1_shape) - 1)]     b1 <- tf$reshape(b1_coeffs, as.integer(b1_shape))     start_index <- start_index + prod(b1_shape)      # Layer 2 kernel.     w2_shape <- c(64, classes)     w2_coeffs <- weights_pred[, start_index:(start_index + prod(w2_shape) - 1)]     w2 <- tf$reshape(w2_coeffs, as.integer(w2_shape))     start_index <- start_index + prod(w2_shape)      # Layer 2 bias.     b2_shape <- c(classes)     b2_coeffs <- weights_pred[, start_index:(start_index + prod(b2_shape) - 1)]     b2 <- tf$reshape(b2_coeffs, as.integer(b2_shape))     start_index <- start_index + prod(b2_shape)      # Set the weight predictions as the weight variables on the outer model.     dense1$kernel <- w1     dense1$bias <- b1     dense2$kernel <- w2     dense2$bias <- b2      # Inference on the outer model.     preds <- main_network(x)     loss <- loss_fn(y, preds)   })    grads <- tape$gradient(loss, hypernetwork$trainable_weights)   optimizer$apply_gradients(zip_lists(grads, hypernetwork$trainable_weights))   loss }  losses <- c()  # Keep track of the losses over time. coro::loop(for (data in dataset) {   x <- data[[1]]   y <- data[[2]]   loss <- train_step(x, y)    # Logging.   losses[length(losses) + 1] <- as.numeric(loss)   if (length(losses) %% 100 == 0) {     cat(\"Step:\", length(losses), \"Loss:\", mean(losses), \"\\n\")   }   # Stop after 1000 steps.   # Training the model to convergence is left   # as an exercise to the reader.   if (length(losses) >= 1000) {     break   } }) ## Step: 100 Loss: 2.536758 ## Step: 200 Loss: 2.236468 ## Step: 300 Loss: 2.119413 ## Step: 400 Loss: 2.040315 ## Step: 500 Loss: 1.949096 ## Step: 600 Loss: 1.859365 ## Step: 700 Loss: 1.845711 ## Step: 800 Loss: 1.820584 ## Step: 900 Loss: 1.771324 ## Step: 1000 Loss: 1.730642"},{"path":"https://keras.posit.co/articles/making_new_layers_and_models_via_subclassing.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Making new layers and models via subclassing","text":"guide cover everything need know build subclassed layers models. particular, ’ll learn following features: Layer class add_weight() method Trainable non-trainable weights build() method Making sure layers can used backend add_loss() method training argument call() mask argument call() Making sure layers can serialized Let’s dive .","code":""},{"path":"https://keras.posit.co/articles/making_new_layers_and_models_via_subclassing.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Making new layers and models via subclassing","text":"","code":"library(keras3) library(tensorflow) ## ## Attaching package: 'tensorflow' ## The following objects are masked from 'package:keras': ## ##     set_random_seed, shape library(tfdatasets) ## ## Attaching package: 'tfdatasets' ## The following object is masked from 'package:keras': ## ##     shape"},{"path":"https://keras.posit.co/articles/making_new_layers_and_models_via_subclassing.html","id":"the-layer-class-the-combination-of-state-weights-and-some-computation","dir":"Articles","previous_headings":"","what":"The Layer class: the combination of state (weights) and some computation","title":"Making new layers and models via subclassing","text":"One central abstractions Keras Layer class. layer encapsulates state (layer’s “weights”) transformation inputs outputs (“call”, layer’s forward pass). ’s densely-connected layer. two state variables: variables w b. use layer calling tensor input(s), much like R function. Note weights w b automatically tracked layer upon set layer attributes:","code":"Linear <- new_layer_class(   \"Linear\",   initialize = function(units = 32, input_dim = 32) {     units <- as.integer(units)     input_dim <- as.integer(input_dim)      super$initialize()     self$w <- self$add_weight(       shape = c(input_dim, units),       initializer = \"random_normal\",       trainable = TRUE     )     self$b <- self$add_weight(       shape = c(units),       initializer = \"zeros\",       trainable = TRUE     )   },   call = function(inputs) {     k_matmul(inputs, self$w) + self$b   } ) x <- k_ones(c(2, 2)) linear_layer <- Linear(units = 4L, input_dim = 2L) y <- linear_layer(x) print(y) ## tf.Tensor( ## [[0.02153057 0.15450525 0.0205495  0.04493225] ##  [0.02153057 0.15450525 0.0205495  0.04493225]], shape=(2, 4), dtype=float32) linear_layer$weights ## [[1]] ## <KerasVariable shape=(2, 4), dtype=float32, path=linear/variable> ## ## [[2]] ## <KerasVariable shape=(4), dtype=float32, path=linear/variable_1>"},{"path":"https://keras.posit.co/articles/making_new_layers_and_models_via_subclassing.html","id":"layers-can-have-non-trainable-weights","dir":"Articles","previous_headings":"","what":"Layers can have non-trainable weights","title":"Making new layers and models via subclassing","text":"Besides trainable weights, can add non-trainable weights layer well. weights meant taken account backpropagation, training layer. ’s add use non-trainable weight: ’s part layer$weights, gets categorized non-trainable weight:","code":"ComputeSum <- new_layer_class(   \"ComputeSum\",   initialize = function(input_dim) {     input_dim <- as.integer(input_dim)     super$initialize()     self$total <- self$add_weight(       initializer = \"zeros\",       shape = c(input_dim),       trainable = FALSE     )   },   call = function(inputs) {     self$total$assign_add(k_sum(inputs, axis = 1))     self$total   } )  x <- k_ones(c(2, 2)) my_sum <- ComputeSum(input_dim = 2) y <- my_sum(x) print(y$numpy()) ## [1] 2 2 y <- my_sum(x) print(y$numpy()) ## [1] 4 4 cat(\"weights:\", length(my_sum$weights)) ## weights: 1 cat(\"non-trainable weights:\", length(my_sum$non_trainable_weights)) ## non-trainable weights: 1 # It's not included in the trainable weights: cat(\"trainable_weights:\", length(my_sum$trainable_weights)) ## trainable_weights: 0"},{"path":"https://keras.posit.co/articles/making_new_layers_and_models_via_subclassing.html","id":"best-practice-deferring-weight-creation-until-the-shape-of-the-inputs-is-known","dir":"Articles","previous_headings":"","what":"Best practice: deferring weight creation until the shape of the inputs is known","title":"Making new layers and models via subclassing","text":"Linear layer took input_dim argument used compute shape weights w b initialize(): many cases, may know advance size inputs, like lazily create weights value becomes known, time instantiating layer. Keras API, recommend creating layer weights build(self, inputs_shape) method layer. Like : call() method layer automatically run build first time called. now layer ’s lazy thus easier use: Implementing build() separately shown nicely separates creating weights using weights every call.","code":"Linear <- new_layer_class(   \"Linear\",   initialize = function(units = 32, input_dim = 32) {     units <- as.integer(units)     input_dim <- as.integer(input_dim)      super$initialize()     self$w <- self$add_weight(       shape = c(input_dim, units),       initializer = \"random_normal\",       trainable = TRUE     )     self$b <- self$add_weight(       shape = c(units),       initializer = \"zeros\",       trainable = TRUE     )   },   call = function(inputs) {     k_matmul(inputs, self$w) + self$b   } ) Linear <- new_layer_class(   \"Linear\",   initialize = function(units = 32) {     self$units <- as.integer(units)     super$initialize()   },   build = function(input_shape) {     self$w <- self$add_weight(       shape = c(tail(input_shape, 1), self$units),       initializer = \"random_normal\",       trainable = TRUE     )     self$b <- self$add_weight(       shape = c(self$units),       initializer = \"zeros\",       trainable = TRUE     )   },   call = function(inputs) {     k_matmul(inputs, self$w) + self$b   } ) # At instantiation, we don't know on what inputs this is going to get called linear_layer <- Linear(units = 32)  # The layer's weights are created dynamically the first time the layer is called y <- linear_layer(x)"},{"path":"https://keras.posit.co/articles/making_new_layers_and_models_via_subclassing.html","id":"layers-are-recursively-composable","dir":"Articles","previous_headings":"","what":"Layers are recursively composable","title":"Making new layers and models via subclassing","text":"assign Layer instance attribute another Layer, outer layer start tracking weights created inner layer. recommend creating sublayers initialize() method leave first call() trigger building weights.","code":"MLPBlock <- new_layer_class(   \"MLPBlock\",   initialize = function() {     super$initialize()     self$linear_1 <- Linear(units = 32)     self$linear_2 <- Linear(units = 32)     self$linear_3 <- Linear(units = 1)   },   call = function(inputs) {     x <- self$linear_1(inputs)     x <- activation_relu(x)     x <- self$linear_2(x)     x <- activation_relu(x)     self$linear_3(x)   } )  mlp = MLPBlock() # The first call to the `mlp` will create the weights y <- mlp(k_ones(shape = c(3, 64)))  cat(\"weights:\", length(mlp$weights)) ## weights: 7 cat(\"trainable weights:\", length(mlp$trainable_weights)) ## trainable weights: 7"},{"path":"https://keras.posit.co/articles/making_new_layers_and_models_via_subclassing.html","id":"backend-agnostic-layers-and-backend-specific-layers","dir":"Articles","previous_headings":"","what":"Backend-agnostic layers and backend-specific layers","title":"Making new layers and models via subclassing","text":"long layer uses APIs ops namespace (ie. using functions starting k_), (Keras namespaces activations_*, random_*, layer_*), can used backend – TensorFlow, JAX, PyTorch. layers ’ve seen far guide work Keras backends. ops namespace gives access : NumPy API, e.g. k_matmul, k_sum, k_reshape, k_stack, etc. Neural networks-specific APIs k_softmax, k_conv, k_binary_crossentropy, k_relu, etc. can also use backend-native APIs layers (tf$nn functions), , layer usable backend question. instance, write following JAX-specific layer using jax$numpy: equivalent TensorFlow-specific layer: equivalent PyTorch-specific layer: cross-backend compatibility tremendously useful property, strongly recommend seek always make layers backend-agnostic leveraging Keras APIs.","code":"jax <- reticulate::import(\"jax\")  Linear <- new_layer_class(   ...   call = function(inputs) {     jax$numpy$matmul(inputs, self$w) + self$b   } ) Linear <- new_layer_class(   ...   call = function(inputs) {     tf$matmul(inputs, self$w) + self$b   } ) torch <- reticulate::import(\"torch\")  Linear <- new_layer_class(   ...   call = function(inputs) {     torch$matmul(inputs, self$w) + self$b   } )"},{"path":"https://keras.posit.co/articles/making_new_layers_and_models_via_subclassing.html","id":"the-add_loss-method","dir":"Articles","previous_headings":"","what":"The add_loss() method","title":"Making new layers and models via subclassing","text":"writing call() method layer, can create loss tensors want use later, writing training loop. doable calling self$add_loss(value): losses (including created inner layer) can retrieved via layer$losses. property reset start every call top-level layer, layer$losses always contains loss values created last forward pass. addition, loss property also contains regularization losses created weights inner layer: losses meant taken account writing custom training loops. also work seamlessly fit() (get automatically summed added main loss, ):","code":"# A layer that creates an activity regularization loss ActivityRegularizationLayer <- new_layer_class(   \"ActivityRegularizationLayer\",   initialize = function(rate = 1e-2) {     self$rate <- as.numeric(rate)     super$initialize()   },   call = function(inputs) {     self$add_loss(self$rate * k_mean(inputs))     inputs   } ) OuterLayer <- new_layer_class(   \"OuterLayer\",   initialize = function() {     super$initialize()     self$activity_reg <- ActivityRegularizationLayer(rate = 1e-2)   },   call = function(inputs) {     self$activity_reg(inputs)     inputs   } )  layer <- OuterLayer() cat(\"losses:\", length(layer$losses))  # No losses yet since the layer has never been called ## losses: 0 x <- layer(k_zeros(c(1, 1))) cat(\"losses:\", length(layer$losses))  # We created one loss value ## losses: 1 # `layer$losses` gets reset at the start of each call x <- layer(k_zeros(c(1, 1))) cat(\"losses:\", length(layer$losses)) # This is the loss created during the call above ## losses: 1 OuterLayerWithKernelRegularizer <- new_layer_class(   \"OuterLayerWithKernelRegularizer\",   initialize = function() {     super$initialize()     self$dense <- layer_dense(units = 32, kernel_regularizer = regularizer_l2(1e-3))   },   call = function(inputs) {     self$dense(inputs)   } )  layer <- OuterLayerWithKernelRegularizer() x <- layer(k_zeros(c(1, 1)))  # This is `1e-3 * sum(layer$dense$kernel ** 2)`, # created by the `kernel_regularizer` above. print(layer$losses) ## [[1]] ## tf.Tensor(0.0017063234, shape=(), dtype=float32) inputs <- layer_input(shape = 3) outputs <- ActivityRegularizationLayer(inputs) model <- keras_model(inputs, outputs)  # If there is a loss passed in `compile`, the regularization # losses get added to it model %>% compile(optimizer=\"adam\", loss=\"mse\") model %>% fit(random_normal(c(2, 3)), random_normal(c(2, 3)), epochs = 1) ## 1/1 - 0s - 42ms/step - loss: 3.3898 # It's also possible not to pass any loss in `compile`, # since the model already has a loss to minimize, via the `add_loss` # call during the forward pass! model %>% compile(optimizer=\"adam\") model %>% fit(random_normal(c(2, 3)), random_normal(c(2, 3)), epochs = 1) ## 1/1 - 0s - 30ms/step - loss: -1.4923e-03"},{"path":"https://keras.posit.co/articles/making_new_layers_and_models_via_subclassing.html","id":"you-can-optionally-enable-serialization-on-your-layers","dir":"Articles","previous_headings":"","what":"You can optionally enable serialization on your layers","title":"Making new layers and models via subclassing","text":"need custom layers serializable part Functional model, can optionally implement get_config() method: Note initialize() method base Layer class takes keyword arguments, particular name dtype. ’s good practice pass arguments parent class initialize() include layer config: need flexibility deserializing layer config, can also override from_config() class method. base implementation from_config(): learn serialization saving, see complete guide saving serializing models.","code":"Linear <- new_layer_class(   \"Linear\",   initialize = function(units = 32) {     self$units <- as.integer(units)     super$initialize()   },   build = function(input_shape) {     self$w <- self$add_weight(       shape = c(tail(input_shape, 1), self$units),       initializer = \"random_normal\",       trainable = TRUE     )     self$b <- self$add_weight(       shape = c(self$units),       initializer = \"zeros\",       trainable = TRUE     )   },   call = function(inputs) {     k_matmul(inputs, self$w) + self$b   },   get_config = function() {     list(units = self$units)   } )  # Now you can recreate the layer from its config: layer <- Linear(units = 64) config <- get_config(layer) print(config) ## {'units': 64} new_layer <- from_config(config) Linear <- new_layer_class(   \"Linear\",   initialize = function(units = 32, ...) {     self$units <- as.integer(units)     super$initialize(...)   },   build = function(input_shape) {     self$w <- self$add_weight(       shape = c(tail(input_shape, 1), self$units),       initializer = \"random_normal\",       trainable = TRUE     )     self$b <- self$add_weight(       shape = c(self$units),       initializer = \"zeros\",       trainable = TRUE     )   },   call = function(inputs) {     k_matmul(inputs, self$w) + self$b   },   get_config = function() {     list(units = self$units)   } )  layer <- Linear(units = 64) config <- get_config(layer) print(config) ## {'units': 64} new_layer <- from_config(config) from_config <- function(config) {   do.call(cls, config) }"},{"path":"https://keras.posit.co/articles/making_new_layers_and_models_via_subclassing.html","id":"privileged-training-argument-in-the-call-method","dir":"Articles","previous_headings":"","what":"Privileged training argument in the call() method","title":"Making new layers and models via subclassing","text":"layers, particular BatchNormalization layer Dropout layer, different behaviors training inference. layers, standard practice expose training (boolean) argument call() method. exposing argument call(), enable built-training evaluation loops (e.g. fit()) correctly use layer training inference.","code":"CustomDropout <- new_layer_class(   \"CustomDropout\",   initialize = function(rate, ...) {     super$initialize(...)     self$rate <- rate   },   call = function(inputs, training = NULL) {     if (!is.null(training) && training) {       return(random_dropout(inputs, rate = self$rate))     }     inputs   } )"},{"path":"https://keras.posit.co/articles/making_new_layers_and_models_via_subclassing.html","id":"privileged-mask-argument-in-the-call-method","dir":"Articles","previous_headings":"","what":"Privileged mask argument in the call() method","title":"Making new layers and models via subclassing","text":"privileged argument supported call() mask argument. find Keras RNN layers. mask boolean tensor (one boolean value per timestep input) used skip certain input timesteps processing timeseries data. Keras automatically pass correct mask argument call() layers support , mask generated prior layer. Mask-generating layers Embedding layer configured mask_zero = TRUE, Masking layer.","code":""},{"path":"https://keras.posit.co/articles/making_new_layers_and_models_via_subclassing.html","id":"the-model-class","dir":"Articles","previous_headings":"","what":"The Model class","title":"Making new layers and models via subclassing","text":"general, use Layer class define inner computation blocks, use Model class define outer model – object train. instance, ResNet50 model, several ResNet blocks subclassing Layer, single Model encompassing entire ResNet50 network. Model class API Layer, following differences: exposes built-training, evaluation, prediction loops (fit(), evaluate(), predict()). exposes list inner layers, via model$layers property. exposes saving serialization APIs (save(), save_weights()…) Effectively, Layer class corresponds refer literature “layer” (“convolution layer” “recurrent layer”) “block” (“ResNet block” “Inception block”). Meanwhile, Model class corresponds referred literature “model” (“deep learning model”) “network” (“deep neural network”). ’re wondering, “use Layer class Model class?”, ask : need call fit() ? need call save() ? , go Model. (either class just block bigger system, writing training & saving code ), use Layer. instance, take mini-resnet example , use build Model train fit(), save save_weights():","code":"ResNet <- new_model_class(   \"ResNet\",   initialize = function(num_classes = 1000, ...) {     super$initialize(...)     self$block_1 <- ResNetBlock()     self$block_2 <- ResNetBlock()     self$global_pool <- layers$GlobalAveragePooling2D()     self$classifier <- layers$Dense(num_classes)   },   call = function(inputs) {     x <- self$block_1(inputs)     x <- self$block_2(x)     x <- self$global_pool(x)     self$classifier(x)   } )  resnet <- ResNet() dataset = ... resnet %>% fit(dataset, epochs=10) resnet$save(\"filepath.keras\")"},{"path":"https://keras.posit.co/articles/making_new_layers_and_models_via_subclassing.html","id":"putting-it-all-together-an-end-to-end-example","dir":"Articles","previous_headings":"","what":"Putting it all together: an end-to-end example","title":"Making new layers and models via subclassing","text":"’s ’ve learned far: Layer encapsulate state (created initialize() build()) computation (defined call()). Layers can recursively nested create new, bigger computation blocks. Layers backend-agnostic long use Keras APIs. can use backend-native APIs (jax.numpy, torch.nn tf.nn), layer usable specific backend. Layers can create track losses (typically regularization losses) via add_loss(). outer container, thing want train, Model. Model just like Layer, added training serialization utilities. Let’s put things together end--end example: ’re going implement Variational AutoEncoder (VAE) backend-agnostic fashion – runs TensorFlow, JAX, PyTorch. ’ll train MNIST digits. VAE subclass Model, built nested composition layers subclass Layer. feature regularization loss (KL divergence). Let’s train MNIST using fit() API:","code":"Sampling <- new_layer_class(   \"Sampling\",   call = function(inputs) {     c(z_mean, z_log_var) %<-% inputs     batch <- k_shape(z_mean)[[1]]     dim <- k_shape(z_mean)[[2]]     epsilon <- random_normal(shape = c(batch, dim))     z_mean + k_exp(0.5 * z_log_var) * epsilon   } )  # Maps MNIST digits to a triplet (z_mean, z_log_var, z). Encoder <- new_layer_class(   \"Encoder\",   initialize = function(latent_dim = 32, intermediate_dim = 64, ...) {     super$initialize(...)     self$dense_proj <- layer_dense(units = intermediate_dim, activation = \"relu\")     self$dense_mean <- layer_dense(units = latent_dim)     self$dense_log_var <- layer_dense(units = latent_dim)     self$sampling <- Sampling()   },   call = function(inputs) {     x <- self$dense_proj(inputs)     z_mean <- self$dense_mean(x)     z_log_var <- self$dense_log_var(x)     z <- self$sampling(list(z_mean, z_log_var))     list(z_mean, z_log_var, z)   } )  # Converts z, the encoded digit vector, back into a readable digit. Decoder <- new_layer_class(   \"Decoder\",   initialize = function(original_dim, intermediate_dim = 64, ...) {     super$initialize(...)     self$dense_proj <- layer_dense(units = intermediate_dim, activation = \"relu\")     self$dense_output <- layer_dense(units = original_dim, activation = \"sigmoid\")   },   call = function(inputs) {     x <- self$dense_proj(inputs)     self$dense_output(x)   } )  # Combines the encoder and decoder into an end-to-end model for training. VariationalAutoEncoder <- new_model_class(   \"VariationalAutoEncoder\",   initialize = function(original_dim,         intermediate_dim=64,         latent_dim=32,         name=\"autoencoder\", ...) {     super$initialize(name = name, ...)     self$original_dim <- original_dim     self$encoder <- Encoder(       latent_dim = latent_dim,       intermediate_dim = intermediate_dim     )     self$decoder <- Decoder(       original_dim = original_dim,       intermediate_dim = intermediate_dim     )   },   call = function(inputs) {     c(z_mean, z_log_var, z) %<-% self$encoder(inputs)     reconstructed <- self$decoder(z)     # Add KL divergence regularization loss.     kl_loss <- -0.5 * k_mean(       z_log_var - k_square(z_mean) - k_exp(z_log_var) + 1     )     self$add_loss(kl_loss)     reconstructed   } ) c(c(x_train, .), .) %<-% dataset_mnist() x_train <- x_train %>%   keras_array() %>%   k_reshape(new_shape = c(60000, 784)) %>%   k_cast(\"float32\") %>%   k_divide(255)  original_dim <- 784 vae <- VariationalAutoEncoder(   original_dim = 784,   intermediate_dim = 64,   latent_dim = 32 )  optimizer <- optimizer_adam(learning_rate = 1e-3) vae %>% compile(optimizer, loss = loss_mean_squared_error())  vae %>% fit(x_train, x_train, epochs=2, batch_size=64) ## Epoch 1/2 ## 938/938 - 2s - 2ms/step - loss: 0.0748 ## Epoch 2/2 ## 938/938 - 1s - 1ms/step - loss: 0.0676"},{"path":"https://keras.posit.co/articles/mnist_convnet.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Simple MNIST convnet","text":"","code":"library(keras3)"},{"path":"https://keras.posit.co/articles/mnist_convnet.html","id":"prepare-the-data","dir":"Articles","previous_headings":"","what":"Prepare the data","title":"Simple MNIST convnet","text":"","code":"# Model / data parameters num_classes <- 10 input_shape <- c(28, 28, 1)  # Load the data and split it between train and test sets c(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_mnist()  # Scale images to the [0, 1] range x_train <- x_train / 255 x_test <- x_test / 255 # Make sure images have shape (28, 28, 1) x_train <- op_expand_dims(x_train, -1) x_test <- op_expand_dims(x_test, -1)   dim(x_train) ## [1] 60000    28    28     1 dim(x_test) ## [1] 10000    28    28     1 # convert class vectors to binary class matrices y_train <- to_categorical(y_train, num_classes) y_test <- to_categorical(y_test, num_classes)"},{"path":"https://keras.posit.co/articles/mnist_convnet.html","id":"build-the-model","dir":"Articles","previous_headings":"","what":"Build the model","title":"Simple MNIST convnet","text":"","code":"model <- keras_model_sequential(input_shape = input_shape) model %>%   layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = \"relu\") %>%   layer_max_pooling_2d(pool_size = c(2, 2)) %>%   layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = \"relu\") %>%   layer_max_pooling_2d(pool_size = c(2, 2)) %>%   layer_flatten() %>%   layer_dropout(rate = 0.5) %>%   layer_dense(units = num_classes, activation = \"softmax\")  summary(model) ## [1mModel: \"sequential\"[0m ## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┓ ## ┃[1m [0m[1mLayer (type)                 [0m[1m [0m┃[1m [0m[1mOutput Shape             [0m[1m [0m┃[1m [0m[1m  Param #[0m[1m [0m┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━┩ ## │ conv2d_1 ([38;5;33mConv2D[0m)             │ ([38;5;45mNone[0m, [38;5;34m26[0m, [38;5;34m26[0m, [38;5;34m32[0m)        │       [38;5;34m320[0m │ ## ├───────────────────────────────┼───────────────────────────┼───────────┤ ## │ max_pooling2d_1               │ ([38;5;45mNone[0m, [38;5;34m13[0m, [38;5;34m13[0m, [38;5;34m32[0m)        │         [38;5;34m0[0m │ ## │ ([38;5;33mMaxPooling2D[0m)                │                           │           │ ## ├───────────────────────────────┼───────────────────────────┼───────────┤ ## │ conv2d ([38;5;33mConv2D[0m)               │ ([38;5;45mNone[0m, [38;5;34m11[0m, [38;5;34m11[0m, [38;5;34m64[0m)        │    [38;5;34m18,496[0m │ ## ├───────────────────────────────┼───────────────────────────┼───────────┤ ## │ max_pooling2d ([38;5;33mMaxPooling2D[0m)  │ ([38;5;45mNone[0m, [38;5;34m5[0m, [38;5;34m5[0m, [38;5;34m64[0m)          │         [38;5;34m0[0m │ ## ├───────────────────────────────┼───────────────────────────┼───────────┤ ## │ flatten ([38;5;33mFlatten[0m)             │ ([38;5;45mNone[0m, [38;5;34m1600[0m)              │         [38;5;34m0[0m │ ## ├───────────────────────────────┼───────────────────────────┼───────────┤ ## │ dropout ([38;5;33mDropout[0m)             │ ([38;5;45mNone[0m, [38;5;34m1600[0m)              │         [38;5;34m0[0m │ ## ├───────────────────────────────┼───────────────────────────┼───────────┤ ## │ dense ([38;5;33mDense[0m)                 │ ([38;5;45mNone[0m, [38;5;34m10[0m)                │    [38;5;34m16,010[0m │ ## └───────────────────────────────┴───────────────────────────┴───────────┘ ## [1m Total params: [0m[38;5;34m34,826[0m (136.04 KB) ## [1m Trainable params: [0m[38;5;34m34,826[0m (136.04 KB) ## [1m Non-trainable params: [0m[38;5;34m0[0m (0.00 B)"},{"path":"https://keras.posit.co/articles/mnist_convnet.html","id":"train-the-model","dir":"Articles","previous_headings":"","what":"Train the model","title":"Simple MNIST convnet","text":"","code":"batch_size <- 128 epochs <- 1  model %>% compile(   loss = \"categorical_crossentropy\",   optimizer = \"adam\",   metrics = \"accuracy\" )  model %>% fit(   x_train, y_train,   batch_size = batch_size,   epochs = epochs,   validation_split = 0.1 ) ## 422/422 - 6s - 15ms/step - accuracy: 0.8816 - loss: 0.3836 - val_accuracy: 0.9787 - val_loss: 0.0822"},{"path":"https://keras.posit.co/articles/mnist_convnet.html","id":"evaluate-the-trained-model","dir":"Articles","previous_headings":"","what":"Evaluate the trained model","title":"Simple MNIST convnet","text":"","code":"score <- model %>% evaluate(x_test, y_test, verbose=0) score ##            loss compile_metrics ##      0.09020368      0.97509998"},{"path":"https://keras.posit.co/articles/preprocessing_layers.html","id":"keras-preprocessing","dir":"Articles","previous_headings":"","what":"Keras preprocessing","title":"Working with preprocessing layers","text":"Keras preprocessing layers API allows developers build Keras-native input processing pipelines. input processing pipelines can used independent preprocessing code non-Keras workflows, combined directly Keras models, exported part Keras SavedModel. Keras preprocessing layers, can build export models truly end--end: models accept raw images raw structured data input; models handle feature normalization feature value indexing .","code":""},{"path":[]},{"path":"https://keras.posit.co/articles/preprocessing_layers.html","id":"text-preprocessing","dir":"Articles","previous_headings":"Available preprocessing","what":"Text preprocessing","title":"Working with preprocessing layers","text":"layer_text_vectorization: turns raw strings encoded representation can read Embedding layer Dense layer.","code":""},{"path":"https://keras.posit.co/articles/preprocessing_layers.html","id":"numerical-features-preprocessing","dir":"Articles","previous_headings":"Available preprocessing","what":"Numerical features preprocessing","title":"Working with preprocessing layers","text":"layer_normalization: performs feature-wise normalization input features. layer_discretization: turns continuous numerical features integer categorical features.","code":""},{"path":"https://keras.posit.co/articles/preprocessing_layers.html","id":"categorical-features-preprocessing","dir":"Articles","previous_headings":"Available preprocessing","what":"Categorical features preprocessing","title":"Working with preprocessing layers","text":"layer_category_encoding: turns integer categorical features one-hot, multi-hot, count dense representations. layer_hashing: performs categorical feature hashing, also known “hashing trick”. layer_string_lookup: turns string categorical values encoded representation can read Embedding layer Dense layer. layer_integer_lookup: turns integer categorical values encoded representation can read Embedding layer Dense layer.","code":""},{"path":"https://keras.posit.co/articles/preprocessing_layers.html","id":"image-preprocessing","dir":"Articles","previous_headings":"Available preprocessing","what":"Image preprocessing","title":"Working with preprocessing layers","text":"layers standardizing inputs image model. layer_resizing: resizes batch images target size. layer_rescaling: rescales offsets values batch images (e.g. go inputs [0, 255] range inputs [0, 1] range. layer_center_crop: returns center crop batch images.","code":""},{"path":"https://keras.posit.co/articles/preprocessing_layers.html","id":"image-data-augmentation","dir":"Articles","previous_headings":"Available preprocessing","what":"Image data augmentation","title":"Working with preprocessing layers","text":"layers apply random augmentation transforms batch images. active training. layer_random_clip layer_random_flip layer_random_translation layer_random_rotation layer_random_zoom layer_random_contrast","code":""},{"path":"https://keras.posit.co/articles/preprocessing_layers.html","id":"the-adapt-method","dir":"Articles","previous_headings":"","what":"The adapt() method","title":"Working with preprocessing layers","text":"preprocessing layers internal state can computed based sample training data. list stateful preprocessing layers : TextVectorization: holds mapping string tokens integer indices StringLookup IntegerLookup: hold mapping input values integer indices. Normalization: holds mean standard deviation features. Discretization: holds information value bucket boundaries. Crucially, layers non-trainable. state set training; must set training, either initializing precomputed constant, “adapting” data. set state preprocessing layer exposing training data, via adapt() method: adapt() method takes either Numpy array tf.data.Dataset object. case StringLookup TextVectorization, can also pass list strings: addition, adaptable layers always expose option directly set state via constructor arguments weight assignment. intended state values known layer construction time, calculated outside adapt() call, can set without relying layer’s internal computation. instance, external vocabulary files TextVectorization, StringLookup, IntegerLookup layers already exist, can loaded directly lookup tables passing path vocabulary file layer’s constructor arguments. ’s example instantiate StringLookup layer precomputed vocabulary:","code":"library(keras3)  data <- rbind(   c(0.1, 0.2, 0.3),   c(0.8, 0.9, 1.0),   c(1.5, 1.6, 1.7) ) layer <- layer_normalization() layer %>% adapt(data) normalized_data <- layer(data)  op_mean(normalized_data) ## tf.Tensor(-2.6490953e-08, shape=(), dtype=float32) op_std(normalized_data) ## tf.Tensor(1.0, shape=(), dtype=float32) data <- c(     \"ξεῖν᾽, ἦ τοι μὲν ὄνειροι ἀμήχανοι ἀκριτόμυθοι\",     \"γίγνοντ᾽, οὐδέ τι πάντα τελείεται ἀνθρώποισι.\",     \"δοιαὶ γάρ τε πύλαι ἀμενηνῶν εἰσὶν ὀνείρων:\",     \"αἱ μὲν γὰρ κεράεσσι τετεύχαται, αἱ δ᾽ ἐλέφαντι:\",     \"τῶν οἳ μέν κ᾽ ἔλθωσι διὰ πριστοῦ ἐλέφαντος,\",     \"οἵ ῥ᾽ ἐλεφαίρονται, ἔπε᾽ ἀκράαντα φέροντες:\",     \"οἱ δὲ διὰ ξεστῶν κεράων ἔλθωσι θύραζε,\",     \"οἵ ῥ᾽ ἔτυμα κραίνουσι, βροτῶν ὅτε κέν τις ἴδηται.\" ) layer <- layer_text_vectorization() layer %>% adapt(data) vectorized_text <- layer(data) vectorized_text ## tf.Tensor( ## [[37 12 25  5  9 20 21  0  0] ##  [51 34 27 33 29 18  0  0  0] ##  [49 52 30 31 19 46 10  0  0] ##  [ 7  5 50 43 28  7 47 17  0] ##  [24 35 39 40  3  6 32 16  0] ##  [ 4  2 15 14 22 23  0  0  0] ##  [36 48  6 38 42  3 45  0  0] ##  [ 4  2 13 41 53  8 44 26 11]], shape=(8, 9), dtype=int64) vocab <- c(\"a\", \"b\", \"c\", \"d\") data <- rbind(c(\"a\", \"c\", \"d\"), c(\"d\", \"z\", \"b\")) layer <- layer_string_lookup(vocabulary=vocab) vectorized_data <- layer(data) vectorized_data ## tf.Tensor( ## [[1 3 4] ##  [4 0 2]], shape=(2, 3), dtype=int64)"},{"path":"https://keras.posit.co/articles/preprocessing_layers.html","id":"preprocessing-data-before-the-model-or-inside-the-model","dir":"Articles","previous_headings":"","what":"Preprocessing data before the model or inside the model","title":"Working with preprocessing layers","text":"two ways using preprocessing layers: Option 1: Make part model, like : option, preprocessing happen device, synchronously rest model execution, meaning benefit GPU acceleration. ’re training GPU, best option Normalization layer, image preprocessing data augmentation layers. Option 2: apply tf.data.Dataset, obtain dataset yields batches preprocessed data, like : option, preprocessing happen CPU, asynchronously, buffered going model. addition, call dataset.prefetch(tf.data.AUTOTUNE) dataset, preprocessing happen efficiently parallel training: best option TextVectorization, structured data preprocessing layers. can also good option ’re training CPU use image preprocessing layers. Note TextVectorization layer can executed CPU, mostly dictionary lookup operation. Therefore, training model GPU TPU, put TextVectorization layer tf.data pipeline get best performance. running TPU, always place preprocessing layers tf$data pipeline (exception Normalization Rescaling, run fine TPU commonly used first layer image model).","code":"inputs <- layer_input(shape=input_shape) x <- preprocessing_layer(inputs) outputs <- rest_of_the_model(x) model <- keras_model(inputs, outputs) dataset <- dataset %>%   dataset_map(function(x, y) list(preprocessing_layer(x), y)) dataset <- dataset %>%   dataset_map(function(x, y) list(preprocessing_layer(x), y)) %>%   dataset_prefetch(tf$data$AUTOTUNE) model %>% fit(dataset, ...)"},{"path":"https://keras.posit.co/articles/preprocessing_layers.html","id":"benefits-of-doing-preprocessing-inside-the-model-at-inference-time","dir":"Articles","previous_headings":"","what":"Benefits of doing preprocessing inside the model at inference time","title":"Working with preprocessing layers","text":"Even go option 2, may later want export inference-end--end model include preprocessing layers. key benefit makes model portable helps reduce training/serving skew. data preprocessing part model, people can load use model without aware feature expected encoded & normalized. inference model able process raw images raw structured data, require users model aware details e.g. tokenization scheme used text, indexing scheme used categorical features, whether image pixel values normalized [-1, +1] [0, 1], etc. especially powerful ’re exporting model another runtime, TensorFlow.js: won’t reimplement preprocessing pipeline JavaScript. initially put preprocessing layers tf.data pipeline, can export inference model packages preprocessing. Simply instantiate new model chains preprocessing layers training model:","code":"inputs <- layer_input(shape=input_shape) x <- preprocessing_layer(inputs) outputs <- training_model(x) inference_model <- keras_model(inputs, outputs)"},{"path":"https://keras.posit.co/articles/preprocessing_layers.html","id":"preprocessing-during-multi-worker-training","dir":"Articles","previous_headings":"","what":"Preprocessing during multi-worker training","title":"Working with preprocessing layers","text":"Preprocessing layers compatible tf$distribute API running training across multiple machines. general, preprocessing layers placed inside tf.distribute.Strategy.scope() called either inside model discussed . details, refer Data preprocessing section Distributed input tutorial.","code":"with (strategy$scope(), {   inputs <- layer_input(shape=input_shape)   preprocessing_layer = layer_hashing(10)   dense_layer = tf.keras.layers.Dense(16) })"},{"path":[]},{"path":"https://keras.posit.co/articles/preprocessing_layers.html","id":"image-data-augmentation-1","dir":"Articles","previous_headings":"Quick recipes","what":"Image data augmentation","title":"Working with preprocessing layers","text":"Note image data augmentation layers active training (similarly Dropout layer). can see similar setup action example image classification scratch.","code":"# Create a data augmentation stage with horizontal flipping, rotations, zooms data_augmentation <- keras_model_sequential() %>%   layer_random_flip(\"horizontal\") %>%   layer_random_rotation(0.1) %>%   layer_random_zoom(0.1)  # Load some data c(c(x_train, y_train), .) %<-% dataset_cifar10() input_shape <- dim(x_train)[-1] classes <- 10  # Create a tf.data pipeline of augmented images (and their labels) train_dataset <- tfdatasets::tensor_slices_dataset(list(x_train, y_train)) %>%   tfdatasets::dataset_batch(16) %>%   tfdatasets::dataset_map(function(x, y) list(data_augmentation(x), y))  # Create a model and train it on the augmented image data inputs <- layer_input(shape=input_shape) x <- layer_rescaling(inputs, 1.0 / 255)# Rescale inputs outputs <- application_resnet50(  # Add the rest of the model     weights=NULL, input_shape=input_shape, classes=classes )(x) model <- keras_model(inputs, outputs) model %>% compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\") model %>% fit(train_dataset, steps_per_epoch=5) ## Epoch 1/10 ## 5/5 - 7s - 1s/step - loss: 7.4957 ## Epoch 2/10 ## 5/5 - 1s - 151ms/step - loss: 4.6604 ## Epoch 3/10 ## 5/5 - 1s - 150ms/step - loss: 3.5557 ## Epoch 4/10 ## 5/5 - 1s - 148ms/step - loss: 3.5326 ## Epoch 5/10 ## 5/5 - 1s - 149ms/step - loss: 3.3773 ## Epoch 6/10 ## 5/5 - 1s - 150ms/step - loss: 4.4809 ## Epoch 7/10 ## 5/5 - 1s - 150ms/step - loss: 4.4726 ## Epoch 8/10 ## 5/5 - 1s - 149ms/step - loss: 4.3798 ## Epoch 9/10 ## 5/5 - 1s - 149ms/step - loss: 3.7721 ## Epoch 10/10 ## 5/5 - 1s - 150ms/step - loss: 3.2425"},{"path":"https://keras.posit.co/articles/preprocessing_layers.html","id":"normalizing-numerical-features","dir":"Articles","previous_headings":"Quick recipes","what":"Normalizing numerical features","title":"Working with preprocessing layers","text":"","code":"# Load some data c(c(x_train, y_train), .) %<-% dataset_cifar10() x_train <- array_reshape(x_train, c(dim(x_train)[1], -1))  input_shape <- dim(x_train)[-1] classes <- 10  # Create a Normalization layer and set its internal state using the training data normalizer <- layer_normalization() normalizer %>% adapt(x_train)  # Create a model that include the normalization layer inputs <- layer_input(shape=input_shape) x <- normalizer(inputs) outputs <- layer_dense(x, classes, activation=\"softmax\") model <- keras_model(inputs, outputs)  # Train the model model %>% compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\") model %>% fit(x_train, y_train, epochs = 1) ## 1563/1563 - 1s - 752us/step - loss: 2.1387"},{"path":"https://keras.posit.co/articles/preprocessing_layers.html","id":"encoding-string-categorical-features-via-one-hot-encoding","dir":"Articles","previous_headings":"Quick recipes","what":"Encoding string categorical features via one-hot encoding","title":"Working with preprocessing layers","text":"Note , , index 0 reserved --vocabulary values (values seen adapt()). can see StringLookup action Structured data classification scratch example.","code":"# Define some toy data data <- rbind(\"a\", \"b\", \"c\", \"b\", \"c\", \"a\")  # Use StringLookup to build an index of the feature values and encode output. lookup <- layer_string_lookup(output_mode=\"one_hot\") lookup %>% adapt(data)  # Convert new test data (which includes unknown feature values) test_data <- rbind(\"a\", \"b\", \"c\", \"b\", \"c\", \"\") encoded_data <- lookup(test_data) encoded_data ## tf.Tensor( ## [[0 0 0 1] ##  [0 0 1 0] ##  [0 1 0 0] ##  [0 0 1 0] ##  [0 1 0 0] ##  [1 0 0 0]], shape=(6, 4), dtype=int64)"},{"path":"https://keras.posit.co/articles/preprocessing_layers.html","id":"encoding-integer-categorical-features-via-one-hot-encoding","dir":"Articles","previous_headings":"Quick recipes","what":"Encoding integer categorical features via one-hot encoding","title":"Working with preprocessing layers","text":"Note index 0 reserved missing values (specify value 0), index 1 reserved --vocabulary values (values seen adapt()). can configure using mask_token oov_token constructor arguments IntegerLookup. can see IntegerLookup action example structured data classification scratch.","code":"# Define some toy data data <- rbind(10, 20, 20, 10, 30, 0)  # Use IntegerLookup to build an index of the feature values and encode output. lookup <- layer_integer_lookup(output_mode=\"one_hot\") lookup %>% adapt(data)  # Convert new test data (which includes unknown feature values) test_data <- rbind(10, 10, 20, 50, 60, 0) encoded_data <- lookup(test_data) encoded_data ## tf.Tensor( ## [[0 0 1 0 0] ##  [0 0 1 0 0] ##  [0 1 0 0 0] ##  [1 0 0 0 0] ##  [1 0 0 0 0] ##  [0 0 0 0 1]], shape=(6, 5), dtype=int64)"},{"path":"https://keras.posit.co/articles/preprocessing_layers.html","id":"applying-the-hashing-trick-to-an-integer-categorical-feature","dir":"Articles","previous_headings":"Quick recipes","what":"Applying the hashing trick to an integer categorical feature","title":"Working with preprocessing layers","text":"categorical feature can take many different values (order 10e3 higher), value appears times data, becomes impractical ineffective index one-hot encode feature values. Instead, can good idea apply “hashing trick”: hash values vector fixed size. keeps size feature space manageable, removes need explicit indexing.","code":"# Sample data: 10,000 random integers with values between 0 and 100,000 data <- random_integer(0, 100000, shape=shape(10000, 1))  # Use the Hashing layer to hash the values to the range [0, 64] hasher <- layer_hashing(num_bins=64, salt=1337)  # Use the CategoryEncoding layer to multi-hot encode the hashed values encoder <- layer_category_encoding(num_tokens=64, output_mode=\"multi_hot\") encoded_data <- encoder(hasher(data)) encoded_data$shape ## TensorShape([10000, 64])"},{"path":"https://keras.posit.co/articles/preprocessing_layers.html","id":"encoding-text-as-a-sequence-of-token-indices","dir":"Articles","previous_headings":"Quick recipes","what":"Encoding text as a sequence of token indices","title":"Working with preprocessing layers","text":"preprocess text passed Embedding layer. can see TextVectorization layer action, combined Embedding mode, example text classification scratch. Note training model, best performance, always use TextVectorization layer part input pipeline.","code":"# Define some text data to adapt the layer adapt_data = c(   \"The Brain is wider than the Sky\",   \"For put them side by side\",   \"The one the other will contain\",   \"With ease and You beside\" )  # Create a TextVectorization layer text_vectorizer <- layer_text_vectorization(output_mode=\"int\") # Index the vocabulary via `adapt()` text_vectorizer %>% adapt(adapt_data)  # Try out the layer text_vectorizer(rbind(\"The Brain is deeper than the sea\")) ## tf.Tensor([[ 2 19 14  1  9  2  1]], shape=(1, 7), dtype=int64) # Create a simple model inputs <- layer_input(shape=shape(NULL), dtype=\"int64\") outputs <- inputs %>%   layer_embedding(input_dim=text_vectorizer$vocabulary_size(), output_dim=16) %>%   layer_gru(units=8) %>%   layer_dense(units=1) model <- keras_model(inputs, outputs)  # Create a labeled dataset (which includes unknown tokens) train_dataset <- tfdatasets::tensor_slices_dataset(list(     rbind(\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"),     c(1, 0) ))  # Preprocess the string inputs, turning them into int sequences train_dataset <- train_dataset %>%   tfdatasets::dataset_batch(2) %>%   tfdatasets::dataset_map(function(x, y) list(text_vectorizer(x), y))  # Train the model on the int sequences model %>% compile(optimizer=\"rmsprop\", loss=\"mse\") model %>% fit(train_dataset) ## Epoch 1/10 ## 1/1 - 1s - 596ms/step - loss: 0.5097 ## Epoch 2/10 ## 1/1 - 0s - 12ms/step - loss: 0.4756 ## Epoch 3/10 ## 1/1 - 0s - 12ms/step - loss: 0.4529 ## Epoch 4/10 ## 1/1 - 0s - 12ms/step - loss: 0.4347 ## Epoch 5/10 ## 1/1 - 0s - 12ms/step - loss: 0.4191 ## Epoch 6/10 ## 1/1 - 0s - 12ms/step - loss: 0.4050 ## Epoch 7/10 ## 1/1 - 0s - 12ms/step - loss: 0.3921 ## Epoch 8/10 ## 1/1 - 0s - 12ms/step - loss: 0.3800 ## Epoch 9/10 ## 1/1 - 0s - 12ms/step - loss: 0.3686 ## Epoch 10/10 ## 1/1 - 0s - 11ms/step - loss: 0.3577 # For inference, you can export a model that accepts strings as input inputs <- layer_input(shape = 1, dtype=\"string\") x <- text_vectorizer(inputs) outputs <- model(x) end_to_end_model <- keras_model(inputs, outputs)  # Call the end-to-end model on test data (which includes unknown tokens) test_data <- rbind(\"The one the other will absorb\") test_output <- end_to_end_model(test_data) test_output ## tf.Tensor([[0.1398402]], shape=(1, 1), dtype=float32)"},{"path":"https://keras.posit.co/articles/preprocessing_layers.html","id":"encoding-text-as-a-dense-matrix-of-n-grams-with-multi-hot-encoding","dir":"Articles","previous_headings":"Quick recipes","what":"Encoding text as a dense matrix of N-grams with multi-hot encoding","title":"Working with preprocessing layers","text":"preprocess text passed Dense layer.","code":"# Define some text data to adapt the layer adapt_data <- rbind(   \"The Brain is wider than the Sky\",   \"For put them side by side\",   \"The one the other will contain\",   \"With ease and You beside\" ) # Instantiate TextVectorization with \"multi_hot\" output_mode # and ngrams=2 (index all bigrams) text_vectorizer <- layer_text_vectorization(output_mode=\"multi_hot\", ngrams=2) # Index the bigrams via `adapt()` text_vectorizer %>% adapt(adapt_data)  # Try out the layer text_vectorizer(rbind(\"The Brain is deeper than the sea\")) ## tf.Tensor( ## [[1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ##   1 1 0 0 0]], shape=(1, 41), dtype=int64) # Create a simple model inputs <- layer_input(shape = text_vectorizer$vocabulary_size()) outputs <- layer_dense(inputs, 1) model <- keras_model(inputs, outputs)  # Create a labeled dataset (which includes unknown tokens) train_dataset <- tfdatasets::tensor_slices_dataset(list(     rbind(\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"),     c(1, 0) ))  # Preprocess the string inputs, turning them into int sequences train_dataset <- train_dataset %>%   tfdatasets::dataset_batch(2) %>%   tfdatasets::dataset_map(function(x,y) list(text_vectorizer(x), y)) # Train the model on the int sequences model %>% compile(optimizer=\"rmsprop\", loss=\"mse\") model %>% fit(train_dataset, epochs = 2) ## Epoch 1/2 ## 1/1 - 0s - 112ms/step - loss: 1.2181 ## Epoch 2/2 ## 1/1 - 0s - 13ms/step - loss: 1.1751 # For inference, you can export a model that accepts strings as input inputs = layer_input(shape=1, dtype=\"string\") x <- text_vectorizer(inputs) outputs <- model(x) end_to_end_model <- keras_model(inputs, outputs)  # Call the end-to-end model on test data (which includes unknown tokens) test_data <- rbind(\"The one the other will absorb\") test_output <- end_to_end_model(test_data) test_output ## tf.Tensor([[-0.17866345]], shape=(1, 1), dtype=float32)"},{"path":"https://keras.posit.co/articles/preprocessing_layers.html","id":"encoding-text-as-a-dense-matrix-of-n-grams-with-tf-idf-weighting","dir":"Articles","previous_headings":"Quick recipes","what":"Encoding text as a dense matrix of N-grams with TF-IDF weighting","title":"Working with preprocessing layers","text":"alternative way preprocessing text passing Dense layer.","code":"# Define some text data to adapt the layer adapt_data <- rbind(   \"The Brain is wider than the Sky\",   \"For put them side by side\",   \"The one the other will contain\",   \"With ease and You beside\" ) # Instantiate TextVectorization with \"tf-idf\" output_mode # (multi-hot with TF-IDF weighting) and ngrams=2 (index all bigrams) text_vectorizer = layer_text_vectorization(output_mode=\"tf-idf\", ngrams=2) # Index the bigrams and learn the TF-IDF weights via `adapt()` text_vectorizer %>% adapt(adapt_data)  # Try out the layer text_vectorizer(rbind(\"The Brain is deeper than the sea\")) ## tf.Tensor( ## [[5.461647  1.6945957 0.        0.        0.        0.        0. ##   0.        0.        0.        0.        0.        0.        0. ##   0.        0.        1.0986123 1.0986123 1.0986123 0.        0. ##   0.        0.        0.        0.        0.        0.        0. ##   1.0986123 0.        0.        0.        0.        0.        0. ##   0.        1.0986123 1.0986123 0.        0.        0.       ]], shape=(1, 41), dtype=float32) # Create a simple model inputs <- layer_input(shape = text_vectorizer$vocabulary_size()) outputs <- layer_dense(inputs, 1) model <- keras_model(inputs, outputs)  # Create a labeled dataset (which includes unknown tokens) train_dataset <- tfdatasets::tensor_slices_dataset(list(     rbind(\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"),     c(1, 0) ))  # Preprocess the string inputs, turning them into int sequences train_dataset <- train_dataset %>%   tfdatasets::dataset_batch(2) %>%   tfdatasets::dataset_map(function(x,y) list(text_vectorizer(x), y))  # Train the model on the int sequences model %>% compile(optimizer=\"rmsprop\", loss=\"mse\") model %>% fit(train_dataset) ## Epoch 1/10 ## 1/1 - 0s - 113ms/step - loss: 7.9392 ## Epoch 2/10 ## 1/1 - 0s - 14ms/step - loss: 7.6545 ## Epoch 3/10 ## 1/1 - 0s - 14ms/step - loss: 7.4529 ## Epoch 4/10 ## 1/1 - 0s - 13ms/step - loss: 7.2871 ## Epoch 5/10 ## 1/1 - 0s - 15ms/step - loss: 7.1420 ## Epoch 6/10 ## 1/1 - 0s - 14ms/step - loss: 7.0108 ## Epoch 7/10 ## 1/1 - 0s - 14ms/step - loss: 6.8896 ## Epoch 8/10 ## 1/1 - 0s - 14ms/step - loss: 6.7760 ## Epoch 9/10 ## 1/1 - 0s - 14ms/step - loss: 6.6684 ## Epoch 10/10 ## 1/1 - 0s - 13ms/step - loss: 6.5657 # For inference, you can export a model that accepts strings as input inputs = layer_input(shape=1, dtype=\"string\") x <- text_vectorizer(inputs) outputs <- model(x) end_to_end_model <- keras_model(inputs, outputs)  # Call the end-to-end model on test data (which includes unknown tokens) test_data <- rbind(\"The one the other will absorb\") test_output <- end_to_end_model(test_data) test_output ## tf.Tensor([[0.35116184]], shape=(1, 1), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/articles/preprocessing_layers.html","id":"working-with-lookup-layers-with-very-large-vocabularies","dir":"Articles","previous_headings":"Important gotchas","what":"Working with lookup layers with very large vocabularies","title":"Working with preprocessing layers","text":"may find working large vocabulary TextVectorization, StringLookup layer, IntegerLookup layer. Typically, vocabulary larger 500MB considered “large”. case, best performance, avoid using adapt(). Instead, pre-compute vocabulary advance (use Apache Beam TF Transform ) store file. load vocabulary layer construction time passing file path vocabulary argument.","code":""},{"path":"https://keras.posit.co/articles/preprocessing_layers.html","id":"using-lookup-layers-on-a-tpu-pod-or-with-parameterserverstrategy-","dir":"Articles","previous_headings":"Important gotchas","what":"Using lookup layers on a TPU pod or with ParameterServerStrategy.","title":"Working with preprocessing layers","text":"outstanding issue causes performance degrade using TextVectorization, StringLookup, IntegerLookup layer training TPU pod multiple machines via ParameterServerStrategy. slated fixed TensorFlow 2.7.","code":""},{"path":"https://keras.posit.co/articles/sequential_model.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"The Sequential model","text":"","code":"library(keras3)"},{"path":"https://keras.posit.co/articles/sequential_model.html","id":"when-to-use-a-sequential-model","dir":"Articles","previous_headings":"","what":"When to use a Sequential model","title":"The Sequential model","text":"Sequential model appropriate plain stack layers layer exactly one input tensor one output tensor. Schematically, following Sequential model: equivalent function: Sequential model appropriate : model multiple inputs multiple outputs layers multiple inputs multiple outputs need layer sharing want non-linear topology (e.g. residual connection, multi-branch model)","code":"model <- keras_model_sequential() %>%   layer_dense(units = 2, activation = \"relu\", name = \"layer1\") %>%   layer_dense(units = 3, activation = \"relu\", name = \"layer2\") %>%   layer_dense(units = 4, name = \"layer3\")  # Call model on a test input x <- op_ones(c(3, 3)) y <- model(x) # Create 3 layers layer1 <- layer_dense(units = 2, activation=\"relu\", name=\"layer1\") layer2 <- layer_dense(units = 3, activation=\"relu\", name=\"layer2\") layer3 <- layer_dense(units = 4, name=\"layer3\")  # Call layers on a test input x <- op_ones(c(3, 3)) y <- x %>% layer1() %>% layer2() %>% layer3()"},{"path":"https://keras.posit.co/articles/sequential_model.html","id":"creating-a-sequential-model","dir":"Articles","previous_headings":"","what":"Creating a Sequential model","title":"The Sequential model","text":"can create Sequential model piping layers keras_model_sequential() object: passing list layers keras_model_sequential(): layers accessible via layers attribute: can also create Sequential model incrementally via add() method: Note ’s also corresponding pop() method remove layers: Sequential model behaves much like list layers. Also note Sequential constructor accepts name argument, just like layer model Keras. useful annotate TensorBoard graphs semantically meaningful names.","code":"model <- keras_model_sequential() %>%   layer_dense(units = 2, activation = \"relu\") %>%   layer_dense(units = 3, activation = \"relu\") %>%   layer_dense(units = 4) model <- keras_model_sequential(layers = list(   layer_dense(units = 2, activation = \"relu\"),   layer_dense(units = 3, activation = \"relu\"),   layer_dense(units = 4) )) model$layers ## [[1]] ## <Dense name=dense_3, built=False> ## ## [[2]] ## <Dense name=dense_4, built=False> ## ## [[3]] ## <Dense name=dense_5, built=False> model <- keras_model_sequential() model$add(layer_dense(units = 2, activation=\"relu\")) model$add(layer_dense(units = 3, activation=\"relu\")) model$add(layer_dense(units = 4)) model$pop() ## <Dense name=dense_8, built=False> length(model$layers)  # 2 ## [1] 2 model <- keras_model_sequential(name = \"my_sequential\") model$add(layer_dense(units = 2, activation=\"relu\", name = \"layer1\")) model$add(layer_dense(units = 3, activation=\"relu\", name = \"layer2\")) model$add(layer_dense(units = 4, name = \"layer3\"))"},{"path":"https://keras.posit.co/articles/sequential_model.html","id":"specifying-the-input-shape-in-advance","dir":"Articles","previous_headings":"","what":"Specifying the input shape in advance","title":"The Sequential model","text":"Generally, layers Keras need know shape inputs order able create weights. create layer like , initially, weights: creates weights first time called input, since shape weights depends shape inputs: Naturally, also applies Sequential models. instantiate Sequential model without input shape, isn’t “built”: weights (calling model$weights results error stating just ). weights created model first sees input data: model “built”, can call summary() method display contents: However, can useful building Sequential model incrementally able display summary model far, including current output shape. case, start model passing input_shape argument model, knows input shape start: Models built predefined input shape like always weights (even seeing data) always defined output shape. general, ’s recommended best practice always specify input shape Sequential model advance know .","code":"layer <- layer_dense(units = 3) layer$weights  # Empty ## list() # Call layer on a test input x <- op_ones(c(1, 4)) y <- layer(x) layer$weights  # Now it has weights, of shape (4, 3) and (3,) ## [[1]] ## <KerasVariable shape=(4, 3), dtype=float32, path=dense_9/kernel> ## ## [[2]] ## <KerasVariable shape=(3), dtype=float32, path=dense_9/bias> model <- keras_model_sequential() %>%   layer_dense(units = 2, activation = \"relu\") %>%   layer_dense(units = 3, activation = \"relu\") %>%   layer_dense(units = 4) # No weights at this stage!  # At this point, you can't do this: # model$weights  # You also can't do this: # summary(model)  # Call the model on a test input x <- op_ones(c(1, 4)) y <- model(x) length(model$weights) ## [1] 6 summary(model) ## Model: \"sequential_4\" ## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓ ## ┃ Layer (type)                      ┃ Output Shape                ┃    Param # ┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩ ## │ dense_12 (Dense)                  │ (1, 2)                      │         10 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ dense_11 (Dense)                  │ (1, 3)                      │          9 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ dense_10 (Dense)                  │ (1, 4)                      │         16 │ ## └───────────────────────────────────┴─────────────────────────────┴────────────┘ ##  Total params: 35 (140.00 B) ##  Trainable params: 35 (140.00 B) ##  Non-trainable params: 0 (0.00 B) model <- keras_model_sequential(input_shape = 4) %>%   layer_dense(units = 2, activation = \"relu\") summary(model) ## Model: \"sequential_5\" ## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓ ## ┃ Layer (type)                      ┃ Output Shape                ┃    Param # ┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩ ## │ dense_13 (Dense)                  │ (None, 2)                   │         10 │ ## └───────────────────────────────────┴─────────────────────────────┴────────────┘ ##  Total params: 10 (40.00 B) ##  Trainable params: 10 (40.00 B) ##  Non-trainable params: 0 (0.00 B) model$layers ## [[1]] ## <Dense name=dense_13, built=True>"},{"path":"https://keras.posit.co/articles/sequential_model.html","id":"a-common-debugging-workflow-add-summary","dir":"Articles","previous_headings":"","what":"A common debugging workflow: add() + summary()","title":"The Sequential model","text":"building new Sequential architecture, ’s useful incrementally stack layers add() %>% frequently print model summaries. instance, enables monitor stack Conv2D MaxPooling2D layers downsampling image feature maps: practical, right? Note %>% equivalent calling model$add(), theory, don’t need reasisgn model model step.","code":"model <- keras_model_sequential(input_shape = c(250, 250, 3)) %>%   layer_conv_2d(filters = 32, kernel_size = 5, strides = 2, activation = \"relu\") %>%   layer_conv_2d(filters = 32, kernel_size = 3, activation = \"relu\") %>%   layer_max_pooling_2d(pool_size = c(3, 3))  # Can you guess what the current output shape is at this point? Probably not. # Let's just print it: summary(model) ## Model: \"sequential_6\" ## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓ ## ┃ Layer (type)                      ┃ Output Shape                ┃    Param # ┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩ ## │ conv2d_1 (Conv2D)                 │ (None, 123, 123, 32)        │      2,432 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d (Conv2D)                   │ (None, 121, 121, 32)        │      9,248 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ max_pooling2d (MaxPooling2D)      │ (None, 40, 40, 32)          │          0 │ ## └───────────────────────────────────┴─────────────────────────────┴────────────┘ ##  Total params: 11,680 (45.62 KB) ##  Trainable params: 11,680 (45.62 KB) ##  Non-trainable params: 0 (0.00 B) # The answer was: (40, 40, 32), so we can keep downsampling...  model <- model %>%   layer_conv_2d(filters = 32, kernel_size = 3, activation = \"relu\") %>%   layer_conv_2d(filters = 32, kernel_size = 3, activation = \"relu\") %>%   layer_max_pooling_2d(pool_size = 3) %>%   layer_conv_2d(filters = 32, kernel_size = 3, activation = \"relu\") %>%   layer_conv_2d(filters = 32, kernel_size = 3, activation = \"relu\") %>%   layer_max_pooling_2d(pool_size = 2)  # And now? summary(model) ## Model: \"sequential_6\" ## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓ ## ┃ Layer (type)                      ┃ Output Shape                ┃    Param # ┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩ ## │ conv2d_1 (Conv2D)                 │ (None, 123, 123, 32)        │      2,432 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d (Conv2D)                   │ (None, 121, 121, 32)        │      9,248 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ max_pooling2d (MaxPooling2D)      │ (None, 40, 40, 32)          │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_5 (Conv2D)                 │ (None, 38, 38, 32)          │      9,248 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_4 (Conv2D)                 │ (None, 36, 36, 32)          │      9,248 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ max_pooling2d_2 (MaxPooling2D)    │ (None, 12, 12, 32)          │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_3 (Conv2D)                 │ (None, 10, 10, 32)          │      9,248 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_2 (Conv2D)                 │ (None, 8, 8, 32)            │      9,248 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ max_pooling2d_1 (MaxPooling2D)    │ (None, 4, 4, 32)            │          0 │ ## └───────────────────────────────────┴─────────────────────────────┴────────────┘ ##  Total params: 48,672 (190.12 KB) ##  Trainable params: 48,672 (190.12 KB) ##  Non-trainable params: 0 (0.00 B) # Now that we have 4x4 feature maps, time to apply global max pooling. model <- model %>%   layer_global_max_pooling_2d()  # Finally, we add a classification layer. model <- model %>%   layer_dense(units = 10, activation = \"softmax\")"},{"path":"https://keras.posit.co/articles/sequential_model.html","id":"what-to-do-once-you-have-a-model","dir":"Articles","previous_headings":"","what":"What to do once you have a model","title":"The Sequential model","text":"model architecture ready, want : Train model, evaluate , run inference. See guide training & evaluation built-loops Save model disk restore . See guide serialization & saving. Speed model training leveraging multiple GPUs. See guide multi-GPU distributed training.","code":""},{"path":"https://keras.posit.co/articles/sequential_model.html","id":"feature-extraction-with-a-sequential-model","dir":"Articles","previous_headings":"","what":"Feature extraction with a Sequential model","title":"The Sequential model","text":"Sequential model built, behaves like Functional API model. means every layer input output attribute. attributes can used neat things, like quickly creating model extracts outputs intermediate layers Sequential model: ’s similar example extract features one layer:","code":"initial_model <- keras_model_sequential(input_shape = c(250, 250, 3)) %>%   layer_conv_2d(filters = 32, kernel_size = 5, strides = 2, activation = \"relu\") %>%   layer_conv_2d(filters = 32, kernel_size = 3, activation = \"relu\") %>%   layer_conv_2d(filters = 32, kernel_size = 3, activation = \"relu\")   feature_extractor <- keras_model(     inputs = initial_model$inputs,     outputs = lapply(initial_model$layers, function(x) x$output), )  # Call feature extractor on test input. x <- op_ones(c(1, 250, 250, 3)) features <- feature_extractor(x) initial_model <- keras_model_sequential(input_shape = c(250, 250, 3)) %>%   layer_conv_2d(filters = 32, kernel_size = 5, strides = 2, activation = \"relu\") %>%   layer_conv_2d(filters = 32, kernel_size = 3, activation = \"relu\", name = \"my_intermediate_layer\") %>%   layer_conv_2d(filters = 32, kernel_size = 3, activation = \"relu\")  feature_extractor <- keras_model(     inputs = initial_model$inputs,     outputs =get_layer(initial_model, \"my_intermediate_layer\")$output, ) # Call feature extractor on test input. x <- op_ones(c(1, 250, 250, 3)) features <- feature_extractor(x)"},{"path":"https://keras.posit.co/articles/sequential_model.html","id":"transfer-learning-with-a-sequential-model","dir":"Articles","previous_headings":"","what":"Transfer learning with a Sequential model","title":"The Sequential model","text":"Transfer learning consists freezing bottom layers model training top layers. aren’t familiar , make sure read guide transfer learning. two common transfer learning blueprint involving Sequential models. First, let’s say Sequential model, want freeze layers except last one. case, simply iterate model.layers set layer.trainable = False layer, except last one. Like : Another common blueprint use Sequential model stack pre-trained model freshly initialized classification layers. Like : transfer learning, probably find frequently using two patterns. ’s need know Sequential models! find building models Keras, see: Guide Functional API Guide making new Layers & Models via subclassing","code":"model <- keras_model_sequential(input_shape = 784) %>%   layer_dense(units = 32, activation = \"relu\") %>%   layer_dense(units = 32, activation = \"relu\") %>%   layer_dense(units = 32, activation = \"relu\") %>%   layer_dense(units = 10)  # Presumably you would want to first load pre-trained weights. model$load_weights(...)  # Freeze all layers except the last one. for (layer in head(model$layers, 3)) {   layer$trainable <- FALSE }  # Recompile and train (this will only update the weights of the last layer). model %>% compile(...) model %>% fit(...) # Load a convolutional base with pre-trained weights base_model <- application_xception(     weights='imagenet',     include_top=FALSE,     pooling='avg')  # Freeze the base model base_model$trainable <- FALSE  # Use a Sequential model to add a trainable classifier on top model <- keras_model_sequential() %>%   base_model() %>%   layer_dense(1000) # Compile & train model %>% compile(...) model %>% fit(...)"},{"path":"https://keras.posit.co/articles/serialization_and_saving.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Save, serialize, and export models","text":"Keras model consists multiple components: architecture, configuration, specifies layers model contain, ’re connected. set weights values (“state model”). optimizer (defined compiling model). set losses metrics (defined compiling model). Keras API saves pieces together unified format, marked .keras extension. zip archive consisting following: JSON-based configuration file (config.json): Records model, layer, trackables’ configuration. H5-based state file, model.weights.h5 (whole model), directory keys layers weights. metadata file JSON, storing things current Keras version. Let’s take look works.","code":""},{"path":"https://keras.posit.co/articles/serialization_and_saving.html","id":"how-to-save-and-load-a-model","dir":"Articles","previous_headings":"","what":"How to save and load a model","title":"Save, serialize, and export models","text":"10 seconds read guide, ’s need know. Saving Keras model: Loading model back: Now, let’s look details.","code":"# Get model (Sequential, Functional Model, or Model subclass) model <- ...  # The filename needs to end with the .keras extension model |> save_model('path/to/location.keras') model <- load_model('path/to/location.keras')"},{"path":"https://keras.posit.co/articles/serialization_and_saving.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Save, serialize, and export models","text":"","code":"library(keras3)"},{"path":"https://keras.posit.co/articles/serialization_and_saving.html","id":"saving","dir":"Articles","previous_headings":"","what":"Saving","title":"Save, serialize, and export models","text":"section saving entire model single file. file include: model’s architecture/config model’s weight values (learned training) model’s compilation information (compile() called) optimizer state, (enables restart training left)","code":""},{"path":"https://keras.posit.co/articles/serialization_and_saving.html","id":"apis","dir":"Articles","previous_headings":"Saving","what":"APIs","title":"Save, serialize, and export models","text":"can save model save_model(). can load back load_model(). supported format Keras 3 “Keras v3” format, uses .keras extension. Example:","code":"get_model <- function() {   # Create a simple model.   inputs <- layer_input(shape(32))   outputs <- inputs |> layer_dense(1)   model <-  keras_model(inputs, outputs)   model |> compile(optimizer = optimizer_adam(), loss = \"mean_squared_error\")   model }  model <- get_model()  # Train the model. test_input <- random_uniform(c(128, 32)) test_target <- random_uniform(c(128, 1)) model |> fit(test_input, test_target)  # Calling `save('my_model.keras')` creates a zip archive `my_model.keras`. model |> save_model(\"my_model.keras\") ## Error: File 'my_model.keras' already exists (pass overwrite = TRUE to force save). # It can be used to reconstruct the model identically. reconstructed_model <- load_model(\"my_model.keras\")  # Let's check: stopifnot(all.equal(   model |> predict(test_input),   reconstructed_model |> predict(test_input) )) ## Error in eval(expr, envir, enclos): predict(model, test_input) and predict(reconstructed_model, test_input) are not equal: ##   Mean relative difference: 1.580108"},{"path":"https://keras.posit.co/articles/serialization_and_saving.html","id":"custom-objects","dir":"Articles","previous_headings":"Saving","what":"Custom objects","title":"Save, serialize, and export models","text":"section covers basic workflows handling custom layers, functions, models Keras saving reloading. saving model includes custom objects, subclassed Layer, must define get_config() method object class. arguments passed constructor (initialize() method) custom object aren’t simple objects (anything types like ints, strings, etc.), must also explicitly deserialize arguments from_config() class method. Like : Please see Defining config methods section details examples. saved .keras file lightweight store Python code custom objects. Therefore, reload model, load_model requires access definition custom objects used one following methods: Registering custom objects (preferred), Passing custom objects directly loading, Using custom object scope examples workflow:","code":"layer_custom <- Layer(   \"CustomLayer\",   initialize = function(sublayer, ...) {     super$initialize(...)     self$sublayer <- layer   },   call = function(x) {     self$sublayer(x)   },   get_config = function() {     base_config <- super$get_config()     config <- list(       sublayer = keras$saving$serialize_keras_object(self$sublayer)     )     c(base_config, config)   },   from_config = function(cls, config) {     sublayer_config <- config$sublayer     sublayer <- keras$saving$deserialize_keras_object(sublayer_config)     cls(sublayer, !!!config)   } )"},{"path":"https://keras.posit.co/articles/serialization_and_saving.html","id":"registering-custom-objects-preferred","dir":"Articles","previous_headings":"Saving > Custom objects","what":"Registering custom objects (preferred)","title":"Save, serialize, and export models","text":"preferred method, custom object registration greatly simplifies saving loading code. Calling register_custom_object() custom object registers object globally master list, allowing Keras recognize object loading model. Let’s create custom model involving custom layer custom activation function demonstrate . Example:","code":"# Clear all previously registered custom objects clear_registered_custom_objects() ## Error in clear_registered_custom_objects(): could not find function \"clear_registered_custom_objects\" layer_custom <- Layer(   \"CustomLayer\",   initialize = function(self, factor) {     super$initialize()     self$factor = factor   },    call = function(self, x) {     x * self$factor   },    get_config = function(self) {     list(factor = self$factor)   } )  # Upon registration, you can optionally specify a package or a name. # If left blank, the package defaults to \"Custom\" and the name defaults to # the class name. register_custom_object(layer_custom, package = \"MyLayers\") ## Registered S3 methods overwritten by 'keras': ##   method                                                     from ##   $.py_R6ClassGenerator                                      keras3 ##   +.keras.backend.common.keras_tensor.KerasTensor            keras3 ##   ==.keras.backend.common.keras_tensor.KerasTensor           keras3 ##   [.keras_shape                                              keras3 ##   as.array.keras.backend.common.variables.KerasVariable      keras3 ##   as.data.frame.keras_training_history                       keras3 ##   as.double.keras.backend.common.variables.KerasVariable     keras3 ##   as.integer.keras.backend.common.variables.KerasVariable    keras3 ##   as.integer.keras_shape                                     keras3 ##   as.list.keras_shape                                        keras3 ##   as.numeric.keras.backend.common.variables.KerasVariable    keras3 ##   compile.keras.models.model.Model                           keras3 ##   destructure.keras_shape                                    keras3 ##   evaluate.keras.models.model.Model                          keras3 ##   fit.keras.models.model.Model                               keras3 ##   format.keras.models.model.Model                            keras3 ##   format.keras_shape                                         keras3 ##   plot.keras.models.model.Model                              keras3 ##   plot.keras_training_history                                keras3 ##   predict.keras.models.model.Model                           keras3 ##   print.keras.models.model.Model                             keras3 ##   print.keras_shape                                          keras3 ##   print.keras_training_history                               keras3 ##   print.kerastools.model.RModel                              keras3 ##   print.py_R6ClassGenerator                                  keras3 ##   py_str.keras.models.model.Model                            keras3 ##   py_to_r.keras.utils.generic_utils.SharedObjectConfig       keras3 ##   py_to_r.tensorflow.python.ops.gen_linalg_ops.Qr            keras3 ##   py_to_r.tensorflow.python.ops.gen_nn_ops.TopKV2            keras3 ##   py_to_r_wrapper.keras.initializers.initializer.Initializer keras3 ##   py_to_r_wrapper.keras.layers.layer.Layer                   keras3 ##   py_to_r_wrapper.keras.losses.loss.Loss                     keras3 ##   py_to_r_wrapper.keras.models.model.Model                   keras3 ##   py_to_r_wrapper.kerastools.model.RModel                    keras3 ##   r_to_py.R6ClassGenerator                                   keras3 ##   r_to_py.keras_layer_wrapper                                keras3 ##   r_to_py.keras_shape                                        keras3 ##   summary.keras.models.model.Model                           keras3 ##   summary.kerastools.model.RModel                            keras3 custom_fn <- keras3:::py_func2(function(x) x^2, name = \"custom_fn\", convert = TRUE)  register_custom_object(custom_fn, name=\"custom_fn\", package=\"my_package\")   # Create the model. get_model <- function() {   inputs <- layer_input(shape(4))   mid <- inputs |> layer_custom(0.5)   outputs <- mid |> layer_dense(1, activation = custom_fn)   model <- keras_model(inputs, outputs)   model |> compile(optimizer = \"rmsprop\", loss = \"mean_squared_error\")   model }   # Train the model. train_model <- function(model) {   input <- random_uniform(c(4, 4))   target <- random_uniform(c(4, 1))   model |> fit(input, target, verbose = FALSE, epochs = 1)   model }  test_input <- random_uniform(c(4, 4)) test_target <- random_uniform(c(4, 1))  model <- get_model() |> train_model() model |> save_model(\"custom_model.keras\", overwrite = TRUE)  # Now, we can simply load without worrying about our custom objects. reconstructed_model <- load_model(\"custom_model.keras\")  # model$get_config()$layers[[3]]$config$activation -> z # keras$saving$deserialize_keras_object(z)  # Let's check: stopifnot(all.equal(   model |> predict(test_input, verbose = FALSE),   reconstructed_model |> predict(test_input, verbose = FALSE) ))"},{"path":"https://keras.posit.co/articles/serialization_and_saving.html","id":"passing-custom-objects-to-load_model","dir":"Articles","previous_headings":"Saving > Custom objects","what":"Passing custom objects to load_model()","title":"Save, serialize, and export models","text":"","code":"model <- get_model() |> train_model()  # Calling `save_model('my_model.keras')` creates a zip archive `my_model.keras`. model |> save_model(\"custom_model.keras\", overwrite = TRUE)  # Upon loading, pass a named list containing the custom objects used in the # `custom_objects` argument of `load_model()`. reconstructed_model <-  load_model(   \"custom_model.keras\",   custom_objects = list(CustomLayer = layer_custom,                         custom_fn = custom_fn), )  # Let's check: stopifnot(all.equal(   model |> predict(test_input, verbose = FALSE),   reconstructed_model |> predict(test_input, verbose = FALSE) ))"},{"path":"https://keras.posit.co/articles/serialization_and_saving.html","id":"using-a-custom-object-scope","dir":"Articles","previous_headings":"Saving > Custom objects","what":"Using a custom object scope","title":"Save, serialize, and export models","text":"code within custom object scope able recognize custom objects passed scope argument. Therefore, loading model within scope allow loading custom objects. Example:","code":"model <- get_model() |> train_model() model |> save_model(\"custom_model.keras\", overwrite = TRUE)  # Pass the custom objects dictionary to a custom object scope and place # the `keras.models.load_model()` call within the scope. custom_objects <- list(CustomLayer = layer_custom, custom_fn = custom_fn)  with_custom_object_scope(custom_objects, {   reconstructed_model <- load_model(\"custom_model.keras\") })  # Let's check: stopifnot(all.equal(   model |> predict(test_input, verbose = FALSE),   reconstructed_model |> predict(test_input, verbose = FALSE) ))"},{"path":"https://keras.posit.co/articles/serialization_and_saving.html","id":"model-serialization","dir":"Articles","previous_headings":"Saving","what":"Model serialization","title":"Save, serialize, and export models","text":"section saving model’s configuration, without state. model’s configuration (architecture) specifies layers model contains, layers connected. configuration model, model can created freshly initialized state (weights compilation information).","code":""},{"path":"https://keras.posit.co/articles/serialization_and_saving.html","id":"apis-1","dir":"Articles","previous_headings":"Saving > Model serialization","what":"APIs","title":"Save, serialize, and export models","text":"following serialization APIs available: clone_model(model): make (randomly initialized) copy model. get_config() cls.from_config(): retrieve configuration layer model, recreate model instance config, respectively. keras.models.model_to_json() keras.models.model_from_json(): similar, JSON strings. keras.saving.serialize_keras_object(): retrieve configuration arbitrary Keras object. keras.saving.deserialize_keras_object(): recreate object instance configuration.","code":""},{"path":"https://keras.posit.co/articles/serialization_and_saving.html","id":"in-memory-model-cloning","dir":"Articles","previous_headings":"Saving > Model serialization","what":"In-memory model cloning","title":"Save, serialize, and export models","text":"can -memory cloning model via clone_model(). equivalent getting config recreating model config (preserve compilation information layer weights values). Example:","code":"new_model <- clone_model(model)"},{"path":"https://keras.posit.co/articles/serialization_and_saving.html","id":"get_config-and-from_config","dir":"Articles","previous_headings":"Saving > Model serialization","what":"get_config() and from_config()","title":"Save, serialize, and export models","text":"Calling get_config(model) get_config(layer) return named list containing configuration model layer, respectively. define get_config() contain arguments needed initialize() method model layer. loading time, from_config(config) method call initialize() arguments reconstruct model layer. Layer example: Now let’s reconstruct layer using from_config() method: Sequential model example: Functional model example:","code":"layer <- layer_dense(, 3, activation=\"relu\") layer_config <- get_config(layer) str(layer_config) ## Dict (12 items) new_layer <- from_config(layer_config) model <- keras_model_sequential(input_shape = c(32)) |>   layer_dense(1) config <- get_config(model) new_model <- from_config(config) inputs <- layer_input(c(32)) outputs <- inputs |> layer_dense(1) model <- keras_model(inputs, outputs) config <- get_config(model) new_model <- from_config(config)"},{"path":"https://keras.posit.co/articles/serialization_and_saving.html","id":"save_model_config-and-load_model_config","dir":"Articles","previous_headings":"Saving > Model serialization","what":"save_model_config() and load_model_config()","title":"Save, serialize, and export models","text":"similar get_config / from_config, except turns model JSON file, can loaded without original model class. also specific models, isn’t meant layers. Example:","code":"model <- keras_model_sequential(input_shape = c(32)) |>   layer_dense(1) save_model_config(model, \"model_config.json\") new_model <- load_model_config(\"model_config.json\") unlink(\"model_config.json\")"},{"path":"https://keras.posit.co/articles/serialization_and_saving.html","id":"arbitrary-object-serialization-and-deserialization","dir":"Articles","previous_headings":"Saving > Model serialization","what":"Arbitrary object serialization and deserialization","title":"Save, serialize, and export models","text":"serialize_keras_object() deserialize_keras_object() APIs general-purpose APIs can used serialize deserialize Keras object custom object. foundation saving model architecture behind serialize()/deserialize() calls keras. Example: Note serialization format containing necessary information proper reconstruction: module containing name Keras module identifying module object comes class_name containing name object’s class. config information needed reconstruct object registered_name custom objects. See . Now can reconstruct regularizer.","code":"my_reg <- regularizer_l1(0.005) config <- serialize_keras_object(my_reg) ## Error in serialize_keras_object(my_reg): could not find function \"serialize_keras_object\" str(config) ## {'name': 'functional_11', 'trainable': True, 'layers': [{'module': 'keras.src.layers.core.input_layer', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 32), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_5'}, 'registered_name': 'InputLayer', 'name': 'input_layer_5', 'inbound_nodes': []}, {'module': 'keras.src.layers.core.dense', 'class_name': 'Dense', 'config': {'name': 'dense_6', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.src.initializers.random_initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': 'GlorotUniform'}, 'bias_initializer': {'module': 'keras.src.initializers.constant_initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': 'Zeros'}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': 'Dense', 'build_config': {'input_shape': (None, 32)}, 'name': 'dense_6', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 32), 'dtype': 'float32', 'keras_history': ['input_layer_5', 0, 0]}},), 'kwargs': {}}]}], 'input_layers': [['input_layer_5', 0, 0]], 'output_layers': [['dense_6', 0, 0]]} new_reg <- deserialize_keras_object(config) ## Error in deserialize_keras_object(config): could not find function \"deserialize_keras_object\" new_reg ## Error in eval(expr, envir, enclos): object 'new_reg' not found"},{"path":"https://keras.posit.co/articles/serialization_and_saving.html","id":"model-weights-saving","dir":"Articles","previous_headings":"Saving","what":"Model weights saving","title":"Save, serialize, and export models","text":"can choose save & load model’s weights. can useful : need model inference: case won’t need restart training, don’t need compilation information optimizer state. transfer learning: case training new model reusing state prior model, don’t need compilation information prior model.","code":""},{"path":"https://keras.posit.co/articles/serialization_and_saving.html","id":"apis-for-in-memory-weight-transfer","dir":"Articles","previous_headings":"Saving > Model weights saving","what":"APIs for in-memory weight transfer","title":"Save, serialize, and export models","text":"Weights can copied different objects using get_weights() set_weights(): get_weights(<layer>): Returns list arrays weight values. set_weights(<layer>weights): Sets model/layer weights values provided (arrays). Examples: Transfering weights one layer another, memory Transfering weights one model another model compatible architecture, memory case stateless layers stateless layers change order number weights, models can compatible architectures even extra/missing stateless layers.","code":"create_layer <- function() {   layer <- layer_dense(, 64, activation = \"relu\", name = \"dense_2\")   layer$build(shape(NA, 784))   layer }  layer_1 <- create_layer() layer_2 <- create_layer()  # Copy weights from layer 1 to layer 2 layer_2 |> set_weights(get_weights(layer_1)) # Create a simple functional model inputs <- layer_input(shape=c(784), name=\"digits\") outputs <- inputs |>   layer_dense(64, activation = \"relu\", name = \"dense_1\") |>   layer_dense(64, activation = \"relu\", name = \"dense_2\") |>   layer_dense(10, name = \"predictions\") functional_model <- keras_model(inputs = inputs, outputs = outputs,                                name = \"3_layer_mlp\")  # Define a subclassed model with the same architecture SubclassedModel <- new_model_class(   \"SubclassedModel\",   initialize = function(output_dim, name = NULL) {     super$initialize(name = name)     self$output_dim <- output_dim |> as.integer()     self$dense_1 <- layer_dense(, 64, activation = \"relu\",                                 name = \"dense_1\")     self$dense_2 <- layer_dense(, 64, activation = \"relu\",                                 name = \"dense_2\")     self$dense_3 <- layer_dense(, self$output_dim,                                 name = \"predictions\")   },    call = function(inputs) {     inputs |>       self$dense_1() |>       self$dense_2() |>       self$dense_3()   },    get_config = function(self) {     list(output_dim = self$output_dim,          name = self$name)   } )   subclassed_model <- SubclassedModel(10) # Call the subclassed model once to create the weights. subclassed_model(op_ones(c(1, 784))) |> invisible()  # Copy weights from functional_model to subclassed_model. set_weights(subclassed_model, get_weights(functional_model))  stopifnot(all.equal(   get_weights(functional_model),   get_weights(subclassed_model) )) input <- layer_input(shape = c(784), name = \"digits\") output <- input |>   layer_dense(64, activation = \"relu\", name = \"dense_1\") |>   layer_dense(64, activation = \"relu\", name = \"dense_2\") |>   layer_dense(10, name = \"predictions\") functional_model <- keras_model(inputs, outputs,                                 name = \"3_layer_mlp\")  input <- layer_input(shape = c(784), name = \"digits\") output <- input |>   layer_dense(64, activation = \"relu\", name = \"dense_1\") |>   layer_dense(64, activation = \"relu\", name = \"dense_2\") |>   # Add a dropout layer, which does not contain any weights.   layer_dropout(0.5) |>   layer_dense(10, name = \"predictions\")  functional_model_with_dropout <-   keras_model(input, output, name = \"3_layer_mlp\")  set_weights(functional_model_with_dropout,             get_weights(functional_model))"},{"path":"https://keras.posit.co/articles/serialization_and_saving.html","id":"apis-for-saving-weights-to-disk-loading-them-back","dir":"Articles","previous_headings":"Saving > Model weights saving","what":"APIs for saving weights to disk & loading them back","title":"Save, serialize, and export models","text":"Weights can saved disk calling save_model_weights(filepath). filename end .weights.h5. Example: Note using freeze_weights() may result different output get_weights(layer) ordering model contains nested layers.","code":"sequential_model = keras_model_sequential(input_shape = c(784),                                           input_layer_name = \"digits\") |>   layer_dense(64, activation = \"relu\", name = \"dense_1\") |>   layer_dense(64, activation = \"relu\", name = \"dense_2\") |>   layer_dense(10, name = \"predictions\") sequential_model |> save_model_weights(\"my_model.weights.h5\") sequential_model |> load_model_weights(\"my_model.weights.h5\")"},{"path":"https://keras.posit.co/articles/serialization_and_saving.html","id":"transfer-learning-example","dir":"Articles","previous_headings":"Saving > Model weights saving > APIs for saving weights to disk & loading them back","what":"Transfer learning example","title":"Save, serialize, and export models","text":"loading pretrained weights weights file, recommended load weights original checkpointed model, extract desired weights/layers new model. Example:","code":"create_functional_model <- function() {   inputs <- layer_input(shape = c(784), name = \"digits\")   outputs <- inputs |>     layer_dense(64, activation = \"relu\", name = \"dense_1\") |>     layer_dense(64, activation = \"relu\", name = \"dense_2\") |>     layer_dense(10, name = \"predictions\")   keras_model(inputs, outputs, name = \"3_layer_mlp\") }  functional_model <- create_functional_model() functional_model |> save_model_weights(\"pretrained.weights.h5\") ## Error: File 'pretrained.weights.h5' already exists (pass overwrite = TRUE to force save). # In a separate program: pretrained_model <- create_functional_model() pretrained_model |> load_model_weights(\"pretrained.weights.h5\")  # Create a new model by extracting layers from the original model: extracted_layers <- pretrained_model$layers |> head(-1) model <- keras_model_sequential(extracted_layers) |>   layer_dense(5, name = \"dense_3\") summary(model) ## Model: \"sequential_4\" ## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓ ## ┃ Layer (type)                      ┃ Output Shape                ┃    Param # ┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩ ## │ dense_1 (Dense)                   │ (None, 64)                  │     50,240 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ dense_2 (Dense)                   │ (None, 64)                  │      4,160 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ dense_3 (Dense)                   │ (None, 5)                   │        325 │ ## └───────────────────────────────────┴─────────────────────────────┴────────────┘ ##  Total params: 54,725 (213.77 KB) ##  Trainable params: 54,725 (213.77 KB) ##  Non-trainable params: 0 (0.00 B)"},{"path":[]},{"path":"https://keras.posit.co/articles/serialization_and_saving.html","id":"defining-the-config-methods","dir":"Articles","previous_headings":"Saving > Appendix: Handling custom objects","what":"Defining the config methods","title":"Save, serialize, and export models","text":"Specifications: get_config() return JSON-serializable named list order compatible Keras architecture model-saving APIs. from_config(config) (class method) return new layer model object created config. default implementation returns .call(cls, config). NOTE: constructor arguments already serializable, e.g. strings ints, non-custom Keras objects, overriding from_config() necessary. However, complex objects layers models passed initialize(), deserialization must handled explicitly either initialize overriding from_config() method. Example: Note overriding from_config unnecessary MyDense hidden_units, kernel_initializer, kernel_regularizer ints, strings, built-Keras object, respectively. means default from_config implementation cls(!!!config) work intended. complex objects, layers models passed initialize(), example, must explicitly deserialize objects. Let’s take look example model from_config override necessary. Example:","code":"layer_my_dense <- register_custom_object(   package = \"MyLayers\", name = \"KernelMult\",   object = Layer(     \"MyDense\",     initialize = function(units,                           ...,                           kernel_regularizer = NULL,                           kernel_initializer = NULL,                           nested_model = NULL) {       super$initialize(...)       self$hidden_units <- units       self$kernel_regularizer <- kernel_regularizer       self$kernel_initializer <- kernel_initializer       self$nested_model <- nested_model     },     get_config = function() {       config <- super$get_config()       # Update the config with the custom layer's parameters       config <- modifyList(         config,         list(           units = self$hidden_units,           kernel_regularizer = self$kernel_regularizer,           kernel_initializer = self$kernel_initializer,           nested_model = self$nested_model         )       )       config     },     build = function(input_shape) {       input_units <- tail(input_shape, 1)       self$kernel <- self$add_weight(         name = \"kernel\",         shape = shape(input_units, self$hidden_units),         regularizer = self$kernel_regularizer,         initializer = self$kernel_initializer,       )     },     call = function(inputs) {       op_matmul(inputs, self$kernel)     }   ) ) ## wrong parameter order: variadic keyword parameter before keyword-only parameter layer <- layer_my_dense(units = 16,                         kernel_regularizer = \"l1\",                         kernel_initializer = \"ones\") ## Error in layer_my_dense(units = 16, kernel_regularizer = \"l1\", kernel_initializer = \"ones\"): could not find function \"layer_my_dense\" layer3 <- layer_my_dense(units = 64, nested_model = layer) ## Error in layer_my_dense(units = 64, nested_model = layer): could not find function \"layer_my_dense\" config <- serialize_keras_object(layer3) ## Error in serialize_keras_object(layer3): could not find function \"serialize_keras_object\" str(config) ## {'name': 'functional_11', 'trainable': True, 'layers': [{'module': 'keras.src.layers.core.input_layer', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 32), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_5'}, 'registered_name': 'InputLayer', 'name': 'input_layer_5', 'inbound_nodes': []}, {'module': 'keras.src.layers.core.dense', 'class_name': 'Dense', 'config': {'name': 'dense_6', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.src.initializers.random_initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': 'GlorotUniform'}, 'bias_initializer': {'module': 'keras.src.initializers.constant_initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': 'Zeros'}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': 'Dense', 'build_config': {'input_shape': (None, 32)}, 'name': 'dense_6', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 32), 'dtype': 'float32', 'keras_history': ['input_layer_5', 0, 0]}},), 'kwargs': {}}]}], 'input_layers': [['input_layer_5', 0, 0]], 'output_layers': [['dense_6', 0, 0]]} new_layer <- deserialize_keras_object()(config) ## Error in deserialize_keras_object(): could not find function \"deserialize_keras_object\" new_layer ## <Dense name=dense_4, built=False> layer_custom_model <- register_custom_object(   package = \"ComplexModels\",   object = Layer(     \"CustomModel\",     initialize = function(first_layer, second_layer = NULL, ...) {       super$initialize(...)       self$first_layer <- first_layer       self$second_layer <- second_layer %||% layer_dense(, 8)     },      get_config = function() {       config <- super$get_config()       config <- modifyList(config, list(         first_layer = self$first_layer,         second_layer = self$second_layer       ))       config     },      from_config = py_eval(\"classmethod\")(function(cls, config) {       config$first_layer %<>% deserialize_keras_object()       config$second_layer %<>% deserialize_keras_object()       # note that the class is available in methods under the classname symbol,       # (`CustomModel` for this class), and also under the symbol `__class__`       cls(!!!config)       # CustomModel(!!!config)     }),     call = function(self, inputs) {       inputs |>         self$first_layer() |>         self$second_layer()     }   ) ) ## Error in py_eval(\"classmethod\"): could not find function \"py_eval\" # Let's make our first layer the custom layer from the previous example (MyDense) inputs <- layer_input(c(32)) outputs <-  inputs |> layer_custom_model(first_layer=layer) ## Error in layer_custom_model(inputs, first_layer = layer): could not find function \"layer_custom_model\" model <- keras_model(inputs, outputs)  config <- get_config(model) new_model <- from_config(config)"},{"path":"https://keras.posit.co/articles/serialization_and_saving.html","id":"how-custom-objects-are-serialized","dir":"Articles","previous_headings":"Saving > Appendix: Handling custom objects","what":"How custom objects are serialized","title":"Save, serialize, and export models","text":"serialization format special key custom objects registered via register_custom_object(). registered_name key allows easy retrieval loading/deserialization time also allowing users add custom naming. Let’s take look config serializing custom layer MyDense defined . Example: shown, registered_name key contains lookup information Keras master list, including package MyLayers custom name KernelMult gave calling register_custom_objects(). Take look custom class definition/registration . Note class_name key contains original name class, allowing proper re-initialization from_config. Additionally, note module key NULL since custom object.","code":"layer <- layer_my_dense(   units = 16,   kernel_regularizer = regularizer_l1_l2(l1 = 1e-5, l2 = 1e-4),   kernel_initializer = \"ones\", ) ## Error in layer_my_dense(units = 16, kernel_regularizer = regularizer_l1_l2(l1 = 1e-05, : could not find function \"layer_my_dense\" config <- serialize_keras_object(layer) ## Error in serialize_keras_object(layer): could not find function \"serialize_keras_object\" str(config) ## {'name': 'functional_21', 'trainable': True, 'layers': [{'module': 'keras.src.layers.core.input_layer', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 784), 'dtype': 'float32', 'sparse': False, 'name': 'digits'}, 'registered_name': 'InputLayer', 'name': 'digits', 'inbound_nodes': []}, {'module': 'keras.src.layers.core.dense', 'class_name': 'Dense', 'config': {'name': 'dense_1', 'trainable': True, 'dtype': 'float32', 'units': 64, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.src.initializers.random_initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': 'GlorotUniform'}, 'bias_initializer': {'module': 'keras.src.initializers.constant_initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': 'Zeros'}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': 'Dense', 'build_config': {'input_shape': (None, 784)}, 'name': 'dense_1', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 784), 'dtype': 'float32', 'keras_history': ['digits', 0, 0]}},), 'kwargs': {}}]}, {'module': 'keras.src.layers.core.dense', 'class_name': 'Dense', 'config': {'name': 'dense_2', 'trainable': True, 'dtype': 'float32', 'units': 64, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.src.initializers.random_initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': 'GlorotUniform'}, 'bias_initializer': {'module': 'keras.src.initializers.constant_initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': 'Zeros'}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': 'Dense', 'build_config': {'input_shape': (None, 64)}, 'name': 'dense_2', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 64), 'dtype': 'float32', 'keras_history': ['dense_1', 0, 0]}},), 'kwargs': {}}]}, {'module': 'keras.src.layers.core.input_layer', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 32), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_8'}, 'registered_name': 'InputLayer', 'name': 'input_layer_8', 'inbound_nodes': []}, {'module': 'keras.src.layers.core.dense', 'class_name': 'Dense', 'config': {'name': 'predictions', 'trainable': True, 'dtype': 'float32', 'units': 10, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.src.initializers.random_initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': 'GlorotUniform'}, 'bias_initializer': {'module': 'keras.src.initializers.constant_initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': 'Zeros'}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': 'Dense', 'build_config': {'input_shape': (None, 64)}, 'name': 'predictions', 'inbound_nodes': [{'args': ({'class_name': '__keras_tensor__', 'config': {'shape': (None, 64), 'dtype': 'float32', 'keras_history': ['dense_2', 0, 0]}},), 'kwargs': {}}]}], 'input_layers': [['input_layer_8', 0, 0]], 'output_layers': [['predictions', 0, 0]]}"},{"path":"https://keras.posit.co/articles/training_with_built_in_methods.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Training & evaluation with the built-in methods","text":"","code":"library(keras3)"},{"path":"https://keras.posit.co/articles/training_with_built_in_methods.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Training & evaluation with the built-in methods","text":"guide covers training, evaluation, prediction (inference) models using built-APIs training & validation (Model$fit(), Model$evaluate() Model$predict()). interested leveraging fit() specifying training step function, see Customizing happens fit() guide. interested writing training & evaluation loops scratch, see guide “writing training loop scratch”. general, whether using built-loops writing , model training & evaluation works strictly way across every kind Keras model – Sequential models, models built Functional API, models written scratch via model subclassing. guide doesn’t cover distributed training, covered guide multi-GPU & distributed training.","code":""},{"path":"https://keras.posit.co/articles/training_with_built_in_methods.html","id":"api-overview-a-first-end-to-end-example","dir":"Articles","previous_headings":"","what":"API overview: a first end-to-end example","title":"Training & evaluation with the built-in methods","text":"passing data built-training loops model, either use: Arrays (data small fits memory) Subclasses keras$utils$PyDataset tf_dataset objects PyTorch DataLoader instances next paragraphs, ’ll use MNIST dataset NumPy arrays, order demonstrate use optimizers, losses, metrics. Afterwards, ’ll take close look options. Let’s consider following model (, build Functional API, Sequential model subclassed model well): ’s typical end--end workflow looks like, consisting : Training Validation holdout set generated original training data Evaluation test data ’ll use MNIST data example. specify training configuration (optimizer, loss, metrics): call fit(), train model slicing data “batches” size batch_size, repeatedly iterating entire dataset given number epochs. returned history object holds record loss values metric values training: evaluate model test data via evaluate(): Now, let’s review piece workflow detail.","code":"inputs <- layer_input(shape = 784, name=\"digits\") outputs <- inputs %>%   layer_dense(units = 64, activation = \"relu\", name = \"dense_1\") %>%   layer_dense(units = 64, activation = \"relu\", name = \"dense_2\") %>%   layer_dense(units = 10, activation = \"softmax\", name = \"predictions\") model <- keras_model(inputs=inputs, outputs=outputs) summary(model) ## Model: \"functional_1\" ## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓ ## ┃ Layer (type)                    ┃ Output Shape              ┃    Param # ┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩ ## │ digits (InputLayer)             │ (None, 784)               │          0 │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ dense_1 (Dense)                 │ (None, 64)                │     50,240 │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ dense_2 (Dense)                 │ (None, 64)                │      4,160 │ ## ├─────────────────────────────────┼───────────────────────────┼────────────┤ ## │ predictions (Dense)             │ (None, 10)                │        650 │ ## └─────────────────────────────────┴───────────────────────────┴────────────┘ ##  Total params: 55,050 (215.04 KB) ##  Trainable params: 55,050 (215.04 KB) ##  Non-trainable params: 0 (0.00 B) c(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_mnist()  # Preprocess the data (these are NumPy arrays) x_train <- array_reshape(x_train, c(60000, 784)) / 255 x_test <- array_reshape(x_test, c(10000, 784)) / 255  # Reserve 10,000 samples for validation x_val <- x_train[1:10000,] y_val <- y_train[1:10000] x_train <- x_train[-c(1:10000),] y_train <- y_train[-c(1:10000)] model %>% compile(     optimizer = optimizer_rmsprop(),  # Optimizer     # Loss function to minimize     loss = loss_sparse_categorical_crossentropy(),     # List of metrics to monitor     metrics = list(metric_sparse_categorical_accuracy()) ) history <- model %>% fit(   x_train,   y_train,   batch_size = 64,   epochs = 2,   # We pass some validation for   # monitoring validation loss and metrics   # at the end of each epoch   validation_data = list(x_val, y_val) ) ## Epoch 1/2 ## 782/782 - 1s - 1ms/step - loss: 0.3481 - sparse_categorical_accuracy: 0.9004 - val_loss: 0.1968 - val_sparse_categorical_accuracy: 0.9444 ## Epoch 2/2 ## 782/782 - 1s - 839us/step - loss: 0.1651 - sparse_categorical_accuracy: 0.9519 - val_loss: 0.1387 - val_sparse_categorical_accuracy: 0.9602 history ## ## Final epoch (plot to see history): ##                            loss: 0.1651 ##     sparse_categorical_accuracy: 0.9519 ##                        val_loss: 0.1387 ## val_sparse_categorical_accuracy: 0.9602 # Evaluate the model on the test data using `evaluate` results <- model %>% evaluate(x_test, y_test, batch_size=128) ## 79/79 - 0s - 560us/step - loss: 0.1347 - sparse_categorical_accuracy: 0.9591 results ## $loss ## [1] 0.1347092 ## ## $sparse_categorical_accuracy ## [1] 0.9591 # Generate predictions (probabilities -- the output of the last layer) # on new data using `predict` predictions <- model %>% predict(x_test[1:2,]) ## 1/1 - 0s - 22ms/step dim(predictions) ## [1]  2 10"},{"path":"https://keras.posit.co/articles/training_with_built_in_methods.html","id":"the-compile-method-specifying-a-loss-metrics-and-an-optimizer","dir":"Articles","previous_headings":"","what":"The compile() method: specifying a loss, metrics, and an optimizer","title":"Training & evaluation with the built-in methods","text":"train model fit(), need specify loss function, optimizer, optionally, metrics monitor. pass model arguments compile() method: metrics argument list – model can number metrics. model multiple outputs, can specify different losses metrics output, can modulate contribution output total loss model. find details Passing data multi-input, multi-output models section. Note ’re satisfied default settings, many cases optimizer, loss, metrics can specified via string identifiers shortcut: later reuse, let’s put model definition compile step functions; call several times across different examples guide.","code":"model %>% compile(     optimizer = optimizer_rmsprop(learning_rate=1e-3),     loss = loss_sparse_categorical_crossentropy(),     metrics = list(metric_sparse_categorical_accuracy()) ) model %>% compile(   optimizer = \"rmsprop\",   loss = \"sparse_categorical_crossentropy\",   metrics = c(\"sparse_categorical_accuracy\") ) get_uncompiled_model <- function() {   inputs <- layer_input(shape = 784, name = \"digits\")   outputs <- inputs %>%     layer_dense(units = 64, activation = \"relu\", name = \"dense_1\") %>%     layer_dense(units = 64, activation = \"relu\", name = \"dense_2\") %>%     layer_dense(units = 10, activation = \"softmax\", name = \"predictions\")   keras_model(inputs = inputs, outputs = outputs) }  get_compiled_model <- function() {   model <- get_uncompiled_model()   model %>% compile(     optimizer = \"rmsprop\",     loss = \"sparse_categorical_crossentropy\",     metrics = c(\"sparse_categorical_accuracy\")   )   model }"},{"path":"https://keras.posit.co/articles/training_with_built_in_methods.html","id":"many-built-in-optimizers-losses-and-metrics-are-available","dir":"Articles","previous_headings":"The compile() method: specifying a loss, metrics, and an optimizer","what":"Many built-in optimizers, losses, and metrics are available","title":"Training & evaluation with the built-in methods","text":"general, won’t create losses, metrics, optimizers scratch, need likely already part Keras API: Optimizers: SGD() (without momentum) RMSprop() Adam() etc. Losses: MeanSquaredError() KLDivergence() CosineSimilarity() etc. Metrics: AUC() Precision() Recall() etc.","code":""},{"path":"https://keras.posit.co/articles/training_with_built_in_methods.html","id":"custom-losses","dir":"Articles","previous_headings":"The compile() method: specifying a loss, metrics, and an optimizer","what":"Custom losses","title":"Training & evaluation with the built-in methods","text":"need create custom loss, Keras provides three ways . first method involves creating function accepts inputs y_true y_pred. following example shows loss function computes mean squared error real data predictions: need loss function takes parameters beside y_true y_pred, can subclass keras.losses.Loss class using new_loss_class implement following two methods: initialize(): accept parameters pass call loss function call(y_true, y_pred): use targets (y_true) model predictions (y_pred) compute model’s loss Let’s say want use mean squared error, added term de-incentivize prediction values far 0.5 (assume categorical targets one-hot encoded take values 0 1). creates incentive model confident, may help reduce overfitting (won’t know works try!). ’s :","code":"custom_mean_squared_error <- function(y_true, y_pred) {   op_mean(op_square(y_true - y_pred), axis=-1) }  model <- get_uncompiled_model() model %>% compile(optimizer=\"adam\", loss=custom_mean_squared_error)  # We need to one-hot encode the labels to use MSE y_train_one_hot <- op_one_hot(y_train, num_classes=10) model %>% fit(x_train, y_train_one_hot, batch_size=64, epochs=1) ## 782/782 - 1s - 1ms/step - loss: 0.0157 loss_custom_mse <- new_loss_class(   \"CustomMSE\",   initialize = function(regularization_factor = 0.1, name = \"custom_mse\") {     super$initialize(name = name)     self$regularization_factor <- regularization_factor   },   call = function(y_true, y_pred) {     mse <- op_mean(op_square(y_true - y_pred), axis = -1)     reg <- op_mean(op_square(0.5 - y_pred), axis = -1)     mse + reg * self$regularization_factor   } )  model <- get_uncompiled_model() model %>% compile(optimizer=\"adam\", loss = loss_custom_mse())  y_train_one_hot <- op_one_hot(y_train, num_classes=10) model %>% fit(x_train, y_train_one_hot, batch_size=64, epochs=1) ## 782/782 - 1s - 1ms/step - loss: 0.0383"},{"path":"https://keras.posit.co/articles/training_with_built_in_methods.html","id":"custom-metrics","dir":"Articles","previous_headings":"The compile() method: specifying a loss, metrics, and an optimizer","what":"Custom metrics","title":"Training & evaluation with the built-in methods","text":"need metric isn’t part API, can easily create custom metrics subclassing keras.metrics.Metric class using new_metric_class. need implement 4 methods: initialize(), create state variables metric. update_state(y_true, y_pred, sample_weight=None), uses targets y_true model predictions y_pred update state variables. result(), uses state variables compute final results. reset_state(), reinitializes state metric. State update results computation kept separate (update_state() result(), respectively) cases, results computation might expensive done periodically. ’s simple example showing implement CategoricalTruePositives metric counts many samples correctly classified belonging given class:","code":"metric_categorical_true_positives <- new_metric_class(   \"CategoricalTruePositives\",   initialize = function(name = \"categorical_true_positives\", ...) {     super$initialize(name = name, ...)     self$true_positives <- self$add_variable(shape = shape(),                                              name = \"ctp\",                                              initializer = \"zeros\")   },   update_state = function(y_true, y_pred, sample_weight = NULL) {     y_pred <- op_reshape(op_argmax(y_pred, axis = 2), shape(-1, 1))     values <- op_cast(y_true, \"int32\") == op_cast(y_pred, \"int32\")     values <- op_cast(values, \"float32\")     if (!is.null(sample_weight)) {       sample_weight <- op_cast(sample_weight, \"float32\")       values <- op_multiply(values, sample_weight)     }     self$true_positives$assign_add(op_sum(values))   },   result = function() {     self$true_positives   },   reset_state = function() {     self$true_positives$assign(0.0)   } )  model <- get_uncompiled_model() model %>% compile(   optimizer = optimizer_rmsprop(learning_rate = 1e-3),   loss = loss_sparse_categorical_crossentropy(),   metrics = list(metric_categorical_true_positives()) ) history <- model %>% fit(x_train, y_train, batch_size = 64, epochs = 3) ## Epoch 1/3 ## 782/782 - 1s - 871us/step - categorical_true_positives: 360544.0000 - loss: 0.3421 ## Epoch 2/3 ## 782/782 - 1s - 692us/step - categorical_true_positives: 362643.0000 - loss: 0.1622 ## Epoch 3/3 ## 782/782 - 1s - 688us/step - categorical_true_positives: 363298.0000 - loss: 0.1178"},{"path":"https://keras.posit.co/articles/training_with_built_in_methods.html","id":"handling-losses-and-metrics-that-dont-fit-the-standard-signature","dir":"Articles","previous_headings":"The compile() method: specifying a loss, metrics, and an optimizer","what":"Handling losses and metrics that don’t fit the standard signature","title":"Training & evaluation with the built-in methods","text":"overwhelming majority losses metrics can computed y_true y_pred, y_pred output model – . instance, regularization loss may require activation layer (targets case), activation may model output. cases, can call self.add_loss(loss_value) inside call method custom layer. Losses added way get added “main” loss training (one passed compile()). ’s simple example adds activity regularization (note activity regularization built-Keras layers – layer just sake providing concrete example): Note pass losses via add_loss(), becomes possible call compile() without loss function, since model already loss minimize. Consider following LogisticEndpoint layer: takes inputs targets & logits, tracks crossentropy loss via add_loss(). can use model two inputs (input data & targets), compiled without loss argument, like : information training multi-input models, see section Passing data multi-input, multi-output models.","code":"layer_custom_activity_regularizer <- Layer(   \"ActivityRegularization\",   call = function(inputs) {     self$add_loss(op_sum(inputs) * 0.1)     inputs  # Pass-through layer.   } )  inputs <- layer_input(shape = 784, name=\"digits\") outputs <- inputs %>%   layer_dense(units=32, activation=\"relu\", name=\"dense_1\") %>%   layer_custom_activity_regularizer() %>%   layer_dense(units=64, activation = \"relu\", name=\"dense_2\") %>%   layer_dense(units=10, name=\"predictions\")  model <- keras_model(inputs=inputs, outputs=outputs) model %>% compile(     optimizer= optimizer_rmsprop(learning_rate=1e-3),     loss=loss_sparse_categorical_crossentropy(from_logits=TRUE) )  # The displayed loss will be much higher than before # due to the regularization component. model %>% fit(x_train, y_train, batch_size=64, epochs=1) ## 782/782 - 1s - 845us/step - loss: 2.3949 layer_logistic_endpoint <- Layer(   \"LogisticEndpoint\",   initialize = function(name = NULL) {     super$initialize(name = name)     self$loss_fn <- loss_binary_crossentropy(from_logits = TRUE)   },   call = function(targets, logits, sample_weights = NULL) {     # Compute the training-time loss value and add it     # to the layer using `self.add_loss()`.     loss <- self$loss_fn(targets, logits, sample_weights)     self$add_loss(loss)      # Return the inference-time prediction tensor (for `predict()`).     op_softmax(logits)   } ) inputs <- layer_input(shape = 3, name=\"inputs\") targets <- layer_input(shape = 10, name=\"targets\")  logits <- inputs |> layer_dense(10) predictions <- layer_logistic_endpoint(name = \"predictions\")(targets, logits)  model <- keras_model(inputs = list(inputs, targets),                      outputs = predictions) model %>% compile(optimizer = \"adam\")  # No loss argument!  data <- list(   inputs = random_normal(c(3, 3)),   targets = random_normal(c(3, 10)) ) model %>% fit(data, epochs = 1) ## 1/1 - 0s - 185ms/step - loss: 1.0566"},{"path":"https://keras.posit.co/articles/training_with_built_in_methods.html","id":"automatically-setting-apart-a-validation-holdout-set","dir":"Articles","previous_headings":"The compile() method: specifying a loss, metrics, and an optimizer","what":"Automatically setting apart a validation holdout set","title":"Training & evaluation with the built-in methods","text":"first end--end example saw, used validation_data argument pass tuple NumPy arrays (x_val, y_val) model evaluating validation loss validation metrics end epoch. ’s another option: argument validation_split allows automatically reserve part training data validation. argument value represents fraction data reserved validation, set number higher 0 lower 1. instance, validation_split=0.2 means “use 20% data validation”, validation_split=0.6 means “use 60% data validation”. way validation computed taking last x% samples arrays received fit() call, shuffling. Note can use validation_split training NumPy data.","code":"model <- get_compiled_model() model |> fit(x_train, y_train,              batch_size = 64,              validation_split = 0.2, epochs = 1) ## 625/625 - 1s - 1ms/step - loss: 0.3694 - sparse_categorical_accuracy: 0.8949 - val_loss: 0.1905 - val_sparse_categorical_accuracy: 0.9461"},{"path":"https://keras.posit.co/articles/training_with_built_in_methods.html","id":"training-evaluation-using-tf_dataset-objects","dir":"Articles","previous_headings":"","what":"Training & evaluation using tf_dataset objects","title":"Training & evaluation with the built-in methods","text":"past paragraphs, ’ve seen handle losses, metrics, optimizers, ’ve seen use validation_data validation_split arguments fit(), data passed NumPy arrays. Another option use iterator-like, tf.data.Dataset, PyTorch DataLoader, Keras PyDataset. Let’s take look former. tf.data API set utilities TensorFlow 2.0 loading preprocessing data way ’s fast scalable. complete guide creating Datasets, see tf.data documentation. can use tf.data train Keras models regardless backend ’re using – whether ’s JAX, PyTorch, TensorFlow. can pass Dataset instance directly methods fit(), evaluate(), predict(): Note Dataset reset end epoch, can reused next epoch. want run training specific number batches Dataset, can pass steps_per_epoch argument, specifies many training steps model run using Dataset moving next epoch. can also pass Dataset instance validation_data argument fit(): end epoch, model iterate validation dataset compute validation loss validation metrics. want run validation specific number batches dataset, can pass validation_steps argument, specifies many validation steps model run validation dataset interrupting validation moving next epoch: Note validation dataset reset use (always evaluating samples epoch epoch). argument validation_split (generating holdout set training data) supported training Dataset objects, since feature requires ability index samples datasets, possible general Dataset API.","code":"library(tfdatasets) ## ## Attaching package: 'tfdatasets' ## The following object is masked from 'package:keras3': ## ##     shape model <- get_compiled_model()  # First, let's create a training Dataset instance. # For the sake of our example, we'll use the same MNIST data as before. train_dataset <- tensor_slices_dataset(list(x_train, y_train)) # Shuffle and slice the dataset. train_dataset <- train_dataset %>%   dataset_shuffle(buffer_size=1024) %>%   dataset_batch(64)  # Now we get a test dataset. test_dataset <- tensor_slices_dataset(list(x_test, y_test)) test_dataset <- test_dataset %>%   dataset_batch(64)  # Since the dataset already takes care of batching, # we don't pass a `batch_size` argument. model %>% fit(train_dataset, epochs=3) ## Epoch 1/3 ## 782/782 - 1s - 967us/step - loss: 0.3417 - sparse_categorical_accuracy: 0.9033 ## Epoch 2/3 ## 782/782 - 1s - 676us/step - loss: 0.1602 - sparse_categorical_accuracy: 0.9524 ## Epoch 3/3 ## 782/782 - 1s - 656us/step - loss: 0.1162 - sparse_categorical_accuracy: 0.9654 # You can also evaluate or predict on a dataset. result <- model %>% evaluate(test_dataset) ## 157/157 - 0s - 685us/step - loss: 0.1145 - sparse_categorical_accuracy: 0.9631 result ## $loss ## [1] 0.1145203 ## ## $sparse_categorical_accuracy ## [1] 0.9631 model <- get_compiled_model()  # Prepare the training dataset train_dataset <- tensor_slices_dataset(list(x_train, y_train)) train_dataset <- train_dataset %>%   dataset_shuffle(buffer_size=1024) %>%   dataset_batch(64)  # Only use the 100 batches per epoch (that's 64 * 100 samples) model %>% fit(train_dataset, epochs=3, steps_per_epoch=100) ## Epoch 1/3 ## 100/100 - 0s - 3ms/step - loss: 0.8506 - sparse_categorical_accuracy: 0.7683 ## Epoch 2/3 ## 100/100 - 0s - 666us/step - loss: 0.3705 - sparse_categorical_accuracy: 0.8947 ## Epoch 3/3 ## 100/100 - 0s - 648us/step - loss: 0.3060 - sparse_categorical_accuracy: 0.9130 model <- get_compiled_model()  # Prepare the training dataset train_dataset <- tensor_slices_dataset(list(x_train, y_train)) train_dataset <- train_dataset %>%   dataset_shuffle(buffer_size=1024) %>%   dataset_batch(64)  # Prepare the validation dataset val_dataset = tensor_slices_dataset(list(x_val, y_val)) val_dataset <- val_dataset %>% dataset_batch(64)  model %>% fit(train_dataset, epochs=1, validation_data=val_dataset) ## 782/782 - 1s - 1ms/step - loss: 0.3355 - sparse_categorical_accuracy: 0.9046 - val_loss: 0.2198 - val_sparse_categorical_accuracy: 0.9313 model = get_compiled_model()  # Prepare the training dataset train_dataset <- tensor_slices_dataset(list(x_train, y_train)) train_dataset <- train_dataset %>%   dataset_shuffle(buffer_size=1024) %>%   dataset_batch(64)  # Prepare the validation dataset val_dataset = tensor_slices_dataset(list(x_val, y_val)) val_dataset <- val_dataset %>% dataset_batch(64)  model %>% fit(     train_dataset,     epochs=1,     # Only run validation using the first 10 batches of the dataset     # using the `validation_steps` argument     validation_data=val_dataset,     validation_steps=10, ) ## 782/782 - 1s - 1ms/step - loss: 0.3350 - sparse_categorical_accuracy: 0.9049 - val_loss: 0.2111 - val_sparse_categorical_accuracy: 0.9359"},{"path":"https://keras.posit.co/articles/training_with_built_in_methods.html","id":"using-sample-weighting-and-class-weighting","dir":"Articles","previous_headings":"","what":"Using sample weighting and class weighting","title":"Training & evaluation with the built-in methods","text":"default settings weight sample decided frequency dataset. two methods weight data, independent sample frequency: Class weights Sample weights","code":""},{"path":"https://keras.posit.co/articles/training_with_built_in_methods.html","id":"class-weights","dir":"Articles","previous_headings":"Using sample weighting and class weighting","what":"Class weights","title":"Training & evaluation with the built-in methods","text":"set passing dictionary class_weight argument fit(). dictionary maps class indices weight used samples belonging class. can used balance classes without resampling, train model gives importance particular class. instance, class “0” half represented class “1” data, use Model.fit(..., class_weight={0: 1., 1: 0.5}). ’s NumPy example use class weights sample weights give importance correct classification class #5 (digit “5” MNIST dataset).","code":"class_weight = list(     \"0\" = 1.0,     \"1\" = 1.0,     \"2\" = 1.0,     \"3\" = 1.0,     \"4\" = 1.0,     # Set weight \"2\" for class \"5\",     # making this class 2x more important     \"5\" = 2.0,     \"6\" = 1.0,     \"7\" = 1.0,     \"8\" = 1.0,     \"9\" = 1.0 )  model <- get_compiled_model() model %>% fit(x_train, y_train, class_weight=class_weight, batch_size=64, epochs=1) ## 782/782 - 1s - 942us/step - loss: 0.3687 - sparse_categorical_accuracy: 0.9026"},{"path":"https://keras.posit.co/articles/training_with_built_in_methods.html","id":"sample-weights","dir":"Articles","previous_headings":"Using sample weighting and class weighting","what":"Sample weights","title":"Training & evaluation with the built-in methods","text":"fine grained control, building classifier, can use “sample weights”. training NumPy data: Pass sample_weight argument fit(). training tf_dataset sort iterator: Yield (input_batch, label_batch, sample_weight_batch) tuples. “sample weights” array array numbers specify much weight sample batch computing total loss. commonly used imbalanced classification problems (idea give weight rarely-seen classes). weights used ones zeros, array can used mask loss function (entirely discarding contribution certain samples total loss). ’s matching Dataset example:","code":"sample_weight <- rep(1.0, length(y_train)) sample_weight[y_train == 5] <- 2.0  model <- get_compiled_model() model %>% fit(     x_train, y_train, sample_weight=sample_weight, batch_size=64, epochs=1 ) ## 782/782 - 1s - 916us/step - loss: 0.3685 - sparse_categorical_accuracy: 0.9024 sample_weight <- rep(1.0, length(y_train)) sample_weight[y_train == 5] <- 2.0  # Create a Dataset that includes sample weights # (3rd element in the return tuple). train_dataset <- tensor_slices_dataset(list(     x_train, y_train, sample_weight ))  # Shuffle and slice the dataset. train_dataset <- train_dataset %>%   dataset_shuffle(buffer_size=1024) %>%   dataset_batch(64)  model <- get_compiled_model() model %>% fit(train_dataset, epochs=1) ## 782/782 - 1s - 1ms/step - loss: 0.3843 - sparse_categorical_accuracy: 0.8999"},{"path":"https://keras.posit.co/articles/training_with_built_in_methods.html","id":"passing-data-to-multi-input-multi-output-models","dir":"Articles","previous_headings":"","what":"Passing data to multi-input, multi-output models","title":"Training & evaluation with the built-in methods","text":"previous examples, considering model single input (tensor shape (764,)) single output (prediction tensor shape (10,)). models multiple inputs outputs? Consider following model, image input shape (32, 32, 3) (’s (height, width, channels)) time series input shape (None, 10) (’s (timesteps, features)). model two outputs computed combination inputs: “score” (shape (1,)) probability distribution five classes (shape (5,)). Let’s plot model, can clearly see ’re (note shapes shown plot batch shapes, rather per-sample shapes). compilation time, can specify different losses different outputs, passing loss functions list: passed single loss function model, loss function applied every output (appropriate ). Likewise metrics: Since gave names output layers, also specify per-output losses metrics via dict: recommend use explicit names dicts 2 outputs. ’s possible give different weights different output-specific losses (instance, one might wish privilege “score” loss example, giving 2x importance class loss), using loss_weights argument: also choose compute loss certain outputs, outputs meant prediction training: Passing data multi-input multi-output model fit() works similar way specifying loss function compile: can pass lists arrays (1:1 mapping outputs received loss function) dicts mapping output names arrays. ’s Dataset use case: similarly NumPy arrays, Dataset return tuple dicts.","code":"image_input <- layer_input(shape=c(32, 32, 3), name=\"img_input\") timeseries_input <- layer_input(shape=shape(NULL, 10), name=\"ts_input\")  x1 <- image_input %>%   layer_conv_2d(filters=3, kernel_size=c(3, 3)) %>%   layer_global_max_pooling_2d()  x2 <- timeseries_input %>%   layer_conv_1d(filters=3, kernel_size=3) %>%   layer_global_max_pooling_1d()  x <- layer_concatenate(list(x1, x2))  score_output <- layer_dense(x, 1, name=\"score_output\") class_output <- layer_dense(x, 5, name=\"class_output\")  model <- keras_model(     inputs = list(image_input, timeseries_input),     outputs = list(score_output, class_output) ) plot(model, show_shapes = TRUE) model %>% compile(     optimizer = optimizer_rmsprop(1e-3),     loss = list(         loss_mean_squared_error(),         loss_categorical_crossentropy()     ) ) model %>% compile(   optimizer = optimizer_rmsprop(1e-3),   loss = list(     loss_mean_squared_error(),     loss_categorical_crossentropy()   ),   metrics = list(     list(       metric_mean_absolute_error(),       metric_mean_absolute_percentage_error()     ),     list(metric_categorical_accuracy())   ) ) model %>% compile(   optimizer = optimizer_rmsprop(1e-3),   loss = list(     score_output = loss_mean_squared_error(),     class_output = loss_categorical_crossentropy()   ),   metrics = list(     score_output = list(       metric_mean_absolute_error(),       metric_mean_absolute_percentage_error()     ),     class_output = list(metric_categorical_accuracy())   ) ) model %>% compile(   optimizer = optimizer_rmsprop(1e-3),   loss = list(     score_output = loss_mean_squared_error(),     class_output = loss_categorical_crossentropy()   ),   metrics = list(     score_output = list(       metric_mean_absolute_error(),       metric_mean_absolute_percentage_error()     ),     class_output = list(metric_categorical_accuracy())   ),   loss_weights = list(score_output = 2.0, class_output = 1.0) ) # List loss version model %>% compile(     optimizer = optimizer_rmsprop(1e-3),     loss = list(NULL, loss_categorical_crossentropy()) )  # Or dict loss version model %>% compile(     optimizer = optimizer_rmsprop(1e-3),     loss = list(class_output = loss_categorical_crossentropy()) ) model %>% compile(   optimizer = optimizer_rmsprop(1e-3),   loss = list(     loss_mean_squared_error(),     loss_categorical_crossentropy()   ) )  # Generate dummy NumPy data img_data <- random_normal(c(100, 32, 32, 3)) ts_data <- random_normal(c(100, 20, 10)) score_targets <- random_normal(c(100, 1)) class_targets <- random_normal(c(100, 5))  # Fit on lists model %>% fit(     list(img_data, ts_data),     list(score_targets, class_targets),     batch_size=32,     epochs=1 ) ## 4/4 - 0s - 89ms/step - loss: 0.5888 # Alternatively, fit on dicts model %>% fit(   list(img_input = img_data, ts_input = ts_data),   list(score_output = score_targets, class_output = class_targets),   batch_size=32,   epochs=1, ) ## 4/4 - 0s - 51ms/step - loss: -1.3254e+00 train_dataset <- tensor_slices_dataset(list(   list(img_input = img_data, ts_input = ts_data),   list(score_output = score_targets, class_output = class_targets) )) train_dataset <- train_dataset %>%   dataset_shuffle(buffer_size=1024) %>%   dataset_batch(64)  model %>% fit(train_dataset, epochs=1) ## 2/2 - 0s - 4ms/step - loss: 1.7666"},{"path":"https://keras.posit.co/articles/training_with_built_in_methods.html","id":"using-callbacks","dir":"Articles","previous_headings":"","what":"Using callbacks","title":"Training & evaluation with the built-in methods","text":"Callbacks Keras objects called different points training (start epoch, end batch, end epoch, etc.). can used implement certain behaviors, : validation different points training (beyond built-per-epoch validation) Checkpointing model regular intervals exceeds certain accuracy threshold Changing learning rate model training seems plateauing fine-tuning top layers training seems plateauing Sending email instant message notifications training ends certain performance threshold exceeded Etc. Callbacks can passed list call fit():","code":"model <- get_compiled_model()  callbacks <- list(   callback_early_stopping(     # Stop training when `val_loss` is no longer improving     monitor=\"val_loss\",     # \"no longer improving\" being defined as \"no better than 1e-2 less\"     min_delta=1e-2,     # \"no longer improving\" being further defined as \"for at least 2 epochs\"     patience=2,     verbose=1   ) ) model %>% fit(     x_train,     y_train,     epochs=20,     batch_size=64,     callbacks=callbacks,     validation_split=0.2, ) ## Epoch 1/20 ## 625/625 - 1s - 1ms/step - loss: 0.3727 - sparse_categorical_accuracy: 0.8946 - val_loss: 0.1909 - val_sparse_categorical_accuracy: 0.9440 ## Epoch 2/20 ## 625/625 - 1s - 847us/step - loss: 0.1740 - sparse_categorical_accuracy: 0.9485 - val_loss: 0.1441 - val_sparse_categorical_accuracy: 0.9572 ## Epoch 3/20 ## 625/625 - 1s - 809us/step - loss: 0.1252 - sparse_categorical_accuracy: 0.9626 - val_loss: 0.1289 - val_sparse_categorical_accuracy: 0.9623 ## Epoch 4/20 ## 625/625 - 0s - 788us/step - loss: 0.0983 - sparse_categorical_accuracy: 0.9710 - val_loss: 0.1205 - val_sparse_categorical_accuracy: 0.9651 ## Epoch 5/20 ## 625/625 - 1s - 819us/step - loss: 0.0808 - sparse_categorical_accuracy: 0.9765 - val_loss: 0.1157 - val_sparse_categorical_accuracy: 0.9659 ## Epoch 6/20 ## 625/625 - 1s - 844us/step - loss: 0.0679 - sparse_categorical_accuracy: 0.9801 - val_loss: 0.1145 - val_sparse_categorical_accuracy: 0.9684 ## Epoch 7/20 ## 625/625 - 0s - 788us/step - loss: 0.0574 - sparse_categorical_accuracy: 0.9840 - val_loss: 0.1140 - val_sparse_categorical_accuracy: 0.9701 ## Epoch 8/20 ## 625/625 - 1s - 837us/step - loss: 0.0492 - sparse_categorical_accuracy: 0.9864 - val_loss: 0.1134 - val_sparse_categorical_accuracy: 0.9710 ## Epoch 9/20 ## 625/625 - 0s - 794us/step - loss: 0.0427 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.1156 - val_sparse_categorical_accuracy: 0.9717 ## Epoch 10/20 ## 625/625 - 1s - 856us/step - loss: 0.0372 - sparse_categorical_accuracy: 0.9898 - val_loss: 0.1186 - val_sparse_categorical_accuracy: 0.9725 ## Epoch 10: early stopping"},{"path":"https://keras.posit.co/articles/training_with_built_in_methods.html","id":"many-built-in-callbacks-are-available","dir":"Articles","previous_headings":"Using callbacks","what":"Many built-in callbacks are available","title":"Training & evaluation with the built-in methods","text":"many built-callbacks already available Keras, : ModelCheckpoint: Periodically save model. EarlyStopping: Stop training training longer improving validation metrics. TensorBoard: periodically write model logs can visualized TensorBoard (details section “Visualization”). CSVLogger: streams loss metrics data CSV file. etc. See callbacks documentation complete list.","code":""},{"path":"https://keras.posit.co/articles/training_with_built_in_methods.html","id":"writing-your-own-callback","dir":"Articles","previous_headings":"Using callbacks","what":"Writing your own callback","title":"Training & evaluation with the built-in methods","text":"can create custom callback extending base class keras.callbacks.Callback. callback access associated model class property self.model. Make sure read complete guide writing custom callbacks. ’s simple example saving list per-batch loss values training:","code":"LossHistory <- new_callback_class(   \"LossHistory\",   on_train_begin = function(logs) {     self$per_batch_losses <- list()   },   on_batch_end = function(batch, logs) {     self$per_batch_losses <- c(self$per_batch_losses, logs$get(\"loss\"))   } )"},{"path":"https://keras.posit.co/articles/training_with_built_in_methods.html","id":"checkpointing-models","dir":"Articles","previous_headings":"","what":"Checkpointing models","title":"Training & evaluation with the built-in methods","text":"’re training model relatively large datasets, ’s crucial save checkpoints model frequent intervals. easiest way achieve ModelCheckpoint callback: ModelCheckpoint callback can used implement fault-tolerance: ability restart training last saved state model case training gets randomly interrupted. ’s basic example: call also write callback saving restoring models. complete guide serialization saving, see guide saving serializing Models.","code":"model <- get_compiled_model()  callbacks <- list(   callback_model_checkpoint(     # Path where to save the model     # The two parameters below mean that we will overwrite     # the current checkpoint if and only if     # the `val_loss` score has improved.     # The saved model name will include the current epoch.     filepath=\"mymodel_{epoch}.keras\",     save_best_only=TRUE,  # Only save a model if `val_loss` has improved.     monitor=\"val_loss\",     verbose=1   ) ) model %>% fit(     x_train,     y_train,     epochs=2,     batch_size=64,     callbacks=callbacks,     validation_split=0.2, ) ## Epoch 1/2 ## ## Epoch 1: val_loss improved from inf to 0.19317, saving model to mymodel_1.keras ## 625/625 - 1s - 1ms/step - loss: 0.3759 - sparse_categorical_accuracy: 0.8940 - val_loss: 0.1932 - val_sparse_categorical_accuracy: 0.9447 ## Epoch 2/2 ## ## Epoch 2: val_loss improved from 0.19317 to 0.14907, saving model to mymodel_2.keras ## 625/625 - 1s - 851us/step - loss: 0.1797 - sparse_categorical_accuracy: 0.9468 - val_loss: 0.1491 - val_sparse_categorical_accuracy: 0.9572 # Prepare a directory to store all the checkpoints. checkpoint_dir <- \"./ckpt\" if (!dir.exists(checkpoint_dir)) {   dir.create(checkpoint_dir) }  make_or_restore_model <- function() {   # Either restore the latest model, or create a fresh one   # if there is no checkpoint available.   checkpoints <- list.files(checkpoint_dir, full.names=TRUE)    if (length(checkpoints) > 0) {     latest_checkpoint <- tail(checkpoints, 1)     load_model(latest_checkpoint)   } else {     get_compiled_model()   } }  model <- make_or_restore_model() callbacks <- list(   # This callback saves the model every 100 batches.   # We include the training loss in the saved model name.   callback_model_checkpoint(     filepath = file.path(checkpoint_dir, \"model-loss={loss:.2f}.keras\"),     save_freq=100   ) ) model %>% fit(x_train, y_train, epochs=1, callbacks=callbacks) ## 1563/1563 - 1s - 766us/step - loss: 0.2470 - sparse_categorical_accuracy: 0.9270"},{"path":"https://keras.posit.co/articles/training_with_built_in_methods.html","id":"using-learning-rate-schedules","dir":"Articles","previous_headings":"","what":"Using learning rate schedules","title":"Training & evaluation with the built-in methods","text":"common pattern training deep learning models gradually reduce learning training progresses. generally known “learning rate decay”. learning decay schedule static (fixed advance, function current epoch current batch index), dynamic (responding current behavior model, particular validation loss).","code":""},{"path":"https://keras.posit.co/articles/training_with_built_in_methods.html","id":"passing-a-schedule-to-an-optimizer","dir":"Articles","previous_headings":"Using learning rate schedules","what":"Passing a schedule to an optimizer","title":"Training & evaluation with the built-in methods","text":"can easily use static learning rate decay schedule passing schedule object learning_rate argument optimizer: Several built-schedules available: ExponentialDecay, PiecewiseConstantDecay, PolynomialDecay, InverseTimeDecay.","code":"initial_learning_rate <- 0.1 lr_schedule <- learning_rate_schedule_exponential_decay(     initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=TRUE )  optimizer <- optimizer_rmsprop(learning_rate=lr_schedule)"},{"path":"https://keras.posit.co/articles/training_with_built_in_methods.html","id":"using-callbacks-to-implement-a-dynamic-learning-rate-schedule","dir":"Articles","previous_headings":"Using learning rate schedules","what":"Using callbacks to implement a dynamic learning rate schedule","title":"Training & evaluation with the built-in methods","text":"dynamic learning rate schedule (instance, decreasing learning rate validation loss longer improving) achieved schedule objects, since optimizer access validation metrics. However, callbacks access metrics, including validation metrics! can thus achieve pattern using callback modifies current learning rate optimizer. fact, even built-ReduceLROnPlateau callback.","code":""},{"path":"https://keras.posit.co/articles/training_with_built_in_methods.html","id":"visualizing-loss-and-metrics-during-training-with-tensorboard","dir":"Articles","previous_headings":"","what":"Visualizing loss and metrics during training with TensorBoard","title":"Training & evaluation with the built-in methods","text":"best way keep eye model training use TensorBoard – browser-based application can run locally provides : Live plots loss metrics training evaluation (optionally) Visualizations histograms layer activations (optionally) 3D visualizations embedding spaces learned Embedding layers installed TensorFlow pip, able launch TensorBoard command line: using:","code":"tensorboard --logdir=/full_path_to_your_logs tensorflow::tensorboard(logdir=\"/full_path_to_your_logs\")"},{"path":"https://keras.posit.co/articles/training_with_built_in_methods.html","id":"using-the-tensorboard-callback","dir":"Articles","previous_headings":"Visualizing loss and metrics during training with TensorBoard","what":"Using the TensorBoard callback","title":"Training & evaluation with the built-in methods","text":"easiest way use TensorBoard Keras model fit() method TensorBoard callback. simplest case, just specify want callback write logs, ’re good go: information, see documentation TensorBoard callback.","code":"tb_callback <- callback_tensorboard(     log_dir=\"/full_path_to_your_logs\",     histogram_freq=0,  # How often to log histogram visualizations     embeddings_freq=0,  # How often to log embedding visualizations     update_freq=\"epoch\", )  # How often to write logs (default: once per epoch)"},{"path":"https://keras.posit.co/articles/transfer_learning.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Transfer learning & fine-tuning","text":"","code":"library(keras3)"},{"path":"https://keras.posit.co/articles/transfer_learning.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Transfer learning & fine-tuning","text":"Transfer learning consists taking features learned one problem, leveraging new, similar problem. instance, features model learned identify racoons may useful kick-start model meant identify tanukis. Transfer learning usually done tasks dataset little data train full-scale model scratch. common incarnation transfer learning context deep learning following workflow: Take layers previously trained model. Freeze , avoid destroying information contain future training rounds. Add new, trainable layers top frozen layers. learn turn old features predictions new dataset. Train new layers dataset. last, optional step, fine-tuning, consists unfreezing entire model obtained (part ), re-training new data low learning rate. can potentially achieve meaningful improvements, incrementally adapting pretrained features new data. First, go Keras trainable API detail, underlies transfer learning & fine-tuning workflows. , ’ll demonstrate typical workflow taking model pretrained ImageNet dataset, retraining Kaggle “cats vs dogs” classification dataset. adapted Deep Learning Python 2016 blog post “building powerful image classification models using little data”.","code":""},{"path":"https://keras.posit.co/articles/transfer_learning.html","id":"freezing-layers-understanding-the-trainable-attribute","dir":"Articles","previous_headings":"","what":"Freezing layers: understanding the trainable attribute","title":"Transfer learning & fine-tuning","text":"Layers & models three weight attributes: weights list weights variables layer. trainable_weights list meant updated (via gradient descent) minimize loss training. non_trainable_weights list aren’t meant trained. Typically updated model forward pass. Example: Dense layer 2 trainable weights (kernel & bias) general, weights trainable weights. built-layer non-trainable weights BatchNormalization layer. uses non-trainable weights keep track mean variance inputs training. learn use non-trainable weights custom layers, see guide writing new layers scratch. Example: BatchNormalization layer 2 trainable weights 2 non-trainable weights Layers & models also feature boolean attribute trainable. value can changed. Setting layer$trainable FALSE moves layer’s weights trainable non-trainable. called “freezing” layer: state frozen layer won’t updated training (either training fit() training custom loop relies trainable_weights apply gradient updates). Example: setting trainable False trainable weight becomes non-trainable, value longer updated training. confuse layer$trainable attribute argument training layer$call (controls whether layer run forward pass inference mode training mode). information, see Keras FAQ.","code":"layer <- layer_dense(units = 3) layer$build(shape(NULL, 4))  # Create the weights  length(layer$weights) ## [1] 2 length(layer$trainable_weights) ## [1] 2 length(layer$non_trainable_weights) ## [1] 0 layer <- layer_batch_normalization() layer$build(shape(NULL, 4))  # Create the weights  length(layer$weights) ## [1] 4 length(layer$trainable_weights) ## [1] 2 length(layer$non_trainable_weights) ## [1] 2 layer <- layer_dense(units = 3) layer$build(shape(NULL, 4))  # Create the weights layer$trainable <- FALSE  # Freeze the layer  length(layer$weights) ## [1] 2 length(layer$trainable_weights) ## [1] 0 length(layer$non_trainable_weights) ## [1] 2 # Make a model with 2 layers layer1 <- layer_dense(units = 3, activation=\"relu\") layer2 <- layer_dense(units = 3, activation=\"sigmoid\") model <- keras_model_sequential(input_shape = 3) %>%   layer1() %>%   layer2()  # Freeze the first layer layer1$trainable <- FALSE  # Keep a copy of the weights of layer1 for later reference initial_layer1_weights_values <- get_weights(layer1)  # Train the model model %>% compile(optimizer=\"adam\", loss=\"mse\") model %>% fit(random_normal(c(2, 3)), random_normal(c(2, 3)), epochs = 1) ## 1/1 - 0s - 294ms/step - loss: 2.1868 # Check that the weights of layer1 have not changed during training final_layer1_weights_values <- get_weights(layer1)  all.equal(initial_layer1_weights_values, final_layer1_weights_values) ## [1] TRUE"},{"path":"https://keras.posit.co/articles/transfer_learning.html","id":"recursive-setting-of-the-trainable-attribute","dir":"Articles","previous_headings":"","what":"Recursive setting of the trainable attribute","title":"Transfer learning & fine-tuning","text":"set trainable = False model layer sublayers, children layers become non-trainable well. Example:","code":"inner_model <- keras_model_sequential(input_shape = 3) %>%   layer_dense(units = 3, activation=\"relu\") %>%   layer_dense(units = 3, activation=\"relu\")  model <- keras_model_sequential(input_shape = 3) %>%   inner_model %>%   layer_dense(units = 3, activation=\"sigmoid\")  model$trainable <- FALSE  # Freeze the outer model  inner_model$trainable  # All layers in `model` are now frozen ## [1] FALSE inner_model$layers[[1]]$trainable  # `trainable` is propagated recursively ## [1] FALSE"},{"path":"https://keras.posit.co/articles/transfer_learning.html","id":"the-typical-transfer-learning-workflow","dir":"Articles","previous_headings":"","what":"The typical transfer-learning workflow","title":"Transfer learning & fine-tuning","text":"leads us typical transfer learning workflow can implemented Keras: Instantiate base model load pre-trained weights . Freeze layers base model setting trainable = False. Create new model top output one (several) layers base model. Train new model new dataset. Note alternative, lightweight workflow also : Instantiate base model load pre-trained weights . Run new dataset record output one (several) layers base model. called feature extraction. Use output input data new, smaller model. key advantage second workflow run base model data, rather per epoch training. ’s lot faster & cheaper. issue second workflow, though, doesn’t allow dynamically modify input data new model training, required data augmentation, instance. Transfer learning typically used tasks new dataset little data train full-scale model scratch, scenarios data augmentation important. follows, focus first workflow. ’s first workflow looks like Keras: First, instantiate base model pre-trained weights. , freeze base model. Create new model top. Train model new data.","code":"base_model <- application_xception(     weights='imagenet',  # Load weights pre-trained on ImageNet.     input_shape=c(150, 150, 3),     include_top=FALSE)  # Do not include the ImageNet classifier at the top. base_model$trainable <- FALSE inputs <- layer_input(shape = c(150, 150, 3)) # We make sure that the base_model is running in inference mode here, # by passing `training=False`. This is important for fine-tuning, as you will # learn in a few paragraphs. outputs <- inputs %>%   base_model(training = FALSE) %>%   # Convert features of shape `base_model.output_shape[1:]` to vectors   layer_global_average_pooling_2d() %>%   # A Dense classifier with a single unit (binary classification)   layer_dense(1) model <- keras_model(inputs, outputs) model %>% compile(   optimizer = optimizer_adam(),   loss = loss_binary_crossentropy(from_logits = TRUE),   metrics = list(metric_binary_accuracy()) ) model %>% fit(new_dataset, epochs=20, callbacks=..., validation_data=...)"},{"path":"https://keras.posit.co/articles/transfer_learning.html","id":"fine-tuning","dir":"Articles","previous_headings":"","what":"Fine-tuning","title":"Transfer learning & fine-tuning","text":"model converged new data, can try unfreeze part base model retrain whole model end--end low learning rate. optional last step can potentially give incremental improvements. also potentially lead quick overfitting – keep mind. critical step model frozen layers trained convergence. mix randomly-initialized trainable layers trainable layers hold pre-trained features, randomly-initialized layers cause large gradient updates training, destroy pre-trained features. ’s also critical use low learning rate stage, training much larger model first round training, dataset typically small. result, risk overfitting quickly apply large weight updates. , want readapt pretrained weights incremental way. implement fine-tuning whole base model: Important note compile() trainable Calling compile() model meant “freeze” behavior model. implies trainable attribute values time model compiled preserved throughout lifetime model, compile called . Hence, change trainable value, make sure call compile() model changes taken account. Important notes BatchNormalization layer Many image models contain BatchNormalization layers. layer special case every imaginable count. things keep mind. BatchNormalization contains 2 non-trainable weights get updated training. variables tracking mean variance inputs. set bn_layer.trainable = False, BatchNormalization layer run inference mode, update mean & variance statistics. case layers general, weight trainability & inference/training modes two orthogonal concepts. two tied case BatchNormalization layer. unfreeze model contains BatchNormalization layers order fine-tuning, keep BatchNormalization layers inference mode passing training=False calling base model. Otherwise updates applied non-trainable weights suddenly destroy model learned. ’ll see pattern action end--end example end guide.","code":"# Unfreeze the base model base_model$trainable <- TRUE  # It's important to recompile your model after you make any changes # to the `trainable` attribute of any inner layer, so that your changes # are take into account model %>% compile(   optimizer = optimizer_adam(1e-5),  # Very low learning rate   loss = loss_binary_crossentropy(from_logits=TRUE),   metrics = list(metric_binary_accuracy()) )  # Train end-to-end. Be careful to stop before you overfit! model %>% fit(new_dataset, epochs=10, callbacks=..., validation_data=...)"},{"path":"https://keras.posit.co/articles/transfer_learning.html","id":"an-end-to-end-example-fine-tuning-an-image-classification-model-on-a-cats-vs--dogs-dataset","dir":"Articles","previous_headings":"","what":"An end-to-end example: fine-tuning an image classification model on a cats vs. dogs dataset","title":"Transfer learning & fine-tuning","text":"solidify concepts, let’s walk concrete end--end transfer learning & fine-tuning example. load Xception model, pre-trained ImageNet, use Kaggle “cats vs. dogs” classification dataset.","code":""},{"path":"https://keras.posit.co/articles/transfer_learning.html","id":"getting-the-data","dir":"Articles","previous_headings":"An end-to-end example: fine-tuning an image classification model on a cats vs. dogs dataset","what":"Getting the data","title":"Transfer learning & fine-tuning","text":"First, let’s fetch cats vs. dogs dataset using TFDS. dataset, ’ll probably want use utility keras.utils.image_dataset_from_directory generate similar labeled dataset objects set images disk filed class-specific folders. Transfer learning useful working small datasets. keep dataset small, use 40% original training data (25,000 images) training, 10% validation, 10% testing. first 9 images training dataset – can see, ’re different sizes. can also see label 1 “dog” label 0 “cat”.","code":"# reticulate::py_install(\"tensorflow-datasets\") tfds <- reticulate::import(\"tensorflow_datasets\") tfds$disable_progress_bar()  c(train_ds, validation_ds, test_ds) %<-% tfds$load(     \"cats_vs_dogs\",     # Reserve 10% for validation and 10% for test     split = c(\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"),     as_supervised=TRUE  # Include labels ) ## Downloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /Users/tomasz/tensorflow_datasets/cats_vs_dogs/4.0.1... ## Dataset cats_vs_dogs downloaded and prepared to /Users/tomasz/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data. length(train_ds) ## [1] 9305 library(tfdatasets) ## ## Attaching package: 'tfdatasets' ## The following object is masked from 'package:keras3': ## ##     shape par(mfrow = c(3, 3), mar = c(1,0,1.5,0)) train_ds %>%   dataset_take(9) %>%   as_array_iterator() %>%   iterate(function(batch) {     c(image, label) %<-% batch     plot(as.raster(image, max = 255))     title(sprintf(\"label: %s   size: %s\",                   label, paste(dim(image), collapse = \" x \")))   })"},{"path":"https://keras.posit.co/articles/transfer_learning.html","id":"standardizing-the-data","dir":"Articles","previous_headings":"An end-to-end example: fine-tuning an image classification model on a cats vs. dogs dataset","what":"Standardizing the data","title":"Transfer learning & fine-tuning","text":"raw images variety sizes. addition, pixel consists 3 integer values 0 255 (RGB level values). isn’t great fit feeding neural network. need 2 things: Standardize fixed image size. pick 150x150. Normalize pixel values -1 1. ’ll using Normalization layer part model . general, ’s good practice develop models take raw data input, opposed models take already-preprocessed data. reason , model expects preprocessed data, time export model use elsewhere (web browser, mobile app), ’ll need reimplement exact preprocessing pipeline. gets tricky quickly. least possible amount preprocessing hitting model. , ’ll image resizing data pipeline (deep neural network can process contiguous batches data), ’ll input value scaling part model, create . Let’s resize images 150x150:","code":"resize_fn <- layer_resizing(width = 150, height = 150) resize_pair <- function(x, y) list(resize_fn(x), y)  train_ds <- train_ds %>% dataset_map(resize_pair) validation_ds <- validation_ds %>% dataset_map(resize_pair) test_ds <- test_ds %>% dataset_map(resize_pair)"},{"path":"https://keras.posit.co/articles/transfer_learning.html","id":"using-random-data-augmentation","dir":"Articles","previous_headings":"An end-to-end example: fine-tuning an image classification model on a cats vs. dogs dataset","what":"Using random data augmentation","title":"Transfer learning & fine-tuning","text":"don’t large image dataset, ’s good practice artificially introduce sample diversity applying random yet realistic transformations training images, random horizontal flipping small random rotations. helps expose model different aspects training data slowing overfitting. Let’s batch data use prefetching optimize loading speed. Let’s visualize first image first batch looks like various random transformations:","code":"data_augmentation <- keras_model_sequential() %>%   layer_random_flip(\"horizontal\") %>%   layer_random_rotation(.1)  train_ds <- train_ds %>%   dataset_map(function(x, y) list(data_augmentation(x), y)) library(tensorflow) ## ## Attaching package: 'tensorflow' ## The following objects are masked from 'package:keras3': ## ##     set_random_seed, shape batch_size <- 64  train_ds <- train_ds %>%   dataset_batch(batch_size) %>%   dataset_prefetch(tf$data$AUTOTUNE) %>%   dataset_cache() validation_ds <- validation_ds %>%   dataset_batch(batch_size) %>%   dataset_prefetch(tf$data$AUTOTUNE) %>%   dataset_cache() test_ds <- test_ds %>%   dataset_batch(batch_size) %>%   dataset_prefetch(tf$data$AUTOTUNE) %>%   dataset_cache() batch <- train_ds %>%   dataset_take(1) %>%   as_iterator() %>%   iter_next()  c(images, labels) %<-% batch first_image <- images[1, all_dims(), drop = TRUE] augmented_image <- data_augmentation(first_image, training = TRUE)  plot_image <- function(image, main = deparse1(substitute(image))) {   image %>%     as.array() %>%   # convert from tensor to R array     as.raster(max = 255) %>%     plot()    if(!is.null(main))     title(main) }  par(mfrow = c(2, 2), mar = c(1, 1, 1.5, 1)) plot_image(first_image) plot_image(augmented_image) plot_image(data_augmentation(first_image, training = TRUE), \"augmented 2\") plot_image(data_augmentation(first_image, training = TRUE), \"augmented 3\")"},{"path":"https://keras.posit.co/articles/transfer_learning.html","id":"build-a-model","dir":"Articles","previous_headings":"","what":"Build a model","title":"Transfer learning & fine-tuning","text":"Now let’s built model follows blueprint ’ve explained earlier. Note : add Rescaling layer scale input values (initially [0, 255] range) [-1, 1] range. add Dropout layer classification layer, regularization. make sure pass training=False calling base model, runs inference mode, batchnorm statistics don’t get updated even unfreeze base model fine-tuning.","code":"base_model <- application_xception(     weights=\"imagenet\",  # Load weights pre-trained on ImageNet.     input_shape=c(150, 150, 3),     include_top=FALSE, )  # Do not include the ImageNet classifier at the top.  # Freeze the base_model base_model$trainable <- FALSE  # Create new model on top inputs <- layer_input(shape=c(150, 150, 3))  # Pre-trained Xception weights requires that input be scaled # from (0, 255) to a range of (-1., +1.), the rescaling layer # outputs: `(inputs * scale) + offset` scale_layer <- layer_rescaling(scale=1 / 127.5, offset=-1) x <- scale_layer(inputs)  # The base model contains batchnorm layers. We want to keep them in inference mode # when we unfreeze the base model for fine-tuning, so we make sure that the # base_model is running in inference mode here. outputs <- x %>%   base_model(training = FALSE) %>%   layer_global_average_pooling_2d() %>%   layer_dropout(0.2) %>%   layer_dense(1)  model <- keras_model(inputs, outputs)  summary(model, show_trainable=TRUE) ## Model: \"functional_10\" ## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━┓ ## ┃ Layer (type)                ┃ Output Shape             ┃ Param # ┃ Trai… ┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━┩ ## │ input_layer_7 (InputLayer)  │ (None, 150, 150, 3)      │       0 │   -   │ ## ├─────────────────────────────┼──────────────────────────┼─────────┼───────┤ ## │ rescaling (Rescaling)       │ (None, 150, 150, 3)      │       0 │   -   │ ## ├─────────────────────────────┼──────────────────────────┼─────────┼───────┤ ## │ xception (Functional)       │ (None, 5, 5, 2048)       │ 20,861… │   N   │ ## ├─────────────────────────────┼──────────────────────────┼─────────┼───────┤ ## │ global_average_pooling2d_1  │ (None, 2048)             │       0 │   -   │ ## │ (GlobalAveragePooling2D)    │                          │         │       │ ## ├─────────────────────────────┼──────────────────────────┼─────────┼───────┤ ## │ dropout (Dropout)           │ (None, 2048)             │       0 │   -   │ ## ├─────────────────────────────┼──────────────────────────┼─────────┼───────┤ ## │ dense_8 (Dense)             │ (None, 1)                │   2,049 │   Y   │ ## └─────────────────────────────┴──────────────────────────┴─────────┴───────┘ ##  Total params: 20,863,529 (79.59 MB) ##  Trainable params: 2,049 (8.00 KB) ##  Non-trainable params: 20,861,480 (79.58 MB)"},{"path":"https://keras.posit.co/articles/transfer_learning.html","id":"train-the-top-layer","dir":"Articles","previous_headings":"","what":"Train the top layer","title":"Transfer learning & fine-tuning","text":"","code":"model %>% compile(     optimizer = optimizer_adam(),     loss = loss_binary_crossentropy(from_logits = TRUE),     metrics = list(metric_binary_accuracy()) )  epochs <- 1 model %>% fit(train_ds, epochs=epochs, validation_data=validation_ds) ## 146/146 - 120s - 825ms/step - binary_accuracy: 0.9239 - loss: 0.1776 - val_binary_accuracy: 0.9650 - val_loss: 0.0886"},{"path":"https://keras.posit.co/articles/transfer_learning.html","id":"do-a-round-of-fine-tuning-of-the-entire-model","dir":"Articles","previous_headings":"","what":"Do a round of fine-tuning of the entire model","title":"Transfer learning & fine-tuning","text":"Finally, let’s unfreeze base model train entire model end--end low learning rate. Importantly, although base model becomes trainable, still running inference mode since passed training=False calling built model. means batch normalization layers inside won’t update batch statistics. , wreck havoc representations learned model far. 10 epochs, fine-tuning gains us nice improvement . Let’s evaluate model test dataset:","code":"# Unfreeze the base_model. Note that it keeps running in inference mode # since we passed `training=False` when calling it. This means that # the batchnorm layers will not update their batch statistics. # This prevents the batchnorm layers from undoing all the training # we've done so far. base_model$trainable <- TRUE summary(model, show_trainable=TRUE) ## Model: \"functional_10\" ## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━┓ ## ┃ Layer (type)                ┃ Output Shape             ┃ Param # ┃ Trai… ┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━┩ ## │ input_layer_7 (InputLayer)  │ (None, 150, 150, 3)      │       0 │   -   │ ## ├─────────────────────────────┼──────────────────────────┼─────────┼───────┤ ## │ rescaling (Rescaling)       │ (None, 150, 150, 3)      │       0 │   -   │ ## ├─────────────────────────────┼──────────────────────────┼─────────┼───────┤ ## │ xception (Functional)       │ (None, 5, 5, 2048)       │ 20,861… │   Y   │ ## ├─────────────────────────────┼──────────────────────────┼─────────┼───────┤ ## │ global_average_pooling2d_1  │ (None, 2048)             │       0 │   -   │ ## │ (GlobalAveragePooling2D)    │                          │         │       │ ## ├─────────────────────────────┼──────────────────────────┼─────────┼───────┤ ## │ dropout (Dropout)           │ (None, 2048)             │       0 │   -   │ ## ├─────────────────────────────┼──────────────────────────┼─────────┼───────┤ ## │ dense_8 (Dense)             │ (None, 1)                │   2,049 │   Y   │ ## └─────────────────────────────┴──────────────────────────┴─────────┴───────┘ ##  Total params: 20,867,629 (79.60 MB) ##  Trainable params: 20,809,001 (79.38 MB) ##  Non-trainable params: 54,528 (213.00 KB) ##  Optimizer params: 4,100 (16.02 KB) model %>% compile(   optimizer = optimizer_adam(1e-5),  # Low learning rate   loss = loss_binary_crossentropy(from_logits=TRUE),   metrics = list(metric_binary_accuracy()) )  epochs <- 1 model %>% fit(train_ds, epochs=epochs, validation_data=validation_ds) ## 146/146 - 496s - 3s/step - binary_accuracy: 0.8600 - loss: 0.3399 - val_binary_accuracy: 0.9515 - val_loss: 0.1133 model %>% evaluate(test_ds) ## 37/37 - 22s - 592ms/step - binary_accuracy: 0.9421 - loss: 0.1391 ## $binary_accuracy ## [1] 0.9421453 ## ## $loss ## [1] 0.1390722"},{"path":"https://keras.posit.co/articles/understanding_masking_and_padding.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Understanding masking & padding","text":"","code":"library(keras3)"},{"path":"https://keras.posit.co/articles/understanding_masking_and_padding.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Understanding masking & padding","text":"Masking way tell sequence-processing layers certain timesteps input missing, thus skipped processing data. Padding special form masking masked steps start end sequence. Padding comes need encode sequence data contiguous batches: order make sequences batch fit given standard length, necessary pad truncate sequences. Let’s take close look.","code":""},{"path":"https://keras.posit.co/articles/understanding_masking_and_padding.html","id":"padding-sequence-data","dir":"Articles","previous_headings":"","what":"Padding sequence data","title":"Understanding masking & padding","text":"processing sequence data, common individual samples different lengths. Consider following example (text tokenized words): vocabulary lookup, data might vectorized integers, e.g.: data nested list individual samples length 3, 5, 6, respectively. Since input data deep learning model must single tensor (shape e.g. (batch_size, 6, vocab_size) case), samples shorter longest item need padded placeholder value (alternatively, one might also truncate long samples padding short samples). Keras provides utility function truncate pad Python lists common length: pad_sequences.","code":"data <- list(   c(\"Hello\", \"world\", \"!\"),   c(\"How\", \"are\", \"you\", \"doing\", \"today\"),   c(\"The\", \"weather\", \"will\", \"be\", \"nice\", \"tomorrow\") ) data <- list(   c(71, 1331, 4231),   c(73, 8, 3215, 55, 927),   c(83, 91, 1, 645, 1253, 927) ) raw_inputs <- list(   c(711, 632, 71),   c(73, 8, 3215, 55, 927),   c(83, 91, 1, 645, 1253, 927) )  # By default, this will pad using 0s; it is configurable via the # \"value\" parameter. # Note that you could use \"pre\" padding (at the beginning) or # \"post\" padding (at the end). # We recommend using \"post\" padding when working with RNN layers # (in order to be able to use the # CuDNN implementation of the layers). padded_inputs <- pad_sequences(raw_inputs, padding=\"post\") padded_inputs ##      [,1] [,2] [,3] [,4] [,5] [,6] ## [1,]  711  632   71    0    0    0 ## [2,]   73    8 3215   55  927    0 ## [3,]   83   91    1  645 1253  927"},{"path":"https://keras.posit.co/articles/understanding_masking_and_padding.html","id":"masking","dir":"Articles","previous_headings":"","what":"Masking","title":"Understanding masking & padding","text":"Now samples uniform length, model must informed part data actually padding ignored. mechanism masking. three ways introduce input masks Keras models: Add layer_masking layer. Configure layer_embedding layer mask_zero=TRUE. Pass mask argument manually calling layers support argument (e.g. RNN layers).","code":""},{"path":"https://keras.posit.co/articles/understanding_masking_and_padding.html","id":"mask-generating-layers-embedding-and-masking","dir":"Articles","previous_headings":"","what":"Mask-generating layers: Embedding and Masking","title":"Understanding masking & padding","text":"hood, layers create mask tensor (2D tensor shape (batch, sequence_length)), attach tensor output returned Masking Embedding layer. can see printed result, mask 2D boolean tensor shape (batch_size, sequence_length), individual FALSE entry indicates corresponding timestep ignored processing.","code":"embedding <- layer_embedding(input_dim=5000, output_dim=16, mask_zero=TRUE) masked_output <- embedding(padded_inputs)  masked_output$`_keras_mask` ## tf.Tensor( ## [[ True  True  True False False False] ##  [ True  True  True  True  True False] ##  [ True  True  True  True  True  True]], shape=(3, 6), dtype=bool) masking_layer <- layer_masking() # Simulate the embedding lookup by expanding the 2D input to 3D, # with embedding dimension of 10. unmasked_embedding <- op_cast(     op_tile(op_expand_dims(padded_inputs, axis=-1), c(1L, 1L, 10L)),     dtype=\"float32\" )  masked_embedding <- masking_layer(unmasked_embedding) masked_embedding$`_keras_mask` ## tf.Tensor( ## [[ True  True  True False False False] ##  [ True  True  True  True  True False] ##  [ True  True  True  True  True  True]], shape=(3, 6), dtype=bool)"},{"path":"https://keras.posit.co/articles/understanding_masking_and_padding.html","id":"mask-propagation-in-the-functional-api-and-sequential-api","dir":"Articles","previous_headings":"","what":"Mask propagation in the Functional API and Sequential API","title":"Understanding masking & padding","text":"using Functional API Sequential API, mask generated Embedding Masking layer propagated network layer capable using (example, RNN layers). Keras automatically fetch mask corresponding input pass layer knows use . instance, following Sequential model, LSTM layer automatically receive mask, means ignore padded values: also case following Functional API model:","code":"model <- keras_model_sequential() %>%   layer_embedding(input_dim=5000, output_dim=16, mask_zero=TRUE) %>%   layer_lstm(units=32) inputs <- layer_input(shape = shape(NULL), dtype=\"int32\") outputs <- inputs %>%   layer_embedding(input_dim=5000, output_dim=16, mask_zero=TRUE) %>%   layer_lstm(units=32)  model <- keras_model(inputs, outputs)"},{"path":"https://keras.posit.co/articles/understanding_masking_and_padding.html","id":"passing-mask-tensors-directly-to-layers","dir":"Articles","previous_headings":"","what":"Passing mask tensors directly to layers","title":"Understanding masking & padding","text":"Layers can handle masks (LSTM layer) mask argument call method. Meanwhile, layers produce mask (e.g. Embedding) expose compute_mask(input, previous_mask) method can call. Thus, can pass output compute_mask() method mask-producing layer call method mask-consuming layer, like :","code":"MyLayer <- new_layer_class(   \"MyLayer\",   initialize = function(...) {     super$initialize(...)     self$embedding <- layer_embedding(       input_dim=5000, output_dim=16, mask_zero=TRUE     )     self$lstm <- layer_lstm(units=32)   },   call = function(inputs) {     inputs %>%       self$embedding() %>%       # Note that you could also prepare a `mask` tensor manually.       # It only needs to be a boolean tensor       # with the right shape, i.e. (batch_size, timesteps).       self$lstm(mask=self$embedding$compute_mask(inputs))   } )  layer <- MyLayer() x <- random_integer(c(32, 10), 0, 100) layer(x) ## tf.Tensor( ## [[ 0.00130048 -0.00113368 -0.0071567  ... -0.00107615 -0.00162071 ##    0.00135018] ##  [-0.004185    0.00726349  0.00520932 ...  0.00119117  0.00230441 ##    0.00174123] ##  [-0.00537032 -0.00164898 -0.00238435 ... -0.00154158 -0.0038603 ##   -0.00105811] ##  ... ##  [ 0.00622132 -0.00905907 -0.00599518 ...  0.00025823 -0.00142478 ##   -0.00125036] ##  [-0.00523904  0.00336683 -0.00299453 ...  0.00876718  0.00172073 ##    0.00903089] ##  [-0.00393721  0.00058538  0.00503809 ... -0.00203075  0.00325884 ##   -0.00299755]], shape=(32, 32), dtype=float32)"},{"path":"https://keras.posit.co/articles/understanding_masking_and_padding.html","id":"supporting-masking-in-your-custom-layers","dir":"Articles","previous_headings":"","what":"Supporting masking in your custom layers","title":"Understanding masking & padding","text":"Sometimes, may need write layers generate mask (like Embedding), layers need modify current mask. instance, layer produces tensor different time dimension input, Concatenate layer concatenates time dimension, need modify current mask downstream layers able properly take masked timesteps account. , layer implement layer.compute_mask() method, produces new mask given input current mask. example TemporalSplit layer needs modify current mask. another example CustomEmbedding layer capable generating mask input values: Note: details format limitations related masking, see serialization guide.","code":"TemporalSplit <- new_layer_class(   \"TemporalSplit\",   call = function(inputs) {     # Expect the input to be 3D and mask to be 2D, split the input tensor into 2     # subtensors along the time axis (axis 1).     op_split(inputs, 2, axis=2)   },   compute_mask = function(inputs, mask = NULL) {     # Also split the mask into 2 if it presents.     if (!is.null(mask)) {       op_split(mask, 2, axis=2)     } else {       NULL     }   } )  c(first_half, second_half) %<-% TemporalSplit(masked_embedding) first_half$`_keras_mask` ## tf.Tensor( ## [[ True  True  True] ##  [ True  True  True] ##  [ True  True  True]], shape=(3, 3), dtype=bool) second_half$`_keras_mask` ## tf.Tensor( ## [[False False False] ##  [ True  True False] ##  [ True  True  True]], shape=(3, 3), dtype=bool) CustomEmbedding <- new_layer_class(   \"CustomEmbedding\",   initialize = function(input_dim, output_dim, mask_zero=FALSE, ...) {     super$initialize(...)     self$input_dim <- as.integer(input_dim)     self$output_dim <- as.integer(output_dim)     self$mask_zero <- mask_zero   },   build = function(input_shape) {     self$embeddings <- self$add_weight(       shape=c(self$input_dim, self$output_dim),       initializer=\"random_normal\",       dtype=\"float32\"     )   },   call = function(inputs) {     inputs <- op_cast(inputs, \"int32\")     op_take(self$embeddings, inputs)   },   compute_mask = function(inputs, mask=NULL) {     if (!self$mask_zero) {       NULL     } else {       op_not_equal(inputs, 0)     }   } )  layer <- CustomEmbedding(input_dim = 10, output_dim = 32, mask_zero=TRUE) x <- random_integer(c(3, 10), 0, 9)  y <- layer(x) mask <- layer$compute_mask(x)  mask ## tf.Tensor( ## [[ True  True  True  True  True  True  True  True  True  True] ##  [ True  True  True  True  True  True  True  True  True  True] ##  [False False  True  True  True  True  True  True  True  True]], shape=(3, 10), dtype=bool)"},{"path":"https://keras.posit.co/articles/understanding_masking_and_padding.html","id":"opting-in-to-mask-propagation-on-compatible-layers","dir":"Articles","previous_headings":"","what":"Opting-in to mask propagation on compatible layers","title":"Understanding masking & padding","text":"layers don’t modify time dimension, don’t need modify current mask. However, may still want able propagate current mask, unchanged, next layer. opt-behavior. default, custom layer destroy current mask (since framework way tell whether propagating mask safe ). custom layer modify time dimension, want able propagate current input mask, set self.supports_masking = True layer constructor. case, default behavior compute_mask() just pass current mask . ’s example layer whitelisted mask propagation: can now use custom layer -mask-generating layer (like Embedding) mask-consuming layer (like LSTM), pass mask along reaches mask-consuming layer.","code":"MyActivation <- new_layer_class(   \"MyActivation\",   initialize = function(...) {     super$initialize(...)     self$supports_masking <- TRUE   },   call = function(inputs) {     op_relu(inputs)   } ) inputs <- layer_input(shape = shape(NULL), dtype=\"int32\") outputs <- inputs %>%   layer_embedding(input_dim=5000, output_dim=16, mask_zero=TRUE) %>%   MyActivation() %>%   layer_lstm(units=32)  model <- keras_model(inputs, outputs) y <- model(random_integer(c(32, 100), 0, 5000))"},{"path":"https://keras.posit.co/articles/understanding_masking_and_padding.html","id":"writing-layers-that-need-mask-information","dir":"Articles","previous_headings":"","what":"Writing layers that need mask information","title":"Understanding masking & padding","text":"layers mask consumers: accept mask argument call use determine whether skip certain time steps. write layer, can simply add mask=None argument call signature. mask associated inputs passed layer whenever available. ’s simple example : layer computes softmax time dimension (axis 1) input sequence, discarding masked timesteps.","code":"TemporalSoftmax <- new_layer_class(   \"TemporalSoftmax\",   initialize = function(...) {     super$initialize(...)     self$supports_masking <- TRUE   },   call = function(inputs, mask=NULL) {     if (is.null(mask)) {       stop(\"`TemporalSoftmax` layer requires a previous layer to support masking.\")     }     broadcast_float_mask <- op_expand_dims(op_cast(mask, \"float32\"), -1)     inputs_exp <- op_exp(inputs) * broadcast_float_mask     inputs_sum <- op_sum(inputs_exp * broadcast_float_mask, axis=-1, keepdims=TRUE)     inputs_exp / inputs_sum   } )  inputs <- layer_input(shape = shape(NULL), dtype=\"int32\") outputs <- inputs %>%   layer_embedding(input_dim=10, output_dim=32, mask_zero=TRUE) %>%   layer_dense(1) %>%   TemporalSoftmax()  model <- keras_model(inputs, outputs) y <- model(random_integer(c(32, 100), 0, 10))"},{"path":"https://keras.posit.co/articles/understanding_masking_and_padding.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Understanding masking & padding","text":"need know padding & masking Keras. recap: “Masking” layers able know skip / ignore certain timesteps sequence inputs. layers mask-generators: Embedding can generate mask input values (mask_zero=TRUE), can Masking layer. layers mask-consumers: expose mask argument call method. case RNN layers. Functional API Sequential API, mask information propagated automatically. using layers standalone way, can pass mask arguments layers manually. can easily write layers modify current mask, generate new mask, consume mask associated inputs.","code":""},{"path":"https://keras.posit.co/articles/working_with_rnns.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Working with RNNs","text":"Recurrent neural networks (RNN) class neural networks powerful modeling sequence data time series natural language. Schematically, RNN layer uses loop iterate timesteps sequence, maintaining internal state encodes information timesteps seen far. Keras RNN API designed focus : Ease use: built-keras.layers.RNN, keras.layers.LSTM, keras.layers.GRU layers enable quickly build recurrent models without make difficult configuration choices. Ease customization: can also define RNN cell layer (inner part loop) custom behavior, use generic keras.layers.RNN layer (loop ). allows quickly prototype different research ideas flexible way minimal code.","code":""},{"path":"https://keras.posit.co/articles/working_with_rnns.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Working with RNNs","text":"","code":"import numpy as np import tensorflow as tf import keras from keras import layers"},{"path":"https://keras.posit.co/articles/working_with_rnns.html","id":"built-in-rnn-layers-a-simple-example","dir":"Articles","previous_headings":"","what":"Built-in RNN layers: a simple example","title":"Working with RNNs","text":"three built-RNN layers Keras: keras.layers.SimpleRNN, fully-connected RNN output previous timestep fed next timestep. keras.layers.GRU, first proposed Cho et al., 2014. keras.layers.LSTM, first proposed Hochreiter & Schmidhuber, 1997. early 2015, Keras first reusable open-source Python implementations LSTM GRU. simple example Sequential model processes sequences integers, embeds integer 64-dimensional vector, processes sequence vectors using LSTM layer. Built-RNNs support number useful features: Recurrent dropout, via dropout recurrent_dropout arguments Ability process input sequence reverse, via go_backwards argument Loop unrolling (can lead large speedup processing short sequences CPU), via unroll argument …. information, see RNN API documentation.","code":"model = keras.Sequential() # Add an Embedding layer expecting input vocab of size 1000, and # output embedding dimension of size 64. model.add(layers.Embedding(input_dim=1000, output_dim=64))  # Add a LSTM layer with 128 internal units. model.add(layers.LSTM(128))  # Add a Dense layer with 10 units. model.add(layers.Dense(10))  model.summary()"},{"path":"https://keras.posit.co/articles/working_with_rnns.html","id":"outputs-and-states","dir":"Articles","previous_headings":"","what":"Outputs and states","title":"Working with RNNs","text":"default, output RNN layer contains single vector per sample. vector RNN cell output corresponding last timestep, containing information entire input sequence. shape output (batch_size, units) units corresponds units argument passed layer’s constructor. RNN layer can also return entire sequence outputs sample (one vector per timestep per sample), set return_sequences=True. shape output (batch_size, timesteps, units). addition, RNN layer can return final internal state(s). returned states can used resume RNN execution later, initialize another RNN. setting commonly used encoder-decoder sequence--sequence model, encoder final state used initial state decoder. configure RNN layer return internal state, set return_state parameter True creating layer. Note LSTM 2 state tensors, GRU one. configure initial state layer, just call layer additional keyword argument initial_state. Note shape state needs match unit size layer, like example .","code":"model = keras.Sequential() model.add(layers.Embedding(input_dim=1000, output_dim=64))  # The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256) model.add(layers.GRU(256, return_sequences=True))  # The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128) model.add(layers.SimpleRNN(128))  model.add(layers.Dense(10))  model.summary() encoder_vocab = 1000 decoder_vocab = 2000  encoder_input = layers.Input(shape=(None,)) encoder_embedded = layers.Embedding(input_dim=encoder_vocab, output_dim=64)(     encoder_input )  # Return states in addition to output output, state_h, state_c = layers.LSTM(64, return_state=True, name=\"encoder\")(     encoder_embedded ) encoder_state = [state_h, state_c]  decoder_input = layers.Input(shape=(None,)) decoder_embedded = layers.Embedding(input_dim=decoder_vocab, output_dim=64)(     decoder_input )  # Pass the 2 states to a new LSTM layer, as initial state decoder_output = layers.LSTM(64, name=\"decoder\")(     decoder_embedded, initial_state=encoder_state ) output = layers.Dense(10)(decoder_output)  model = keras.Model([encoder_input, decoder_input], output) model.summary()"},{"path":"https://keras.posit.co/articles/working_with_rnns.html","id":"rnn-layers-and-rnn-cells","dir":"Articles","previous_headings":"","what":"RNN layers and RNN cells","title":"Working with RNNs","text":"addition built-RNN layers, RNN API also provides cell-level APIs. Unlike RNN layers, processes whole batches input sequences, RNN cell processes single timestep. cell inside loop RNN layer. Wrapping cell inside keras.layers.RNN layer gives layer capable processing batches sequences, e.g. RNN(LSTMCell(10)). Mathematically, RNN(LSTMCell(10)) produces result LSTM(10). fact, implementation layer TF v1.x just creating corresponding RNN cell wrapping RNN layer. However using built-GRU LSTM layers enable use CuDNN may see better performance. three built-RNN cells, corresponding matching RNN layer. keras.layers.SimpleRNNCell corresponds SimpleRNN layer. keras.layers.GRUCell corresponds GRU layer. keras.layers.LSTMCell corresponds LSTM layer. cell abstraction, together generic keras.layers.RNN class, make easy implement custom RNN architectures research.","code":""},{"path":"https://keras.posit.co/articles/working_with_rnns.html","id":"cross-batch-statefulness","dir":"Articles","previous_headings":"","what":"Cross-batch statefulness","title":"Working with RNNs","text":"processing long sequences (possibly infinite), may want use pattern cross-batch statefulness. Normally, internal state RNN layer reset every time sees new batch (.e. every sample seen layer assumed independent past). layer maintain state processing given sample. long sequences though, useful break shorter sequences, feed shorter sequences sequentially RNN layer without resetting layer’s state. way, layer can retain information entirety sequence, even though ’s seeing one sub-sequence time. can setting stateful=True constructor. sequence s = [t0, t1, ... t1546, t1547], split e.g. process via: want clear state, can use layer.reset_states(). Note: setup, sample given batch assumed continuation sample previous batch. means batches contain number samples (batch size). E.g. batch contains [sequence_A_from_t0_to_t100, sequence_B_from_t0_to_t100], next batch contain [sequence_A_from_t101_to_t200,  sequence_B_from_t101_to_t200]. complete example:","code":"s1 = [t0, t1, ... t100] s2 = [t101, ... t201] ... s16 = [t1501, ... t1547] lstm_layer = layers.LSTM(64, stateful=True) for s in sub_sequences:   output = lstm_layer(s) paragraph1 = np.random.random((20, 10, 50)).astype(np.float32) paragraph2 = np.random.random((20, 10, 50)).astype(np.float32) paragraph3 = np.random.random((20, 10, 50)).astype(np.float32)  lstm_layer = layers.LSTM(64, stateful=True) output = lstm_layer(paragraph1) output = lstm_layer(paragraph2) output = lstm_layer(paragraph3)  # reset_states() will reset the cached state to the original initial_state. # If no initial_state was provided, zero-states will be used by default. lstm_layer.reset_states()"},{"path":"https://keras.posit.co/articles/working_with_rnns.html","id":"rnn-state-reuse","dir":"Articles","previous_headings":"Cross-batch statefulness","what":"RNN State Reuse","title":"Working with RNNs","text":"recorded states RNN layer included layer.weights(). like reuse state RNN layer, can retrieve states value layer.states use initial state new layer via Keras functional API like new_layer(inputs, initial_state=layer.states), model subclassing. Please also note sequential model might used case since supports layers single input output, extra input initial state makes impossible use .","code":"paragraph1 = np.random.random((20, 10, 50)).astype(np.float32) paragraph2 = np.random.random((20, 10, 50)).astype(np.float32) paragraph3 = np.random.random((20, 10, 50)).astype(np.float32)  lstm_layer = layers.LSTM(64, stateful=True) output = lstm_layer(paragraph1) output = lstm_layer(paragraph2)  existing_state = lstm_layer.states  new_lstm_layer = layers.LSTM(64) new_output = new_lstm_layer(paragraph3, initial_state=existing_state)"},{"path":"https://keras.posit.co/articles/working_with_rnns.html","id":"bidirectional-rnns","dir":"Articles","previous_headings":"","what":"Bidirectional RNNs","title":"Working with RNNs","text":"sequences time series (e.g. text), often case RNN model can perform better processes sequence start end, also backwards. example, predict next word sentence, often useful context around word, just words come . Keras provides easy API build bidirectional RNNs: keras.layers.Bidirectional wrapper. hood, Bidirectional copy RNN layer passed , flip go_backwards field newly copied layer, process inputs reverse order. output Bidirectional RNN , default, concatenation forward layer output backward layer output. need different merging behavior, e.g. concatenation, change merge_mode parameter Bidirectional wrapper constructor. details Bidirectional, please check API docs.","code":"model = keras.Sequential()  model.add(     layers.Bidirectional(layers.LSTM(64, return_sequences=True), input_shape=(5, 10)) ) model.add(layers.Bidirectional(layers.LSTM(32))) model.add(layers.Dense(10))  model.summary()"},{"path":"https://keras.posit.co/articles/working_with_rnns.html","id":"performance-optimization-and-cudnn-kernels","dir":"Articles","previous_headings":"","what":"Performance optimization and CuDNN kernels","title":"Working with RNNs","text":"TensorFlow 2.0, built-LSTM GRU layers updated leverage CuDNN kernels default GPU available. change, prior keras.layers.CuDNNLSTM/CuDNNGRU layers deprecated, can build model without worrying hardware run . Since CuDNN kernel built certain assumptions, means layer able use CuDNN kernel change defaults built-LSTM GRU layers. E.g.: Changing activation function tanh something else. Changing recurrent_activation function sigmoid something else. Using recurrent_dropout > 0. Setting unroll True, forces LSTM/GRU decompose inner tf.while_loop unrolled loop. Setting use_bias False. Using masking input data strictly right padded (mask corresponds strictly right padded data, CuDNN can still used. common case). detailed list constraints, please see documentation LSTM GRU layers.","code":""},{"path":"https://keras.posit.co/articles/working_with_rnns.html","id":"using-cudnn-kernels-when-available","dir":"Articles","previous_headings":"Performance optimization and CuDNN kernels","what":"Using CuDNN kernels when available","title":"Working with RNNs","text":"Let’s build simple LSTM model demonstrate performance difference. ’ll use input sequences sequence rows MNIST digits (treating row pixels timestep), ’ll predict digit’s label. Let’s load MNIST dataset: Let’s create model instance train . choose sparse_categorical_crossentropy loss function model. output model shape [batch_size, 10]. target model integer vector, integer range 0 9. Now, let’s compare model use CuDNN kernel: running machine NVIDIA GPU CuDNN installed, model built CuDNN much faster train compared model uses regular TensorFlow kernel. CuDNN-enabled model can also used run inference CPU-environment. tf.device annotation just forcing device placement. model run CPU default GPU available. simply don’t worry hardware ’re running anymore. Isn’t pretty cool?","code":"batch_size = 64 # Each MNIST image batch is a tensor of shape (batch_size, 28, 28). # Each input sequence will be of size (28, 28) (height is treated like time). input_dim = 28  units = 64 output_size = 10  # labels are from 0 to 9   # Build the RNN model def build_model(allow_cudnn_kernel=True):     # CuDNN is only available at the layer level, and not at the cell level.     # This means `LSTM(units)` will use the CuDNN kernel,     # while RNN(LSTMCell(units)) will run on non-CuDNN kernel.     if allow_cudnn_kernel:         # The LSTM layer with default options uses CuDNN.         lstm_layer = keras.layers.LSTM(units, input_shape=(None, input_dim))     else:         # Wrapping a LSTMCell in a RNN layer will not use CuDNN.         lstm_layer = keras.layers.RNN(             keras.layers.LSTMCell(units), input_shape=(None, input_dim)         )     model = keras.models.Sequential(         [             lstm_layer,             keras.layers.BatchNormalization(),             keras.layers.Dense(output_size),         ]     )     return model mnist = keras.datasets.mnist  (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 sample, sample_label = x_train[0], y_train[0] model = build_model(allow_cudnn_kernel=True)  model.compile(     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),     optimizer=\"sgd\",     metrics=[\"accuracy\"], )   model.fit(     x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=1 ) noncudnn_model = build_model(allow_cudnn_kernel=False) noncudnn_model.set_weights(model.get_weights()) noncudnn_model.compile(     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),     optimizer=\"sgd\",     metrics=[\"accuracy\"], ) noncudnn_model.fit(     x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=1 ) import matplotlib.pyplot as plt  with tf.device(\"CPU:0\"):     cpu_model = build_model(allow_cudnn_kernel=True)     cpu_model.set_weights(model.get_weights())     result = tf.argmax(cpu_model.predict_on_batch(tf.expand_dims(sample, 0)), axis=1)     print(         \"Predicted result is: %s, target result is: %s\" % (result.numpy(), sample_label)     )     plt.imshow(sample, cmap=plt.get_cmap(\"gray\"))"},{"path":"https://keras.posit.co/articles/working_with_rnns.html","id":"rnns-with-listdict-inputs-or-nested-inputs","dir":"Articles","previous_headings":"","what":"RNNs with list/dict inputs, or nested inputs","title":"Working with RNNs","text":"Nested structures allow implementers include information within single timestep. example, video frame audio video input time. data shape case : [batch, timestep, {\"video\": [height, width, channel], \"audio\": [frequency]}] another example, handwriting data coordinates x y current position pen, well pressure information. data representation : [batch, timestep, {\"location\": [x, y], \"pressure\": [force]}] following code provides example build custom RNN cell accepts structured inputs.","code":""},{"path":"https://keras.posit.co/articles/working_with_rnns.html","id":"define-a-custom-cell-that-supports-nested-inputoutput","dir":"Articles","previous_headings":"RNNs with list/dict inputs, or nested inputs","what":"Define a custom cell that supports nested input/output","title":"Working with RNNs","text":"See Making new Layers & Models via subclassing details writing layers.","code":"@keras.saving.register_keras_serializable() class NestedCell(keras.layers.Layer):     def __init__(self, unit_1, unit_2, unit_3, **kwargs):         self.unit_1 = unit_1         self.unit_2 = unit_2         self.unit_3 = unit_3         self.state_size = [tf.TensorShape([unit_1]), tf.TensorShape([unit_2, unit_3])]         self.output_size = [tf.TensorShape([unit_1]), tf.TensorShape([unit_2, unit_3])]         super().__init__(**kwargs)      def build(self, input_shapes):         # expect input_shape to contain 2 items, [(batch, i1), (batch, i2, i3)]         i1 = input_shapes[0][1]         i2 = input_shapes[1][1]         i3 = input_shapes[1][2]          self.kernel_1 = self.add_weight(             shape=(i1, self.unit_1), initializer=\"uniform\", name=\"kernel_1\"         )         self.kernel_2_3 = self.add_weight(             shape=(i2, i3, self.unit_2, self.unit_3),             initializer=\"uniform\",             name=\"kernel_2_3\",         )      def call(self, inputs, states):         # inputs should be in [(batch, input_1), (batch, input_2, input_3)]         # state should be in shape [(batch, unit_1), (batch, unit_2, unit_3)]         input_1, input_2 = tf.nest.flatten(inputs)         s1, s2 = states          output_1 = tf.matmul(input_1, self.kernel_1)         output_2_3 = tf.einsum(\"bij,ijkl->bkl\", input_2, self.kernel_2_3)         state_1 = s1 + output_1         state_2_3 = s2 + output_2_3          output = (output_1, output_2_3)         new_states = (state_1, state_2_3)          return output, new_states      def get_config(self):         return {\"unit_1\": self.unit_1, \"unit_2\": unit_2, \"unit_3\": self.unit_3}"},{"path":"https://keras.posit.co/articles/working_with_rnns.html","id":"build-a-rnn-model-with-nested-inputoutput","dir":"Articles","previous_headings":"RNNs with list/dict inputs, or nested inputs","what":"Build a RNN model with nested input/output","title":"Working with RNNs","text":"Let’s build Keras model uses keras.layers.RNN layer custom cell just defined.","code":"unit_1 = 10 unit_2 = 20 unit_3 = 30  i1 = 32 i2 = 64 i3 = 32 batch_size = 64 num_batches = 10 timestep = 50  cell = NestedCell(unit_1, unit_2, unit_3) rnn = keras.layers.RNN(cell)  input_1 = keras.Input((None, i1)) input_2 = keras.Input((None, i2, i3))  outputs = rnn((input_1, input_2))  model = keras.models.Model([input_1, input_2], outputs)  model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])"},{"path":"https://keras.posit.co/articles/working_with_rnns.html","id":"train-the-model-with-randomly-generated-data","dir":"Articles","previous_headings":"RNNs with list/dict inputs, or nested inputs","what":"Train the model with randomly generated data","title":"Working with RNNs","text":"Since isn’t good candidate dataset model, use random Numpy data demonstration. Keras keras.layers.RNN layer, expected define math logic individual step within sequence, keras.layers.RNN layer handle sequence iteration . ’s incredibly powerful way quickly prototype new kinds RNNs (e.g. LSTM variant). details, please visit API docs.","code":"input_1_data = np.random.random((batch_size * num_batches, timestep, i1)) input_2_data = np.random.random((batch_size * num_batches, timestep, i2, i3)) target_1_data = np.random.random((batch_size * num_batches, unit_1)) target_2_data = np.random.random((batch_size * num_batches, unit_2, unit_3)) input_data = [input_1_data, input_2_data] target_data = [target_1_data, target_2_data]  model.fit(input_data, target_data, batch_size=batch_size)"},{"path":"https://keras.posit.co/articles/writing_a_custom_training_loop_in_jax.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Writing a training loop from scratch in JAX","text":"","code":"import os  # This guide can only be run with the jax backend. os.environ[\"KERAS_BACKEND\"] = \"jax\"  import jax  # We import TF so we can use tf.data. import tensorflow as tf import keras import numpy as np"},{"path":"https://keras.posit.co/articles/writing_a_custom_training_loop_in_jax.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Writing a training loop from scratch in JAX","text":"Keras provides default training evaluation loops, fit() evaluate(). usage covered guide Training & evaluation built-methods. want customize learning algorithm model still leveraging convenience fit() (instance, train GAN using fit()), can subclass Model class implement train_step() method, called repeatedly fit(). Now, want low-level control training & evaluation, write training & evaluation loops scratch. guide .","code":""},{"path":"https://keras.posit.co/articles/writing_a_custom_training_loop_in_jax.html","id":"a-first-end-to-end-example","dir":"Articles","previous_headings":"","what":"A first end-to-end example","title":"Writing a training loop from scratch in JAX","text":"write custom training loop, need following ingredients: model train, course. optimizer. either use optimizer keras.optimizers, one optax package. loss function. dataset. standard JAX ecosystem load data via tf.data, ’s ’ll use. Let’s line . First, let’s get model MNIST dataset: Next, ’s loss function optimizer. ’ll use Keras optimizer case.","code":"def get_model():     inputs = keras.Input(shape=(784,), name=\"digits\")     x1 = keras.layers.Dense(64, activation=\"relu\")(inputs)     x2 = keras.layers.Dense(64, activation=\"relu\")(x1)     outputs = keras.layers.Dense(10, name=\"predictions\")(x2)     model = keras.Model(inputs=inputs, outputs=outputs)     return model   model = get_model()  # Prepare the training dataset. batch_size = 32 (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data() x_train = np.reshape(x_train, (-1, 784)).astype(\"float32\") x_test = np.reshape(x_test, (-1, 784)).astype(\"float32\") y_train = keras.utils.to_categorical(y_train) y_test = keras.utils.to_categorical(y_test)  # Reserve 10,000 samples for validation. x_val = x_train[-10000:] y_val = y_train[-10000:] x_train = x_train[:-10000] y_train = y_train[:-10000]  # Prepare the training dataset. train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)) train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)  # Prepare the validation dataset. val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)) val_dataset = val_dataset.batch(batch_size) # Instantiate a loss function. loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)  # Instantiate an optimizer. optimizer = keras.optimizers.Adam(learning_rate=1e-3)"},{"path":"https://keras.posit.co/articles/writing_a_custom_training_loop_in_jax.html","id":"getting-gradients-in-jax","dir":"Articles","previous_headings":"A first end-to-end example","what":"Getting gradients in JAX","title":"Writing a training loop from scratch in JAX","text":"Let’s train model using mini-batch gradient custom training loop. JAX, gradients computed via metaprogramming: call jax.grad (jax.value_and_grad function order create gradient-computing function first function. first thing need function returns loss value. ’s function ’ll use generate gradient function. Something like : function, can compute gradients via metaprogramming : Typically, don’t just want get gradient values, also want get loss value. can using jax.value_and_grad instead jax.grad:","code":"def compute_loss(x, y):     ...     return loss grad_fn = jax.grad(compute_loss) grads = grad_fn(x, y) grad_fn = jax.value_and_grad(compute_loss) loss, grads = grad_fn(x, y)"},{"path":"https://keras.posit.co/articles/writing_a_custom_training_loop_in_jax.html","id":"jax-computation-is-purely-stateless","dir":"Articles","previous_headings":"A first end-to-end example","what":"JAX computation is purely stateless","title":"Writing a training loop from scratch in JAX","text":"JAX, everything must stateless function – loss computation function must stateless well. means Keras variables (e.g. weight tensors) must passed function inputs, variable updated forward pass must returned function output. function side effect. forward pass, non-trainable variables Keras model might get updated. variables , instance, RNG seed state variables BatchNormalization statistics. ’re going need return . need something like : function, can get gradient function specifying hax_aux value_and_grad: tells JAX loss computation function returns outputs just loss. Note loss always first output. Now established basics, let’s implement compute_loss_and_updates function. Keras models stateless_call method come handy . works just like model.__call__, requires explicitly pass value variables model, returns just __call__ outputs also (potentially updated) non-trainable variables. Let’s get gradient function:","code":"def compute_loss_and_updates(trainable_variables, non_trainable_variables, x, y):     ...     return loss, non_trainable_variables grad_fn = jax.value_and_grad(compute_loss_and_updates, has_aux=True) (loss, non_trainable_variables), grads = grad_fn(     trainable_variables, non_trainable_variables, x, y ) def compute_loss_and_updates(     trainable_variables, non_trainable_variables, x, y ):     y_pred, non_trainable_variables = model.stateless_call(         trainable_variables, non_trainable_variables, x     )     loss = loss_fn(y, y_pred)     return loss, non_trainable_variables grad_fn = jax.value_and_grad(compute_loss_and_updates, has_aux=True)"},{"path":"https://keras.posit.co/articles/writing_a_custom_training_loop_in_jax.html","id":"the-training-step-function","dir":"Articles","previous_headings":"A first end-to-end example","what":"The training step function","title":"Writing a training loop from scratch in JAX","text":"Next, let’s implement end--end training step, function run forward pass, compute loss, compute gradients, also use optimizer update trainable variables. function also needs stateless, get input state tuple includes every state element ’re going use: trainable_variables non_trainable_variables: model’s variables. optimizer_variables: optimizer’s state variables, momentum accumulators. update trainable variables, use optimizer’s stateless method stateless_apply. ’s equivalent optimizer.apply(), requires always passing trainable_variables optimizer_variables. returns updated trainable variables updated optimizer_variables.","code":"def train_step(state, data):     trainable_variables, non_trainable_variables, optimizer_variables = state     x, y = data     (loss, non_trainable_variables), grads = grad_fn(         trainable_variables, non_trainable_variables, x, y     )     trainable_variables, optimizer_variables = optimizer.stateless_apply(         optimizer_variables, grads, trainable_variables     )     # Return updated state     return loss, (         trainable_variables,         non_trainable_variables,         optimizer_variables,     )"},{"path":"https://keras.posit.co/articles/writing_a_custom_training_loop_in_jax.html","id":"make-it-fast-with-jax-jit","dir":"Articles","previous_headings":"A first end-to-end example","what":"Make it fast with jax.jit","title":"Writing a training loop from scratch in JAX","text":"default, JAX operations run eagerly, just like TensorFlow eager mode PyTorch eager mode. just like TensorFlow eager mode PyTorch eager mode, ’s pretty slow – eager mode better used debugging environment, way actual work. let’s make train_step fast compiling . stateless JAX function, can compile XLA via @jax.jit decorator. get traced first execution, subsequent executions executing traced graph (just like @tf.function(jit_compile=True). Let’s try : ’re now ready train model. training loop trivial: just repeatedly call loss, state = train_step(state, data). Note: convert TF tensors yielded tf.data.Dataset NumPy passing JAX function. variables must built beforehand: model must built optimizer must built. Since ’re using Functional API model, ’s already built, subclassed model ’d need call batch data build . key thing notice loop entirely stateless – variables attached model (model.weights) never getting updated loop. new values stored state tuple. means point, saving model, attaching new variable values back model. Just call variable.assign(new_value) model variable want update:","code":"@jax.jit def train_step(state, data):     trainable_variables, non_trainable_variables, optimizer_variables = state     x, y = data     (loss, non_trainable_variables), grads = grad_fn(         trainable_variables, non_trainable_variables, x, y     )     trainable_variables, optimizer_variables = optimizer.stateless_apply(         optimizer_variables, grads, trainable_variables     )     # Return updated state     return loss, (         trainable_variables,         non_trainable_variables,         optimizer_variables,     ) # Build optimizer variables. optimizer.build(model.trainable_variables)  trainable_variables = model.trainable_variables non_trainable_variables = model.non_trainable_variables optimizer_variables = optimizer.variables state = trainable_variables, non_trainable_variables, optimizer_variables  # Training loop for step, data in enumerate(train_dataset):     data = (data[0].numpy(), data[1].numpy())     loss, state = train_step(state, data)     # Log every 100 batches.     if step % 100 == 0:         print(f\"Training loss (for 1 batch) at step {step}: {float(loss):.4f}\")         print(f\"Seen so far: {(step + 1) * batch_size} samples\") trainable_variables, non_trainable_variables, optimizer_variables = state for variable, value in zip(model.trainable_variables, trainable_variables):     variable.assign(value) for variable, value in zip(     model.non_trainable_variables, non_trainable_variables ):     variable.assign(value)"},{"path":"https://keras.posit.co/articles/writing_a_custom_training_loop_in_jax.html","id":"low-level-handling-of-metrics","dir":"Articles","previous_headings":"","what":"Low-level handling of metrics","title":"Writing a training loop from scratch in JAX","text":"Let’s add metrics monitoring basic training loop. can readily reuse built-Keras metrics (custom ones wrote) training loops written scratch. ’s flow: Instantiate metric start loop Include metric_variables train_step arguments compute_loss_and_updates arguments. Call metric.stateless_update_state() compute_loss_and_updates function. ’s equivalent update_state() – stateless. need display current value metric, outside train_step (eager scope), attach new metric variable values metric object vall metric.result(). Call metric.reset_state() need clear state metric (typically end epoch) Let’s use knowledge compute CategoricalAccuracy training validation data end training: ’ll also prepare evaluation step function: loops:","code":"# Get a fresh model model = get_model()  # Instantiate an optimizer to train the model. optimizer = keras.optimizers.Adam(learning_rate=1e-3) # Instantiate a loss function. loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)  # Prepare the metrics. train_acc_metric = keras.metrics.CategoricalAccuracy() val_acc_metric = keras.metrics.CategoricalAccuracy()   def compute_loss_and_updates(     trainable_variables, non_trainable_variables, metric_variables, x, y ):     y_pred, non_trainable_variables = model.stateless_call(         trainable_variables, non_trainable_variables, x     )     loss = loss_fn(y, y_pred)     metric_variables = train_acc_metric.stateless_update_state(         metric_variables, y, y_pred     )     return loss, (non_trainable_variables, metric_variables)   grad_fn = jax.value_and_grad(compute_loss_and_updates, has_aux=True)   @jax.jit def train_step(state, data):     (         trainable_variables,         non_trainable_variables,         optimizer_variables,         metric_variables,     ) = state     x, y = data     (loss, (non_trainable_variables, metric_variables)), grads = grad_fn(         trainable_variables, non_trainable_variables, metric_variables, x, y     )     trainable_variables, optimizer_variables = optimizer.stateless_apply(         optimizer_variables, grads, trainable_variables     )     # Return updated state     return loss, (         trainable_variables,         non_trainable_variables,         optimizer_variables,         metric_variables,     ) @jax.jit def eval_step(state, data):     trainable_variables, non_trainable_variables, metric_variables = state     x, y = data     y_pred, non_trainable_variables = model.stateless_call(         trainable_variables, non_trainable_variables, x     )     loss = loss_fn(y, y_pred)     metric_variables = val_acc_metric.stateless_update_state(         metric_variables, y, y_pred     )     return loss, (         trainable_variables,         non_trainable_variables,         metric_variables,     ) # Build optimizer variables. optimizer.build(model.trainable_variables)  trainable_variables = model.trainable_variables non_trainable_variables = model.non_trainable_variables optimizer_variables = optimizer.variables metric_variables = train_acc_metric.variables state = (     trainable_variables,     non_trainable_variables,     optimizer_variables,     metric_variables, )  # Training loop for step, data in enumerate(train_dataset):     data = (data[0].numpy(), data[1].numpy())     loss, state = train_step(state, data)     # Log every 100 batches.     if step % 100 == 0:         print(f\"Training loss (for 1 batch) at step {step}: {float(loss):.4f}\")         _, _, _, metric_variables = state         for variable, value in zip(             train_acc_metric.variables, metric_variables         ):             variable.assign(value)         print(f\"Training accuracy: {train_acc_metric.result()}\")         print(f\"Seen so far: {(step + 1) * batch_size} samples\")  metric_variables = val_acc_metric.variables (     trainable_variables,     non_trainable_variables,     optimizer_variables,     metric_variables, ) = state state = trainable_variables, non_trainable_variables, metric_variables  # Eval loop for step, data in enumerate(val_dataset):     data = (data[0].numpy(), data[1].numpy())     loss, state = eval_step(state, data)     # Log every 100 batches.     if step % 100 == 0:         print(             f\"Validation loss (for 1 batch) at step {step}: {float(loss):.4f}\"         )         _, _, metric_variables = state         for variable, value in zip(val_acc_metric.variables, metric_variables):             variable.assign(value)         print(f\"Validation accuracy: {val_acc_metric.result()}\")         print(f\"Seen so far: {(step + 1) * batch_size} samples\")"},{"path":"https://keras.posit.co/articles/writing_a_custom_training_loop_in_jax.html","id":"low-level-handling-of-losses-tracked-by-the-model","dir":"Articles","previous_headings":"","what":"Low-level handling of losses tracked by the model","title":"Writing a training loop from scratch in JAX","text":"Layers & models recursively track losses created forward pass layers call self.add_loss(value). resulting list scalar loss values available via property model.losses end forward pass. want using loss components, sum add main loss training step. Consider layer, creates activity regularization loss: Let’s build really simple model uses : ’s compute_loss_and_updates function look like now: Pass return_losses=True model.stateless_call(). Sum resulting losses add main loss. ’s !","code":"class ActivityRegularizationLayer(keras.layers.Layer):     def call(self, inputs):         self.add_loss(1e-2 * jax.numpy.sum(inputs))         return inputs inputs = keras.Input(shape=(784,), name=\"digits\") x = keras.layers.Dense(64, activation=\"relu\")(inputs) # Insert activity regularization as a layer x = ActivityRegularizationLayer()(x) x = keras.layers.Dense(64, activation=\"relu\")(x) outputs = keras.layers.Dense(10, name=\"predictions\")(x)  model = keras.Model(inputs=inputs, outputs=outputs) def compute_loss_and_updates(     trainable_variables, non_trainable_variables, metric_variables, x, y ):     y_pred, non_trainable_variables, losses = model.stateless_call(         trainable_variables, non_trainable_variables, x, return_losses=True     )     loss = loss_fn(y, y_pred)     if losses:         loss += jax.numpy.sum(losses)     metric_variables = train_acc_metric.stateless_update_state(         metric_variables, y, y_pred     )     return loss, non_trainable_variables, metric_variables"},{"path":"https://keras.posit.co/articles/writing_a_custom_training_loop_in_tensorflow.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Writing a training loop from scratch in TensorFlow","text":"","code":"library(tensorflow) ## ## Attaching package: 'tensorflow' ## The following objects are masked from 'package:keras3': ## ##     set_random_seed, shape library(keras3) library(tfdatasets) ## ## Attaching package: 'tfdatasets' ## The following object is masked from 'package:keras3': ## ##     shape"},{"path":"https://keras.posit.co/articles/writing_a_custom_training_loop_in_tensorflow.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Writing a training loop from scratch in TensorFlow","text":"Keras provides default training evaluation loops, fit() evaluate(). usage covered guide Training & evaluation built-methods. want customize learning algorithm model still leveraging convenience fit() (instance, train GAN using fit()), can subclass Model class implement train_step() method, called repeatedly fit(). Now, want low-level control training & evaluation, write training & evaluation loops scratch. guide .","code":""},{"path":"https://keras.posit.co/articles/writing_a_custom_training_loop_in_tensorflow.html","id":"a-first-end-to-end-example","dir":"Articles","previous_headings":"","what":"A first end-to-end example","title":"Writing a training loop from scratch in TensorFlow","text":"Let’s consider simple MNIST model: Let’s train using mini-batch gradient custom training loop. First, ’re going need optimizer, loss function, dataset: Calling model inside GradientTape scope enables retrieve gradients trainable weights layer respect loss value. Using optimizer instance, can use gradients update variables (can retrieve using model.trainable_weights). ’s training loop, step step: open loop iterates epochs epoch, open loop iterates dataset, batches batch, open GradientTape() scope Inside scope, call model (forward pass) compute loss Outside scope, retrieve gradients weights model regard loss Finally, use optimizer update weights model based gradients","code":"get_model <- function() {   inputs <- layer_input(shape = 784, name=\"digits\")   outputs <- inputs %>%     layer_dense(units = 64, activation = \"relu\") %>%     layer_dense(units = 64, activation = \"relu\") %>%     layer_dense(units = 10, name = \"predictions\")   model <- keras_model(inputs=inputs, outputs=outputs) } # Instantiate an optimizer. optimizer <- optimizer_sgd(learning_rate=1e-3) # Instantiate a loss function. loss_fn <- loss_sparse_categorical_crossentropy(from_logits=TRUE)  # Prepare the training dataset. batch_size <- 64 c(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_mnist() x_train <- array_reshape(x_train, c(-1, 784)) x_test <- array_reshape(x_test, c(-1, 784))  # Reserve 10,000 samples for validation. x_val <- x_train[1:10000,] y_val <- y_train[1:10000] x_train = x_train[-c(1:10000),] y_train = y_train[-c(1:10000)]  # Prepare the training dataset. train_dataset <- list(x_train, y_train) %>%   tensor_slices_dataset() %>%   dataset_shuffle(buffer_size=1024) %>%   dataset_batch(batch_size)  # Prepare the validation dataset. val_dataset <- list(x_val, y_val) %>%   tensor_slices_dataset() %>%   dataset_batch(batch_size) epochs <- 2  for (epoch in seq_len(epochs)) {   cat(\"Start of epoch \", epoch, \"\\n\")    # Iterate over the batches of the dataset.   step <- 0   coro::loop(for(data in train_dataset) {     step <- step + 1     # Open a GradientTape to record the operations run     # during the forward pass, which enables auto-differentiation.     with(tf$GradientTape() %as% tape, {       # Run the forward pass of the layer.       # The operations that the layer applies       # to its inputs are going to be recorded       # on the GradientTape.       logits <- model(data[[1]], training = TRUE)        # Compute the loss value for this minibatch.       loss_value <- loss_fn(data[[2]], logits)     })      # Use the gradient tape to automatically retrieve     # the gradients of the trainable variables with respect to the loss.     gradients <- tape$gradient(loss_value, model$trainable_weights)      # Run one step of gradient descent by updating     # the value of the variables to minimize the loss.     optimizer$apply_gradients(zip_lists(gradients, model$trainable_weights))      # Log every 200 batches.     if (step %% 200 == 0) {       cat(sprintf(         \"Training loss (for one batch) at step %d: %.4f\\n\", step, loss_value       ))       cat(sprintf(\"Seen so far: %d samples \\n\", (step * batch_size)))     }   }) } ## Start of epoch  1 ## Error in model(data[[1]], training = TRUE): could not find function \"model\""},{"path":"https://keras.posit.co/articles/writing_a_custom_training_loop_in_tensorflow.html","id":"low-level-handling-of-metrics","dir":"Articles","previous_headings":"","what":"Low-level handling of metrics","title":"Writing a training loop from scratch in TensorFlow","text":"Let’s add metrics monitoring basic loop. can readily reuse built-metrics (custom ones wrote) training loops written scratch. ’s flow: Instantiate metric start loop Call metric$update_state() batch Call metric$result() need display current value metric Call metric$reset_state() need clear state metric (typically end epoch) Let’s use knowledge compute SparseCategoricalAccuracy training validation data end epoch: ’s training & evaluation loop:","code":"# Get a fresh model model <- get_model()  # Instantiate an optimizer to train the model. optimizer <- optimizer_sgd(learning_rate=1e-3) # Instantiate a loss function. loss_fn <- loss_sparse_categorical_crossentropy(from_logits=TRUE)  # Prepare the metrics. train_acc_metric <- metric_sparse_categorical_accuracy() val_acc_metric <- metric_sparse_categorical_accuracy() epochs <- 2 time <- Sys.time() for (epoch in seq_len(epochs)) {   cat(\"Start of epoch \", epoch, \"\\n\")    # Iterate over the batches of the dataset.   step <- 0   coro::loop(for(data in train_dataset) {     step <- step + 1     with(tf$GradientTape() %as% tape, {       logits <- model(data[[1]], training = TRUE)       loss_value <- loss_fn(data[[2]], logits)     })     gradients <- tape$gradient(loss_value, model$trainable_weights)     optimizer$apply_gradients(zip_lists(gradients, model$trainable_weights))      # Update training metric.     train_acc_metric$update_state(data[[2]], logits)      if (step %% 200 == 0) {       cat(sprintf(         \"Training loss (for one batch) at step %d: %.4f\\n\", step, loss_value       ))       cat(sprintf(\"Seen so far: %d samples \\n\", (step * batch_size)))     }   })    # Display metrics at the end of each epoch.   train_acc <- train_acc_metric$result()   cat(sprintf(\"Training acc over epoch: %.4f\\n\", train_acc))    # Reset training metrics at the end of each epoch   train_acc_metric$reset_state()    coro::loop(for(data in val_dataset) {     val_logits <- model(data[[1]], training=FALSE)     # Update val metrics     val_acc_metric$update_state(data[[2]], val_logits)   })    val_acc <- val_acc_metric$result()   val_acc_metric$reset_state()   cat(sprintf(\"Validation acc: %.4f\\n\", val_acc)) } ## Start of epoch  1 ## Training loss (for one batch) at step 200: 1.0514 ## Seen so far: 12800 samples ## Training loss (for one batch) at step 400: 1.0502 ## Seen so far: 25600 samples ## Training loss (for one batch) at step 600: 0.6835 ## Seen so far: 38400 samples ## Training acc over epoch: 0.6316 ## Validation acc: 0.7797 ## Start of epoch  2 ## Training loss (for one batch) at step 200: 0.6663 ## Seen so far: 12800 samples ## Training loss (for one batch) at step 400: 0.5877 ## Seen so far: 25600 samples ## Training loss (for one batch) at step 600: 0.6235 ## Seen so far: 38400 samples ## Training acc over epoch: 0.8011 ## Validation acc: 0.8365 Sys.time() - time ## Time difference of 15.73774 secs"},{"path":"https://keras.posit.co/articles/writing_a_custom_training_loop_in_tensorflow.html","id":"speeding-up-your-training-step-with-tf-function","dir":"Articles","previous_headings":"","what":"Speeding-up your training step with tf.function","title":"Writing a training loop from scratch in TensorFlow","text":"default runtime TensorFlow eager execution. , training loop executes eagerly. great debugging, graph compilation definite performance advantage. Describing computation static graph enables framework apply global performance optimizations. impossible framework constrained greedily execute one operation another, knowledge comes next. can compile static graph function takes tensors input. Just add @tf.function decorator , like : Let’s evaluation step: Now, let’s re-run training loop compiled training step: Much faster, isn’t ?","code":"train_step <- tf_function(function(x, y) {   with(tf$GradientTape() %as% tape, {     logits <- model(x, training = TRUE)     loss_value <- loss_fn(y, logits)   })   gradients <- tape$gradient(loss_value, model$trainable_weights)   optimizer$apply_gradients(zip_lists(gradients, model$trainable_weights))   train_acc_metric$update_state(y, logits) }) test_step <- tf_function(function(x, y) {   val_logits <- model(x, training=FALSE)   val_acc_metric$update_state(y, val_logits) }) epochs <- 2 time <- Sys.time() for (epoch in seq_len(epochs)) {   cat(\"Start of epoch \", epoch, \"\\n\")    # Iterate over the batches of the dataset.   step <- 0   coro::loop(for(data in train_dataset) {     step <- step + 1     train_step(data[[1]], data[[2]])      if (step %% 200 == 0) {       cat(sprintf(         \"Training loss (for one batch) at step %d: %.4f\\n\", step, loss_value       ))       cat(sprintf(\"Seen so far: %d samples \\n\", (step * batch_size)))     }   })    # Display metrics at the end of each epoch.   train_acc <- train_acc_metric$result()   cat(sprintf(\"Training acc over epoch: %.4f\\n\", train_acc))   train_acc_metric$reset_state()    coro::loop(for(data in val_dataset) {     test_step(data[[1]], data[[2]])   })    val_acc <- val_acc_metric$result()   val_acc_metric$reset_state()   cat(sprintf(\"Validation acc: %.4f\\n\", val_acc)) } ## Start of epoch  1 ## Training loss (for one batch) at step 200: 0.2641 ## Seen so far: 12800 samples ## Training loss (for one batch) at step 400: 0.2641 ## Seen so far: 25600 samples ## Training loss (for one batch) at step 600: 0.2641 ## Seen so far: 38400 samples ## Training acc over epoch: 0.8412 ## Validation acc: 0.8604 ## Start of epoch  2 ## Training loss (for one batch) at step 200: 0.2641 ## Seen so far: 12800 samples ## Training loss (for one batch) at step 400: 0.2641 ## Seen so far: 25600 samples ## Training loss (for one batch) at step 600: 0.2641 ## Seen so far: 38400 samples ## Training acc over epoch: 0.8622 ## Validation acc: 0.8715 Sys.time() - time ## Time difference of 1.89366 secs"},{"path":"https://keras.posit.co/articles/writing_a_custom_training_loop_in_tensorflow.html","id":"low-level-handling-of-losses-tracked-by-the-model","dir":"Articles","previous_headings":"","what":"Low-level handling of losses tracked by the model","title":"Writing a training loop from scratch in TensorFlow","text":"Layers & models recursively track losses created forward pass layers call self.add_loss(value). resulting list scalar loss values available via property model.losses end forward pass. want using loss components, sum add main loss training step. Consider layer, creates activity regularization loss: Let’s build really simple model uses : ’s training step look like now:","code":"ActivityRegularizationLayer <- new_layer_class(   \"ActivityRegularizationLayer\",   call = function(inputs) {     self$add_loss(0.1 * tf$reduce_mean(inputs))     inputs   } ) inputs <- layer_input(shape = 784, name=\"digits\") outputs <- inputs %>%   layer_dense(units = 64, activation = \"relu\") %>%   ActivityRegularizationLayer() %>%   layer_dense(units = 64, activation = \"relu\") %>%   layer_dense(units = 10, name = \"predictions\") model <- keras_model(inputs = inputs, outputs = outputs) train_step <- tf_function(function(x, y) {   with(tf$GradientTape() %as% tape, {     logits <- model(x, training = TRUE)     loss_value <- loss_fn(y, logits)     loss_value <- loss_value + Reduce(`+`, model$losses)   })   gradients <- tape$gradient(loss_value, model$trainable_weights)   optimizer$apply_gradients(zip_lists(gradients, model$trainable_weights))   train_acc_metric$update_state(y, logits) })"},{"path":"https://keras.posit.co/articles/writing_a_custom_training_loop_in_tensorflow.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Writing a training loop from scratch in TensorFlow","text":"Now know everything know using built-training loops writing scratch. conclude, ’s simple end--end example ties together everything ’ve learned guide: DCGAN trained MNIST digits.","code":""},{"path":"https://keras.posit.co/articles/writing_a_custom_training_loop_in_tensorflow.html","id":"end-to-end-example-a-gan-training-loop-from-scratch","dir":"Articles","previous_headings":"","what":"End-to-end example: a GAN training loop from scratch","title":"Writing a training loop from scratch in TensorFlow","text":"may familiar Generative Adversarial Networks (GANs). GANs can generate new images look almost real, learning latent distribution training dataset images (“latent space” images). GAN made two parts: “generator” model maps points latent space points image space, “discriminator” model, classifier can tell difference real images (training dataset) fake images (output generator network). GAN training loop looks like : Train discriminator. Sample batch random points latent space. Turn points fake images via “generator” model. Get batch real images combine generated images. Train “discriminator” model classify generated vs. real images. Train generator. Sample random points latent space. Turn points fake images via “generator” network. Get batch real images combine generated images. Train “generator” model “fool” discriminator classify fake images real. much detailed overview GANs works, see Deep Learning Python. Let’s implement training loop. First, create discriminator meant classify fake vs real digits: let’s create generator network, turns latent vectors outputs shape (28, 28, 1) (representing MNIST digits): ’s key bit: training loop. can see quite straightforward. training step function takes 17 lines. Let’s train GAN, repeatedly calling train_step batches images. Since discriminator generator convnets, ’re going want run code GPU. ’s ! ’ll get nice-looking fake MNIST digits just ~30s training Colab GPU.","code":"# Create the discriminator discriminator <- keras_model_sequential(name = \"discriminator\", input_shape = c(28, 28, 1)) %>%   layer_conv_2d(filters = 64, kernel_size = c(3, 3), strides = c(2, 2), padding = \"same\") %>%   layer_activation_leaky_relu(negative_slope = 0.2) %>%   layer_conv_2d(filters = 128, kernel_size = c(3, 3), strides = c(2, 2), padding = \"same\") %>%   layer_activation_leaky_relu(negative_slope = 0.2) %>%   layer_global_max_pooling_2d() %>%   layer_dense(units = 1) summary(discriminator) ## Model: \"discriminator\" ## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓ ## ┃ Layer (type)                      ┃ Output Shape                ┃    Param # ┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩ ## │ conv2d_1 (Conv2D)                 │ (None, 14, 14, 64)          │        640 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ leaky_re_lu_1 (LeakyReLU)         │ (None, 14, 14, 64)          │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d (Conv2D)                   │ (None, 7, 7, 128)           │     73,856 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ leaky_re_lu (LeakyReLU)           │ (None, 7, 7, 128)           │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ global_max_pooling2d              │ (None, 128)                 │          0 │ ## │ (GlobalMaxPooling2D)              │                             │            │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ dense_4 (Dense)                   │ (None, 1)                   │        129 │ ## └───────────────────────────────────┴─────────────────────────────┴────────────┘ ##  Total params: 74,625 (291.50 KB) ##  Trainable params: 74,625 (291.50 KB) ##  Non-trainable params: 0 (0.00 B) latent_dim <- 128L  generator <- keras_model_sequential(name = \"generator\", input_shape = latent_dim) %>%   layer_dense(7 * 7 * 128) %>%   layer_activation_leaky_relu(negative_slope = 0.2) %>%   layer_reshape(target_shape = c(7, 7, 128)) %>%   layer_conv_2d_transpose(filters = 128, kernel_size = c(4, 4), strides = c(2, 2), padding = \"same\") %>%   layer_activation_leaky_relu(negative_slope = 0.2) %>%   layer_conv_2d_transpose(filters = 128, kernel_size = c(4, 4), strides = c(2, 2), padding = \"same\") %>%   layer_activation_leaky_relu(negative_slope = 0.2) %>%   layer_conv_2d(filters = 1, kernel_size = c(7, 7), padding = \"same\", activation = \"sigmoid\") summary(generator) ## Model: \"generator\" ## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓ ## ┃ Layer (type)                      ┃ Output Shape                ┃    Param # ┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩ ## │ dense_5 (Dense)                   │ (None, 6272)                │    809,088 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ leaky_re_lu_4 (LeakyReLU)         │ (None, 6272)                │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ reshape (Reshape)                 │ (None, 7, 7, 128)           │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_transpose_1                │ (None, 14, 14, 128)         │    262,272 │ ## │ (Conv2DTranspose)                 │                             │            │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ leaky_re_lu_3 (LeakyReLU)         │ (None, 14, 14, 128)         │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_transpose                  │ (None, 28, 28, 128)         │    262,272 │ ## │ (Conv2DTranspose)                 │                             │            │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ leaky_re_lu_2 (LeakyReLU)         │ (None, 28, 28, 128)         │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_2 (Conv2D)                 │ (None, 28, 28, 1)           │      6,273 │ ## └───────────────────────────────────┴─────────────────────────────┴────────────┘ ##  Total params: 1,339,905 (5.11 MB) ##  Trainable params: 1,339,905 (5.11 MB) ##  Non-trainable params: 0 (0.00 B) # Instantiate one optimizer for the discriminator and another for the generator. d_optimizer <- optimizer_adam(learning_rate=0.0003) g_optimizer <- optimizer_adam(learning_rate=0.0004)  # Instantiate a loss function. loss_fn <- loss_binary_crossentropy(from_logits=TRUE)  train_step <- tf_function(function(real_images) {   # Sample random points in the latent space   batch_size <- tf$shape(real_images)[1]   random_latent_vectors <-     tf$random$normal(shape = c(batch_size, latent_dim))    # Decode them to fake images   generated_images <- generator(random_latent_vectors)    # Combine them with real images   combined_images <-     tf$concat(list(generated_images, real_images),               axis = 0L)    # Assemble labels discriminating real from fake images   labels <-     tf$concat(list(tf$ones(c(batch_size, 1L)),                    tf$zeros(c(batch_size, 1L))),               axis = 0L)    # Add random noise to the labels - important trick!   labels %<>% `+`(tf$random$uniform(tf$shape(.), maxval = 0.05))    # Train the discriminator   with(tf$GradientTape() %as% tape, {     predictions <- discriminator(combined_images)     d_loss <- loss_fn(labels, predictions)   })   grads <- tape$gradient(d_loss, discriminator$trainable_weights)   d_optimizer$apply_gradients(     zip_lists(grads, discriminator$trainable_weights))     # Sample random points in the latent space   random_latent_vectors <-     tf$random$normal(shape = c(batch_size, latent_dim))    # Assemble labels that say \"all real images\"   misleading_labels <- tf$zeros(c(batch_size, 1L))    # Train the generator (note that we should *not* update the weights   # of the discriminator)!   with(tf$GradientTape() %as% tape, {     predictions <- discriminator(generator(random_latent_vectors))     g_loss <- loss_fn(misleading_labels, predictions)   })    grads <- tape$gradient(g_loss, generator$trainable_weights)   g_optimizer$apply_gradients(     zip_lists(grads, generator$trainable_weights))    list(d_loss = d_loss, g_loss = g_loss, generated_images = generated_images) }) # Prepare the dataset. We use both the training & test MNIST digits. batch_size <- 64 c(c(x_train, .), c(x_test, .)) %<-% dataset_mnist() all_digits <- op_concatenate(list(x_train, x_test)) all_digits <- op_reshape(all_digits, c(-1, 28, 28, 1)) dataset <- all_digits %>%   tfdatasets::tensor_slices_dataset() %>%   tfdatasets::dataset_map(function(x) tf$cast(x, \"float\")/255) %>%   tfdatasets::dataset_shuffle(buffer_size = 1024) %>%   tfdatasets::dataset_batch(batch_size = batch_size)  epochs <- 1  # In practice you need at least 20 epochs to generate nice digits. save_dir <- \"./\"  for (epoch in seq_len(epochs)) {   cat(\"Start epoch: \", epoch, \"\\n\")   step <- 0   coro::loop(for(real_images in dataset) {     step <- step + 1     # Train the discriminator & generator on one batch of real images.     c(d_loss, g_loss, generated_images) %<-% train_step(real_images)      # Logging.     if (step %% 200 == 0) {       # Print metrics       cat(sprintf(\"discriminator loss at step %d: %.2f\\n\", step, d_loss))       cat(sprintf(\"adversarial loss at step %d: %.2f\\n\", step, g_loss))     }      # To limit execution time we stop after 10 steps.     # Remove the lines below to actually train the model!     if (step > 10){       break     }   }) } ## Start epoch:  1"},{"path":"https://keras.posit.co/articles/writing_a_custom_training_loop_in_torch.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Writing a training loop from scratch in PyTorch","text":"","code":"import os  # This guide can only be run with the torch backend. os.environ[\"KERAS_BACKEND\"] = \"torch\"  import torch import keras import numpy as np"},{"path":"https://keras.posit.co/articles/writing_a_custom_training_loop_in_torch.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Writing a training loop from scratch in PyTorch","text":"Keras provides default training evaluation loops, fit() evaluate(). usage covered guide Training & evaluation built-methods. want customize learning algorithm model still leveraging convenience fit() (instance, train GAN using fit()), can subclass Model class implement train_step() method, called repeatedly fit(). Now, want low-level control training & evaluation, write training & evaluation loops scratch. guide .","code":""},{"path":"https://keras.posit.co/articles/writing_a_custom_training_loop_in_torch.html","id":"a-first-end-to-end-example","dir":"Articles","previous_headings":"","what":"A first end-to-end example","title":"Writing a training loop from scratch in PyTorch","text":"write custom training loop, need following ingredients: model train, course. optimizer. either use keras.optimizers optimizer, native PyTorch optimizer torch.optim. loss function. either use keras.losses loss, native PyTorch loss torch.nn. dataset. use format: tf.data.Dataset, PyTorch DataLoader, Python generator, etc. Let’s line . ’ll use torch-native objects case – except, course, Keras model. First, let’s get model MNIST dataset: Next, ’s PyTorch optimizer PyTorch loss function: Let’s train model using mini-batch gradient custom training loop. Calling loss.backward() loss tensor triggers backpropagation. ’s done, optimizer magically aware gradients variable can update variables, done via optimizer.step(). Tensors, variables, optimizers interconnected one another via hidden global state. Also, don’t forget call model.zero_grad() loss.backward(), won’t get right gradients variables. ’s training loop, step step: open loop iterates epochs epoch, open loop iterates dataset, batches batch, call model input data retrive predictions, use compute loss value call loss.backward() Outside scope, retrieve gradients weights model regard loss Finally, use optimizer update weights model based gradients alternative, let’s look loop looks like using Keras optimizer Keras loss function. Important differences: retrieve gradients variables via v.value.grad, called trainable variable. update variables via optimizer.apply(), must called torch.no_grad() scope. Also, big gotcha: NumPy/TensorFlow/JAX/Keras APIs well Python unittest APIs use argument order convention fn(y_true, y_pred) (reference values first, predicted values second), PyTorch actually uses fn(y_pred, y_true) losses. make sure invert order logits targets.","code":"# Let's consider a simple MNIST model def get_model():     inputs = keras.Input(shape=(784,), name=\"digits\")     x1 = keras.layers.Dense(64, activation=\"relu\")(inputs)     x2 = keras.layers.Dense(64, activation=\"relu\")(x1)     outputs = keras.layers.Dense(10, name=\"predictions\")(x2)     model = keras.Model(inputs=inputs, outputs=outputs)     return model   # Create load up the MNIST dataset and put it in a torch DataLoader # Prepare the training dataset. batch_size = 32 (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data() x_train = np.reshape(x_train, (-1, 784)).astype(\"float32\") x_test = np.reshape(x_test, (-1, 784)).astype(\"float32\") y_train = keras.utils.to_categorical(y_train) y_test = keras.utils.to_categorical(y_test)  # Reserve 10,000 samples for validation. x_val = x_train[-10000:] y_val = y_train[-10000:] x_train = x_train[:-10000] y_train = y_train[:-10000]  # Create torch Datasets train_dataset = torch.utils.data.TensorDataset(     torch.from_numpy(x_train), torch.from_numpy(y_train) ) val_dataset = torch.utils.data.TensorDataset(     torch.from_numpy(x_val), torch.from_numpy(y_val) )  # Create DataLoaders for the Datasets train_dataloader = torch.utils.data.DataLoader(     train_dataset, batch_size=batch_size, shuffle=True ) val_dataloader = torch.utils.data.DataLoader(     val_dataset, batch_size=batch_size, shuffle=False ) # Instantiate a torch optimizer model = get_model() optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # Instantiate a torch loss function loss_fn = torch.nn.CrossEntropyLoss() epochs = 3 for epoch in range(epochs):     for step, (inputs, targets) in enumerate(train_dataloader):         # Forward pass         logits = model(inputs)         loss = loss_fn(logits, targets)          # Backward pass         model.zero_grad()         loss.backward()          # Optimizer variable updates         optimizer.step()          # Log every 100 batches.         if step % 100 == 0:             print(                 f\"Training loss (for 1 batch) at step {step}: {loss.detach().numpy():.4f}\"             )             print(f\"Seen so far: {(step + 1) * batch_size} samples\") model = get_model() optimizer = keras.optimizers.Adam(learning_rate=1e-3) loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)  for epoch in range(epochs):     print(f\"\\nStart of epoch {epoch}\")     for step, (inputs, targets) in enumerate(train_dataloader):         # Forward pass         logits = model(inputs)         loss = loss_fn(targets, logits)          # Backward pass         model.zero_grad()         trainable_weights = [v for v in model.trainable_weights]          # Call torch.Tensor.backward() on the loss to compute gradients         # for the weights.         loss.backward()         gradients = [v.value.grad for v in trainable_weights]          # Update weights         with torch.no_grad():             optimizer.apply(gradients, trainable_weights)          # Log every 100 batches.         if step % 100 == 0:             print(                 f\"Training loss (for 1 batch) at step {step}: {loss.detach().numpy():.4f}\"             )             print(f\"Seen so far: {(step + 1) * batch_size} samples\")"},{"path":"https://keras.posit.co/articles/writing_a_custom_training_loop_in_torch.html","id":"low-level-handling-of-metrics","dir":"Articles","previous_headings":"","what":"Low-level handling of metrics","title":"Writing a training loop from scratch in PyTorch","text":"Let’s add metrics monitoring basic training loop. can readily reuse built-Keras metrics (custom ones wrote) training loops written scratch. ’s flow: Instantiate metric start loop Call metric.update_state() batch Call metric.result() need display current value metric Call metric.reset_state() need clear state metric (typically end epoch) Let’s use knowledge compute CategoricalAccuracy training validation data end epoch: ’s training & evaluation loop:","code":"# Get a fresh model model = get_model()  # Instantiate an optimizer to train the model. optimizer = keras.optimizers.Adam(learning_rate=1e-3) # Instantiate a loss function. loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)  # Prepare the metrics. train_acc_metric = keras.metrics.CategoricalAccuracy() val_acc_metric = keras.metrics.CategoricalAccuracy() for epoch in range(epochs):     print(f\"\\nStart of epoch {epoch}\")     for step, (inputs, targets) in enumerate(train_dataloader):         # Forward pass         logits = model(inputs)         loss = loss_fn(targets, logits)          # Backward pass         model.zero_grad()         trainable_weights = [v for v in model.trainable_weights]          # Call torch.Tensor.backward() on the loss to compute gradients         # for the weights.         loss.backward()         gradients = [v.value.grad for v in trainable_weights]          # Update weights         with torch.no_grad():             optimizer.apply(gradients, trainable_weights)          # Update training metric.         train_acc_metric.update_state(targets, logits)          # Log every 100 batches.         if step % 100 == 0:             print(                 f\"Training loss (for 1 batch) at step {step}: {loss.detach().numpy():.4f}\"             )             print(f\"Seen so far: {(step + 1) * batch_size} samples\")      # Display metrics at the end of each epoch.     train_acc = train_acc_metric.result()     print(f\"Training acc over epoch: {float(train_acc):.4f}\")      # Reset training metrics at the end of each epoch     train_acc_metric.reset_state()      # Run a validation loop at the end of each epoch.     for x_batch_val, y_batch_val in val_dataloader:         val_logits = model(x_batch_val, training=False)         # Update val metrics         val_acc_metric.update_state(y_batch_val, val_logits)     val_acc = val_acc_metric.result()     val_acc_metric.reset_state()     print(f\"Validation acc: {float(val_acc):.4f}\")"},{"path":"https://keras.posit.co/articles/writing_a_custom_training_loop_in_torch.html","id":"low-level-handling-of-losses-tracked-by-the-model","dir":"Articles","previous_headings":"","what":"Low-level handling of losses tracked by the model","title":"Writing a training loop from scratch in PyTorch","text":"Layers & models recursively track losses created forward pass layers call self.add_loss(value). resulting list scalar loss values available via property model.losses end forward pass. want using loss components, sum add main loss training step. Consider layer, creates activity regularization loss: Let’s build really simple model uses : ’s training loop look like now: ’s !","code":"class ActivityRegularizationLayer(keras.layers.Layer):     def call(self, inputs):         self.add_loss(1e-2 * torch.sum(inputs))         return inputs inputs = keras.Input(shape=(784,), name=\"digits\") x = keras.layers.Dense(64, activation=\"relu\")(inputs) # Insert activity regularization as a layer x = ActivityRegularizationLayer()(x) x = keras.layers.Dense(64, activation=\"relu\")(x) outputs = keras.layers.Dense(10, name=\"predictions\")(x)  model = keras.Model(inputs=inputs, outputs=outputs) # Get a fresh model model = get_model()  # Instantiate an optimizer to train the model. optimizer = keras.optimizers.Adam(learning_rate=1e-3) # Instantiate a loss function. loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)  # Prepare the metrics. train_acc_metric = keras.metrics.CategoricalAccuracy() val_acc_metric = keras.metrics.CategoricalAccuracy()  for epoch in range(epochs):     print(f\"\\nStart of epoch {epoch}\")     for step, (inputs, targets) in enumerate(train_dataloader):         # Forward pass         logits = model(inputs)         loss = loss_fn(targets, logits)         if model.losses:             loss = loss + torch.sum(*model.losses)          # Backward pass         model.zero_grad()         trainable_weights = [v for v in model.trainable_weights]          # Call torch.Tensor.backward() on the loss to compute gradients         # for the weights.         loss.backward()         gradients = [v.value.grad for v in trainable_weights]          # Update weights         with torch.no_grad():             optimizer.apply(gradients, trainable_weights)          # Update training metric.         train_acc_metric.update_state(targets, logits)          # Log every 100 batches.         if step % 100 == 0:             print(                 f\"Training loss (for 1 batch) at step {step}: {loss.detach().numpy():.4f}\"             )             print(f\"Seen so far: {(step + 1) * batch_size} samples\")      # Display metrics at the end of each epoch.     train_acc = train_acc_metric.result()     print(f\"Training acc over epoch: {float(train_acc):.4f}\")      # Reset training metrics at the end of each epoch     train_acc_metric.reset_state()      # Run a validation loop at the end of each epoch.     for x_batch_val, y_batch_val in val_dataloader:         val_logits = model(x_batch_val, training=False)         # Update val metrics         val_acc_metric.update_state(y_batch_val, val_logits)     val_acc = val_acc_metric.result()     val_acc_metric.reset_state()     print(f\"Validation acc: {float(val_acc):.4f}\")"},{"path":"https://keras.posit.co/articles/writing_a_training_loop_from_scratch.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Writing a training loop from scratch","text":"","code":"library(keras3) library(tensorflow) ## ## Attaching package: 'tensorflow' ## The following objects are masked from 'package:keras': ## ##     set_random_seed, shape library(tfdatasets) ## ## Attaching package: 'tfdatasets' ## The following object is masked from 'package:keras': ## ##     shape"},{"path":"https://keras.posit.co/articles/writing_a_training_loop_from_scratch.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Writing a training loop from scratch","text":"Keras provides default training evaluation loops, fit() evaluate(). usage covered guide Training & evaluation built-methods. want customize learning algorithm model still leveraging convenience fit() (instance, train GAN using fit()), can subclass Model class implement train_step() method, called repeatedly fit(). covered guide Customizing happens fit(). Now, want low-level control training & evaluation, write training & evaluation loops scratch. guide .","code":""},{"path":"https://keras.posit.co/articles/writing_a_training_loop_from_scratch.html","id":"using-the-gradienttape-a-first-end-to-end-example","dir":"Articles","previous_headings":"","what":"Using the GradientTape: a first end-to-end example","title":"Writing a training loop from scratch","text":"Calling model inside GradientTape scope enables retrieve gradients trainable weights layer respect loss value. Using optimizer instance, can use gradients update variables (can retrieve using model.trainable_weights). Let’s consider simple MNIST model: Let’s train using mini-batch gradient custom training loop. First, ’re going need optimizer, loss function, dataset: ’s training loop: open loop iterates epochs epoch, open loop iterates dataset, batches batch, open GradientTape() scope Inside scope, call model (forward pass) compute loss Outside scope, retrieve gradients weights model regard loss Finally, use optimizer update weights model based gradients","code":"inputs <- layer_input(shape = 784, name=\"digits\") outputs <- inputs %>%   layer_dense(units = 64, activation = \"relu\") %>%   layer_dense(units = 64, activation = \"relu\") %>%   layer_dense(units = 10, name = \"predictions\") model <- keras_model(inputs=inputs, outputs=outputs) # Instantiate an optimizer. optimizer <- optimizer_sgd(learning_rate=1e-3) # Instantiate a loss function. loss_fn <- loss_sparse_categorical_crossentropy(from_logits=TRUE)  # Prepare the training dataset. batch_size <- 64 c(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_mnist() x_train <- array_reshape(x_train, c(-1, 784)) x_test <- array_reshape(x_test, c(-1, 784))  # Reserve 10,000 samples for validation. x_val <- x_train[1:10000,] y_val <- y_train[1:10000] x_train = x_train[-c(1:10000),] y_train = y_train[-c(1:10000)]  # Prepare the training dataset. train_dataset <- list(x_train, y_train) %>%   tensor_slices_dataset() %>%   dataset_shuffle(buffer_size=1024) %>%   dataset_batch(batch_size)  # Prepare the validation dataset. val_dataset <- list(x_val, y_val) %>%   tensor_slices_dataset() %>%   dataset_batch(batch_size) epochs <- 2  for (epoch in seq_len(epochs)) {   cat(\"Start of epoch \", epoch, \"\\n\")    # Iterate over the batches of the dataset.   step <- 0   coro::loop(for(data in train_dataset) {     step <- step + 1     # Open a GradientTape to record the operations run     # during the forward pass, which enables auto-differentiation.     with(tf$GradientTape() %as% tape, {       # Run the forward pass of the layer.       # The operations that the layer applies       # to its inputs are going to be recorded       # on the GradientTape.       logits <- model(data[[1]], training = TRUE)        # Compute the loss value for this minibatch.       loss_value <- loss_fn(data[[2]], logits)     })      # Use the gradient tape to automatically retrieve     # the gradients of the trainable variables with respect to the loss.     gradients <- tape$gradient(loss_value, model$trainable_weights)      # Run one step of gradient descent by updating     # the value of the variables to minimize the loss.     optimizer$apply_gradients(zip_lists(gradients, model$trainable_weights))      # Log every 200 batches.     if (step %% 200 == 0) {       cat(sprintf(         \"Training loss (for one batch) at step %d: %.4f\\n\", step, loss_value       ))       cat(sprintf(\"Seen so far: %d samples \\n\", (step * batch_size)))     }   }) } ## Start of epoch  1 ## Training loss (for one batch) at step 200: 1.6825 ## Seen so far: 12800 samples ## Training loss (for one batch) at step 400: 0.8280 ## Seen so far: 25600 samples ## Training loss (for one batch) at step 600: 0.6472 ## Seen so far: 38400 samples ## Start of epoch  2 ## Training loss (for one batch) at step 200: 0.7153 ## Seen so far: 12800 samples ## Training loss (for one batch) at step 400: 0.6430 ## Seen so far: 25600 samples ## Training loss (for one batch) at step 600: 0.4325 ## Seen so far: 38400 samples"},{"path":"https://keras.posit.co/articles/writing_a_training_loop_from_scratch.html","id":"low-level-handling-of-metrics","dir":"Articles","previous_headings":"","what":"Low-level handling of metrics","title":"Writing a training loop from scratch","text":"Let’s add metrics monitoring basic loop. can readily reuse built-metrics (custom ones wrote) training loops written scratch. ’s flow: Instantiate metric start loop Call metric$update_state() batch Call metric$result() need display current value metric Call metric$reset_states() need clear state metric (typically end epoch) Let’s use knowledge compute SparseCategoricalAccuracy validation data end epoch: ’s training & evaluation loop:","code":"# Get model inputs <- layer_input(shape = 784, name=\"digits\") outputs <- inputs %>%   layer_dense(units = 64, activation = \"relu\") %>%   layer_dense(units = 64, activation = \"relu\") %>%   layer_dense(units = 10, name = \"predictions\") model <- keras_model(inputs=inputs, outputs=outputs)  # Instantiate an optimizer to train the model. optimizer <- optimizer_sgd(learning_rate=1e-3) # Instantiate a loss function. loss_fn <- loss_sparse_categorical_crossentropy(from_logits=TRUE)  # Prepare the metrics. train_acc_metric <- metric_sparse_categorical_accuracy() val_acc_metric <- metric_sparse_categorical_accuracy() epochs <- 2 time <- Sys.time() for (epoch in seq_len(epochs)) {   cat(\"Start of epoch \", epoch, \"\\n\")    # Iterate over the batches of the dataset.   step <- 0   coro::loop(for(data in train_dataset) {     step <- step + 1     with(tf$GradientTape() %as% tape, {       logits <- model(data[[1]], training = TRUE)       loss_value <- loss_fn(data[[2]], logits)     })     gradients <- tape$gradient(loss_value, model$trainable_weights)     optimizer$apply_gradients(zip_lists(gradients, model$trainable_weights))      # Update training metric.     train_acc_metric$update_state(data[[2]], logits)      if (step %% 200 == 0) {       cat(sprintf(         \"Training loss (for one batch) at step %d: %.4f\\n\", step, loss_value       ))       cat(sprintf(\"Seen so far: %d samples \\n\", (step * batch_size)))     }   })    # Display metrics at the end of each epoch.   train_acc <- train_acc_metric$result()   cat(sprintf(\"Training acc over epoch: %.4f\\n\", train_acc))    # Reset training metrics at the end of each epoch   train_acc_metric$reset_state()    coro::loop(for(data in val_dataset) {     val_logits <- model(data[[1]], training=FALSE)     # Update val metrics     val_acc_metric$update_state(data[[2]], val_logits)   })    val_acc <- val_acc_metric$result()   val_acc_metric$reset_state()   cat(sprintf(\"Validation acc: %.4f\\n\", val_acc)) } ## Start of epoch  1 ## Training loss (for one batch) at step 200: 1.0199 ## Seen so far: 12800 samples ## Training loss (for one batch) at step 400: 0.7246 ## Seen so far: 25600 samples ## Training loss (for one batch) at step 600: 0.7681 ## Seen so far: 38400 samples ## Training acc over epoch: 0.6977 ## Validation acc: 0.7766 ## Start of epoch  2 ## Training loss (for one batch) at step 200: 0.7923 ## Seen so far: 12800 samples ## Training loss (for one batch) at step 400: 0.6582 ## Seen so far: 25600 samples ## Training loss (for one batch) at step 600: 0.5754 ## Seen so far: 38400 samples ## Training acc over epoch: 0.8043 ## Validation acc: 0.8180 Sys.time() - time ## Time difference of 13.14381 secs"},{"path":"https://keras.posit.co/articles/writing_a_training_loop_from_scratch.html","id":"speeding-up-your-training-step-with-tf_function","dir":"Articles","previous_headings":"","what":"Speeding-up your training step with tf_function","title":"Writing a training loop from scratch","text":"default runtime TensorFlow 2 eager execution. , training loop executes eagerly. great debugging, graph compilation definite performance advantage. Describing computation static graph enables framework apply global performance optimizations. impossible framework constrained greedily execute one operation another, knowledge comes next. can compile static graph function takes tensors input. Just add @tf.function decorator , like : Let’s evaluation step: Now, let’s re-run training loop compiled training step: Much faster, isn’t ?","code":"train_step <- tf_function(function(x, y) {   with(tf$GradientTape() %as% tape, {     logits <- model(x, training = TRUE)     loss_value <- loss_fn(y, logits)   })   gradients <- tape$gradient(loss_value, model$trainable_weights)   optimizer$apply_gradients(zip_lists(gradients, model$trainable_weights))   train_acc_metric$update_state(y, logits) }) test_step <- tf_function(function(x, y) {   val_logits <- model(x, training=FALSE)   val_acc_metric$update_state(y, val_logits) }) epochs <- 2 time <- Sys.time() for (epoch in seq_len(epochs)) {   cat(\"Start of epoch \", epoch, \"\\n\")    # Iterate over the batches of the dataset.   step <- 0   coro::loop(for(data in train_dataset) {     step <- step + 1     train_step(data[[1]], data[[2]])      if (step %% 200 == 0) {       cat(sprintf(         \"Training loss (for one batch) at step %d: %.4f\\n\", step, loss_value       ))       cat(sprintf(\"Seen so far: %d samples \\n\", (step * batch_size)))     }   })    # Display metrics at the end of each epoch.   train_acc <- train_acc_metric$result()   cat(sprintf(\"Training acc over epoch: %.4f\\n\", train_acc))   train_acc_metric$reset_state()    coro::loop(for(data in val_dataset) {     test_step(data[[1]], data[[2]])   })    val_acc <- val_acc_metric$result()   val_acc_metric$reset_state()   cat(sprintf(\"Validation acc: %.4f\\n\", val_acc)) } ## Start of epoch  1 ## Training loss (for one batch) at step 200: 0.9200 ## Seen so far: 12800 samples ## Training loss (for one batch) at step 400: 0.9200 ## Seen so far: 25600 samples ## Training loss (for one batch) at step 600: 0.9200 ## Seen so far: 38400 samples ## Training acc over epoch: 0.8450 ## Validation acc: 0.8472 ## Start of epoch  2 ## Training loss (for one batch) at step 200: 0.9200 ## Seen so far: 12800 samples ## Training loss (for one batch) at step 400: 0.9200 ## Seen so far: 25600 samples ## Training loss (for one batch) at step 600: 0.9200 ## Seen so far: 38400 samples ## Training acc over epoch: 0.8676 ## Validation acc: 0.8698 Sys.time() - time ## Time difference of 1.836063 secs"},{"path":"https://keras.posit.co/articles/writing_a_training_loop_from_scratch.html","id":"low-level-handling-of-losses-tracked-by-the-model","dir":"Articles","previous_headings":"","what":"Low-level handling of losses tracked by the model","title":"Writing a training loop from scratch","text":"Layers & models recursively track losses created forward pass layers call self.add_loss(value). resulting list scalar loss values available via property model.losses end forward pass. want using loss components, sum add main loss training step. Consider layer, creates activity regularization loss: Let’s build really simple model uses : ’s training step look like now:","code":"ActivityRegularizationLayer <- new_layer_class(   \"ActivityRegularizationLayer\",   call = function(inputs) {     self$add_loss(0.1 * tf$reduce_mean(inputs))     inputs   } ) inputs <- layer_input(shape = 784, name=\"digits\") outputs <- inputs %>%   layer_dense(units = 64, activation = \"relu\") %>%   ActivityRegularizationLayer() %>%   layer_dense(units = 64, activation = \"relu\") %>%   layer_dense(units = 10, name = \"predictions\") model <- keras_model(inputs = inputs, outputs = outputs) train_step <- tf_function(function(x, y) {   with(tf$GradientTape() %as% tape, {     logits <- model(x, training = TRUE)     loss_value <- loss_fn(y, logits)     loss_value <- loss_value + Reduce(`+`, model$losses)   })   gradients <- tape$gradient(loss_value, model$trainable_weights)   optimizer$apply_gradients(zip_lists(gradients, model$trainable_weights))   train_acc_metric$update_state(y, logits) })"},{"path":"https://keras.posit.co/articles/writing_a_training_loop_from_scratch.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Writing a training loop from scratch","text":"Now know everything know using built-training loops writing scratch. conclude, ’s simple end--end example ties together everything ’ve learned guide: DCGAN trained MNIST digits.","code":""},{"path":"https://keras.posit.co/articles/writing_a_training_loop_from_scratch.html","id":"end-to-end-example-a-gan-training-loop-from-scratch","dir":"Articles","previous_headings":"","what":"End-to-end example: a GAN training loop from scratch","title":"Writing a training loop from scratch","text":"may familiar Generative Adversarial Networks (GANs). GANs can generate new images look almost real, learning latent distribution training dataset images (“latent space” images). GAN made two parts: “generator” model maps points latent space points image space, “discriminator” model, classifier can tell difference real images (training dataset) fake images (output generator network). GAN training loop looks like : Train discriminator. Sample batch random points latent space. Turn points fake images via “generator” model. Get batch real images combine generated images. Train “discriminator” model classify generated vs. real images. Train generator. Sample random points latent space. Turn points fake images via “generator” network. Get batch real images combine generated images. Train “generator” model “fool” discriminator classify fake images real. much detailed overview GANs works, see Deep Learning Python. Let’s implement training loop. First, create discriminator meant classify fake vs real digits: let’s create generator network, turns latent vectors outputs shape (28, 28, 1) (representing MNIST digits): ’s key bit: training loop. can see quite straightforward. training step function takes 17 lines. Let’s train GAN, repeatedly calling train_step batches images. Since discriminator generator convnets, ’re going want run code GPU. ’s ! ’ll get nice-looking fake MNIST digits just ~30s training Colab GPU.","code":"# Create the discriminator discriminator <- keras_model_sequential(name = \"discriminator\", input_shape = c(28, 28, 1)) %>%   layer_conv_2d(filters = 64, kernel_size = c(3, 3), strides = c(2, 2), padding = \"same\") %>%   layer_activation_leaky_relu(negative_slope = 0.2) %>%   layer_conv_2d(filters = 128, kernel_size = c(3, 3), strides = c(2, 2), padding = \"same\") %>%   layer_activation_leaky_relu(negative_slope = 0.2) %>%   layer_global_max_pooling_2d() %>%   layer_dense(units = 1) summary(discriminator) ## Model: \"discriminator\" ## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓ ## ┃ Layer (type)                      ┃ Output Shape                ┃    Param # ┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩ ## │ conv2d_1 (Conv2D)                 │ (None, 14, 14, 64)          │        640 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ leaky_re_lu_1 (LeakyReLU)         │ (None, 14, 14, 64)          │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d (Conv2D)                   │ (None, 7, 7, 128)           │     73,856 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ leaky_re_lu (LeakyReLU)           │ (None, 7, 7, 128)           │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ global_max_pooling2d              │ (None, 128)                 │          0 │ ## │ (GlobalMaxPooling2D)              │                             │            │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ dense_6 (Dense)                   │ (None, 1)                   │        129 │ ## └───────────────────────────────────┴─────────────────────────────┴────────────┘ ##  Total params: 74,625 (291.50 KB) ##  Trainable params: 74,625 (291.50 KB) ##  Non-trainable params: 0 (0.00 B) latent_dim <- 128L  generator <- keras_model_sequential(name = \"generator\", input_shape = latent_dim) %>%   layer_dense(7 * 7 * 128) %>%   layer_activation_leaky_relu(negative_slope = 0.2) %>%   layer_reshape(target_shape = c(7, 7, 128)) %>%   layer_conv_2d_transpose(filters = 128, kernel_size = c(4, 4), strides = c(2, 2), padding = \"same\") %>%   layer_activation_leaky_relu(negative_slope = 0.2) %>%   layer_conv_2d_transpose(filters = 128, kernel_size = c(4, 4), strides = c(2, 2), padding = \"same\") %>%   layer_activation_leaky_relu(negative_slope = 0.2) %>%   layer_conv_2d(filters = 1, kernel_size = c(7, 7), padding = \"same\", activation = \"sigmoid\") summary(generator) ## Model: \"generator\" ## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓ ## ┃ Layer (type)                      ┃ Output Shape                ┃    Param # ┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩ ## │ dense_7 (Dense)                   │ (None, 6272)                │    809,088 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ leaky_re_lu_4 (LeakyReLU)         │ (None, 6272)                │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ reshape (Reshape)                 │ (None, 7, 7, 128)           │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_transpose_1                │ (None, 14, 14, 128)         │    262,272 │ ## │ (Conv2DTranspose)                 │                             │            │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ leaky_re_lu_3 (LeakyReLU)         │ (None, 14, 14, 128)         │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_transpose                  │ (None, 28, 28, 128)         │    262,272 │ ## │ (Conv2DTranspose)                 │                             │            │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ leaky_re_lu_2 (LeakyReLU)         │ (None, 28, 28, 128)         │          0 │ ## ├───────────────────────────────────┼─────────────────────────────┼────────────┤ ## │ conv2d_2 (Conv2D)                 │ (None, 28, 28, 1)           │      6,273 │ ## └───────────────────────────────────┴─────────────────────────────┴────────────┘ ##  Total params: 1,339,905 (5.11 MB) ##  Trainable params: 1,339,905 (5.11 MB) ##  Non-trainable params: 0 (0.00 B) # Instantiate one optimizer for the discriminator and another for the generator. d_optimizer <- optimizer_adam(learning_rate=0.0003) g_optimizer <- optimizer_adam(learning_rate=0.0004)  # Instantiate a loss function. loss_fn <- loss_binary_crossentropy(from_logits=TRUE)  train_step <- tf_function(function(real_images) {   # Sample random points in the latent space   batch_size <- tf$shape(real_images)[1]   random_latent_vectors <-     tf$random$normal(shape = c(batch_size, latent_dim))    # Decode them to fake images   generated_images <- generator(random_latent_vectors)    # Combine them with real images   combined_images <-     tf$concat(list(generated_images, real_images),               axis = 0L)    # Assemble labels discriminating real from fake images   labels <-     tf$concat(list(tf$ones(c(batch_size, 1L)),                    tf$zeros(c(batch_size, 1L))),               axis = 0L)    # Add random noise to the labels - important trick!   labels %<>% `+`(tf$random$uniform(tf$shape(.), maxval = 0.05))    # Train the discriminator   with(tf$GradientTape() %as% tape, {     predictions <- discriminator(combined_images)     d_loss <- loss_fn(labels, predictions)   })   grads <- tape$gradient(d_loss, discriminator$trainable_weights)   d_optimizer$apply_gradients(     zip_lists(grads, discriminator$trainable_weights))     # Sample random points in the latent space   random_latent_vectors <-     tf$random$normal(shape = c(batch_size, latent_dim))    # Assemble labels that say \"all real images\"   misleading_labels <- tf$zeros(c(batch_size, 1L))    # Train the generator (note that we should *not* update the weights   # of the discriminator)!   with(tf$GradientTape() %as% tape, {     predictions <- discriminator(generator(random_latent_vectors))     g_loss <- loss_fn(misleading_labels, predictions)   })    grads <- tape$gradient(g_loss, generator$trainable_weights)   g_optimizer$apply_gradients(     zip_lists(grads, generator$trainable_weights))    list(d_loss = d_loss, g_loss = g_loss, generated_images = generated_images) }) # Prepare the dataset. We use both the training & test MNIST digits. batch_size <- 64 c(c(x_train, .), c(x_test, .)) %<-% dataset_mnist() all_digits <- op_concatenate(list(x_train, x_test)) all_digits <- op_reshape(all_digits, c(-1, 28, 28, 1)) dataset <- all_digits %>%   tfdatasets::tensor_slices_dataset() %>%   tfdatasets::dataset_map(function(x) tf$cast(x, \"float\")/255) %>%   tfdatasets::dataset_shuffle(buffer_size = 1024) %>%   tfdatasets::dataset_batch(batch_size = batch_size)  epochs <- 1  # In practice you need at least 20 epochs to generate nice digits. save_dir <- \"./\"  for (epoch in seq_len(epochs)) {   cat(\"Start epoch: \", epoch, \"\\n\")   step <- 0   coro::loop(for(real_images in dataset) {     step <- step + 1     # Train the discriminator & generator on one batch of real images.     c(d_loss, g_loss, generated_images) %<-% train_step(real_images)      # Logging.     if (step %% 200 == 0) {       # Print metrics       cat(sprintf(\"discriminator loss at step %d: %.2f\\n\", step, d_loss))       cat(sprintf(\"adversarial loss at step %d: %.2f\\n\", step, g_loss))     }      # To limit execution time we stop after 10 steps.     # Remove the lines below to actually train the model!     if (step > 10){       break     }   }) } ## Start epoch:  1"},{"path":"https://keras.posit.co/articles/writing_your_own_callbacks.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Writing your own callbacks","text":"callback powerful tool customize behavior Keras model training, evaluation, inference. Examples include keras.callbacks.TensorBoard visualize training progress results TensorBoard, keras.callbacks.ModelCheckpoint periodically save model training. guide, learn Keras callback , can , can build . provide demos simple callback applications get started.","code":""},{"path":"https://keras.posit.co/articles/writing_your_own_callbacks.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Writing your own callbacks","text":"","code":"library(keras3) `add<-` <- `+` envir::import_from(dplyr, last)"},{"path":"https://keras.posit.co/articles/writing_your_own_callbacks.html","id":"keras-callbacks-overview","dir":"Articles","previous_headings":"","what":"Keras callbacks overview","title":"Writing your own callbacks","text":"callbacks subclass keras.callbacks.Callback class, override set methods called various stages training, testing, predicting. Callbacks useful get view internal states statistics model training. can pass list callbacks (keyword argument callbacks) following model methods: fit() evaluate() predict()","code":""},{"path":[]},{"path":[]},{"path":"https://keras.posit.co/articles/writing_your_own_callbacks.html","id":"on_traintestpredict_beginlogs-null","dir":"Articles","previous_headings":"An overview of callback methods > Global methods","what":"on_(train|test|predict)_begin(logs = NULL)","title":"Writing your own callbacks","text":"Called beginning fit/evaluate/predict.","code":""},{"path":"https://keras.posit.co/articles/writing_your_own_callbacks.html","id":"on_traintestpredict_endlogs-null","dir":"Articles","previous_headings":"An overview of callback methods > Global methods","what":"on_(train|test|predict)_end(logs = NULL)","title":"Writing your own callbacks","text":"Called end fit/evaluate/predict.","code":""},{"path":[]},{"path":"https://keras.posit.co/articles/writing_your_own_callbacks.html","id":"on_traintestpredict_batch_beginbatch-logs-null","dir":"Articles","previous_headings":"An overview of callback methods > Batch-level methods for training/testing/predicting","what":"on_(train|test|predict)_batch_begin(batch, logs = NULL)","title":"Writing your own callbacks","text":"Called right processing batch training/testing/predicting.","code":""},{"path":"https://keras.posit.co/articles/writing_your_own_callbacks.html","id":"on_traintestpredict_batch_endbatch-logs-null","dir":"Articles","previous_headings":"An overview of callback methods > Batch-level methods for training/testing/predicting","what":"on_(train|test|predict)_batch_end(batch, logs = NULL)","title":"Writing your own callbacks","text":"Called end training/testing/predicting batch. Within method, logs named list containing metrics results.","code":""},{"path":[]},{"path":"https://keras.posit.co/articles/writing_your_own_callbacks.html","id":"on_epoch_beginepoch-logs-null","dir":"Articles","previous_headings":"An overview of callback methods > Epoch-level methods (training only)","what":"on_epoch_begin(epoch, logs = NULL)","title":"Writing your own callbacks","text":"Called beginning epoch training.","code":""},{"path":"https://keras.posit.co/articles/writing_your_own_callbacks.html","id":"on_epoch_endepoch-logs-null","dir":"Articles","previous_headings":"An overview of callback methods > Epoch-level methods (training only)","what":"on_epoch_end(epoch, logs = NULL)","title":"Writing your own callbacks","text":"Called end epoch training.","code":""},{"path":"https://keras.posit.co/articles/writing_your_own_callbacks.html","id":"a-basic-example","dir":"Articles","previous_headings":"","what":"A basic example","title":"Writing your own callbacks","text":"Let’s take look concrete example. get started, let’s import tensorflow define simple Sequential Keras model: , load MNIST data training testing Keras datasets API: Now, define simple custom callback logs: fit/evaluate/predict starts & ends epoch starts & ends training batch starts & ends evaluation (test) batch starts & ends inference (prediction) batch starts & ends Let’s try :","code":"# Define the Keras model to add callbacks to get_model <- function() {   model <- keras_model_sequential()   model |> layer_dense(units = 1)   model |> compile(     optimizer = optimizer_rmsprop(learning_rate = 0.1),     loss = \"mean_squared_error\",     metrics = \"mean_absolute_error\"   )   model } # Load example MNIST data and pre-process it mnist <- dataset_mnist()  flatten_and_rescale <- function(x) {   x <- array_reshape(x, c(-1, 784))   x <- x / 255   x }  mnist$train$x <- flatten_and_rescale(mnist$train$x) mnist$test$x  <- flatten_and_rescale(mnist$test$x)  # limit to 1000 samples n <- 1000 mnist$train$x <- mnist$train$x[1:n,] mnist$train$y <- mnist$train$y[1:n] mnist$test$x  <- mnist$test$x[1:n,] mnist$test$y  <- mnist$test$y[1:n] show <- function(msg, logs) {   cat(glue::glue(msg, .envir = parent.frame()),       \"got logs: \", sep = \"; \")   str(logs); cat(\"\\n\") }  callback_custom <- new_callback_class(   \"CustomCallback\",   on_train_begin         = function(logs = NULL) show(\"Starting training\", logs),   on_epoch_begin         = function(epoch, logs = NULL) show(\"Start epoch {epoch} of training\", logs),   on_train_batch_begin   = function(batch, logs = NULL) show(\"...Training: start of batch {batch}\", logs),   on_train_batch_end     = function(batch, logs = NULL) show(\"...Training: end of batch {batch}\",  logs),   on_epoch_end           = function(epoch, logs = NULL) show(\"End epoch {epoch} of training\", logs),   on_train_end           = function(logs = NULL) show(\"Stop training\", logs),     on_test_begin          = function(logs = NULL) show(\"Start testing\", logs),   on_test_batch_begin    = function(batch, logs = NULL) show(\"...Evaluating: start of batch {batch}\", logs),   on_test_batch_end      = function(batch, logs = NULL) show(\"...Evaluating: end of batch {batch}\", logs),   on_test_end            = function(logs = NULL) show(\"Stop testing\", logs),    on_predict_begin       = function(logs = NULL) show(\"Start predicting\", logs),   on_predict_end         = function(logs = NULL) show(\"Stop predicting\", logs),   on_predict_batch_begin = function(batch, logs = NULL) show(\"...Predicting: start of batch {batch}\", logs),   on_predict_batch_end   = function(batch, logs = NULL) show(\"...Predicting: end of batch {batch}\", logs), ) model <- get_model() model |> fit(   mnist$train$x, mnist$train$y,   batch_size = 128,   epochs = 2,   verbose = 0,   validation_split = 0.5,   callbacks = list(callback_custom()) ) ## Starting training; got logs:  Named list() ## ## Start epoch 1 of training; got logs:  Named list() ## ## ...Training: start of batch 1; got logs:  Named list() ## ## ...Training: end of batch 1; got logs: List of 2 ##  $ loss               : num 25.9 ##  $ mean_absolute_error: num 4.19 ## ## ...Training: start of batch 2; got logs:  Named list() ## ## ...Training: end of batch 2; got logs: List of 2 ##  $ loss               : num 433 ##  $ mean_absolute_error: num 15.5 ## ## ...Training: start of batch 3; got logs:  Named list() ## ## ...Training: end of batch 3; got logs: List of 2 ##  $ loss               : num 297 ##  $ mean_absolute_error: num 11.8 ## ## ...Training: start of batch 4; got logs:  Named list() ## ## ...Training: end of batch 4; got logs: List of 2 ##  $ loss               : num 226 ##  $ mean_absolute_error: num 9.52 ## ## Start testing; got logs:  Named list() ## ## ...Evaluating: start of batch 1; got logs:  Named list() ## ## ...Evaluating: end of batch 1; got logs: List of 2 ##  $ loss               : num 8.1 ##  $ mean_absolute_error: num 2.3 ## ## ...Evaluating: start of batch 2; got logs:  Named list() ## ## ...Evaluating: end of batch 2; got logs: List of 2 ##  $ loss               : num 7.58 ##  $ mean_absolute_error: num 2.23 ## ## ...Evaluating: start of batch 3; got logs:  Named list() ## ## ...Evaluating: end of batch 3; got logs: List of 2 ##  $ loss               : num 7.38 ##  $ mean_absolute_error: num 2.21 ## ## ...Evaluating: start of batch 4; got logs:  Named list() ## ## ...Evaluating: end of batch 4; got logs: List of 2 ##  $ loss               : num 7.29 ##  $ mean_absolute_error: num 2.21 ## ## Stop testing; got logs: List of 2 ##  $ loss               : num 7.29 ##  $ mean_absolute_error: num 2.21 ## ## End epoch 1 of training; got logs: List of 4 ##  $ loss                   : num 226 ##  $ mean_absolute_error    : num 9.52 ##  $ val_loss               : num 7.29 ##  $ val_mean_absolute_error: num 2.21 ## ## Start epoch 2 of training; got logs:  Named list() ## ## ...Training: start of batch 1; got logs:  Named list() ## ## ...Training: end of batch 1; got logs: List of 2 ##  $ loss               : num 7.44 ##  $ mean_absolute_error: num 2.27 ## ## ...Training: start of batch 2; got logs:  Named list() ## ## ...Training: end of batch 2; got logs: List of 2 ##  $ loss               : num 6.81 ##  $ mean_absolute_error: num 2.16 ## ## ...Training: start of batch 3; got logs:  Named list() ## ## ...Training: end of batch 3; got logs: List of 2 ##  $ loss               : num 6.12 ##  $ mean_absolute_error: num 2.06 ## ## ...Training: start of batch 4; got logs:  Named list() ## ## ...Training: end of batch 4; got logs: List of 2 ##  $ loss               : num 6.07 ##  $ mean_absolute_error: num 2.03 ## ## Start testing; got logs:  Named list() ## ## ...Evaluating: start of batch 1; got logs:  Named list() ## ## ...Evaluating: end of batch 1; got logs: List of 2 ##  $ loss               : num 5.54 ##  $ mean_absolute_error: num 1.92 ## ## ...Evaluating: start of batch 2; got logs:  Named list() ## ## ...Evaluating: end of batch 2; got logs: List of 2 ##  $ loss               : num 5.31 ##  $ mean_absolute_error: num 1.87 ## ## ...Evaluating: start of batch 3; got logs:  Named list() ## ## ...Evaluating: end of batch 3; got logs: List of 2 ##  $ loss               : num 5.11 ##  $ mean_absolute_error: num 1.8 ## ## ...Evaluating: start of batch 4; got logs:  Named list() ## ## ...Evaluating: end of batch 4; got logs: List of 2 ##  $ loss               : num 5.15 ##  $ mean_absolute_error: num 1.82 ## ## Stop testing; got logs: List of 2 ##  $ loss               : num 5.15 ##  $ mean_absolute_error: num 1.82 ## ## End epoch 2 of training; got logs: List of 4 ##  $ loss                   : num 6.07 ##  $ mean_absolute_error    : num 2.03 ##  $ val_loss               : num 5.15 ##  $ val_mean_absolute_error: num 1.82 ## ## Stop training; got logs: List of 4 ##  $ loss                   : num 6.07 ##  $ mean_absolute_error    : num 2.03 ##  $ val_loss               : num 5.15 ##  $ val_mean_absolute_error: num 1.82 res <- model |> evaluate(   mnist$test$x, mnist$test$y,   batch_size = 128, verbose = 0,   callbacks = list(callback_custom()) ) ## Start testing; got logs:  Named list() ## ## ...Evaluating: start of batch 1; got logs:  Named list() ## ## ...Evaluating: end of batch 1; got logs: List of 2 ##  $ loss               : num 5.2 ##  $ mean_absolute_error: num 1.84 ## ## ...Evaluating: start of batch 2; got logs:  Named list() ## ## ...Evaluating: end of batch 2; got logs: List of 2 ##  $ loss               : num 4.62 ##  $ mean_absolute_error: num 1.73 ## ## ...Evaluating: start of batch 3; got logs:  Named list() ## ## ...Evaluating: end of batch 3; got logs: List of 2 ##  $ loss               : num 4.61 ##  $ mean_absolute_error: num 1.74 ## ## ...Evaluating: start of batch 4; got logs:  Named list() ## ## ...Evaluating: end of batch 4; got logs: List of 2 ##  $ loss               : num 4.65 ##  $ mean_absolute_error: num 1.75 ## ## ...Evaluating: start of batch 5; got logs:  Named list() ## ## ...Evaluating: end of batch 5; got logs: List of 2 ##  $ loss               : num 4.84 ##  $ mean_absolute_error: num 1.77 ## ## ...Evaluating: start of batch 6; got logs:  Named list() ## ## ...Evaluating: end of batch 6; got logs: List of 2 ##  $ loss               : num 4.76 ##  $ mean_absolute_error: num 1.76 ## ## ...Evaluating: start of batch 7; got logs:  Named list() ## ## ...Evaluating: end of batch 7; got logs: List of 2 ##  $ loss               : num 4.74 ##  $ mean_absolute_error: num 1.76 ## ## ...Evaluating: start of batch 8; got logs:  Named list() ## ## ...Evaluating: end of batch 8; got logs: List of 2 ##  $ loss               : num 4.66 ##  $ mean_absolute_error: num 1.74 ## ## Stop testing; got logs: List of 2 ##  $ loss               : num 4.66 ##  $ mean_absolute_error: num 1.74 res <- model |> predict(   mnist$test$x,   batch_size = 128, verbose = 0,   callbacks = list(callback_custom()) ) ## Start predicting; got logs:  Named list() ## ## ...Predicting: start of batch 1; got logs:  Named list() ## ## ...Predicting: end of batch 1; got logs: List of 1 ##  $ outputs:<tf.Tensor: shape=(128, 1), dtype=float32, numpy=…> ## ## ...Predicting: start of batch 2; got logs:  Named list() ## ## ...Predicting: end of batch 2; got logs: List of 1 ##  $ outputs:<tf.Tensor: shape=(128, 1), dtype=float32, numpy=…> ## ## ...Predicting: start of batch 3; got logs:  Named list() ## ## ...Predicting: end of batch 3; got logs: List of 1 ##  $ outputs:<tf.Tensor: shape=(128, 1), dtype=float32, numpy=…> ## ## ...Predicting: start of batch 4; got logs:  Named list() ## ## ...Predicting: end of batch 4; got logs: List of 1 ##  $ outputs:<tf.Tensor: shape=(128, 1), dtype=float32, numpy=…> ## ## ...Predicting: start of batch 5; got logs:  Named list() ## ## ...Predicting: end of batch 5; got logs: List of 1 ##  $ outputs:<tf.Tensor: shape=(128, 1), dtype=float32, numpy=…> ## ## ...Predicting: start of batch 6; got logs:  Named list() ## ## ...Predicting: end of batch 6; got logs: List of 1 ##  $ outputs:<tf.Tensor: shape=(128, 1), dtype=float32, numpy=…> ## ## ...Predicting: start of batch 7; got logs:  Named list() ## ## ...Predicting: end of batch 7; got logs: List of 1 ##  $ outputs:<tf.Tensor: shape=(128, 1), dtype=float32, numpy=…> ## ## ...Predicting: start of batch 8; got logs:  Named list() ## ## ...Predicting: end of batch 8; got logs: List of 1 ##  $ outputs:<tf.Tensor: shape=(104, 1), dtype=float32, numpy=…> ## ## Stop predicting; got logs:  Named list()"},{"path":"https://keras.posit.co/articles/writing_your_own_callbacks.html","id":"usage-of-logs-dict","dir":"Articles","previous_headings":"A basic example","what":"Usage of logs dict","title":"Writing your own callbacks","text":"logs named list contains loss value, metrics end batch epoch. Example includes loss mean absolute error. information callbacks, can check Keras callback API documentation.","code":"callback_print_loss_and_mae <- new_callback_class(   \"LossAndErrorPrintingCallback\",    on_train_batch_end = function(batch, logs = NULL)     cat(sprintf(\"Up to batch %i, the average loss is %7.2f.\\n\",                 batch,  logs$loss)),    on_test_batch_end = function(batch, logs = NULL)     cat(sprintf(\"Up to batch %i, the average loss is %7.2f.\\n\",                 batch, logs$loss)),    on_epoch_end = function(epoch, logs = NULL)     cat(sprintf(       \"The average loss for epoch %2i is %9.2f and mean absolute error is %7.2f.\\n\",       epoch, logs$loss, logs$mean_absolute_error     )) )   model <- get_model() model |> fit(   mnist$train$x, mnist$train$y,   epochs = 2, verbose = 0, batch_size = 128,   callbacks = list(callback_print_loss_and_mae()) ) ## Up to batch 1, the average loss is   25.12. ## Up to batch 2, the average loss is  398.92. ## Up to batch 3, the average loss is  274.04. ## Up to batch 4, the average loss is  208.32. ## Up to batch 5, the average loss is  168.15. ## Up to batch 6, the average loss is  141.31. ## Up to batch 7, the average loss is  122.19. ## Up to batch 8, the average loss is  107.60. ## The average loss for epoch  1 is    107.60 and mean absolute error is    5.70. ## Up to batch 1, the average loss is    4.71. ## Up to batch 2, the average loss is    4.74. ## Up to batch 3, the average loss is    4.81. ## Up to batch 4, the average loss is    5.07. ## Up to batch 5, the average loss is    5.08. ## Up to batch 6, the average loss is    5.09. ## Up to batch 7, the average loss is    5.19. ## Up to batch 8, the average loss is    5.57. ## The average loss for epoch  2 is      5.57 and mean absolute error is    1.91. res = model |> evaluate(   mnist$test$x, mnist$test$y,   verbose = 0, batch_size = 128,   callbacks = list(callback_print_loss_and_mae()) ) ## Up to batch 1, the average loss is   15.86. ## Up to batch 2, the average loss is   16.13. ## Up to batch 3, the average loss is   16.02. ## Up to batch 4, the average loss is   16.11. ## Up to batch 5, the average loss is   16.23. ## Up to batch 6, the average loss is   16.68. ## Up to batch 7, the average loss is   16.61. ## Up to batch 8, the average loss is   16.53."},{"path":"https://keras.posit.co/articles/writing_your_own_callbacks.html","id":"usage-of-selfmodel-attribute","dir":"Articles","previous_headings":"","what":"Usage of self$model attribute","title":"Writing your own callbacks","text":"addition receiving log information one methods called, callbacks access model associated current round training/evaluation/inference: self$model. things can self$model callback: Set self$model$stop_training <- TRUE immediately interrupt training. Mutate hyperparameters optimizer (available self$model$optimizer), self$model$optimizer$learning_rate. Save model period intervals. Record output model |> predict() test samples end epoch, use sanity check training. Extract visualizations intermediate features end epoch, monitor model learning time. etc. Let’s see action couple examples.","code":""},{"path":[]},{"path":"https://keras.posit.co/articles/writing_your_own_callbacks.html","id":"early-stopping-at-minimum-loss","dir":"Articles","previous_headings":"Examples of Keras callback applications","what":"Early stopping at minimum loss","title":"Writing your own callbacks","text":"first example shows creation Callback stops training minimum loss reached, setting attribute self$model$stop_training (boolean). Optionally, can provide argument patience specify many epochs wait stopping reached local minimum. callback_early_stopping() provides complete general implementation.","code":"callback_early_stopping_at_min_loss <- new_callback_class(   \"EarlyStoppingAtMinLoss\",   `__doc__` =     \"Stop training when the loss is at its min, i.e. the loss stops decreasing.      Arguments:         patience: Number of epochs to wait after min has been hit. After this         number of no improvement, training stops.     \",    initialize = function(patience = 0) {     super$initialize()     self$patience <- patience     # best_weights to store the weights at which the minimum loss occurs.     self$best_weights <- NULL   },    on_train_begin = function(logs = NULL) {     # The number of epoch it has waited when loss is no longer minimum.     self$wait <- 0     # The epoch the training stops at.     self$stopped_epoch <- 0     # Initialize the best as infinity.     self$best <- Inf   },    on_epoch_end = function(epoch, logs = NULL) {     current <- logs$loss     if (current < self$best) {       self$best <- current       self$wait <- 0       # Record the best weights if current results is better (less).       self$best_weights <- self$model$get_weights()     } else {       add(self$wait) <- 1       if (self$wait >= self$patience) {         self$stopped_epoch <- epoch         model <- self$model         model$stop_training <- TRUE         cat(\"Restoring model weights from the end of the best epoch.\\n\")         model$set_weights(self$best_weights)       }     }   },    on_train_end = function(logs = NULL)     if (self$stopped_epoch > 0)       cat(sprintf(\"Epoch %05d: early stopping\\n\", self$stopped_epoch + 1)) )   model <- get_model() model |> fit(   mnist$train$x,   mnist$train$y,   epochs = 30,   batch_size = 64,   verbose = 0,   callbacks = list(callback_print_loss_and_mae(),                    callback_early_stopping_at_min_loss()) ) ## Up to batch 1, the average loss is   30.54. ## Up to batch 2, the average loss is  513.27. ## Up to batch 3, the average loss is  352.60. ## Up to batch 4, the average loss is  266.37. ## Up to batch 5, the average loss is  214.68. ## Up to batch 6, the average loss is  179.97. ## Up to batch 7, the average loss is  155.06. ## Up to batch 8, the average loss is  136.59. ## Up to batch 9, the average loss is  121.96. ## Up to batch 10, the average loss is  110.28. ## Up to batch 11, the average loss is  100.72. ## Up to batch 12, the average loss is   92.71. ## Up to batch 13, the average loss is   85.95. ## Up to batch 14, the average loss is   80.21. ## Up to batch 15, the average loss is   75.17. ## Up to batch 16, the average loss is   70.97. ## The average loss for epoch  1 is     70.97 and mean absolute error is    4.04. ## Up to batch 1, the average loss is    7.98. ## Up to batch 2, the average loss is    9.92. ## Up to batch 3, the average loss is   12.88. ## Up to batch 4, the average loss is   16.61. ## Up to batch 5, the average loss is   20.49. ## Up to batch 6, the average loss is   26.14. ## Up to batch 7, the average loss is   30.44. ## Up to batch 8, the average loss is   33.76. ## Up to batch 9, the average loss is   36.32. ## Up to batch 10, the average loss is   35.26. ## Up to batch 11, the average loss is   34.22. ## Up to batch 12, the average loss is   33.53. ## Up to batch 13, the average loss is   32.84. ## Up to batch 14, the average loss is   31.80. ## Up to batch 15, the average loss is   31.39. ## Up to batch 16, the average loss is   31.49. ## The average loss for epoch  2 is     31.49 and mean absolute error is    4.83. ## Up to batch 1, the average loss is   39.60. ## Up to batch 2, the average loss is   41.95. ## Up to batch 3, the average loss is   41.29. ## Up to batch 4, the average loss is   36.77. ## Up to batch 5, the average loss is   32.08. ## Up to batch 6, the average loss is   28.17. ## Up to batch 7, the average loss is   25.33. ## Up to batch 8, the average loss is   23.56. ## Up to batch 9, the average loss is   22.28. ## Up to batch 10, the average loss is   21.22. ## Up to batch 11, the average loss is   20.87. ## Up to batch 12, the average loss is   22.25. ## Up to batch 13, the average loss is   25.08. ## Up to batch 14, the average loss is   27.87. ## Up to batch 15, the average loss is   31.72. ## Up to batch 16, the average loss is   34.05. ## The average loss for epoch  3 is     34.05 and mean absolute error is    4.86. ## Restoring model weights from the end of the best epoch. ## Epoch 00004: early stopping"},{"path":"https://keras.posit.co/articles/writing_your_own_callbacks.html","id":"learning-rate-scheduling","dir":"Articles","previous_headings":"Examples of Keras callback applications","what":"Learning rate scheduling","title":"Writing your own callbacks","text":"example, show custom Callback can used dynamically change learning rate optimizer course training. See keras$callbacks$LearningRateScheduler general implementations (RStudio, press F1 cursor LearningRateScheduler browser open page).","code":"callback_custom_learning_rate_scheduler <- new_callback_class(   \"CustomLearningRateScheduler\",   `__doc__` =   \"Learning rate scheduler which sets the learning rate according to schedule.      Arguments:         schedule: a function that takes an epoch index             (integer, indexed from 0) and current learning rate             as inputs and returns a new learning rate as output (float).     \",    initialize = function(schedule) {     super$initialize()     self$schedule <- schedule   },    on_epoch_begin = function(epoch, logs = NULL) {     ## When in doubt about what types of objects are in scope (e.g., self$model)     ## use a debugger to interact with the actual objects at the console!     # browser()      if (!\"learning_rate\" %in% names(self$model$optimizer))       stop('Optimizer must have a \"learning_rate\" attribute.')      # # Get the current learning rate from model's optimizer.     # use as.numeric() to convert the keras variablea to an R numeric     lr <- as.numeric(self$model$optimizer$learning_rate)     # # Call schedule function to get the scheduled learning rate.     scheduled_lr <- self$schedule(epoch, lr)     # # Set the value back to the optimizer before this epoch starts     optimizer <- self$model$optimizer     optimizer$learning_rate <- scheduled_lr     cat(sprintf(\"\\nEpoch %03d: Learning rate is %6.4f.\\n\", epoch, scheduled_lr))   } )  LR_SCHEDULE <- tibble::tribble(   ~start_epoch, ~learning_rate,              0,            0.1,              3,           0.05,              6,           0.01,              9,          0.005,             12,          0.001,   )  lr_schedule <- function(epoch, learning_rate) {   \"Helper function to retrieve the scheduled learning rate based on epoch.\"   with(LR_SCHEDULE, learning_rate[last(which(epoch >= start_epoch))]) }  model <- get_model() model |> fit(   mnist$train$x,   mnist$train$y,   batch_size = 64,   verbose = 0,   callbacks = list(     callback_print_loss_and_mae(),     callback_custom_learning_rate_scheduler(lr_schedule)   ) ) ## ## Epoch 001: Learning rate is 0.1000. ## Up to batch 1, the average loss is   29.36. ## Up to batch 2, the average loss is  513.95. ## Up to batch 3, the average loss is  352.70. ## Up to batch 4, the average loss is  266.46. ## Up to batch 5, the average loss is  214.73. ## Up to batch 6, the average loss is  180.00. ## Up to batch 7, the average loss is  155.05. ## Up to batch 8, the average loss is  136.64. ## Up to batch 9, the average loss is  121.97. ## Up to batch 10, the average loss is  110.30. ## Up to batch 11, the average loss is  100.76. ## Up to batch 12, the average loss is   92.74. ## Up to batch 13, the average loss is   85.95. ## Up to batch 14, the average loss is   80.18. ## Up to batch 15, the average loss is   75.11. ## Up to batch 16, the average loss is   70.84. ## The average loss for epoch  1 is     70.84 and mean absolute error is    4.00. ## ## Epoch 002: Learning rate is 0.1000. ## Up to batch 1, the average loss is    6.95. ## Up to batch 2, the average loss is    8.71. ## Up to batch 3, the average loss is   11.42. ## Up to batch 4, the average loss is   15.15. ## Up to batch 5, the average loss is   19.28. ## Up to batch 6, the average loss is   25.54. ## Up to batch 7, the average loss is   30.38. ## Up to batch 8, the average loss is   33.95. ## Up to batch 9, the average loss is   36.58. ## Up to batch 10, the average loss is   35.46. ## Up to batch 11, the average loss is   34.34. ## Up to batch 12, the average loss is   33.51. ## Up to batch 13, the average loss is   32.67. ## Up to batch 14, the average loss is   31.54. ## Up to batch 15, the average loss is   31.05. ## Up to batch 16, the average loss is   31.11. ## The average loss for epoch  2 is     31.11 and mean absolute error is    4.78. ## ## Epoch 003: Learning rate is 0.0500. ## Up to batch 1, the average loss is   40.40. ## Up to batch 2, the average loss is   22.33. ## Up to batch 3, the average loss is   16.18. ## Up to batch 4, the average loss is   13.09. ## Up to batch 5, the average loss is   11.48. ## Up to batch 6, the average loss is   10.21. ## Up to batch 7, the average loss is    9.22. ## Up to batch 8, the average loss is    8.70. ## Up to batch 9, the average loss is    8.16. ## Up to batch 10, the average loss is    7.80. ## Up to batch 11, the average loss is    7.50. ## Up to batch 12, the average loss is    7.17. ## Up to batch 13, the average loss is    6.89. ## Up to batch 14, the average loss is    6.70. ## Up to batch 15, the average loss is    6.52. ## Up to batch 16, the average loss is    6.56. ## The average loss for epoch  3 is      6.56 and mean absolute error is    1.93. ## ## Epoch 004: Learning rate is 0.0500. ## Up to batch 1, the average loss is    8.74. ## Up to batch 2, the average loss is    8.34. ## Up to batch 3, the average loss is    9.09. ## Up to batch 4, the average loss is    9.72. ## Up to batch 5, the average loss is   10.48. ## Up to batch 6, the average loss is   11.69. ## Up to batch 7, the average loss is   11.83. ## Up to batch 8, the average loss is   11.56. ## Up to batch 9, the average loss is   11.24. ## Up to batch 10, the average loss is   10.84. ## Up to batch 11, the average loss is   10.66. ## Up to batch 12, the average loss is   10.44. ## Up to batch 13, the average loss is   10.21. ## Up to batch 14, the average loss is   10.06. ## Up to batch 15, the average loss is   10.00. ## Up to batch 16, the average loss is   10.31. ## The average loss for epoch  4 is     10.31 and mean absolute error is    2.72. ## ## Epoch 005: Learning rate is 0.0500. ## Up to batch 1, the average loss is   17.26. ## Up to batch 2, the average loss is   14.09. ## Up to batch 3, the average loss is   12.67. ## Up to batch 4, the average loss is   11.44. ## Up to batch 5, the average loss is   10.54. ## Up to batch 6, the average loss is   10.10. ## Up to batch 7, the average loss is    9.53. ## Up to batch 8, the average loss is    9.17. ## Up to batch 9, the average loss is    8.78. ## Up to batch 10, the average loss is    8.49. ## Up to batch 11, the average loss is    8.50. ## Up to batch 12, the average loss is    8.59. ## Up to batch 13, the average loss is    8.68. ## Up to batch 14, the average loss is    8.86. ## Up to batch 15, the average loss is    9.17. ## Up to batch 16, the average loss is    9.74. ## The average loss for epoch  5 is      9.74 and mean absolute error is    2.61. ## ## Epoch 006: Learning rate is 0.0100. ## Up to batch 1, the average loss is   17.04. ## Up to batch 2, the average loss is   14.85. ## Up to batch 3, the average loss is   11.53. ## Up to batch 4, the average loss is    9.65. ## Up to batch 5, the average loss is    8.44. ## Up to batch 6, the average loss is    7.50. ## Up to batch 7, the average loss is    6.74. ## Up to batch 8, the average loss is    6.56. ## Up to batch 9, the average loss is    6.18. ## Up to batch 10, the average loss is    5.87. ## Up to batch 11, the average loss is    5.63. ## Up to batch 12, the average loss is    5.45. ## Up to batch 13, the average loss is    5.23. ## Up to batch 14, the average loss is    5.12. ## Up to batch 15, the average loss is    4.96. ## Up to batch 16, the average loss is    4.89. ## The average loss for epoch  6 is      4.89 and mean absolute error is    1.66. ## ## Epoch 007: Learning rate is 0.0100. ## Up to batch 1, the average loss is    3.65. ## Up to batch 2, the average loss is    3.04. ## Up to batch 3, the average loss is    2.88. ## Up to batch 4, the average loss is    2.85. ## Up to batch 5, the average loss is    2.88. ## Up to batch 6, the average loss is    2.81. ## Up to batch 7, the average loss is    2.70. ## Up to batch 8, the average loss is    2.96. ## Up to batch 9, the average loss is    2.96. ## Up to batch 10, the average loss is    2.93. ## Up to batch 11, the average loss is    2.95. ## Up to batch 12, the average loss is    2.98. ## Up to batch 13, the average loss is    2.97. ## Up to batch 14, the average loss is    3.01. ## Up to batch 15, the average loss is    3.00. ## Up to batch 16, the average loss is    3.08. ## The average loss for epoch  7 is      3.08 and mean absolute error is    1.34. ## ## Epoch 008: Learning rate is 0.0100. ## Up to batch 1, the average loss is    3.69. ## Up to batch 2, the average loss is    3.21. ## Up to batch 3, the average loss is    3.00. ## Up to batch 4, the average loss is    2.91. ## Up to batch 5, the average loss is    2.94. ## Up to batch 6, the average loss is    2.85. ## Up to batch 7, the average loss is    2.72. ## Up to batch 8, the average loss is    2.95. ## Up to batch 9, the average loss is    2.97. ## Up to batch 10, the average loss is    2.93. ## Up to batch 11, the average loss is    2.96. ## Up to batch 12, the average loss is    2.98. ## Up to batch 13, the average loss is    2.99. ## Up to batch 14, the average loss is    3.05. ## Up to batch 15, the average loss is    3.08. ## Up to batch 16, the average loss is    3.17. ## The average loss for epoch  8 is      3.17 and mean absolute error is    1.36. ## ## Epoch 009: Learning rate is 0.0050. ## Up to batch 1, the average loss is    3.71. ## Up to batch 2, the average loss is    2.93. ## Up to batch 3, the average loss is    2.76. ## Up to batch 4, the average loss is    2.70. ## Up to batch 5, the average loss is    2.76. ## Up to batch 6, the average loss is    2.69. ## Up to batch 7, the average loss is    2.57. ## Up to batch 8, the average loss is    2.79. ## Up to batch 9, the average loss is    2.80. ## Up to batch 10, the average loss is    2.77. ## Up to batch 11, the average loss is    2.79. ## Up to batch 12, the average loss is    2.80. ## Up to batch 13, the average loss is    2.78. ## Up to batch 14, the average loss is    2.81. ## Up to batch 15, the average loss is    2.80. ## Up to batch 16, the average loss is    2.84. ## The average loss for epoch  9 is      2.84 and mean absolute error is    1.28. ## ## Epoch 010: Learning rate is 0.0050. ## Up to batch 1, the average loss is    3.02. ## Up to batch 2, the average loss is    2.69. ## Up to batch 3, the average loss is    2.58. ## Up to batch 4, the average loss is    2.57. ## Up to batch 5, the average loss is    2.65. ## Up to batch 6, the average loss is    2.60. ## Up to batch 7, the average loss is    2.48. ## Up to batch 8, the average loss is    2.72. ## Up to batch 9, the average loss is    2.74. ## Up to batch 10, the average loss is    2.71. ## Up to batch 11, the average loss is    2.74. ## Up to batch 12, the average loss is    2.75. ## Up to batch 13, the average loss is    2.74. ## Up to batch 14, the average loss is    2.77. ## Up to batch 15, the average loss is    2.77. ## Up to batch 16, the average loss is    2.82. ## The average loss for epoch 10 is      2.82 and mean absolute error is    1.28."},{"path":"https://keras.posit.co/articles/writing_your_own_callbacks.html","id":"built-in-keras-callbacks","dir":"Articles","previous_headings":"Examples of Keras callback applications","what":"Built-in Keras callbacks","title":"Writing your own callbacks","text":"sure check existing Keras callbacks reading API docs. Applications include logging CSV, saving model, visualizing metrics TensorBoard, lot !","code":""},{"path":"https://keras.posit.co/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Tomasz Kalinowski. Author, copyright holder, maintainer. Daniel Falbel. Contributor, copyright holder. JJ Allaire. Author, copyright holder. François Chollet. Author, copyright holder. Posit Software, PBC. Copyright holder, funder. Google. Copyright holder, funder. Yuan Tang. Contributor, copyright holder. Wouter Van Der Bijl. Contributor, copyright holder. Martin Studer. Contributor, copyright holder. Sigrid Keydana. Contributor.","code":""},{"path":"https://keras.posit.co/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Kalinowski T, Allaire J, Chollet F (2024). keras3: R Interface 'Keras'. R package version 2.13.0.9000, https:/keras.posit.co/.","code":"@Manual{,   title = {keras3: R Interface to 'Keras'},   author = {Tomasz Kalinowski and JJ Allaire and François Chollet},   year = {2024},   note = {R package version 2.13.0.9000},   url = {https:/keras.posit.co/}, }"},{"path":"https://keras.posit.co/index.html","id":"r-interface-to-keras","dir":"","previous_headings":"","what":"R Interface to Keras","title":"R Interface to Keras","text":"Keras high-level neural networks API developed focus enabling fast experimentation. able go idea result least possible delay key good research. Keras following key features: Allows code run CPU GPU, seamlessly. User-friendly API makes easy quickly prototype deep learning models. Built-support convolutional networks (computer vision), recurrent networks (sequence processing), combination . Supports arbitrary network architectures: multi-input multi-output models, layer sharing, model sharing, etc. means Keras appropriate building essentially deep learning model, memory network neural Turing machine. See package website https://keras.posit.co complete documentation.","code":""},{"path":"https://keras.posit.co/reference/Callback.html","id":null,"dir":"Reference","previous_headings":"","what":"Define a custom Callback — Callback","title":"Define a custom Callback — Callback","text":"Callbacks can passed keras methods fit(), evaluate(), predict() order hook various stages model training, evaluation, inference lifecycle. create custom callback, call Callback() override method associated stage interest.","code":""},{"path":"https://keras.posit.co/reference/Callback.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define a custom Callback — Callback","text":"","code":"Callback(   classname,   on_epoch_begin = NULL,   on_epoch_end = NULL,   on_train_begin = NULL,   on_train_end = NULL,   on_train_batch_begin = NULL,   on_train_batch_end = NULL,   on_test_begin = NULL,   on_test_end = NULL,   on_test_batch_begin = NULL,   on_test_batch_end = NULL,   on_predict_begin = NULL,   on_predict_end = NULL,   on_predict_batch_begin = NULL,   on_predict_batch_end = NULL,   ...,   public = list(),   private = list(),   inherit = keras$callbacks$Callback,   parent_env = parent.frame() )  new_callback_class(   classname,   on_epoch_begin = NULL,   on_epoch_end = NULL,   on_train_begin = NULL,   on_train_end = NULL,   on_train_batch_begin = NULL,   on_train_batch_end = NULL,   on_test_begin = NULL,   on_test_end = NULL,   on_test_batch_begin = NULL,   on_test_batch_end = NULL,   on_predict_begin = NULL,   on_predict_end = NULL,   on_predict_batch_begin = NULL,   on_predict_batch_end = NULL,   ...,   public = list(),   private = list(),   inherit = keras$callbacks$Callback,   parent_env = parent.frame() )"},{"path":"https://keras.posit.co/reference/Callback.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define a custom Callback — Callback","text":"classname name callback class. CamelCase convention. on_epoch_begin   Called start epoch. Subclasses override actions run. function called TRAIN mode. Args: epoch: Integer, index epoch. logs: Named List. Currently data passed argument method may change future. on_epoch_end   Called end epoch. Subclasses override actions run. function called TRAIN mode. Args: epoch: Integer, index epoch. logs: Named List, metric results training epoch, validation epoch validation performed. Validation result keys prefixed val_. training epoch, values Model's metrics returned. Example: list(loss = 0.2, accuracy = 0.7). on_train_begin   Called beginning training. Subclasses override actions run. Args: logs: Named list. Currently data passed argument method may change future. on_train_end   Called end training. Subclasses override actions run. Args: logs: Named list. Currently output last call on_epoch_end() passed argument method may change future. on_train_batch_begin   Called beginning training batch fit() methods. Subclasses override actions run. Note steps_per_execution argument compile Model set N, method called every N batches. Args: batch: Integer, index batch within current epoch. logs: Named list. Currently data passed argument method may change future. on_train_batch_end   Called end training batch fit() methods. Subclasses override actions run. Note steps_per_execution argument compile Model set N, method called every N batches. Args: batch: Integer, index batch within current epoch. logs: Named list. Aggregated metric results batch. on_test_begin   Called beginning evaluation validation. Subclasses override actions run. Args: logs: Named list. Currently data passed argument method may change future. on_test_end   Called end evaluation validation. Subclasses override actions run. Args: logs: Named list. Currently output last call on_test_batch_end() passed argument method may change future. on_test_batch_begin   Called beginning batch evaluate() methods. Also called beginning validation batch fit() methods, validation data provided. Subclasses override actions run. Note steps_per_execution argument compile() Model set N, method called every N batches. Args: batch: Integer, index batch within current epoch. logs: Named list. Currently data passed argument method may change future. on_test_batch_end   Called end batch evaluate() methods. Also called end validation batch fit() methods, validation data provided. Subclasses override actions run. Note steps_per_execution argument compile() Model set N, method called every N batches. Args: batch: Integer, index batch within current epoch. logs: Named list. Aggregated metric results batch. on_predict_begin   Called beginning prediction. Subclasses override actions run. Args: logs: Named list. Currently data passed argument method may change future. on_predict_end   Called end prediction. Subclasses override actions run. Args: logs: Named list. Currently data passed argument method may change future. on_predict_batch_begin   Called beginning batch predict() methods. Subclasses override actions run. Note steps_per_execution argument compile() Model set N, method called every N batches. Args: batch: Integer, index batch within current epoch. logs: Named list. Currently data passed argument method may change future. on_predict_batch_end   Called end batch predict() methods. Subclasses override actions run. Note steps_per_execution argument compile Model set N, method called every N batches. Args: batch: Integer, index batch within current epoch. logs: Named list. Aggregated metric results batch. ..., public Additional methods public members custom class. private Named list R objects (typically, functions) include instance private environments. private symbol scope class methods, resolving R environment populated list provided. instance private environment. methods (functions) private scope self __class__ symbols. objects private invisible Keras framework Python runtime. inherit Callback class inherit . default, base Callback. parent_env environment class methods grandparent.","code":"\\(epoch, logs = NULL) \\(epoch, logs = NULL) \\(logs = NULL) \\(logs = NULL) \\(batch, logs = NULL) \\(batch, logs=NULL) \\(logs = NULL) \\(logs = NULL) \\(batch, logs = NULL) \\(batch, logs = NULL) \\(logs = NULL) \\(logs = NULL) \\(batch, logs = NULL) \\(batch, logs = NULL)"},{"path":"https://keras.posit.co/reference/Callback.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Define a custom Callback — Callback","text":"want use Callback objects custom training loop: pack callbacks single keras$callbacks$CallbackList can called together. need manually call on_* methods appropriate locations loop. Like : Example:","code":"CallbackList <- function(...)   reticulate::import(\"keras\")$callbacks$CallbackList(list(...)) enumerate <- reticulate::import_builtins()$enumerate callbacks <- CallbackList(callback1(), callback2(), ...) callbacks$append(callback3()) callbacks$on_train_begin(...) for (epoch in seq(0, len = EPOCHS)) {   callbacks$on_epoch_begin(epoch)   ds_iterator <- as_iterator(enumerate(dataset))   while (!is.null(c(i, batch) %<-% iter_next(ds_iterator))) {     callbacks$on_train_batch_begin(i)     batch_logs <- model$train_step(batch)     callbacks$on_train_batch_end(i, batch_logs)   }   epoch_logs <- ...   callbacks$on_epoch_end(epoch, epoch_logs) } final_logs <- ... callbacks$on_train_end(final_logs)"},{"path":"https://keras.posit.co/reference/Callback.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Define a custom Callback — Callback","text":"R function custom methods (public private) following symbols scope: self: Layer instance. super: Layer superclass. private: R environment specific class instance. objects defined invisible Keras framework. __class__ current class type object. also available alias symbol, value supplied Layer(classname = )","code":"training_finished <- FALSE callback_mark_finished <- Callback(\"MarkFinished\",   on_train_end = function(logs = NULL) {     training_finished <<- TRUE   } )  model <- keras_model_sequential(input_shape = c(1)) |>   layer_dense(1) model |> compile(loss = 'mean_squared_error') model |> fit(op_ones(c(1, 1)), op_ones(c(1, 1)),              callbacks = callback_mark_finished()) stopifnot(isTRUE(training_finished))"},{"path":"https://keras.posit.co/reference/Callback.html","id":"attributes-accessible-via-self-","dir":"Reference","previous_headings":"","what":"Attributes (accessible via self$)","title":"Define a custom Callback — Callback","text":"params: Named list, Training parameters (e.g. verbosity, batch size, number epochs, ...). model: Instance Model. Reference model trained. logs named list callback methods take argument contain keys quantities relevant current batch epoch (see method-specific docstrings).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/KerasCallback.html","id":null,"dir":"Reference","previous_headings":"","what":"(Deprecated) Base R6 class for Keras callbacks — KerasCallback","title":"(Deprecated) Base R6 class for Keras callbacks — KerasCallback","text":"New custom callbacks implemented R6 classes encouraged inherit keras$callbacks$Callback directly.","code":""},{"path":"https://keras.posit.co/reference/KerasCallback.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"(Deprecated) Base R6 class for Keras callbacks — KerasCallback","text":"R6Class generator object","code":""},{"path":"https://keras.posit.co/reference/KerasCallback.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"(Deprecated) Base R6 class for Keras callbacks — KerasCallback","text":"KerasCallback.","code":""},{"path":"https://keras.posit.co/reference/KerasCallback.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"(Deprecated) Base R6 class for Keras callbacks — KerasCallback","text":"logs named list callback methods take argument contain keys quantities relevant current batch epoch. Currently, fit.keras.models.model.Model() method sequential models include following quantities logs passes callbacks: on_epoch_end: logs include acc loss, optionally include val_loss (validation enabled fit), val_acc (validation accuracy monitoring enabled). on_batch_begin: logs include size, number samples current batch. on_batch_end: logs include loss, optionally acc (accuracy monitoring enabled).","code":""},{"path":"https://keras.posit.co/reference/KerasCallback.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"(Deprecated) Base R6 class for Keras callbacks — KerasCallback","text":"params Named list training parameters (eg. verbosity, batch size, number epochs...). model Reference Keras model trained.","code":""},{"path":"https://keras.posit.co/reference/KerasCallback.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"(Deprecated) Base R6 class for Keras callbacks — KerasCallback","text":"on_epoch_begin(epoch, logs) Called beginning epoch. on_epoch_end(epoch, logs) Called end epoch. on_batch_begin(batch, logs) Called beginning batch. on_batch_end(batch, logs) Called end batch. on_train_begin(logs) Called beginning training. on_train_end(logs) Called end training.","code":""},{"path":"https://keras.posit.co/reference/KerasCallback.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"(Deprecated) Base R6 class for Keras callbacks — KerasCallback","text":"","code":"if (FALSE) { library(keras3)  LossHistory <- R6::R6Class(\"LossHistory\",   inherit = KerasCallback,    public = list(      losses = NULL,      on_batch_end = function(batch, logs = list()) {       self$losses <- c(self$losses, logs[[\"loss\"]])     }   ) ) }"},{"path":"https://keras.posit.co/reference/KerasConstraint.html","id":null,"dir":"Reference","previous_headings":"","what":"(Deprecated) Base R6 class for Keras constraints — KerasConstraint","title":"(Deprecated) Base R6 class for Keras constraints — KerasConstraint","text":"New custom constraints encouraged subclass keras$constraints$Constraint directly.","code":""},{"path":"https://keras.posit.co/reference/KerasConstraint.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"(Deprecated) Base R6 class for Keras constraints — KerasConstraint","text":"R6Class generator object","code":""},{"path":"https://keras.posit.co/reference/KerasConstraint.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"(Deprecated) Base R6 class for Keras constraints — KerasConstraint","text":"can implement custom constraint either creating R function accepts weights (w) parameter, creating R6 class derives KerasConstraint implements call method.","code":""},{"path":"https://keras.posit.co/reference/KerasConstraint.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"(Deprecated) Base R6 class for Keras constraints — KerasConstraint","text":"Models use custom constraints serialized using save_model(). Rather, weights model saved restored using save_model_weights().","code":""},{"path":"https://keras.posit.co/reference/KerasConstraint.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"(Deprecated) Base R6 class for Keras constraints — KerasConstraint","text":"call(w) Constrain specified weights.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/KerasConstraint.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"(Deprecated) Base R6 class for Keras constraints — KerasConstraint","text":"","code":"if (FALSE) { CustomNonNegConstraint <- R6::R6Class(   \"CustomNonNegConstraint\",   inherit = KerasConstraint,   public = list(     call = function(x) {        w * op_cast(w >= 0, config_floatx())     }   ) )  layer_dense(units = 32, input_shape = c(784),             kernel_constraint = CustomNonNegConstraint$new()) }"},{"path":"https://keras.posit.co/reference/KerasLayer.html","id":null,"dir":"Reference","previous_headings":"","what":"(Deprecated) Base R6 class for Keras layers — KerasLayer","title":"(Deprecated) Base R6 class for Keras layers — KerasLayer","text":"Custom R6 layers can now inherit directly keras$layers$Layer layers.","code":""},{"path":"https://keras.posit.co/reference/KerasLayer.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"(Deprecated) Base R6 class for Keras layers — KerasLayer","text":"R6Class generator object","code":""},{"path":"https://keras.posit.co/reference/KerasLayer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"(Deprecated) Base R6 class for Keras layers — KerasLayer","text":"KerasLayer.","code":""},{"path":"https://keras.posit.co/reference/KerasLayer.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"(Deprecated) Base R6 class for Keras layers — KerasLayer","text":"build(input_shape) Creates layer weights (must implemented layers weights) call(inputs,mask) Call layer input tensor. compute_output_shape(input_shape) Compute output shape layer. add_loss(losses, inputs) Add losses layer. add_weight(name,shape,dtype,initializer,regularizer,trainable,constraint) Adds weight variable layer.","code":""},{"path":"https://keras.posit.co/reference/Layer.html","id":null,"dir":"Reference","previous_headings":"","what":"Define a custom Layer. — Layer","title":"Define a custom Layer. — Layer","text":"layer callable object takes input one tensors outputs one tensors. involves computation, defined call() method, state (weight variables). State can created: initialize(), instance via self$add_weight(); optional build() method, invoked first call() layer, supplies shape(s) input(s), may known initialization time. Layers recursively composable: assign Layer instance attribute another Layer, outer layer start tracking weights created inner layer. Nested layers instantiated initialize() method build() method. Users just instantiate layer treat callable. R function custom methods (public private) following symbols scope: self: Layer instance. super: Layer superclass. private: R environment specific class instance. objects defined invisible Keras framework. __class__ current class type object. also available alias symbol, value supplied Layer(classname = )","code":""},{"path":"https://keras.posit.co/reference/Layer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define a custom Layer. — Layer","text":"","code":"Layer(   classname,   initialize = NULL,   call = NULL,   build = NULL,   get_config = NULL,   ...,   public = list(),   private = list(),   inherit = keras$layers$Layer,   parent_env = parent.frame() )  new_layer_class(   classname,   initialize = NULL,   call = NULL,   build = NULL,   get_config = NULL,   ...,   public = list(),   private = list(),   inherit = keras$layers$Layer,   parent_env = parent.frame() )"},{"path":"https://keras.posit.co/reference/Layer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define a custom Layer. — Layer","text":"classname String, name custom class. (Conventionally, CamelCase). initialize, call, build, get_config Recommended methods implement. See description section. ..., public Additional methods public members custom class. Recommended methods initialize, call can optionally provided elements public, take precedence value supplied argument. private Named list R objects (typically, functions) include instance private environments. private symbol scope class methods, resolving R environment populated list provided. instance private environment. methods (functions) private scope self __class__ symbols. objects private invisible Keras framework Python runtime. inherit new layer subclass. default, base keras Layer class. parent_env R environment class methods grandparent.","code":""},{"path":"https://keras.posit.co/reference/Layer.html","id":"attributes","dir":"Reference","previous_headings":"","what":"Attributes","title":"Define a custom Layer. — Layer","text":"name: name layer (string). dtype: Dtype layer's weights. Alias layer$variable_dtype. variable_dtype: Dtype layer's weights. compute_dtype: dtype layer's computations. Layers automatically cast inputs dtype, causes computations output also dtype. mixed precision used keras$mixed_precision$DTypePolicy, different variable_dtype. trainable_weights: List variables included backprop. non_trainable_weights: List variables included backprop. weights: concatenation lists trainable_weights non_trainable_weights (order). trainable: Whether layer trained (boolean), .e. whether potentially-trainable weights returned part layer$trainable_weights. input_spec: Optional (list ) InputSpec object(s) specifying constraints inputs can accepted layer. recommend custom Layers implement following methods: initialize(): Defines custom layer attributes, creates layer weights depend input shapes, using add_weight(), state. build(input_shape): method can used create weights depend shape(s) input(s), using add_weight(), state. Calling call() automatically build layer (built yet) calling build(). call(...): Method called making sure build() called. call() performs logic applying layer input arguments. Two reserved arguments can optionally use call() : training (boolean, whether call inference mode training mode). mask (boolean tensor encoding masked timesteps input, used e.g. RNN layers). typical signature method call(inputs), user optionally add training mask layer need . get_config(): Returns named list containing configuration used initialize layer. list names differ arguments initialize(), override from_config() well. method used saving layer model contains layer.","code":""},{"path":"https://keras.posit.co/reference/Layer.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Define a custom Layer. — Layer","text":"basic example: layer two variables, w b, returns y <- (w %*% x) + b. shows implement build() call(). Variables set attributes layer tracked weights layers (layer$weights).   Besides trainable weights, updated via backpropagation training, layers can also non-trainable weights. weights meant updated manually call(). example layer computes running sum inputs:","code":"layer_simple_dense <- Layer(   \"SimpleDense\",   initialize = function(units = 32) {     super$initialize()     self$units <- units   },    # Create the state of the layer (weights)   build = function(input_shape) {     self$kernel <- self$add_weight(       shape = shape(tail(input_shape, 1), self$units),       initializer = \"glorot_uniform\",       trainable = TRUE,       name = \"kernel\"     )     self$bias = self$add_weight(       shape = shape(self$units),       initializer = \"zeros\",       trainable = TRUE,       name = \"bias\"     )   },    # Defines the computation   call = function(self, inputs) {     op_matmul(inputs, self$kernel) + self$bias   } )  # Instantiates the layer. # Supply missing `object` arg to skip invoking `call()` and instead return # the Layer instance linear_layer <- layer_simple_dense(, 4)  # This will call `build(input_shape)` and create the weights, # and then invoke `call()`. y <- linear_layer(op_ones(c(2, 2))) stopifnot(length(linear_layer$weights) == 2)  # These weights are trainable, so they're listed in `trainable_weights`: stopifnot(length(linear_layer$trainable_weights) == 2) layer_compute_sum <- Layer(\"ComputeSum\",     initialize = function(input_dim){        super$initialize()       # Create a non-trainable weight.       self$total = self$add_weight(         shape=shape(),         initializer=\"zeros\",         trainable=FALSE,         name=\"total\"       )    },     call = function(inputs){      # TODO: where is assign() documented?       self$total$assign(self$total + op_sum(inputs))       self$total    } )  my_sum <- layer_compute_sum(,2) x <- op_ones(c(2, 2)) y <- my_sum(x)  stopifnot(exprs = {   all.equal(my_sum$weights,               list(my_sum$total))   all.equal(my_sum$non_trainable_weights, list(my_sum$total))   all.equal(my_sum$trainable_weights,     list()) })"},{"path":"https://keras.posit.co/reference/Layer.html","id":"methods-available","dir":"Reference","previous_headings":"","what":"Methods available","title":"Define a custom Layer. — Layer","text":"Can called inside call() method add scalar loss. Example:         Alias add_weight().    Add weight variable layer. Args: shape: shape variable (defined keras3::shape()) Must fully-defined (NA/NULL entries). Defaults () (scalar) unspecified. initializer: Initializer object use populate initial variable value, string name built-initializer (e.g. \"random_normal\"). unspecified, defaults \"glorot_uniform\" floating-point variables \"zeros\" types (e.g. int, bool). dtype: Dtype variable create, e.g. \"float32\". unspecified, defaults layer's variable dtype (defaults \"float32\" unspecified). trainable: Boolean, whether variable trainable via backprop whether updates managed manually. constraint: Constraint object call variable optimizer update, string name built-constraint. name: String name variable. Useful debugging purposes.       Builds layer's states supplied config (named list args). default, method calls .call(build, config$input_shape) method, creates weights based layer's input shape supplied config. config contains information needed load layer's state, override method. Args: config: Named list containing input shape associated layer.    See description             Count total number scalars composing weights. Returns: integer count.      Returns named list layer's input shape. method returns config (named list) can used build_from_config(config) create states (e.g. Variables Lookup tables) needed layer. default, config contains input shape layer built . writing custom layer creates state unusual way, override method make sure state already created Keras attempts load value upon model loading. Returns: named list containing input shape associated layer.    Returns config object. object config named list (serializable) containing information needed re-instantiate . config expected serializable JSON, expected consist (potentially complex, nested) structure names lists consisting simple objects like strings, ints.    Return values layer$weights list R NumPy arrays.    Loads state layer. can override method take full control state layer loaded upon calling load_model(). Args: store: Named list state model loaded.    Saves state layer. can override method take full control state layer saved upon calling save_model(). Args: store: Named list state model saved.    Sets values weights list R NumPy arrays.    Call layer without side effects. Args: trainable_variables: List trainable variables model. non_trainable_variables: List non-trainable variables model. ...: Positional named arguments passed call(). return_losses: TRUE, stateless_call() return list losses created call() part return values. Returns: unnamed list. default, returns list(outputs, non_trainable_variables). return_losses = TRUE, returns list(outputs, non_trainable_variables, losses). Note: non_trainable_variables include non-trainable weights BatchNormalization statistics, also RNG seed state (random operations part layer, dropout), Metric state (metrics attached layer). elements state layer. Example:         Creates layer config. class method, meaning, R function self symbol (class instance) scope. Use __class__ classname symbol provided Layer() constructed) resolve class definition. default implementation :   method reverse get_config(), capable instantiating layer config named list. handle layer connectivity (handled Network), weights (handled set_weights()). Args: config: named list, typically output get_config(). Returns: layer instance.   Initialize self. method typically called custom initialize() method. Example:","code":"add_loss(loss) Layer(\"MyLayer\",   ...   call = function(x) {     self$add_loss(op_sum(x))     x   } add_metric() add_variable(...) add_weight(shape = NULL,            initializer = NULL,            dtype = NULL,            trainable = TRUE,            regularizer = NULL,            constraint = NULL,            name = NULL) build(input_shape) build_from_config(config) call(...) compute_mask(inputs, previous_mask) compute_output_shape(...) compute_output_spec(...) count_params()  get_build_config() get_config() get_weights() load_own_variables(store) save_own_variables(store) set_weights(weights) stateless_call(trainable_variables, non_trainable_variables,                ..., return_losses = FALSE) model <- ... data <- ... trainable_variables <- model$trainable_variables non_trainable_variables <- model$non_trainable_variables # Call the model with zero side effects c(outputs, non_trainable_variables) %<-% model$stateless_call(     trainable_variables,     non_trainable_variables,     data ) # Attach the updated state to the model # (until you do this, the model is still in its pre-call state). purrr::walk2(   model$non_trainable_variables, non_trainable_variables,   \\(variable, value) variable$assign(value)) symbolic_call(...) from_config(config) from_config = function(config) {   do.call(`__class__`, config) } initialize(...,            activity_regularizer = NULL,            trainable = TRUE,            dtype = NULL,            autocast = TRUE,            name = NULL) layer_my_layer <- Layer(\"MyLayer\",   initialize = function(units, ..., dtype = NULL, name = NULL) {     super$initialize(..., dtype = dtype, name = name)     # .... finish initializing `self` instance   } )"},{"path":"https://keras.posit.co/reference/Layer.html","id":"readonly-properties-","dir":"Reference","previous_headings":"","what":"Readonly properties:","title":"Define a custom Layer. — Layer","text":"compute_dtype dtype computations performed layer. dtype Alias layer$variable_dtype. input_dtype dtype layer inputs converted . losses List scalar losses add_loss(), regularizers sublayers. metrics_variables List metric variables. non_trainable_variables List non-trainable layer state. extends layer$non_trainable_weights include state used layer including state metrics SeedGenerators. non_trainable_weights List non-trainable weight variables layer. weights updated optimizer training. Unlike, layer$non_trainable_variables excludes metric state random seeds. trainable_variables List trainable layer state. equivalent layer$trainable_weights. trainable_weights List trainable weight variables layer. weights get updated optimizer training. variable_dtype dtype state (weights) layer. variables List layer state, including random seeds. extends layer$weights include state used layer including SeedGenerators. Note metrics variables included , use metrics_variables visit metric variables. weights List weight variables layer. Unlike, layer$variables excludes metric state random seeds. input Retrieves input tensor(s) symbolic operation. returns tensor(s) corresponding first time operation called. Returns: Input tensor list input tensors. output Retrieves output tensor(s) layer. returns tensor(s) corresponding first time operation called. Returns: Output tensor list output tensors.","code":""},{"path":"https://keras.posit.co/reference/Layer.html","id":"data-descriptors-attributes-","dir":"Reference","previous_headings":"","what":"Data descriptors (Attributes):","title":"Define a custom Layer. — Layer","text":"input_spec supports_masking Whether layer supports computing mask using compute_mask. trainable Settable boolean, whether layer trainable .","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/Metric.html","id":null,"dir":"Reference","previous_headings":"","what":"Metric — Metric","title":"Metric — Metric","text":"Metric object encapsulates metric logic state can used track model performance training. returned family metric functions start prefix metric_*.","code":""},{"path":"https://keras.posit.co/reference/Metric.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Metric — Metric","text":"name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/Metric.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Metric — Metric","text":"(subclassed) Metric instance can passed directly compile(metrics = ), used standalone object. See ?Metric example usage.","code":""},{"path":"https://keras.posit.co/reference/Metric.html","id":"usage-with-compile","dir":"Reference","previous_headings":"","what":"Usage with compile","title":"Metric — Metric","text":"","code":"model %>% compile(   optimizer = 'sgd',   loss = 'mse',   metrics = list(metric_SOME_METRIC(), metric_SOME_OTHER_METRIC()) )"},{"path":"https://keras.posit.co/reference/Metric.html","id":"standalone-usage","dir":"Reference","previous_headings":"","what":"Standalone usage","title":"Metric — Metric","text":"","code":"m <- metric_SOME_METRIC() for (e in seq(epochs)) {   for (i in seq(train_steps)) {     c(y_true, y_pred, sample_weight = NULL) %<-% ...     m$update_state(y_true, y_pred, sample_weight)   }   cat('Final epoch result: ', as.numeric(m$result()), \"\\n\")   m$reset_state() }"},{"path":"https://keras.posit.co/reference/Metric.html","id":"custom-metric-subclass-","dir":"Reference","previous_headings":"","what":"Custom Metric (subclass)","title":"Metric — Metric","text":"implemented subclasses: initialize(): state variables created method calling self$add_weight() like:   update_state(): updates state variables like:   result(): Computes returns value metric state variables. Example custom metric subclass:   metric_binary_true_positives built %py_class% like :","code":"self$var <- self$add_weight(...) self$var$assign_add(...) metric_binary_true_positives <- new_metric_class(   classname = \"BinaryTruePositives\",   initialize = function(name = 'binary_true_positives', ...) {     super$initialize(name = name, ...)     self$true_positives <-       self$add_weight(name = 'tp', initializer = 'zeros')   },    update_state = function(y_true, y_pred, sample_weight = NULL) {     y_true <- k_cast(y_true, \"bool\")     y_pred <- k_cast(y_pred, \"bool\")      values <- y_true & y_pred     values <- k_cast(values, self$dtype)     if (!is.null(sample_weight)) {       sample_weight <- k_cast(sample_weight, self$dtype)       sample_weight <- tf$broadcast_to(sample_weight, values$shape)       values <- values * sample_weight     }     self$true_positives$assign_add(tf$reduce_sum(values))   },    result = function()     self$true_positives ) model %>% compile(..., metrics = list(metric_binary_true_positives())) metric_binary_true_positives(keras$metrics$Metric) %py_class% {   initialize <- <same-as-above>,   update_state <- <same-as-above>,   result <- <same-as-above> }"},{"path":"https://keras.posit.co/reference/activation_elu.html","id":null,"dir":"Reference","previous_headings":"","what":"Exponential Linear Unit. — activation_elu","title":"Exponential Linear Unit. — activation_elu","text":"exponential linear unit (ELU) alpha > 0 defined : x x > 0 alpha * exp(x) - 1 x < 0 ELUs negative values pushes mean activations closer zero. Mean activations closer zero enable faster learning bring gradient closer natural gradient. ELUs saturate negative value argument gets smaller. Saturation means small derivative decreases variation information propagated next layer.","code":""},{"path":"https://keras.posit.co/reference/activation_elu.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Exponential Linear Unit. — activation_elu","text":"","code":"activation_elu(x, alpha = 1)"},{"path":"https://keras.posit.co/reference/activation_elu.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Exponential Linear Unit. — activation_elu","text":"x Input tensor. alpha Numeric. See description details.","code":""},{"path":"https://keras.posit.co/reference/activation_elu.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Exponential Linear Unit. — activation_elu","text":"Clevert et al., 2016","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/activation_exponential.html","id":null,"dir":"Reference","previous_headings":"","what":"Exponential activation function. — activation_exponential","title":"Exponential activation function. — activation_exponential","text":"Exponential activation function.","code":""},{"path":"https://keras.posit.co/reference/activation_exponential.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Exponential activation function. — activation_exponential","text":"","code":"activation_exponential(x)"},{"path":"https://keras.posit.co/reference/activation_exponential.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Exponential activation function. — activation_exponential","text":"x Input tensor.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/activation_gelu.html","id":null,"dir":"Reference","previous_headings":"","what":"Gaussian error linear unit (GELU) activation function. — activation_gelu","title":"Gaussian error linear unit (GELU) activation function. — activation_gelu","text":"Gaussian error linear unit (GELU) defined : gelu(x) = x * P(X <= x) P(X) ~ N(0, 1), .e. gelu(x) = 0.5 * x * (1 + erf(x / sqrt(2))). GELU weights inputs value, rather gating inputs sign ReLU.","code":""},{"path":"https://keras.posit.co/reference/activation_gelu.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gaussian error linear unit (GELU) activation function. — activation_gelu","text":"","code":"activation_gelu(x, approximate = FALSE)"},{"path":"https://keras.posit.co/reference/activation_gelu.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gaussian error linear unit (GELU) activation function. — activation_gelu","text":"x Input tensor. approximate bool, whether enable approximation.","code":""},{"path":"https://keras.posit.co/reference/activation_gelu.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Gaussian error linear unit (GELU) activation function. — activation_gelu","text":"Hendrycks et al., 2016","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/activation_hard_sigmoid.html","id":null,"dir":"Reference","previous_headings":"","what":"Hard sigmoid activation function. — activation_hard_sigmoid","title":"Hard sigmoid activation function. — activation_hard_sigmoid","text":"hard sigmoid activation defined : 0 x < -2.5 1 x > 2.5 0.2 * x + 0.5 -2.5 <= x <= 2.5 faster, piecewise linear approximation sigmoid activation.","code":""},{"path":"https://keras.posit.co/reference/activation_hard_sigmoid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hard sigmoid activation function. — activation_hard_sigmoid","text":"","code":"activation_hard_sigmoid(x)"},{"path":"https://keras.posit.co/reference/activation_hard_sigmoid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hard sigmoid activation function. — activation_hard_sigmoid","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/activation_hard_sigmoid.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Hard sigmoid activation function. — activation_hard_sigmoid","text":"Wikipedia \"Hard sigmoid\"","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/activation_leaky_relu.html","id":null,"dir":"Reference","previous_headings":"","what":"Leaky relu activation function. — activation_leaky_relu","title":"Leaky relu activation function. — activation_leaky_relu","text":"Leaky relu activation function.","code":""},{"path":"https://keras.posit.co/reference/activation_leaky_relu.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Leaky relu activation function. — activation_leaky_relu","text":"","code":"activation_leaky_relu(x, negative_slope = 0.2)"},{"path":"https://keras.posit.co/reference/activation_leaky_relu.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Leaky relu activation function. — activation_leaky_relu","text":"x Input tensor. negative_slope float controls slope values lower threshold.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/activation_linear.html","id":null,"dir":"Reference","previous_headings":"","what":"Linear activation function (pass-through). — activation_linear","title":"Linear activation function (pass-through). — activation_linear","text":"\"linear\" activation identity function: returns input, unmodified.","code":""},{"path":"https://keras.posit.co/reference/activation_linear.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Linear activation function (pass-through). — activation_linear","text":"","code":"activation_linear(x)"},{"path":"https://keras.posit.co/reference/activation_linear.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Linear activation function (pass-through). — activation_linear","text":"x Input tensor.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/activation_log_softmax.html","id":null,"dir":"Reference","previous_headings":"","what":"Log-Softmax activation function. — activation_log_softmax","title":"Log-Softmax activation function. — activation_log_softmax","text":"input vector handled independently. axis argument sets axis input function applied along.","code":""},{"path":"https://keras.posit.co/reference/activation_log_softmax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Log-Softmax activation function. — activation_log_softmax","text":"","code":"activation_log_softmax(x, axis = -1L)"},{"path":"https://keras.posit.co/reference/activation_log_softmax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Log-Softmax activation function. — activation_log_softmax","text":"x Input tensor. axis Integer, axis along softmax applied.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/activation_mish.html","id":null,"dir":"Reference","previous_headings":"","what":"Mish activation function. — activation_mish","title":"Mish activation function. — activation_mish","text":"defined : mish(x) = x * tanh(softplus(x)) softplus defined : softplus(x) = log(exp(x) + 1)","code":""},{"path":"https://keras.posit.co/reference/activation_mish.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mish activation function. — activation_mish","text":"","code":"activation_mish(x)"},{"path":"https://keras.posit.co/reference/activation_mish.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Mish activation function. — activation_mish","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/activation_mish.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Mish activation function. — activation_mish","text":"Misra, 2019","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/activation_relu.html","id":null,"dir":"Reference","previous_headings":"","what":"Applies the rectified linear unit activation function. — activation_relu","title":"Applies the rectified linear unit activation function. — activation_relu","text":"default values, returns standard ReLU activation: max(x, 0), element-wise maximum 0 input tensor. Modifying default parameters allows use non-zero thresholds, change max value activation, use non-zero multiple input values threshold.","code":""},{"path":"https://keras.posit.co/reference/activation_relu.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Applies the rectified linear unit activation function. — activation_relu","text":"","code":"activation_relu(x, negative_slope = 0, max_value = NULL, threshold = 0)"},{"path":"https://keras.posit.co/reference/activation_relu.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Applies the rectified linear unit activation function. — activation_relu","text":"x Input tensor. negative_slope numeric controls slope values lower threshold. max_value numeric sets saturation threshold (largest value function return). threshold numeric giving threshold value activation function values damped set zero.","code":""},{"path":"https://keras.posit.co/reference/activation_relu.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Applies the rectified linear unit activation function. — activation_relu","text":"","code":"A tensor with the same shape and dtype as input `x`."},{"path":"https://keras.posit.co/reference/activation_relu.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Applies the rectified linear unit activation function. — activation_relu","text":"","code":"x <- c(-10, -5, 0, 5, 10) activation_relu(x) ## tf.Tensor([ 0.  0.  0.  5. 10.], shape=(5), dtype=float32) activation_relu(x, negative_slope = 0.5) ## tf.Tensor([-5.  -2.5  0.   5.  10. ], shape=(5), dtype=float32) activation_relu(x, max_value = 5) ## tf.Tensor([0. 0. 0. 5. 5.], shape=(5), dtype=float32) activation_relu(x, threshold = 5) ## tf.Tensor([-0. -0.  0.  0. 10.], shape=(5), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/activation_relu6.html","id":null,"dir":"Reference","previous_headings":"","what":"Relu6 activation function. — activation_relu6","title":"Relu6 activation function. — activation_relu6","text":"ReLU function, truncated maximum value 6.","code":""},{"path":"https://keras.posit.co/reference/activation_relu6.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Relu6 activation function. — activation_relu6","text":"","code":"activation_relu6(x)"},{"path":"https://keras.posit.co/reference/activation_relu6.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Relu6 activation function. — activation_relu6","text":"x Input tensor.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/activation_selu.html","id":null,"dir":"Reference","previous_headings":"","what":"Scaled Exponential Linear Unit (SELU). — activation_selu","title":"Scaled Exponential Linear Unit (SELU). — activation_selu","text":"Scaled Exponential Linear Unit (SELU) activation function defined : scale * x x > 0 scale * alpha * (exp(x) - 1) x < 0 alpha scale pre-defined constants (alpha = 1.67326324 scale = 1.05070098). Basically, SELU activation function multiplies scale (> 1) output keras.activations.elu function ensure slope larger one positive inputs. values alpha scale chosen mean variance inputs preserved two consecutive layers long weights initialized correctly (see keras.initializers.LecunNormal initializer) number input units \"large enough\" (see reference paper information).","code":""},{"path":"https://keras.posit.co/reference/activation_selu.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scaled Exponential Linear Unit (SELU). — activation_selu","text":"","code":"activation_selu(x)"},{"path":"https://keras.posit.co/reference/activation_selu.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scaled Exponential Linear Unit (SELU). — activation_selu","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/activation_selu.html","id":"notes","dir":"Reference","previous_headings":"","what":"Notes","title":"Scaled Exponential Linear Unit (SELU). — activation_selu","text":"used together keras.initializers.LecunNormal initializer. used together dropout variant keras.layers.AlphaDropout (rather regular dropout).","code":""},{"path":"https://keras.posit.co/reference/activation_selu.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Scaled Exponential Linear Unit (SELU). — activation_selu","text":"Klambauer et al., 2017","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/activation_sigmoid.html","id":null,"dir":"Reference","previous_headings":"","what":"Sigmoid activation function. — activation_sigmoid","title":"Sigmoid activation function. — activation_sigmoid","text":"defined : sigmoid(x) = 1 / (1 + exp(-x)). small values (<-5), sigmoid returns value close zero, large values (>5) result function gets close 1. Sigmoid equivalent 2-element softmax, second element assumed zero. sigmoid function always returns value 0 1.","code":""},{"path":"https://keras.posit.co/reference/activation_sigmoid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sigmoid activation function. — activation_sigmoid","text":"","code":"activation_sigmoid(x)"},{"path":"https://keras.posit.co/reference/activation_sigmoid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sigmoid activation function. — activation_sigmoid","text":"x Input tensor.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/activation_silu.html","id":null,"dir":"Reference","previous_headings":"","what":"Swish (or Silu) activation function. — activation_silu","title":"Swish (or Silu) activation function. — activation_silu","text":"defined : swish(x) = x * sigmoid(x). Swish (Silu) activation function smooth, non-monotonic function unbounded bounded .","code":""},{"path":"https://keras.posit.co/reference/activation_silu.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Swish (or Silu) activation function. — activation_silu","text":"","code":"activation_silu(x)"},{"path":"https://keras.posit.co/reference/activation_silu.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Swish (or Silu) activation function. — activation_silu","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/activation_silu.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Swish (or Silu) activation function. — activation_silu","text":"Ramachandran et al., 2017","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/activation_softmax.html","id":null,"dir":"Reference","previous_headings":"","what":"Softmax converts a vector of values to a probability distribution. — activation_softmax","title":"Softmax converts a vector of values to a probability distribution. — activation_softmax","text":"elements output vector range [0, 1] sum 1. input vector handled independently. axis argument sets axis input function applied along. Softmax often used activation last layer classification network result interpreted probability distribution. softmax vector x computed exp(x) / sum(exp(x)). input values log-odds resulting probability.","code":""},{"path":"https://keras.posit.co/reference/activation_softmax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Softmax converts a vector of values to a probability distribution. — activation_softmax","text":"","code":"activation_softmax(x, axis = -1L)"},{"path":"https://keras.posit.co/reference/activation_softmax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Softmax converts a vector of values to a probability distribution. — activation_softmax","text":"x Input tensor. axis Integer, axis along softmax applied.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/activation_softplus.html","id":null,"dir":"Reference","previous_headings":"","what":"Softplus activation function. — activation_softplus","title":"Softplus activation function. — activation_softplus","text":"defined : softplus(x) = log(exp(x) + 1).","code":""},{"path":"https://keras.posit.co/reference/activation_softplus.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Softplus activation function. — activation_softplus","text":"","code":"activation_softplus(x)"},{"path":"https://keras.posit.co/reference/activation_softplus.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Softplus activation function. — activation_softplus","text":"x Input tensor.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/activation_softsign.html","id":null,"dir":"Reference","previous_headings":"","what":"Softsign activation function. — activation_softsign","title":"Softsign activation function. — activation_softsign","text":"Softsign defined : softsign(x) = x / (abs(x) + 1).","code":""},{"path":"https://keras.posit.co/reference/activation_softsign.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Softsign activation function. — activation_softsign","text":"","code":"activation_softsign(x)"},{"path":"https://keras.posit.co/reference/activation_softsign.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Softsign activation function. — activation_softsign","text":"x Input tensor.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/activation_tanh.html","id":null,"dir":"Reference","previous_headings":"","what":"Hyperbolic tangent activation function. — activation_tanh","title":"Hyperbolic tangent activation function. — activation_tanh","text":"defined : tanh(x) = sinh(x) / cosh(x), .e. tanh(x) = ((exp(x) - exp(-x)) / (exp(x) + exp(-x))).","code":""},{"path":"https://keras.posit.co/reference/activation_tanh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperbolic tangent activation function. — activation_tanh","text":"","code":"activation_tanh(x)"},{"path":"https://keras.posit.co/reference/activation_tanh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hyperbolic tangent activation function. — activation_tanh","text":"x Input tensor.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/active_property.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an active property class method — active_property","title":"Create an active property class method — active_property","text":"Create active property class method","code":""},{"path":"https://keras.posit.co/reference/active_property.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an active property class method — active_property","text":"","code":"active_property(fn)  mark_active(fn)"},{"path":"https://keras.posit.co/reference/active_property.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an active property class method — active_property","text":"fn R function","code":""},{"path":"https://keras.posit.co/reference/active_property.html","id":"example","dir":"Reference","previous_headings":"","what":"Example","title":"Create an active property class method — active_property","text":"mark_active() backwards compatible alias active_property().","code":"layer_foo <- Model(\"Foo\", ...,   metrics = active_property(function() {     list(self$d_loss_metric,          self$g_loss_metric)   }))"},{"path":"https://keras.posit.co/reference/adapt.html","id":null,"dir":"Reference","previous_headings":"","what":"Fits the state of the preprocessing layer to the data being passed — adapt","title":"Fits the state of the preprocessing layer to the data being passed — adapt","text":"Fits state preprocessing layer data passed","code":""},{"path":"https://keras.posit.co/reference/adapt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fits the state of the preprocessing layer to the data being passed — adapt","text":"","code":"adapt(object, data, ..., batch_size = NULL, steps = NULL)"},{"path":"https://keras.posit.co/reference/adapt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fits the state of the preprocessing layer to the data being passed — adapt","text":"object Preprocessing layer object data data train . can passed either tf.data.Dataset R array. ... Used forwards backwards compatibility. Passed underlying method. batch_size Integer NULL. Number asamples per state update. unspecified, batch_size default 32. specify batch_size data form datasets, generators, keras.utils.Sequence instances (since generate batches). steps Integer NULL. Total number steps (batches samples) training input tensors TensorFlow data tensors, default NULL equal number samples dataset divided batch size, 1 determined. x tf.data.Dataset, steps NULL, epoch run input dataset exhausted. passing infinitely repeating dataset, must specify steps argument. argument supported array inputs.","code":""},{"path":"https://keras.posit.co/reference/adapt.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fits the state of the preprocessing layer to the data being passed — adapt","text":"calling adapt layer, preprocessing layer's state update training. order make preprocessing layers efficient distribution context, kept constant respect compiled tf.Graphs call layer. affect layer use adapting layer , adapt layer multiple times need take care re-compile compiled functions follows: adding preprocessing layer keras.Model, need call compile(model) subsequent call adapt(). calling preprocessing layer inside tfdatasets::dataset_map(), call dataset_map() input tf.data.Dataset adapt(). using tensorflow::tf_function() directly calls preprocessing layer, need call tf_function callable subsequent call adapt(). keras_model example multiple adapts:   tf.data.Dataset example multiple adapts:","code":"layer <- layer_normalization(axis=NULL) adapt(layer, c(0, 2)) model <- keras_model_sequential(layer) predict(model, c(0, 1, 2)) # [1] -1  0  1  adapt(layer, c(-1, 1)) compile(model)  # This is needed to re-compile model.predict! predict(model, c(0, 1, 2)) # [1] 0 1 2 layer <- layer_normalization(axis=NULL) adapt(layer, c(0, 2)) input_ds <- tfdatasets::range_dataset(0, 3) normalized_ds <- input_ds %>%   tfdatasets::dataset_map(layer) str(reticulate::iterate(normalized_ds)) # List of 3 #  $ :tf.Tensor([-1.], shape=(1,), dtype=float32) #  $ :tf.Tensor([0.], shape=(1,), dtype=float32) #  $ :tf.Tensor([1.], shape=(1,), dtype=float32) adapt(layer, c(-1, 1)) normalized_ds <- input_ds %>%   tfdatasets::dataset_map(layer) # Re-map over the input dataset. str(reticulate::iterate(normalized_ds$as_numpy_iterator())) # List of 3 #  $ : num [1(1d)] -1 #  $ : num [1(1d)] 0 #  $ : num [1(1d)] 1"},{"path":[]},{"path":"https://keras.posit.co/reference/application_convnext_base.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the ConvNeXtBase architecture. — application_convnext_base","title":"Instantiates the ConvNeXtBase architecture. — application_convnext_base","text":"Instantiates ConvNeXtBase architecture.","code":""},{"path":"https://keras.posit.co/reference/application_convnext_base.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the ConvNeXtBase architecture. — application_convnext_base","text":"","code":"application_convnext_base(   model_name = \"convnext_base\",   include_top = TRUE,   include_preprocessing = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\" )"},{"path":"https://keras.posit.co/reference/application_convnext_base.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the ConvNeXtBase architecture. — application_convnext_base","text":"model_name String, name model. include_top Whether include fully-connected layer top network. Defaults TRUE. include_preprocessing Boolean, whether include preprocessing layer bottom network. weights One NULL (random initialization), \"imagenet\" (pre-training ImageNet-1k), path weights file loaded. Defaults \"imagenet\". input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. input_shape Optional shape tuple, specified include_top FALSE. exactly 3 inputs channels. pooling Optional pooling mode feature extraction include_top FALSE. Defaults NULL. NULL means output model 4D tensor output last convolutional layer. avg means global average pooling applied output last convolutional layer, thus output model 2D tensor. max means global max pooling applied. classes Optional number classes classify images , specified include_top TRUE, weights argument specified. Defaults 1000 (number ImageNet classes). classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. Defaults \"softmax\". loading pretrained weights, classifier_activation can NULL \"softmax\".","code":""},{"path":"https://keras.posit.co/reference/application_convnext_base.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the ConvNeXtBase architecture. — application_convnext_base","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_convnext_base.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Instantiates the ConvNeXtBase architecture. — application_convnext_base","text":"ConvNet 2020s (CVPR 2022) image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning. base, large, xlarge models first pre-trained ImageNet-21k dataset fine-tuned ImageNet-1k dataset. pre-trained parameters models assembled official repository. get sense parameters converted Keras compatible parameters, please refer repository.","code":""},{"path":"https://keras.posit.co/reference/application_convnext_base.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the ConvNeXtBase architecture. — application_convnext_base","text":"Keras Application expects specific kind input preprocessing. ConvNeXt, preprocessing included model using Normalization layer.  ConvNeXt models expect inputs float uint8 tensors pixels values [0-255] range. calling summary() method instantiating ConvNeXt model, prefer setting expand_nested argument summary() TRUE better investigate instantiated model.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_convnext_large.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the ConvNeXtLarge architecture. — application_convnext_large","title":"Instantiates the ConvNeXtLarge architecture. — application_convnext_large","text":"Instantiates ConvNeXtLarge architecture.","code":""},{"path":"https://keras.posit.co/reference/application_convnext_large.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the ConvNeXtLarge architecture. — application_convnext_large","text":"","code":"application_convnext_large(   model_name = \"convnext_large\",   include_top = TRUE,   include_preprocessing = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\" )"},{"path":"https://keras.posit.co/reference/application_convnext_large.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the ConvNeXtLarge architecture. — application_convnext_large","text":"model_name String, name model. include_top Whether include fully-connected layer top network. Defaults TRUE. include_preprocessing Boolean, whether include preprocessing layer bottom network. weights One NULL (random initialization), \"imagenet\" (pre-training ImageNet-1k), path weights file loaded. Defaults \"imagenet\". input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. input_shape Optional shape tuple, specified include_top FALSE. exactly 3 inputs channels. pooling Optional pooling mode feature extraction include_top FALSE. Defaults NULL. NULL means output model 4D tensor output last convolutional layer. avg means global average pooling applied output last convolutional layer, thus output model 2D tensor. max means global max pooling applied. classes Optional number classes classify images , specified include_top TRUE, weights argument specified. Defaults 1000 (number ImageNet classes). classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. Defaults \"softmax\". loading pretrained weights, classifier_activation can NULL \"softmax\".","code":""},{"path":"https://keras.posit.co/reference/application_convnext_large.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the ConvNeXtLarge architecture. — application_convnext_large","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_convnext_large.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Instantiates the ConvNeXtLarge architecture. — application_convnext_large","text":"ConvNet 2020s (CVPR 2022) image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning. base, large, xlarge models first pre-trained ImageNet-21k dataset fine-tuned ImageNet-1k dataset. pre-trained parameters models assembled official repository. get sense parameters converted Keras compatible parameters, please refer repository.","code":""},{"path":"https://keras.posit.co/reference/application_convnext_large.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the ConvNeXtLarge architecture. — application_convnext_large","text":"Keras Application expects specific kind input preprocessing. ConvNeXt, preprocessing included model using Normalization layer.  ConvNeXt models expect inputs float uint8 tensors pixels values [0-255] range. calling summary() method instantiating ConvNeXt model, prefer setting expand_nested argument summary() TRUE better investigate instantiated model.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_convnext_small.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the ConvNeXtSmall architecture. — application_convnext_small","title":"Instantiates the ConvNeXtSmall architecture. — application_convnext_small","text":"Instantiates ConvNeXtSmall architecture.","code":""},{"path":"https://keras.posit.co/reference/application_convnext_small.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the ConvNeXtSmall architecture. — application_convnext_small","text":"","code":"application_convnext_small(   model_name = \"convnext_small\",   include_top = TRUE,   include_preprocessing = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\" )"},{"path":"https://keras.posit.co/reference/application_convnext_small.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the ConvNeXtSmall architecture. — application_convnext_small","text":"model_name String, name model. include_top Whether include fully-connected layer top network. Defaults TRUE. include_preprocessing Boolean, whether include preprocessing layer bottom network. weights One NULL (random initialization), \"imagenet\" (pre-training ImageNet-1k), path weights file loaded. Defaults \"imagenet\". input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. input_shape Optional shape tuple, specified include_top FALSE. exactly 3 inputs channels. pooling Optional pooling mode feature extraction include_top FALSE. Defaults NULL. NULL means output model 4D tensor output last convolutional layer. avg means global average pooling applied output last convolutional layer, thus output model 2D tensor. max means global max pooling applied. classes Optional number classes classify images , specified include_top TRUE, weights argument specified. Defaults 1000 (number ImageNet classes). classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. Defaults \"softmax\". loading pretrained weights, classifier_activation can NULL \"softmax\".","code":""},{"path":"https://keras.posit.co/reference/application_convnext_small.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the ConvNeXtSmall architecture. — application_convnext_small","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_convnext_small.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Instantiates the ConvNeXtSmall architecture. — application_convnext_small","text":"ConvNet 2020s (CVPR 2022) image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning. base, large, xlarge models first pre-trained ImageNet-21k dataset fine-tuned ImageNet-1k dataset. pre-trained parameters models assembled official repository. get sense parameters converted Keras compatible parameters, please refer repository.","code":""},{"path":"https://keras.posit.co/reference/application_convnext_small.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the ConvNeXtSmall architecture. — application_convnext_small","text":"Keras Application expects specific kind input preprocessing. ConvNeXt, preprocessing included model using Normalization layer.  ConvNeXt models expect inputs float uint8 tensors pixels values [0-255] range. calling summary() method instantiating ConvNeXt model, prefer setting expand_nested argument summary() TRUE better investigate instantiated model.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_convnext_tiny.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the ConvNeXtTiny architecture. — application_convnext_tiny","title":"Instantiates the ConvNeXtTiny architecture. — application_convnext_tiny","text":"Instantiates ConvNeXtTiny architecture.","code":""},{"path":"https://keras.posit.co/reference/application_convnext_tiny.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the ConvNeXtTiny architecture. — application_convnext_tiny","text":"","code":"application_convnext_tiny(   model_name = \"convnext_tiny\",   include_top = TRUE,   include_preprocessing = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\" )"},{"path":"https://keras.posit.co/reference/application_convnext_tiny.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the ConvNeXtTiny architecture. — application_convnext_tiny","text":"model_name String, name model. include_top Whether include fully-connected layer top network. Defaults TRUE. include_preprocessing Boolean, whether include preprocessing layer bottom network. weights One NULL (random initialization), \"imagenet\" (pre-training ImageNet-1k), path weights file loaded. Defaults \"imagenet\". input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. input_shape Optional shape tuple, specified include_top FALSE. exactly 3 inputs channels. pooling Optional pooling mode feature extraction include_top FALSE. Defaults NULL. NULL means output model 4D tensor output last convolutional layer. avg means global average pooling applied output last convolutional layer, thus output model 2D tensor. max means global max pooling applied. classes Optional number classes classify images , specified include_top TRUE, weights argument specified. Defaults 1000 (number ImageNet classes). classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. Defaults \"softmax\". loading pretrained weights, classifier_activation can NULL \"softmax\".","code":""},{"path":"https://keras.posit.co/reference/application_convnext_tiny.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the ConvNeXtTiny architecture. — application_convnext_tiny","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_convnext_tiny.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Instantiates the ConvNeXtTiny architecture. — application_convnext_tiny","text":"ConvNet 2020s (CVPR 2022) image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning. base, large, xlarge models first pre-trained ImageNet-21k dataset fine-tuned ImageNet-1k dataset. pre-trained parameters models assembled official repository. get sense parameters converted Keras compatible parameters, please refer repository.","code":""},{"path":"https://keras.posit.co/reference/application_convnext_tiny.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the ConvNeXtTiny architecture. — application_convnext_tiny","text":"Keras Application expects specific kind input preprocessing. ConvNeXt, preprocessing included model using Normalization layer.  ConvNeXt models expect inputs float uint8 tensors pixels values [0-255] range. calling summary() method instantiating ConvNeXt model, prefer setting expand_nested argument summary() TRUE better investigate instantiated model.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_convnext_xlarge.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the ConvNeXtXLarge architecture. — application_convnext_xlarge","title":"Instantiates the ConvNeXtXLarge architecture. — application_convnext_xlarge","text":"Instantiates ConvNeXtXLarge architecture.","code":""},{"path":"https://keras.posit.co/reference/application_convnext_xlarge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the ConvNeXtXLarge architecture. — application_convnext_xlarge","text":"","code":"application_convnext_xlarge(   model_name = \"convnext_xlarge\",   include_top = TRUE,   include_preprocessing = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\" )"},{"path":"https://keras.posit.co/reference/application_convnext_xlarge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the ConvNeXtXLarge architecture. — application_convnext_xlarge","text":"model_name String, name model. include_top Whether include fully-connected layer top network. Defaults TRUE. include_preprocessing Boolean, whether include preprocessing layer bottom network. weights One NULL (random initialization), \"imagenet\" (pre-training ImageNet-1k), path weights file loaded. Defaults \"imagenet\". input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. input_shape Optional shape tuple, specified include_top FALSE. exactly 3 inputs channels. pooling Optional pooling mode feature extraction include_top FALSE. Defaults NULL. NULL means output model 4D tensor output last convolutional layer. avg means global average pooling applied output last convolutional layer, thus output model 2D tensor. max means global max pooling applied. classes Optional number classes classify images , specified include_top TRUE, weights argument specified. Defaults 1000 (number ImageNet classes). classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. Defaults \"softmax\". loading pretrained weights, classifier_activation can NULL \"softmax\".","code":""},{"path":"https://keras.posit.co/reference/application_convnext_xlarge.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the ConvNeXtXLarge architecture. — application_convnext_xlarge","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_convnext_xlarge.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Instantiates the ConvNeXtXLarge architecture. — application_convnext_xlarge","text":"ConvNet 2020s (CVPR 2022) image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning. base, large, xlarge models first pre-trained ImageNet-21k dataset fine-tuned ImageNet-1k dataset. pre-trained parameters models assembled official repository. get sense parameters converted Keras compatible parameters, please refer repository.","code":""},{"path":"https://keras.posit.co/reference/application_convnext_xlarge.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the ConvNeXtXLarge architecture. — application_convnext_xlarge","text":"Keras Application expects specific kind input preprocessing. ConvNeXt, preprocessing included model using Normalization layer.  ConvNeXt models expect inputs float uint8 tensors pixels values [0-255] range. calling summary() method instantiating ConvNeXt model, prefer setting expand_nested argument summary() TRUE better investigate instantiated model.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_densenet121.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the Densenet121 architecture. — application_densenet121","title":"Instantiates the Densenet121 architecture. — application_densenet121","text":"Instantiates Densenet121 architecture.","code":""},{"path":"https://keras.posit.co/reference/application_densenet121.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the Densenet121 architecture. — application_densenet121","text":"","code":"application_densenet121(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\" )"},{"path":"https://keras.posit.co/reference/application_densenet121.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the Densenet121 architecture. — application_densenet121","text":"include_top whether include fully-connected layer top network. weights one NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. input_tensor optional Keras tensor (.e. output layers.Input()) use image input model. input_shape optional shape tuple, specified include_top FALSE (otherwise input shape (224, 224, 3) ('channels_last' data format) (3, 224, 224) ('channels_first' data format). exactly 3 inputs channels, width height smaller 32. E.g. (200, 200, 3) one valid value. pooling Optional pooling mode feature extraction include_top FALSE. NULL means output model 4D tensor output last convolutional block. avg means global average pooling applied output last convolutional block, thus output model 2D tensor. max means global max pooling applied. classes optional number classes classify images , specified include_top TRUE, weights argument specified. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. loading pretrained weights, classifier_activation can NULL \"softmax\".","code":""},{"path":"https://keras.posit.co/reference/application_densenet121.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the Densenet121 architecture. — application_densenet121","text":"","code":"A Keras model instance."},{"path":"https://keras.posit.co/reference/application_densenet121.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the Densenet121 architecture. — application_densenet121","text":"Densely Connected Convolutional Networks (CVPR 2017) Optionally loads weights pre-trained ImageNet. Note data format convention used model one specified Keras config ~/.keras/keras.json.","code":""},{"path":"https://keras.posit.co/reference/application_densenet121.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the Densenet121 architecture. — application_densenet121","text":"Keras Application expects specific kind input preprocessing. DenseNet, call keras.applications.densenet.preprocess_input inputs passing model.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_densenet169.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the Densenet169 architecture. — application_densenet169","title":"Instantiates the Densenet169 architecture. — application_densenet169","text":"Instantiates Densenet169 architecture.","code":""},{"path":"https://keras.posit.co/reference/application_densenet169.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the Densenet169 architecture. — application_densenet169","text":"","code":"application_densenet169(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\" )"},{"path":"https://keras.posit.co/reference/application_densenet169.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the Densenet169 architecture. — application_densenet169","text":"include_top whether include fully-connected layer top network. weights one NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. input_tensor optional Keras tensor (.e. output layers.Input()) use image input model. input_shape optional shape tuple, specified include_top FALSE (otherwise input shape (224, 224, 3) ('channels_last' data format) (3, 224, 224) ('channels_first' data format). exactly 3 inputs channels, width height smaller 32. E.g. (200, 200, 3) one valid value. pooling Optional pooling mode feature extraction include_top FALSE. NULL means output model 4D tensor output last convolutional block. avg means global average pooling applied output last convolutional block, thus output model 2D tensor. max means global max pooling applied. classes optional number classes classify images , specified include_top TRUE, weights argument specified. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. loading pretrained weights, classifier_activation can NULL \"softmax\".","code":""},{"path":"https://keras.posit.co/reference/application_densenet169.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the Densenet169 architecture. — application_densenet169","text":"","code":"A Keras model instance."},{"path":"https://keras.posit.co/reference/application_densenet169.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the Densenet169 architecture. — application_densenet169","text":"Densely Connected Convolutional Networks (CVPR 2017) Optionally loads weights pre-trained ImageNet. Note data format convention used model one specified Keras config ~/.keras/keras.json.","code":""},{"path":"https://keras.posit.co/reference/application_densenet169.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the Densenet169 architecture. — application_densenet169","text":"Keras Application expects specific kind input preprocessing. DenseNet, call keras.applications.densenet.preprocess_input inputs passing model.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_densenet201.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the Densenet201 architecture. — application_densenet201","title":"Instantiates the Densenet201 architecture. — application_densenet201","text":"Instantiates Densenet201 architecture.","code":""},{"path":"https://keras.posit.co/reference/application_densenet201.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the Densenet201 architecture. — application_densenet201","text":"","code":"application_densenet201(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\" )"},{"path":"https://keras.posit.co/reference/application_densenet201.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the Densenet201 architecture. — application_densenet201","text":"include_top whether include fully-connected layer top network. weights one NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. input_tensor optional Keras tensor (.e. output layers.Input()) use image input model. input_shape optional shape tuple, specified include_top FALSE (otherwise input shape (224, 224, 3) ('channels_last' data format) (3, 224, 224) ('channels_first' data format). exactly 3 inputs channels, width height smaller 32. E.g. (200, 200, 3) one valid value. pooling Optional pooling mode feature extraction include_top FALSE. NULL means output model 4D tensor output last convolutional block. avg means global average pooling applied output last convolutional block, thus output model 2D tensor. max means global max pooling applied. classes optional number classes classify images , specified include_top TRUE, weights argument specified. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. loading pretrained weights, classifier_activation can NULL \"softmax\".","code":""},{"path":"https://keras.posit.co/reference/application_densenet201.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the Densenet201 architecture. — application_densenet201","text":"","code":"A Keras model instance."},{"path":"https://keras.posit.co/reference/application_densenet201.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the Densenet201 architecture. — application_densenet201","text":"Densely Connected Convolutional Networks (CVPR 2017) Optionally loads weights pre-trained ImageNet. Note data format convention used model one specified Keras config ~/.keras/keras.json.","code":""},{"path":"https://keras.posit.co/reference/application_densenet201.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the Densenet201 architecture. — application_densenet201","text":"Keras Application expects specific kind input preprocessing. DenseNet, call keras.applications.densenet.preprocess_input inputs passing model.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_efficientnet_b0.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the EfficientNetB0 architecture. — application_efficientnet_b0","title":"Instantiates the EfficientNetB0 architecture. — application_efficientnet_b0","text":"Instantiates EfficientNetB0 architecture.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_b0.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the EfficientNetB0 architecture. — application_efficientnet_b0","text":"","code":"application_efficientnet_b0(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\",   ... )"},{"path":"https://keras.posit.co/reference/application_efficientnet_b0.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the EfficientNetB0 architecture. — application_efficientnet_b0","text":"include_top Whether include fully-connected layer top network. Defaults TRUE. weights One NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. Defaults \"imagenet\". input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. input_shape Optional shape tuple, specified include_top FALSE. exactly 3 inputs channels. pooling Optional pooling mode feature extraction include_top FALSE. Defaults NULL. NULL means output model 4D tensor output last convolutional layer. avg means global average pooling applied output last convolutional layer, thus output model 2D tensor. max means global max pooling applied. classes Optional number classes classify images , specified include_top TRUE, weights argument specified. 1000 many ImageNet classes . Defaults 1000. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. Defaults 'softmax'. loading pretrained weights, classifier_activation can NULL \"softmax\". ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_b0.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the EfficientNetB0 architecture. — application_efficientnet_b0","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_efficientnet_b0.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the EfficientNetB0 architecture. — application_efficientnet_b0","text":"EfficientNet: Rethinking Model Scaling Convolutional Neural Networks (ICML 2019) function returns Keras image classification model, optionally loaded weights pre-trained ImageNet. image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_b0.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the EfficientNetB0 architecture. — application_efficientnet_b0","text":"Keras Application expects specific kind input preprocessing. EfficientNet, input preprocessing included part model (Rescaling layer), thus keras.applications.efficientnet.preprocess_input actually pass-function. EfficientNet models expect inputs float tensors pixels values [0-255] range.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_efficientnet_b1.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the EfficientNetB1 architecture. — application_efficientnet_b1","title":"Instantiates the EfficientNetB1 architecture. — application_efficientnet_b1","text":"Instantiates EfficientNetB1 architecture.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_b1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the EfficientNetB1 architecture. — application_efficientnet_b1","text":"","code":"application_efficientnet_b1(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\",   ... )"},{"path":"https://keras.posit.co/reference/application_efficientnet_b1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the EfficientNetB1 architecture. — application_efficientnet_b1","text":"include_top Whether include fully-connected layer top network. Defaults TRUE. weights One NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. Defaults \"imagenet\". input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. input_shape Optional shape tuple, specified include_top FALSE. exactly 3 inputs channels. pooling Optional pooling mode feature extraction include_top FALSE. Defaults NULL. NULL means output model 4D tensor output last convolutional layer. avg means global average pooling applied output last convolutional layer, thus output model 2D tensor. max means global max pooling applied. classes Optional number classes classify images , specified include_top TRUE, weights argument specified. 1000 many ImageNet classes . Defaults 1000. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. Defaults 'softmax'. loading pretrained weights, classifier_activation can NULL \"softmax\". ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_b1.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the EfficientNetB1 architecture. — application_efficientnet_b1","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_efficientnet_b1.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the EfficientNetB1 architecture. — application_efficientnet_b1","text":"EfficientNet: Rethinking Model Scaling Convolutional Neural Networks (ICML 2019) function returns Keras image classification model, optionally loaded weights pre-trained ImageNet. image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_b1.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the EfficientNetB1 architecture. — application_efficientnet_b1","text":"Keras Application expects specific kind input preprocessing. EfficientNet, input preprocessing included part model (Rescaling layer), thus keras.applications.efficientnet.preprocess_input actually pass-function. EfficientNet models expect inputs float tensors pixels values [0-255] range.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_efficientnet_b2.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the EfficientNetB2 architecture. — application_efficientnet_b2","title":"Instantiates the EfficientNetB2 architecture. — application_efficientnet_b2","text":"Instantiates EfficientNetB2 architecture.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_b2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the EfficientNetB2 architecture. — application_efficientnet_b2","text":"","code":"application_efficientnet_b2(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\",   ... )"},{"path":"https://keras.posit.co/reference/application_efficientnet_b2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the EfficientNetB2 architecture. — application_efficientnet_b2","text":"include_top Whether include fully-connected layer top network. Defaults TRUE. weights One NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. Defaults \"imagenet\". input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. input_shape Optional shape tuple, specified include_top FALSE. exactly 3 inputs channels. pooling Optional pooling mode feature extraction include_top FALSE. Defaults NULL. NULL means output model 4D tensor output last convolutional layer. avg means global average pooling applied output last convolutional layer, thus output model 2D tensor. max means global max pooling applied. classes Optional number classes classify images , specified include_top TRUE, weights argument specified. 1000 many ImageNet classes . Defaults 1000. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. Defaults 'softmax'. loading pretrained weights, classifier_activation can NULL \"softmax\". ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_b2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the EfficientNetB2 architecture. — application_efficientnet_b2","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_efficientnet_b2.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the EfficientNetB2 architecture. — application_efficientnet_b2","text":"EfficientNet: Rethinking Model Scaling Convolutional Neural Networks (ICML 2019) function returns Keras image classification model, optionally loaded weights pre-trained ImageNet. image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_b2.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the EfficientNetB2 architecture. — application_efficientnet_b2","text":"Keras Application expects specific kind input preprocessing. EfficientNet, input preprocessing included part model (Rescaling layer), thus keras.applications.efficientnet.preprocess_input actually pass-function. EfficientNet models expect inputs float tensors pixels values [0-255] range.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_efficientnet_b3.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the EfficientNetB3 architecture. — application_efficientnet_b3","title":"Instantiates the EfficientNetB3 architecture. — application_efficientnet_b3","text":"Instantiates EfficientNetB3 architecture.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_b3.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the EfficientNetB3 architecture. — application_efficientnet_b3","text":"","code":"application_efficientnet_b3(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\",   ... )"},{"path":"https://keras.posit.co/reference/application_efficientnet_b3.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the EfficientNetB3 architecture. — application_efficientnet_b3","text":"include_top Whether include fully-connected layer top network. Defaults TRUE. weights One NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. Defaults \"imagenet\". input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. input_shape Optional shape tuple, specified include_top FALSE. exactly 3 inputs channels. pooling Optional pooling mode feature extraction include_top FALSE. Defaults NULL. NULL means output model 4D tensor output last convolutional layer. avg means global average pooling applied output last convolutional layer, thus output model 2D tensor. max means global max pooling applied. classes Optional number classes classify images , specified include_top TRUE, weights argument specified. 1000 many ImageNet classes . Defaults 1000. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. Defaults 'softmax'. loading pretrained weights, classifier_activation can NULL \"softmax\". ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_b3.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the EfficientNetB3 architecture. — application_efficientnet_b3","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_efficientnet_b3.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the EfficientNetB3 architecture. — application_efficientnet_b3","text":"EfficientNet: Rethinking Model Scaling Convolutional Neural Networks (ICML 2019) function returns Keras image classification model, optionally loaded weights pre-trained ImageNet. image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_b3.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the EfficientNetB3 architecture. — application_efficientnet_b3","text":"Keras Application expects specific kind input preprocessing. EfficientNet, input preprocessing included part model (Rescaling layer), thus keras.applications.efficientnet.preprocess_input actually pass-function. EfficientNet models expect inputs float tensors pixels values [0-255] range.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_efficientnet_b4.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the EfficientNetB4 architecture. — application_efficientnet_b4","title":"Instantiates the EfficientNetB4 architecture. — application_efficientnet_b4","text":"Instantiates EfficientNetB4 architecture.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_b4.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the EfficientNetB4 architecture. — application_efficientnet_b4","text":"","code":"application_efficientnet_b4(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\",   ... )"},{"path":"https://keras.posit.co/reference/application_efficientnet_b4.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the EfficientNetB4 architecture. — application_efficientnet_b4","text":"include_top Whether include fully-connected layer top network. Defaults TRUE. weights One NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. Defaults \"imagenet\". input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. input_shape Optional shape tuple, specified include_top FALSE. exactly 3 inputs channels. pooling Optional pooling mode feature extraction include_top FALSE. Defaults NULL. NULL means output model 4D tensor output last convolutional layer. avg means global average pooling applied output last convolutional layer, thus output model 2D tensor. max means global max pooling applied. classes Optional number classes classify images , specified include_top TRUE, weights argument specified. 1000 many ImageNet classes . Defaults 1000. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. Defaults 'softmax'. loading pretrained weights, classifier_activation can NULL \"softmax\". ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_b4.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the EfficientNetB4 architecture. — application_efficientnet_b4","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_efficientnet_b4.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the EfficientNetB4 architecture. — application_efficientnet_b4","text":"EfficientNet: Rethinking Model Scaling Convolutional Neural Networks (ICML 2019) function returns Keras image classification model, optionally loaded weights pre-trained ImageNet. image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_b4.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the EfficientNetB4 architecture. — application_efficientnet_b4","text":"Keras Application expects specific kind input preprocessing. EfficientNet, input preprocessing included part model (Rescaling layer), thus keras.applications.efficientnet.preprocess_input actually pass-function. EfficientNet models expect inputs float tensors pixels values [0-255] range.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_efficientnet_b5.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the EfficientNetB5 architecture. — application_efficientnet_b5","title":"Instantiates the EfficientNetB5 architecture. — application_efficientnet_b5","text":"Instantiates EfficientNetB5 architecture.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_b5.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the EfficientNetB5 architecture. — application_efficientnet_b5","text":"","code":"application_efficientnet_b5(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\",   ... )"},{"path":"https://keras.posit.co/reference/application_efficientnet_b5.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the EfficientNetB5 architecture. — application_efficientnet_b5","text":"include_top Whether include fully-connected layer top network. Defaults TRUE. weights One NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. Defaults \"imagenet\". input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. input_shape Optional shape tuple, specified include_top FALSE. exactly 3 inputs channels. pooling Optional pooling mode feature extraction include_top FALSE. Defaults NULL. NULL means output model 4D tensor output last convolutional layer. avg means global average pooling applied output last convolutional layer, thus output model 2D tensor. max means global max pooling applied. classes Optional number classes classify images , specified include_top TRUE, weights argument specified. 1000 many ImageNet classes . Defaults 1000. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. Defaults 'softmax'. loading pretrained weights, classifier_activation can NULL \"softmax\". ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_b5.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the EfficientNetB5 architecture. — application_efficientnet_b5","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_efficientnet_b5.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the EfficientNetB5 architecture. — application_efficientnet_b5","text":"EfficientNet: Rethinking Model Scaling Convolutional Neural Networks (ICML 2019) function returns Keras image classification model, optionally loaded weights pre-trained ImageNet. image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_b5.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the EfficientNetB5 architecture. — application_efficientnet_b5","text":"Keras Application expects specific kind input preprocessing. EfficientNet, input preprocessing included part model (Rescaling layer), thus keras.applications.efficientnet.preprocess_input actually pass-function. EfficientNet models expect inputs float tensors pixels values [0-255] range.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_efficientnet_b6.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the EfficientNetB6 architecture. — application_efficientnet_b6","title":"Instantiates the EfficientNetB6 architecture. — application_efficientnet_b6","text":"Instantiates EfficientNetB6 architecture.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_b6.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the EfficientNetB6 architecture. — application_efficientnet_b6","text":"","code":"application_efficientnet_b6(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\",   ... )"},{"path":"https://keras.posit.co/reference/application_efficientnet_b6.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the EfficientNetB6 architecture. — application_efficientnet_b6","text":"include_top Whether include fully-connected layer top network. Defaults TRUE. weights One NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. Defaults \"imagenet\". input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. input_shape Optional shape tuple, specified include_top FALSE. exactly 3 inputs channels. pooling Optional pooling mode feature extraction include_top FALSE. Defaults NULL. NULL means output model 4D tensor output last convolutional layer. avg means global average pooling applied output last convolutional layer, thus output model 2D tensor. max means global max pooling applied. classes Optional number classes classify images , specified include_top TRUE, weights argument specified. 1000 many ImageNet classes . Defaults 1000. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. Defaults 'softmax'. loading pretrained weights, classifier_activation can NULL \"softmax\". ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_b6.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the EfficientNetB6 architecture. — application_efficientnet_b6","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_efficientnet_b6.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the EfficientNetB6 architecture. — application_efficientnet_b6","text":"EfficientNet: Rethinking Model Scaling Convolutional Neural Networks (ICML 2019) function returns Keras image classification model, optionally loaded weights pre-trained ImageNet. image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_b6.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the EfficientNetB6 architecture. — application_efficientnet_b6","text":"Keras Application expects specific kind input preprocessing. EfficientNet, input preprocessing included part model (Rescaling layer), thus keras.applications.efficientnet.preprocess_input actually pass-function. EfficientNet models expect inputs float tensors pixels values [0-255] range.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_efficientnet_b7.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the EfficientNetB7 architecture. — application_efficientnet_b7","title":"Instantiates the EfficientNetB7 architecture. — application_efficientnet_b7","text":"Instantiates EfficientNetB7 architecture.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_b7.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the EfficientNetB7 architecture. — application_efficientnet_b7","text":"","code":"application_efficientnet_b7(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\",   ... )"},{"path":"https://keras.posit.co/reference/application_efficientnet_b7.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the EfficientNetB7 architecture. — application_efficientnet_b7","text":"include_top Whether include fully-connected layer top network. Defaults TRUE. weights One NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. Defaults \"imagenet\". input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. input_shape Optional shape tuple, specified include_top FALSE. exactly 3 inputs channels. pooling Optional pooling mode feature extraction include_top FALSE. Defaults NULL. NULL means output model 4D tensor output last convolutional layer. avg means global average pooling applied output last convolutional layer, thus output model 2D tensor. max means global max pooling applied. classes Optional number classes classify images , specified include_top TRUE, weights argument specified. 1000 many ImageNet classes . Defaults 1000. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. Defaults 'softmax'. loading pretrained weights, classifier_activation can NULL \"softmax\". ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_b7.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the EfficientNetB7 architecture. — application_efficientnet_b7","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_efficientnet_b7.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the EfficientNetB7 architecture. — application_efficientnet_b7","text":"EfficientNet: Rethinking Model Scaling Convolutional Neural Networks (ICML 2019) function returns Keras image classification model, optionally loaded weights pre-trained ImageNet. image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_b7.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the EfficientNetB7 architecture. — application_efficientnet_b7","text":"Keras Application expects specific kind input preprocessing. EfficientNet, input preprocessing included part model (Rescaling layer), thus keras.applications.efficientnet.preprocess_input actually pass-function. EfficientNet models expect inputs float tensors pixels values [0-255] range.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_efficientnet_v2b0.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the EfficientNetV2B0 architecture. — application_efficientnet_v2b0","title":"Instantiates the EfficientNetV2B0 architecture. — application_efficientnet_v2b0","text":"Instantiates EfficientNetV2B0 architecture.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_v2b0.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the EfficientNetV2B0 architecture. — application_efficientnet_v2b0","text":"","code":"application_efficientnet_v2b0(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\",   include_preprocessing = TRUE )"},{"path":"https://keras.posit.co/reference/application_efficientnet_v2b0.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the EfficientNetV2B0 architecture. — application_efficientnet_v2b0","text":"include_top Boolean, whether include fully-connected layer top network. Defaults TRUE. weights One NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. Defaults \"imagenet\". input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. input_shape Optional shape tuple, specified include_top FALSE. exactly 3 inputs channels. pooling Optional pooling mode feature extraction include_top FALSE. Defaults NULL. NULL means output model 4D tensor output last convolutional layer. \"avg\" means global average pooling applied output last convolutional layer, thus output model 2D tensor. \"max\" means global max pooling applied. classes Optional number classes classify images , specified include_top TRUE, weights argument specified. Defaults 1000 (number ImageNet classes). classifier_activation string callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. Defaults \"softmax\". loading pretrained weights, classifier_activation can NULL \"softmax\". include_preprocessing Boolean, whether include preprocessing layer bottom network.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_v2b0.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the EfficientNetV2B0 architecture. — application_efficientnet_v2b0","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_efficientnet_v2b0.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the EfficientNetV2B0 architecture. — application_efficientnet_v2b0","text":"EfficientNetV2: Smaller Models Faster Training (ICML 2021) function returns Keras image classification model, optionally loaded weights pre-trained ImageNet. image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_v2b0.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the EfficientNetV2B0 architecture. — application_efficientnet_v2b0","text":"Keras Application expects specific kind input preprocessing. EfficientNetV2, default input preprocessing included part model (Rescaling layer), thus keras.applications.efficientnet_v2.preprocess_input actually pass-function. use case, EfficientNetV2 models expect inputs float tensors pixels values [0, 255] range. time, preprocessing part model (.e. Rescaling layer) can disabled setting include_preprocessing argument FALSE. preprocessing disabled EfficientNetV2 models expect inputs float tensors pixels values [-1, 1] range.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_efficientnet_v2b1.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the EfficientNetV2B1 architecture. — application_efficientnet_v2b1","title":"Instantiates the EfficientNetV2B1 architecture. — application_efficientnet_v2b1","text":"Instantiates EfficientNetV2B1 architecture.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_v2b1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the EfficientNetV2B1 architecture. — application_efficientnet_v2b1","text":"","code":"application_efficientnet_v2b1(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\",   include_preprocessing = TRUE )"},{"path":"https://keras.posit.co/reference/application_efficientnet_v2b1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the EfficientNetV2B1 architecture. — application_efficientnet_v2b1","text":"include_top Boolean, whether include fully-connected layer top network. Defaults TRUE. weights One NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. Defaults \"imagenet\". input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. input_shape Optional shape tuple, specified include_top FALSE. exactly 3 inputs channels. pooling Optional pooling mode feature extraction include_top FALSE. Defaults NULL. NULL means output model 4D tensor output last convolutional layer. \"avg\" means global average pooling applied output last convolutional layer, thus output model 2D tensor. \"max\" means global max pooling applied. classes Optional number classes classify images , specified include_top TRUE, weights argument specified. Defaults 1000 (number ImageNet classes). classifier_activation string callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. Defaults \"softmax\". loading pretrained weights, classifier_activation can NULL \"softmax\". include_preprocessing Boolean, whether include preprocessing layer bottom network.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_v2b1.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the EfficientNetV2B1 architecture. — application_efficientnet_v2b1","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_efficientnet_v2b1.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the EfficientNetV2B1 architecture. — application_efficientnet_v2b1","text":"EfficientNetV2: Smaller Models Faster Training (ICML 2021) function returns Keras image classification model, optionally loaded weights pre-trained ImageNet. image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_v2b1.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the EfficientNetV2B1 architecture. — application_efficientnet_v2b1","text":"Keras Application expects specific kind input preprocessing. EfficientNetV2, default input preprocessing included part model (Rescaling layer), thus keras.applications.efficientnet_v2.preprocess_input actually pass-function. use case, EfficientNetV2 models expect inputs float tensors pixels values [0, 255] range. time, preprocessing part model (.e. Rescaling layer) can disabled setting include_preprocessing argument FALSE. preprocessing disabled EfficientNetV2 models expect inputs float tensors pixels values [-1, 1] range.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_efficientnet_v2b2.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the EfficientNetV2B2 architecture. — application_efficientnet_v2b2","title":"Instantiates the EfficientNetV2B2 architecture. — application_efficientnet_v2b2","text":"Instantiates EfficientNetV2B2 architecture.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_v2b2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the EfficientNetV2B2 architecture. — application_efficientnet_v2b2","text":"","code":"application_efficientnet_v2b2(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\",   include_preprocessing = TRUE )"},{"path":"https://keras.posit.co/reference/application_efficientnet_v2b2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the EfficientNetV2B2 architecture. — application_efficientnet_v2b2","text":"include_top Boolean, whether include fully-connected layer top network. Defaults TRUE. weights One NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. Defaults \"imagenet\". input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. input_shape Optional shape tuple, specified include_top FALSE. exactly 3 inputs channels. pooling Optional pooling mode feature extraction include_top FALSE. Defaults NULL. NULL means output model 4D tensor output last convolutional layer. \"avg\" means global average pooling applied output last convolutional layer, thus output model 2D tensor. \"max\" means global max pooling applied. classes Optional number classes classify images , specified include_top TRUE, weights argument specified. Defaults 1000 (number ImageNet classes). classifier_activation string callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. Defaults \"softmax\". loading pretrained weights, classifier_activation can NULL \"softmax\". include_preprocessing Boolean, whether include preprocessing layer bottom network.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_v2b2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the EfficientNetV2B2 architecture. — application_efficientnet_v2b2","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_efficientnet_v2b2.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the EfficientNetV2B2 architecture. — application_efficientnet_v2b2","text":"EfficientNetV2: Smaller Models Faster Training (ICML 2021) function returns Keras image classification model, optionally loaded weights pre-trained ImageNet. image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_v2b2.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the EfficientNetV2B2 architecture. — application_efficientnet_v2b2","text":"Keras Application expects specific kind input preprocessing. EfficientNetV2, default input preprocessing included part model (Rescaling layer), thus keras.applications.efficientnet_v2.preprocess_input actually pass-function. use case, EfficientNetV2 models expect inputs float tensors pixels values [0, 255] range. time, preprocessing part model (.e. Rescaling layer) can disabled setting include_preprocessing argument FALSE. preprocessing disabled EfficientNetV2 models expect inputs float tensors pixels values [-1, 1] range.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_efficientnet_v2b3.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the EfficientNetV2B3 architecture. — application_efficientnet_v2b3","title":"Instantiates the EfficientNetV2B3 architecture. — application_efficientnet_v2b3","text":"Instantiates EfficientNetV2B3 architecture.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_v2b3.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the EfficientNetV2B3 architecture. — application_efficientnet_v2b3","text":"","code":"application_efficientnet_v2b3(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\",   include_preprocessing = TRUE )"},{"path":"https://keras.posit.co/reference/application_efficientnet_v2b3.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the EfficientNetV2B3 architecture. — application_efficientnet_v2b3","text":"include_top Boolean, whether include fully-connected layer top network. Defaults TRUE. weights One NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. Defaults \"imagenet\". input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. input_shape Optional shape tuple, specified include_top FALSE. exactly 3 inputs channels. pooling Optional pooling mode feature extraction include_top FALSE. Defaults NULL. NULL means output model 4D tensor output last convolutional layer. \"avg\" means global average pooling applied output last convolutional layer, thus output model 2D tensor. \"max\" means global max pooling applied. classes Optional number classes classify images , specified include_top TRUE, weights argument specified. Defaults 1000 (number ImageNet classes). classifier_activation string callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. Defaults \"softmax\". loading pretrained weights, classifier_activation can NULL \"softmax\". include_preprocessing Boolean, whether include preprocessing layer bottom network.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_v2b3.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the EfficientNetV2B3 architecture. — application_efficientnet_v2b3","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_efficientnet_v2b3.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the EfficientNetV2B3 architecture. — application_efficientnet_v2b3","text":"EfficientNetV2: Smaller Models Faster Training (ICML 2021) function returns Keras image classification model, optionally loaded weights pre-trained ImageNet. image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_v2b3.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the EfficientNetV2B3 architecture. — application_efficientnet_v2b3","text":"Keras Application expects specific kind input preprocessing. EfficientNetV2, default input preprocessing included part model (Rescaling layer), thus keras.applications.efficientnet_v2.preprocess_input actually pass-function. use case, EfficientNetV2 models expect inputs float tensors pixels values [0, 255] range. time, preprocessing part model (.e. Rescaling layer) can disabled setting include_preprocessing argument FALSE. preprocessing disabled EfficientNetV2 models expect inputs float tensors pixels values [-1, 1] range.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_efficientnet_v2l.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the EfficientNetV2L architecture. — application_efficientnet_v2l","title":"Instantiates the EfficientNetV2L architecture. — application_efficientnet_v2l","text":"Instantiates EfficientNetV2L architecture.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_v2l.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the EfficientNetV2L architecture. — application_efficientnet_v2l","text":"","code":"application_efficientnet_v2l(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\",   include_preprocessing = TRUE )"},{"path":"https://keras.posit.co/reference/application_efficientnet_v2l.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the EfficientNetV2L architecture. — application_efficientnet_v2l","text":"include_top Boolean, whether include fully-connected layer top network. Defaults TRUE. weights One NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. Defaults \"imagenet\". input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. input_shape Optional shape tuple, specified include_top FALSE. exactly 3 inputs channels. pooling Optional pooling mode feature extraction include_top FALSE. Defaults NULL. NULL means output model 4D tensor output last convolutional layer. \"avg\" means global average pooling applied output last convolutional layer, thus output model 2D tensor. \"max\" means global max pooling applied. classes Optional number classes classify images , specified include_top TRUE, weights argument specified. Defaults 1000 (number ImageNet classes). classifier_activation string callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. Defaults \"softmax\". loading pretrained weights, classifier_activation can NULL \"softmax\". include_preprocessing Boolean, whether include preprocessing layer bottom network.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_v2l.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the EfficientNetV2L architecture. — application_efficientnet_v2l","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_efficientnet_v2l.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the EfficientNetV2L architecture. — application_efficientnet_v2l","text":"EfficientNetV2: Smaller Models Faster Training (ICML 2021) function returns Keras image classification model, optionally loaded weights pre-trained ImageNet. image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_v2l.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the EfficientNetV2L architecture. — application_efficientnet_v2l","text":"Keras Application expects specific kind input preprocessing. EfficientNetV2, default input preprocessing included part model (Rescaling layer), thus keras.applications.efficientnet_v2.preprocess_input actually pass-function. use case, EfficientNetV2 models expect inputs float tensors pixels values [0, 255] range. time, preprocessing part model (.e. Rescaling layer) can disabled setting include_preprocessing argument FALSE. preprocessing disabled EfficientNetV2 models expect inputs float tensors pixels values [-1, 1] range.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_efficientnet_v2m.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the EfficientNetV2M architecture. — application_efficientnet_v2m","title":"Instantiates the EfficientNetV2M architecture. — application_efficientnet_v2m","text":"Instantiates EfficientNetV2M architecture.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_v2m.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the EfficientNetV2M architecture. — application_efficientnet_v2m","text":"","code":"application_efficientnet_v2m(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\",   include_preprocessing = TRUE )"},{"path":"https://keras.posit.co/reference/application_efficientnet_v2m.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the EfficientNetV2M architecture. — application_efficientnet_v2m","text":"include_top Boolean, whether include fully-connected layer top network. Defaults TRUE. weights One NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. Defaults \"imagenet\". input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. input_shape Optional shape tuple, specified include_top FALSE. exactly 3 inputs channels. pooling Optional pooling mode feature extraction include_top FALSE. Defaults NULL. NULL means output model 4D tensor output last convolutional layer. \"avg\" means global average pooling applied output last convolutional layer, thus output model 2D tensor. \"max\" means global max pooling applied. classes Optional number classes classify images , specified include_top TRUE, weights argument specified. Defaults 1000 (number ImageNet classes). classifier_activation string callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. Defaults \"softmax\". loading pretrained weights, classifier_activation can NULL \"softmax\". include_preprocessing Boolean, whether include preprocessing layer bottom network.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_v2m.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the EfficientNetV2M architecture. — application_efficientnet_v2m","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_efficientnet_v2m.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the EfficientNetV2M architecture. — application_efficientnet_v2m","text":"EfficientNetV2: Smaller Models Faster Training (ICML 2021) function returns Keras image classification model, optionally loaded weights pre-trained ImageNet. image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_v2m.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the EfficientNetV2M architecture. — application_efficientnet_v2m","text":"Keras Application expects specific kind input preprocessing. EfficientNetV2, default input preprocessing included part model (Rescaling layer), thus keras.applications.efficientnet_v2.preprocess_input actually pass-function. use case, EfficientNetV2 models expect inputs float tensors pixels values [0, 255] range. time, preprocessing part model (.e. Rescaling layer) can disabled setting include_preprocessing argument FALSE. preprocessing disabled EfficientNetV2 models expect inputs float tensors pixels values [-1, 1] range.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_efficientnet_v2s.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the EfficientNetV2S architecture. — application_efficientnet_v2s","title":"Instantiates the EfficientNetV2S architecture. — application_efficientnet_v2s","text":"Instantiates EfficientNetV2S architecture.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_v2s.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the EfficientNetV2S architecture. — application_efficientnet_v2s","text":"","code":"application_efficientnet_v2s(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\",   include_preprocessing = TRUE )"},{"path":"https://keras.posit.co/reference/application_efficientnet_v2s.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the EfficientNetV2S architecture. — application_efficientnet_v2s","text":"include_top Boolean, whether include fully-connected layer top network. Defaults TRUE. weights One NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. Defaults \"imagenet\". input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. input_shape Optional shape tuple, specified include_top FALSE. exactly 3 inputs channels. pooling Optional pooling mode feature extraction include_top FALSE. Defaults NULL. NULL means output model 4D tensor output last convolutional layer. \"avg\" means global average pooling applied output last convolutional layer, thus output model 2D tensor. \"max\" means global max pooling applied. classes Optional number classes classify images , specified include_top TRUE, weights argument specified. Defaults 1000 (number ImageNet classes). classifier_activation string callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. Defaults \"softmax\". loading pretrained weights, classifier_activation can NULL \"softmax\". include_preprocessing Boolean, whether include preprocessing layer bottom network.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_v2s.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the EfficientNetV2S architecture. — application_efficientnet_v2s","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_efficientnet_v2s.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the EfficientNetV2S architecture. — application_efficientnet_v2s","text":"EfficientNetV2: Smaller Models Faster Training (ICML 2021) function returns Keras image classification model, optionally loaded weights pre-trained ImageNet. image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_efficientnet_v2s.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the EfficientNetV2S architecture. — application_efficientnet_v2s","text":"Keras Application expects specific kind input preprocessing. EfficientNetV2, default input preprocessing included part model (Rescaling layer), thus keras.applications.efficientnet_v2.preprocess_input actually pass-function. use case, EfficientNetV2 models expect inputs float tensors pixels values [0, 255] range. time, preprocessing part model (.e. Rescaling layer) can disabled setting include_preprocessing argument FALSE. preprocessing disabled EfficientNetV2 models expect inputs float tensors pixels values [-1, 1] range.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_inception_resnet_v2.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the Inception-ResNet v2 architecture. — application_inception_resnet_v2","title":"Instantiates the Inception-ResNet v2 architecture. — application_inception_resnet_v2","text":"Instantiates Inception-ResNet v2 architecture.","code":""},{"path":"https://keras.posit.co/reference/application_inception_resnet_v2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the Inception-ResNet v2 architecture. — application_inception_resnet_v2","text":"","code":"application_inception_resnet_v2(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\" )"},{"path":"https://keras.posit.co/reference/application_inception_resnet_v2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the Inception-ResNet v2 architecture. — application_inception_resnet_v2","text":"include_top whether include fully-connected layer top network. weights one NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. input_tensor optional Keras tensor (.e. output layers.Input()) use image input model. input_shape optional shape tuple, specified include_top FALSE (otherwise input shape (299, 299, 3) ('channels_last' data format) (3, 299, 299) ('channels_first' data format). exactly 3 inputs channels, width height smaller 75. E.g. (150, 150, 3) one valid value. pooling Optional pooling mode feature extraction include_top FALSE. NULL means output model 4D tensor output last convolutional block. 'avg' means global average pooling applied output last convolutional block, thus output model 2D tensor. 'max' means global max pooling applied. classes optional number classes classify images , specified include_top TRUE, weights argument specified. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. loading pretrained weights, classifier_activation can NULL \"softmax\".","code":""},{"path":"https://keras.posit.co/reference/application_inception_resnet_v2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the Inception-ResNet v2 architecture. — application_inception_resnet_v2","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_inception_resnet_v2.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the Inception-ResNet v2 architecture. — application_inception_resnet_v2","text":"Inception-v4, Inception-ResNet Impact Residual Connections Learning (AAAI 2017) function returns Keras image classification model, optionally loaded weights pre-trained ImageNet. image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_inception_resnet_v2.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the Inception-ResNet v2 architecture. — application_inception_resnet_v2","text":"Keras Application expects specific kind input preprocessing. InceptionResNetV2, call keras.applications.inception_resnet_v2.preprocess_input inputs passing model. inception_resnet_v2.preprocess_input scale input pixels -1 1.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_inception_v3.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the Inception v3 architecture. — application_inception_v3","title":"Instantiates the Inception v3 architecture. — application_inception_v3","text":"Instantiates Inception v3 architecture.","code":""},{"path":"https://keras.posit.co/reference/application_inception_v3.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the Inception v3 architecture. — application_inception_v3","text":"","code":"application_inception_v3(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\" )"},{"path":"https://keras.posit.co/reference/application_inception_v3.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the Inception v3 architecture. — application_inception_v3","text":"include_top Boolean, whether include fully-connected layer top, last layer network. Defaults TRUE. weights One NULL (random initialization), imagenet (pre-training ImageNet), path weights file loaded. Defaults \"imagenet\". input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. input_tensor useful sharing inputs multiple different networks. Defaults NULL. input_shape Optional shape tuple, specified include_top FALSE (otherwise input shape (299, 299, 3) (channels_last data format) (3, 299, 299) (channels_first data format). exactly 3 inputs channels, width height smaller 75. E.g. (150, 150, 3) one valid value. input_shape ignored input_tensor provided. pooling Optional pooling mode feature extraction include_top FALSE. NULL (default) means output model 4D tensor output last convolutional block. avg means global average pooling applied output last convolutional block, thus output model 2D tensor. max means global max pooling applied. classes optional number classes classify images , specified include_top TRUE, weights argument specified. Defaults 1000. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. loading pretrained weights, classifier_activation can NULL \"softmax\".","code":""},{"path":"https://keras.posit.co/reference/application_inception_v3.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the Inception v3 architecture. — application_inception_v3","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_inception_v3.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the Inception v3 architecture. — application_inception_v3","text":"Rethinking Inception Architecture Computer Vision (CVPR 2016) function returns Keras image classification model, optionally loaded weights pre-trained ImageNet. image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_inception_v3.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the Inception v3 architecture. — application_inception_v3","text":"Keras Application expects specific kind input preprocessing. InceptionV3, call keras.applications.inception_v3.preprocess_input inputs passing model. inception_v3.preprocess_input scale input pixels -1 1.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_mobilenet.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the MobileNet architecture. — application_mobilenet","title":"Instantiates the MobileNet architecture. — application_mobilenet","text":"Instantiates MobileNet architecture.","code":""},{"path":"https://keras.posit.co/reference/application_mobilenet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the MobileNet architecture. — application_mobilenet","text":"","code":"application_mobilenet(   input_shape = NULL,   alpha = 1,   depth_multiplier = 1L,   dropout = 0.001,   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\" )"},{"path":"https://keras.posit.co/reference/application_mobilenet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the MobileNet architecture. — application_mobilenet","text":"input_shape Optional shape tuple, specified include_top FALSE (otherwise input shape (224, 224, 3) (\"channels_last\" data format) (3, 224, 224) (\"channels_first\" data format). exactly 3 inputs channels, width height smaller 32. E.g. (200, 200, 3) one valid value. Defaults NULL. input_shape ignored input_tensor provided. alpha Controls width network. known width multiplier MobileNet paper. alpha < 1.0, proportionally decreases number filters layer. alpha > 1.0, proportionally increases number filters layer. alpha == 1, default number filters paper used layer. Defaults 1.0. depth_multiplier Depth multiplier depthwise convolution. called resolution multiplier MobileNet paper. Defaults 1.0. dropout Dropout rate. Defaults 0.001. include_top Boolean, whether include fully-connected layer top network. Defaults TRUE. weights One NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. Defaults \"imagenet\". input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. input_tensor useful sharing inputs multiple different networks. Defaults NULL. pooling Optional pooling mode feature extraction include_top FALSE. NULL (default) means output model 4D tensor output last convolutional block. avg means global average pooling applied output last convolutional block, thus output model 2D tensor. max means global max pooling applied. classes Optional number classes classify images , specified include_top TRUE, weights argument specified. Defaults 1000. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. loading pretrained weights, classifier_activation can NULL \"softmax\".","code":""},{"path":"https://keras.posit.co/reference/application_mobilenet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the MobileNet architecture. — application_mobilenet","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_mobilenet.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the MobileNet architecture. — application_mobilenet","text":"MobileNets: Efficient Convolutional Neural Networks Mobile Vision Applications function returns Keras image classification model, optionally loaded weights pre-trained ImageNet. image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_mobilenet.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the MobileNet architecture. — application_mobilenet","text":"Keras Application expects specific kind input preprocessing. MobileNet, call keras.applications.mobilenet.preprocess_input inputs passing model. mobilenet.preprocess_input scale input pixels -1 1.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_mobilenet_v2.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the MobileNetV2 architecture. — application_mobilenet_v2","title":"Instantiates the MobileNetV2 architecture. — application_mobilenet_v2","text":"MobileNetV2 similar original MobileNet, except uses inverted residual blocks bottlenecking features. drastically lower parameter count original MobileNet. MobileNets support input size greater 32 x 32, larger image sizes offering better performance.","code":""},{"path":"https://keras.posit.co/reference/application_mobilenet_v2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the MobileNetV2 architecture. — application_mobilenet_v2","text":"","code":"application_mobilenet_v2(   input_shape = NULL,   alpha = 1,   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\" )"},{"path":"https://keras.posit.co/reference/application_mobilenet_v2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the MobileNetV2 architecture. — application_mobilenet_v2","text":"input_shape Optional shape tuple, specified include_top FALSE (otherwise input shape (224, 224, 3) (\"channels_last\" data format) (3, 224, 224) (\"channels_first\" data format). exactly 3 inputs channels, width height smaller 32. E.g. (200, 200, 3) one valid value. Defaults NULL. input_shape ignored input_tensor provided. alpha Controls width network. known width multiplier MobileNet paper. alpha < 1.0, proportionally decreases number filters layer. alpha > 1.0, proportionally increases number filters layer. alpha == 1, default number filters paper used layer. Defaults 1.0. include_top Boolean, whether include fully-connected layer top network. Defaults TRUE. weights One NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. Defaults \"imagenet\". input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. input_tensor useful sharing inputs multiple different networks. Defaults NULL. pooling Optional pooling mode feature extraction include_top FALSE. NULL (default) means output model 4D tensor output last convolutional block. avg means global average pooling applied output last convolutional block, thus output model 2D tensor. max means global max pooling applied. classes Optional number classes classify images , specified include_top TRUE, weights argument specified. Defaults 1000. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. loading pretrained weights, classifier_activation can NULL \"softmax\".","code":""},{"path":"https://keras.posit.co/reference/application_mobilenet_v2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the MobileNetV2 architecture. — application_mobilenet_v2","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_mobilenet_v2.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the MobileNetV2 architecture. — application_mobilenet_v2","text":"MobileNetV2: Inverted Residuals Linear Bottlenecks (CVPR 2018) function returns Keras image classification model, optionally loaded weights pre-trained ImageNet. image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_mobilenet_v2.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the MobileNetV2 architecture. — application_mobilenet_v2","text":"Keras Application expects specific kind input preprocessing. MobileNetV2, call keras.applications.mobilenet_v2.preprocess_input inputs passing model. mobilenet_v2.preprocess_input scale input pixels -1 1.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_mobilenet_v3_large.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the MobileNetV3Large architecture. — application_mobilenet_v3_large","title":"Instantiates the MobileNetV3Large architecture. — application_mobilenet_v3_large","text":"Instantiates MobileNetV3Large architecture.","code":""},{"path":"https://keras.posit.co/reference/application_mobilenet_v3_large.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the MobileNetV3Large architecture. — application_mobilenet_v3_large","text":"","code":"application_mobilenet_v3_large(   input_shape = NULL,   alpha = 1,   minimalistic = FALSE,   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   classes = 1000L,   pooling = NULL,   dropout_rate = 0.2,   classifier_activation = \"softmax\",   include_preprocessing = TRUE )"},{"path":"https://keras.posit.co/reference/application_mobilenet_v3_large.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the MobileNetV3Large architecture. — application_mobilenet_v3_large","text":"input_shape Optional shape tuple, specified like use model input image resolution (224, 224, 3). exactly 3 inputs channels. can also omit option like infer input_shape input_tensor. choose include input_tensor input_shape input_shape used match, shapes match throw error. E.g. (160, 160, 3) one valid value. alpha controls width network. known depth multiplier MobileNetV3 paper, name kept consistency MobileNetV1 Keras. alpha < 1.0, proportionally decreases number filters layer. alpha > 1.0, proportionally increases number filters layer. alpha == 1, default number filters paper used layer. minimalistic addition large small models module also contains -called minimalistic models, models per-layer dimensions characteristic MobilenetV3 however, utilize advanced blocks (squeeze--excite units, hard-swish, 5x5 convolutions). models less efficient CPU, much performant GPU/DSP. include_top Boolean, whether include fully-connected layer top network. Defaults TRUE. weights String, one NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. classes Integer, optional number classes classify images , specified include_top TRUE, weights argument specified. pooling String, optional pooling mode feature extraction include_top FALSE. NULL means output model 4D tensor output last convolutional block. avg means global average pooling applied output last convolutional block, thus output model 2D tensor. max means global max pooling applied. dropout_rate fraction input units drop last layer. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. loading pretrained weights, classifier_activation can NULL \"softmax\". include_preprocessing Boolean, whether include preprocessing layer (Rescaling) bottom network. Defaults TRUE.","code":""},{"path":"https://keras.posit.co/reference/application_mobilenet_v3_large.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the MobileNetV3Large architecture. — application_mobilenet_v3_large","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_mobilenet_v3_large.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the MobileNetV3Large architecture. — application_mobilenet_v3_large","text":"Searching MobileNetV3 (ICCV 2019)","code":""},{"path":"https://keras.posit.co/reference/application_mobilenet_v3_large.html","id":"the-following-table-describes-the-performance-of-mobilenets-v-","dir":"Reference","previous_headings":"","what":"The following table describes the performance of MobileNets v3:","title":"Instantiates the MobileNetV3Large architecture. — application_mobilenet_v3_large","text":"MACs stands Multiply Adds image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_mobilenet_v3_large.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the MobileNetV3Large architecture. — application_mobilenet_v3_large","text":"Keras Application expects specific kind input preprocessing. MobileNetV3, default input preprocessing included part model (Rescaling layer), thus keras.applications.mobilenet_v3.preprocess_input actually pass-function. use case, MobileNetV3 models expect inputs float tensors pixels values [0-255] range. time, preprocessing part model (.e. Rescaling layer) can disabled setting include_preprocessing argument FALSE. preprocessing disabled MobileNetV3 models expect inputs float tensors pixels values [-1, 1] range.","code":""},{"path":"https://keras.posit.co/reference/application_mobilenet_v3_large.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"Instantiates the MobileNetV3Large architecture. — application_mobilenet_v3_large","text":"inputs: floating point numpy.array backend-native tensor, 4D 3 color channels, values range [0, 255] include_preprocessing TRUE range [-1, 1] otherwise.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_mobilenet_v3_small.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the MobileNetV3Small architecture. — application_mobilenet_v3_small","title":"Instantiates the MobileNetV3Small architecture. — application_mobilenet_v3_small","text":"Instantiates MobileNetV3Small architecture.","code":""},{"path":"https://keras.posit.co/reference/application_mobilenet_v3_small.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the MobileNetV3Small architecture. — application_mobilenet_v3_small","text":"","code":"application_mobilenet_v3_small(   input_shape = NULL,   alpha = 1,   minimalistic = FALSE,   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   classes = 1000L,   pooling = NULL,   dropout_rate = 0.2,   classifier_activation = \"softmax\",   include_preprocessing = TRUE )"},{"path":"https://keras.posit.co/reference/application_mobilenet_v3_small.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the MobileNetV3Small architecture. — application_mobilenet_v3_small","text":"input_shape Optional shape tuple, specified like use model input image resolution (224, 224, 3). exactly 3 inputs channels. can also omit option like infer input_shape input_tensor. choose include input_tensor input_shape input_shape used match, shapes match throw error. E.g. (160, 160, 3) one valid value. alpha controls width network. known depth multiplier MobileNetV3 paper, name kept consistency MobileNetV1 Keras. alpha < 1.0, proportionally decreases number filters layer. alpha > 1.0, proportionally increases number filters layer. alpha == 1, default number filters paper used layer. minimalistic addition large small models module also contains -called minimalistic models, models per-layer dimensions characteristic MobilenetV3 however, utilize advanced blocks (squeeze--excite units, hard-swish, 5x5 convolutions). models less efficient CPU, much performant GPU/DSP. include_top Boolean, whether include fully-connected layer top network. Defaults TRUE. weights String, one NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. classes Integer, optional number classes classify images , specified include_top TRUE, weights argument specified. pooling String, optional pooling mode feature extraction include_top FALSE. NULL means output model 4D tensor output last convolutional block. avg means global average pooling applied output last convolutional block, thus output model 2D tensor. max means global max pooling applied. dropout_rate fraction input units drop last layer. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. loading pretrained weights, classifier_activation can NULL \"softmax\". include_preprocessing Boolean, whether include preprocessing layer (Rescaling) bottom network. Defaults TRUE.","code":""},{"path":"https://keras.posit.co/reference/application_mobilenet_v3_small.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the MobileNetV3Small architecture. — application_mobilenet_v3_small","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_mobilenet_v3_small.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the MobileNetV3Small architecture. — application_mobilenet_v3_small","text":"Searching MobileNetV3 (ICCV 2019)","code":""},{"path":"https://keras.posit.co/reference/application_mobilenet_v3_small.html","id":"the-following-table-describes-the-performance-of-mobilenets-v-","dir":"Reference","previous_headings":"","what":"The following table describes the performance of MobileNets v3:","title":"Instantiates the MobileNetV3Small architecture. — application_mobilenet_v3_small","text":"MACs stands Multiply Adds image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_mobilenet_v3_small.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the MobileNetV3Small architecture. — application_mobilenet_v3_small","text":"Keras Application expects specific kind input preprocessing. MobileNetV3, default input preprocessing included part model (Rescaling layer), thus keras.applications.mobilenet_v3.preprocess_input actually pass-function. use case, MobileNetV3 models expect inputs float tensors pixels values [0-255] range. time, preprocessing part model (.e. Rescaling layer) can disabled setting include_preprocessing argument FALSE. preprocessing disabled MobileNetV3 models expect inputs float tensors pixels values [-1, 1] range.","code":""},{"path":"https://keras.posit.co/reference/application_mobilenet_v3_small.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"Instantiates the MobileNetV3Small architecture. — application_mobilenet_v3_small","text":"inputs: floating point numpy.array backend-native tensor, 4D 3 color channels, values range [0, 255] include_preprocessing TRUE range [-1, 1] otherwise.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_nasnetlarge.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates a NASNet model in ImageNet mode. — application_nasnetlarge","title":"Instantiates a NASNet model in ImageNet mode. — application_nasnetlarge","text":"Instantiates NASNet model ImageNet mode.","code":""},{"path":"https://keras.posit.co/reference/application_nasnetlarge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates a NASNet model in ImageNet mode. — application_nasnetlarge","text":"","code":"application_nasnetlarge(   input_shape = NULL,   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\" )"},{"path":"https://keras.posit.co/reference/application_nasnetlarge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates a NASNet model in ImageNet mode. — application_nasnetlarge","text":"input_shape Optional shape tuple, specified include_top FALSE (otherwise input shape (331, 331, 3) NASNetLarge. exactly 3 inputs channels, width height smaller 32. E.g. (224, 224, 3) one valid value. include_top Whether include fully-connected layer top network. weights NULL (random initialization) imagenet (ImageNet weights).  loading imagenet weights, input_shape (331, 331, 3) input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. pooling Optional pooling mode feature extraction include_top FALSE. NULL means output model 4D tensor output last convolutional layer. avg means global average pooling applied output last convolutional layer, thus output model 2D tensor. max means global max pooling applied. classes Optional number classes classify images , specified include_top TRUE, weights argument specified. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer.  loading pretrained weights, classifier_activation can NULL \"softmax\".","code":""},{"path":"https://keras.posit.co/reference/application_nasnetlarge.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates a NASNet model in ImageNet mode. — application_nasnetlarge","text":"","code":"A Keras model instance."},{"path":"https://keras.posit.co/reference/application_nasnetlarge.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates a NASNet model in ImageNet mode. — application_nasnetlarge","text":"Learning Transferable Architectures Scalable Image Recognition (CVPR 2018) Optionally loads weights pre-trained ImageNet. Note data format convention used model one specified Keras config ~/.keras/keras.json.","code":""},{"path":"https://keras.posit.co/reference/application_nasnetlarge.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates a NASNet model in ImageNet mode. — application_nasnetlarge","text":"Keras Application expects specific kind input preprocessing. NASNet, call keras.applications.nasnet.preprocess_input inputs passing model.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_nasnetmobile.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates a Mobile NASNet model in ImageNet mode. — application_nasnetmobile","title":"Instantiates a Mobile NASNet model in ImageNet mode. — application_nasnetmobile","text":"Instantiates Mobile NASNet model ImageNet mode.","code":""},{"path":"https://keras.posit.co/reference/application_nasnetmobile.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates a Mobile NASNet model in ImageNet mode. — application_nasnetmobile","text":"","code":"application_nasnetmobile(   input_shape = NULL,   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\" )"},{"path":"https://keras.posit.co/reference/application_nasnetmobile.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates a Mobile NASNet model in ImageNet mode. — application_nasnetmobile","text":"input_shape Optional shape tuple, specified include_top FALSE (otherwise input shape (224, 224, 3) NASNetMobile exactly 3 inputs channels, width height smaller 32. E.g. (224, 224, 3) one valid value. include_top Whether include fully-connected layer top network. weights NULL (random initialization) imagenet (ImageNet weights). loading imagenet weights, input_shape (224, 224, 3) input_tensor Optional Keras tensor (.e. output layers.Input()) use image input model. pooling Optional pooling mode feature extraction include_top FALSE. NULL means output model 4D tensor output last convolutional layer. avg means global average pooling applied output last convolutional layer, thus output model 2D tensor. max means global max pooling applied. classes Optional number classes classify images , specified include_top TRUE, weights argument specified. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer.  loading pretrained weights, classifier_activation can NULL \"softmax\".","code":""},{"path":"https://keras.posit.co/reference/application_nasnetmobile.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates a Mobile NASNet model in ImageNet mode. — application_nasnetmobile","text":"","code":"A Keras model instance."},{"path":"https://keras.posit.co/reference/application_nasnetmobile.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates a Mobile NASNet model in ImageNet mode. — application_nasnetmobile","text":"Learning Transferable Architectures Scalable Image Recognition (CVPR 2018) Optionally loads weights pre-trained ImageNet. Note data format convention used model one specified Keras config ~/.keras/keras.json.","code":""},{"path":"https://keras.posit.co/reference/application_nasnetmobile.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates a Mobile NASNet model in ImageNet mode. — application_nasnetmobile","text":"Keras Application expects specific kind input preprocessing. NASNet, call keras.applications.nasnet.preprocess_input inputs passing model.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_resnet101.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the ResNet101 architecture. — application_resnet101","title":"Instantiates the ResNet101 architecture. — application_resnet101","text":"Instantiates ResNet101 architecture.","code":""},{"path":"https://keras.posit.co/reference/application_resnet101.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the ResNet101 architecture. — application_resnet101","text":"","code":"application_resnet101(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\" )"},{"path":"https://keras.posit.co/reference/application_resnet101.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the ResNet101 architecture. — application_resnet101","text":"include_top whether include fully-connected layer top network. weights one NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. input_tensor optional Keras tensor (.e. output layers.Input()) use image input model. input_shape optional shape tuple, specified include_top FALSE (otherwise input shape (224, 224, 3) (\"channels_last\" data format) (3, 224, 224) (\"channels_first\" data format). exactly 3 inputs channels, width height smaller 32. E.g. (200, 200, 3) one valid value. pooling Optional pooling mode feature extraction include_top FALSE. NULL means output model 4D tensor output last convolutional block. avg means global average pooling applied output last convolutional block, thus output model 2D tensor. max means global max pooling applied. classes optional number classes classify images , specified include_top TRUE, weights argument specified. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. loading pretrained weights, classifier_activation can NULL \"softmax\".","code":""},{"path":"https://keras.posit.co/reference/application_resnet101.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the ResNet101 architecture. — application_resnet101","text":"","code":"A Model instance."},{"path":"https://keras.posit.co/reference/application_resnet101.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the ResNet101 architecture. — application_resnet101","text":"Deep Residual Learning Image Recognition (CVPR 2015) image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_resnet101.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the ResNet101 architecture. — application_resnet101","text":"Keras Application expects specific kind input preprocessing. ResNet, call keras.applications.resnet.preprocess_input inputs passing model. resnet.preprocess_input convert input images RGB BGR, zero-center color channel respect ImageNet dataset, without scaling.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_resnet101_v2.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the ResNet101V2 architecture. — application_resnet101_v2","title":"Instantiates the ResNet101V2 architecture. — application_resnet101_v2","text":"Instantiates ResNet101V2 architecture.","code":""},{"path":"https://keras.posit.co/reference/application_resnet101_v2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the ResNet101V2 architecture. — application_resnet101_v2","text":"","code":"application_resnet101_v2(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\" )"},{"path":"https://keras.posit.co/reference/application_resnet101_v2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the ResNet101V2 architecture. — application_resnet101_v2","text":"include_top whether include fully-connected layer top network. weights one NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. input_tensor optional Keras tensor (.e. output layers.Input()) use image input model. input_shape optional shape tuple, specified include_top FALSE (otherwise input shape (224, 224, 3) (\"channels_last\" data format) (3, 224, 224) (\"channels_first\" data format). exactly 3 inputs channels, width height smaller 32. E.g. (200, 200, 3) one valid value. pooling Optional pooling mode feature extraction include_top FALSE. NULL means output model 4D tensor output last convolutional block. avg means global average pooling applied output last convolutional block, thus output model 2D tensor. max means global max pooling applied. classes optional number classes classify images , specified include_top TRUE, weights argument specified. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. loading pretrained weights, classifier_activation can NULL \"softmax\".","code":""},{"path":"https://keras.posit.co/reference/application_resnet101_v2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the ResNet101V2 architecture. — application_resnet101_v2","text":"","code":"A Model instance."},{"path":"https://keras.posit.co/reference/application_resnet101_v2.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the ResNet101V2 architecture. — application_resnet101_v2","text":"Identity Mappings Deep Residual Networks (CVPR 2016) image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_resnet101_v2.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the ResNet101V2 architecture. — application_resnet101_v2","text":"Keras Application expects specific kind input preprocessing. ResNet, call keras.applications.resnet_v2.preprocess_input inputs passing model. resnet_v2.preprocess_input scale input pixels -1 1.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_resnet152.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the ResNet152 architecture. — application_resnet152","title":"Instantiates the ResNet152 architecture. — application_resnet152","text":"Instantiates ResNet152 architecture.","code":""},{"path":"https://keras.posit.co/reference/application_resnet152.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the ResNet152 architecture. — application_resnet152","text":"","code":"application_resnet152(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\" )"},{"path":"https://keras.posit.co/reference/application_resnet152.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the ResNet152 architecture. — application_resnet152","text":"include_top whether include fully-connected layer top network. weights one NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. input_tensor optional Keras tensor (.e. output layers.Input()) use image input model. input_shape optional shape tuple, specified include_top FALSE (otherwise input shape (224, 224, 3) (\"channels_last\" data format) (3, 224, 224) (\"channels_first\" data format). exactly 3 inputs channels, width height smaller 32. E.g. (200, 200, 3) one valid value. pooling Optional pooling mode feature extraction include_top FALSE. NULL means output model 4D tensor output last convolutional block. avg means global average pooling applied output last convolutional block, thus output model 2D tensor. max means global max pooling applied. classes optional number classes classify images , specified include_top TRUE, weights argument specified. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. loading pretrained weights, classifier_activation can NULL \"softmax\".","code":""},{"path":"https://keras.posit.co/reference/application_resnet152.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the ResNet152 architecture. — application_resnet152","text":"","code":"A Model instance."},{"path":"https://keras.posit.co/reference/application_resnet152.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the ResNet152 architecture. — application_resnet152","text":"Deep Residual Learning Image Recognition (CVPR 2015) image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_resnet152.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the ResNet152 architecture. — application_resnet152","text":"Keras Application expects specific kind input preprocessing. ResNet, call keras.applications.resnet.preprocess_input inputs passing model. resnet.preprocess_input convert input images RGB BGR, zero-center color channel respect ImageNet dataset, without scaling.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_resnet152_v2.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the ResNet152V2 architecture. — application_resnet152_v2","title":"Instantiates the ResNet152V2 architecture. — application_resnet152_v2","text":"Instantiates ResNet152V2 architecture.","code":""},{"path":"https://keras.posit.co/reference/application_resnet152_v2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the ResNet152V2 architecture. — application_resnet152_v2","text":"","code":"application_resnet152_v2(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\" )"},{"path":"https://keras.posit.co/reference/application_resnet152_v2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the ResNet152V2 architecture. — application_resnet152_v2","text":"include_top whether include fully-connected layer top network. weights one NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. input_tensor optional Keras tensor (.e. output layers.Input()) use image input model. input_shape optional shape tuple, specified include_top FALSE (otherwise input shape (224, 224, 3) (\"channels_last\" data format) (3, 224, 224) (\"channels_first\" data format). exactly 3 inputs channels, width height smaller 32. E.g. (200, 200, 3) one valid value. pooling Optional pooling mode feature extraction include_top FALSE. NULL means output model 4D tensor output last convolutional block. avg means global average pooling applied output last convolutional block, thus output model 2D tensor. max means global max pooling applied. classes optional number classes classify images , specified include_top TRUE, weights argument specified. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. loading pretrained weights, classifier_activation can NULL \"softmax\".","code":""},{"path":"https://keras.posit.co/reference/application_resnet152_v2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the ResNet152V2 architecture. — application_resnet152_v2","text":"","code":"A Model instance."},{"path":"https://keras.posit.co/reference/application_resnet152_v2.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the ResNet152V2 architecture. — application_resnet152_v2","text":"Identity Mappings Deep Residual Networks (CVPR 2016) image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_resnet152_v2.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the ResNet152V2 architecture. — application_resnet152_v2","text":"Keras Application expects specific kind input preprocessing. ResNet, call keras.applications.resnet_v2.preprocess_input inputs passing model. resnet_v2.preprocess_input scale input pixels -1 1.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_resnet50.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the ResNet50 architecture. — application_resnet50","title":"Instantiates the ResNet50 architecture. — application_resnet50","text":"Instantiates ResNet50 architecture.","code":""},{"path":"https://keras.posit.co/reference/application_resnet50.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the ResNet50 architecture. — application_resnet50","text":"","code":"application_resnet50(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\" )"},{"path":"https://keras.posit.co/reference/application_resnet50.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the ResNet50 architecture. — application_resnet50","text":"include_top whether include fully-connected layer top network. weights one NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. input_tensor optional Keras tensor (.e. output layers.Input()) use image input model. input_shape optional shape tuple, specified include_top FALSE (otherwise input shape (224, 224, 3) (\"channels_last\" data format) (3, 224, 224) (\"channels_first\" data format). exactly 3 inputs channels, width height smaller 32. E.g. (200, 200, 3) one valid value. pooling Optional pooling mode feature extraction include_top FALSE. NULL means output model 4D tensor output last convolutional block. avg means global average pooling applied output last convolutional block, thus output model 2D tensor. max means global max pooling applied. classes optional number classes classify images , specified include_top TRUE, weights argument specified. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. loading pretrained weights, classifier_activation can NULL \"softmax\".","code":""},{"path":"https://keras.posit.co/reference/application_resnet50.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the ResNet50 architecture. — application_resnet50","text":"","code":"A Model instance."},{"path":"https://keras.posit.co/reference/application_resnet50.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the ResNet50 architecture. — application_resnet50","text":"Deep Residual Learning Image Recognition (CVPR 2015) image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_resnet50.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the ResNet50 architecture. — application_resnet50","text":"Keras Application expects specific kind input preprocessing. ResNet, call keras.applications.resnet.preprocess_input inputs passing model. resnet.preprocess_input convert input images RGB BGR, zero-center color channel respect ImageNet dataset, without scaling.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_resnet50_v2.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the ResNet50V2 architecture. — application_resnet50_v2","title":"Instantiates the ResNet50V2 architecture. — application_resnet50_v2","text":"Instantiates ResNet50V2 architecture.","code":""},{"path":"https://keras.posit.co/reference/application_resnet50_v2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the ResNet50V2 architecture. — application_resnet50_v2","text":"","code":"application_resnet50_v2(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\" )"},{"path":"https://keras.posit.co/reference/application_resnet50_v2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the ResNet50V2 architecture. — application_resnet50_v2","text":"include_top whether include fully-connected layer top network. weights one NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. input_tensor optional Keras tensor (.e. output layers.Input()) use image input model. input_shape optional shape tuple, specified include_top FALSE (otherwise input shape (224, 224, 3) (\"channels_last\" data format) (3, 224, 224) (\"channels_first\" data format). exactly 3 inputs channels, width height smaller 32. E.g. (200, 200, 3) one valid value. pooling Optional pooling mode feature extraction include_top FALSE. NULL means output model 4D tensor output last convolutional block. avg means global average pooling applied output last convolutional block, thus output model 2D tensor. max means global max pooling applied. classes optional number classes classify images , specified include_top TRUE, weights argument specified. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer. loading pretrained weights, classifier_activation can NULL \"softmax\".","code":""},{"path":"https://keras.posit.co/reference/application_resnet50_v2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the ResNet50V2 architecture. — application_resnet50_v2","text":"","code":"A Model instance."},{"path":"https://keras.posit.co/reference/application_resnet50_v2.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the ResNet50V2 architecture. — application_resnet50_v2","text":"Identity Mappings Deep Residual Networks (CVPR 2016) image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning.","code":""},{"path":"https://keras.posit.co/reference/application_resnet50_v2.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the ResNet50V2 architecture. — application_resnet50_v2","text":"Keras Application expects specific kind input preprocessing. ResNet, call keras.applications.resnet_v2.preprocess_input inputs passing model. resnet_v2.preprocess_input scale input pixels -1 1.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_vgg16.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the VGG16 model. — application_vgg16","title":"Instantiates the VGG16 model. — application_vgg16","text":"Instantiates VGG16 model.","code":""},{"path":"https://keras.posit.co/reference/application_vgg16.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the VGG16 model. — application_vgg16","text":"","code":"application_vgg16(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\" )"},{"path":"https://keras.posit.co/reference/application_vgg16.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the VGG16 model. — application_vgg16","text":"include_top whether include 3 fully-connected layers top network. weights one NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. input_tensor optional Keras tensor (.e. output layers.Input()) use image input model. input_shape optional shape tuple, specified include_top FALSE (otherwise input shape (224, 224, 3) (channels_last data format) (3, 224, 224) (\"channels_first\" data format). exactly 3 input channels, width height smaller 32. E.g. (200, 200, 3) one valid value. pooling Optional pooling mode feature extraction include_top FALSE. NULL means output model 4D tensor output last convolutional block. avg means global average pooling applied output last convolutional block, thus output model 2D tensor. max means global max pooling applied. classes optional number classes classify images , specified include_top TRUE, weights argument specified. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer.  loading pretrained weights, classifier_activation can NULL \"softmax\".","code":""},{"path":"https://keras.posit.co/reference/application_vgg16.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the VGG16 model. — application_vgg16","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_vgg16.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the VGG16 model. — application_vgg16","text":"Deep Convolutional Networks Large-Scale Image Recognition (ICLR 2015) image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning. default input size model 224x224.","code":""},{"path":"https://keras.posit.co/reference/application_vgg16.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the VGG16 model. — application_vgg16","text":"Keras Application expects specific kind input preprocessing. VGG16, call keras.applications.vgg16.preprocess_input inputs passing model. vgg16.preprocess_input convert input images RGB BGR, zero-center color channel respect ImageNet dataset, without scaling.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_vgg19.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the VGG19 model. — application_vgg19","title":"Instantiates the VGG19 model. — application_vgg19","text":"Instantiates VGG19 model.","code":""},{"path":"https://keras.posit.co/reference/application_vgg19.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the VGG19 model. — application_vgg19","text":"","code":"application_vgg19(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\" )"},{"path":"https://keras.posit.co/reference/application_vgg19.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the VGG19 model. — application_vgg19","text":"include_top whether include 3 fully-connected layers top network. weights one NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. input_tensor optional Keras tensor (.e. output layers.Input()) use image input model. input_shape optional shape tuple, specified include_top FALSE (otherwise input shape (224, 224, 3) (channels_last data format) (3, 224, 224) (\"channels_first\" data format). exactly 3 input channels, width height smaller 32. E.g. (200, 200, 3) one valid value. pooling Optional pooling mode feature extraction include_top FALSE. NULL means output model 4D tensor output last convolutional block. avg means global average pooling applied output last convolutional block, thus output model 2D tensor. max means global max pooling applied. classes optional number classes classify images , specified include_top TRUE, weights argument specified. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer.  loading pretrained weights, classifier_activation can NULL \"softmax\".","code":""},{"path":"https://keras.posit.co/reference/application_vgg19.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the VGG19 model. — application_vgg19","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_vgg19.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the VGG19 model. — application_vgg19","text":"Deep Convolutional Networks Large-Scale Image Recognition (ICLR 2015) image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning. default input size model 224x224.","code":""},{"path":"https://keras.posit.co/reference/application_vgg19.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the VGG19 model. — application_vgg19","text":"Keras Application expects specific kind input preprocessing. VGG19, call keras.applications.vgg19.preprocess_input inputs passing model. vgg19.preprocess_input convert input images RGB BGR, zero-center color channel respect ImageNet dataset, without scaling.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/application_xception.html","id":null,"dir":"Reference","previous_headings":"","what":"Instantiates the Xception architecture. — application_xception","title":"Instantiates the Xception architecture. — application_xception","text":"Instantiates Xception architecture.","code":""},{"path":"https://keras.posit.co/reference/application_xception.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Instantiates the Xception architecture. — application_xception","text":"","code":"application_xception(   include_top = TRUE,   weights = \"imagenet\",   input_tensor = NULL,   input_shape = NULL,   pooling = NULL,   classes = 1000L,   classifier_activation = \"softmax\" )"},{"path":"https://keras.posit.co/reference/application_xception.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Instantiates the Xception architecture. — application_xception","text":"include_top whether include 3 fully-connected layers top network. weights one NULL (random initialization), \"imagenet\" (pre-training ImageNet), path weights file loaded. input_tensor optional Keras tensor (.e. output layers.Input()) use image input model. input_shape optional shape tuple, specified include_top FALSE (otherwise input shape (299, 299, 3). exactly 3 inputs channels, width height smaller 71. E.g. (150, 150, 3) one valid value. pooling Optional pooling mode feature extraction include_top FALSE. NULL means output model 4D tensor output last convolutional block. avg means global average pooling applied output last convolutional block, thus output model 2D tensor. max means global max pooling applied. classes optional number classes classify images , specified include_top TRUE, weights argument specified. classifier_activation str callable. activation function use \"top\" layer. Ignored unless include_top=TRUE. Set classifier_activation=NULL return logits \"top\" layer.  loading pretrained weights, classifier_activation can NULL \"softmax\".","code":""},{"path":"https://keras.posit.co/reference/application_xception.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Instantiates the Xception architecture. — application_xception","text":"","code":"A model instance."},{"path":"https://keras.posit.co/reference/application_xception.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Instantiates the Xception architecture. — application_xception","text":"Xception: Deep Learning Depthwise Separable Convolutions (CVPR 2017) image classification use cases, see page detailed examples. transfer learning use cases, make sure read guide transfer learning & fine-tuning. default input image size model 299x299.","code":""},{"path":"https://keras.posit.co/reference/application_xception.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Instantiates the Xception architecture. — application_xception","text":"Keras Application expects specific kind input preprocessing. Xception, call keras.applications.xception.preprocess_input inputs passing model. xception.preprocess_input scale input pixels -1 1.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/audio_dataset_from_directory.html","id":null,"dir":"Reference","previous_headings":"","what":"Generates a tf.data.Dataset from audio files in a directory. — audio_dataset_from_directory","title":"Generates a tf.data.Dataset from audio files in a directory. — audio_dataset_from_directory","text":"directory structure :   calling audio_dataset_from_directory(main_directory, labels = 'inferred') return tf.data.Dataset yields batches audio files subdirectories class_a class_b, together labels 0 1 (0 corresponding class_a 1 corresponding class_b). .wav files supported time.","code":"main_directory/ ...class_a/ ......a_audio_1.wav ......a_audio_2.wav ...class_b/ ......b_audio_1.wav ......b_audio_2.wav"},{"path":"https://keras.posit.co/reference/audio_dataset_from_directory.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generates a tf.data.Dataset from audio files in a directory. — audio_dataset_from_directory","text":"","code":"audio_dataset_from_directory(   directory,   labels = \"inferred\",   label_mode = \"int\",   class_names = NULL,   batch_size = 32L,   sampling_rate = NULL,   output_sequence_length = NULL,   ragged = FALSE,   shuffle = TRUE,   seed = NULL,   validation_split = NULL,   subset = NULL,   follow_links = FALSE )"},{"path":"https://keras.posit.co/reference/audio_dataset_from_directory.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generates a tf.data.Dataset from audio files in a directory. — audio_dataset_from_directory","text":"directory Directory data located. labels \"inferred\", contain subdirectories, containing audio files class. Otherwise, directory structure ignored. labels Either \"inferred\" (labels generated directory structure), NULL (labels), list/tuple integer labels size number audio files found directory. Labels sorted according alphanumeric order audio file paths (obtained via os.walk(directory) Python). label_mode String describing encoding labels. Options : \"int\": means labels encoded integers (e.g. sparse_categorical_crossentropy loss). \"categorical\" means labels encoded categorical vector (e.g. categorical_crossentropy loss) \"binary\" means labels (can 2) encoded float32 scalars values 0 1 (e.g. binary_crossentropy). NULL (labels). class_names valid \"labels\" \"inferred\". explicit list class names (must match names subdirectories). Used control order classes (otherwise alphanumerical order used). batch_size Size batches data. Default: 32. NULL, data batched (dataset yield individual samples). sampling_rate Audio sampling rate (samples per second). output_sequence_length Maximum length audio sequence. Audio files longer truncated output_sequence_length. set NULL, sequences batch padded length longest sequence batch. ragged Whether return Ragged dataset (sequence length). Defaults FALSE. shuffle Whether shuffle data. Defaults TRUE. set FALSE, sorts data alphanumeric order. seed Optional random seed shuffling transformations. validation_split Optional float 0 1, fraction data reserve validation. subset Subset data return. One \"training\", \"validation\" \"\". used validation_split set. follow_links Whether visits subdirectories pointed symlinks. Defaults FALSE.","code":""},{"path":"https://keras.posit.co/reference/audio_dataset_from_directory.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generates a tf.data.Dataset from audio files in a directory. — audio_dataset_from_directory","text":"tf.data.Dataset object. label_mode NULL, yields string tensors shape (batch_size,), containing contents batch audio files. Otherwise, yields tuple (audio, labels), audio shape (batch_size, sequence_length, num_channels) labels follows format described . Rules regarding labels format: label_mode int, labels int32 tensor shape (batch_size,). label_mode binary, labels float32 tensor 1s 0s shape (batch_size, 1). label_mode categorical, labels float32 tensor shape (batch_size, num_classes), representing one-hot encoding class index.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/bidirectional.html","id":null,"dir":"Reference","previous_headings":"","what":"Bidirectional wrapper for RNNs. — bidirectional","title":"Bidirectional wrapper for RNNs. — bidirectional","text":"Bidirectional wrapper RNNs.","code":""},{"path":"https://keras.posit.co/reference/bidirectional.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bidirectional wrapper for RNNs. — bidirectional","text":"","code":"bidirectional(   object,   layer,   merge_mode = \"concat\",   weights = NULL,   backward_layer = NULL,   ... )"},{"path":"https://keras.posit.co/reference/bidirectional.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bidirectional wrapper for RNNs. — bidirectional","text":"object Object compose layer . tensor, array, sequential model. layer keras.layers.RNN instance, keras.layers.LSTM keras.layers.GRU. also keras.layers.Layer instance meets following criteria: sequence-processing layer (accepts 3D+ inputs). go_backwards, return_sequences return_state attribute (semantics RNN class). input_spec attribute. Implement serialization via get_config() from_config(). Note recommended way create new RNN layers write custom RNN cell use keras.layers.RNN, instead subclassing keras.layers.Layer directly. return_sequences TRUE, output masked timestep zero regardless layer's original zero_output_for_mask value. merge_mode Mode outputs forward backward RNNs combined. One {\"sum\", \"mul\", \"concat\", \"ave\", NULL}. NULL, outputs combined, returned list. Defaults \"concat\". weights see description backward_layer Optional keras.layers.RNN, keras.layers.Layer instance used handle backwards input processing. backward_layer provided, layer instance passed layer argument used generate backward layer automatically. Note provided backward_layer layer properties matching layer argument, particular values stateful, return_states, return_sequences, etc. addition, backward_layer layer different go_backwards argument values. ValueError raised requirements met. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/bidirectional.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"Bidirectional wrapper for RNNs. — bidirectional","text":"call arguments layer wrapped RNN layer. Beware passing initial_state argument call layer, first half list elements initial_state list passed forward RNN call last half list elements passed backward RNN call.","code":""},{"path":"https://keras.posit.co/reference/bidirectional.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Bidirectional wrapper for RNNs. — bidirectional","text":"instantiating Bidirectional layer existing RNN layer instance reuse weights state RNN layer instance -- Bidirectional layer freshly initialized weights.","code":""},{"path":"https://keras.posit.co/reference/bidirectional.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bidirectional wrapper for RNNs. — bidirectional","text":"","code":"model <- keras_model_sequential(input_shape = c(5, 10)) %>%   bidirectional(layer_lstm(units = 10, return_sequences = TRUE)) %>%   bidirectional(layer_lstm(units = 10)) %>%   layer_dense(5, activation = \"softmax\")  model %>% compile(loss = \"categorical_crossentropy\",                   optimizer = \"rmsprop\")  # With custom backward layer forward_layer <- layer_lstm(units = 10, return_sequences = TRUE) backward_layer <- layer_lstm(units = 10, activation = \"relu\",                              return_sequences = TRUE, go_backwards = TRUE)  model <- keras_model_sequential(input_shape = c(5, 10)) %>%   bidirectional(forward_layer, backward_layer = backward_layer) %>%   layer_dense(5, activation = \"softmax\")  model %>% compile(loss = \"categorical_crossentropy\",                   optimizer = \"rmsprop\")"},{"path":[]},{"path":"https://keras.posit.co/reference/callback_backup_and_restore.html","id":null,"dir":"Reference","previous_headings":"","what":"Callback to back up and restore the training state. — callback_backup_and_restore","title":"Callback to back up and restore the training state. — callback_backup_and_restore","text":"callback_backup_and_restore callback intended recover training interruption happened middle fit execution, backing training states temporary checkpoint file, end epoch. backup overwrites previously written checkpoint file, given time one checkpoint file backup/restoring purpose. training restarts completion, training state (includes model weights epoch number) restored recently saved state beginning new fit run. completion fit run, temporary checkpoint file deleted. Note user responsible bring jobs back interruption. callback important backup restore mechanism fault tolerance purpose, model restored previous checkpoint expected one used back . user changes arguments passed compile fit, checkpoint saved fault tolerance can become invalid.","code":""},{"path":"https://keras.posit.co/reference/callback_backup_and_restore.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Callback to back up and restore the training state. — callback_backup_and_restore","text":"","code":"callback_backup_and_restore(   backup_dir,   save_freq = \"epoch\",   delete_checkpoint = TRUE )"},{"path":"https://keras.posit.co/reference/callback_backup_and_restore.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Callback to back up and restore the training state. — callback_backup_and_restore","text":"backup_dir String, path directory store data needed restore model. directory reused elsewhere store files, e.g. backup_and_restore callback another training run, another callback (e.g. callback_model_checkpoint) training run. save_freq \"epoch\", integer, FALSE. set \"epoch\", callback saves checkpoint end epoch. set integer, callback saves checkpoint every save_freq batches. Set save_freq = FALSE using preemption checkpointing (.e. save_before_preemption = TRUE). delete_checkpoint Boolean, defaults TRUE. backup_and_restore callback works saving checkpoint back training state. delete_checkpoint = TRUE, checkpoint deleted training finished. Use FALSE like keep checkpoint future usage.","code":""},{"path":"https://keras.posit.co/reference/callback_backup_and_restore.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Callback to back up and restore the training state. — callback_backup_and_restore","text":"","code":"callback_interrupting <- new_callback_class(   \"InterruptingCallback\",   on_epoch_begin = function(epoch, logs = NULL) {     if (epoch == 4) {       stop('Interrupting!')     }   } )  backup_dir <- tempfile() callback <- callback_backup_and_restore(backup_dir = backup_dir) model <- keras_model_sequential() %>%   layer_dense(10) model %>% compile(optimizer = optimizer_sgd(), loss = 'mse')  tryCatch({   model %>% fit(x = op_ones(c(5, 20)),                 y = op_zeros(5),                 epochs = 10, batch_size = 1,                 callbacks = list(callback, callback_interrupting()),                 verbose = 0) }, python.builtin.RuntimeError = function(e) message(\"Interrupted!\")) ## Interrupted! model$history$epoch ## [1] 0 1 2 # model$history %>% keras3:::to_keras_training_history() %>% as.data.frame() %>% print()  history <- model %>% fit(x = op_ones(c(5, 20)),                          y = op_zeros(5),                          epochs = 10, batch_size = 1,                          callbacks = list(callback),                          verbose = 0)  # Only 6 more epochs are run, since first training got interrupted at # zero-indexed epoch 4, second training will continue from 4 to 9. nrow(as.data.frame(history)) ## [1] 10"},{"path":[]},{"path":"https://keras.posit.co/reference/callback_csv_logger.html","id":null,"dir":"Reference","previous_headings":"","what":"Callback that streams epoch results to a CSV file. — callback_csv_logger","title":"Callback that streams epoch results to a CSV file. — callback_csv_logger","text":"Supports values can represented string, including 1D iterables atomic vectors.","code":""},{"path":"https://keras.posit.co/reference/callback_csv_logger.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Callback that streams epoch results to a CSV file. — callback_csv_logger","text":"","code":"callback_csv_logger(filename, separator = \",\", append = FALSE)"},{"path":"https://keras.posit.co/reference/callback_csv_logger.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Callback that streams epoch results to a CSV file. — callback_csv_logger","text":"filename Filename CSV file, e.g. 'run/log.csv'. separator String used separate elements CSV file. append Boolean. TRUE: append file exists (useful continuing training). FALSE: overwrite existing file.","code":""},{"path":"https://keras.posit.co/reference/callback_csv_logger.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Callback that streams epoch results to a CSV file. — callback_csv_logger","text":"","code":"csv_logger <- callback_csv_logger('training.log') model %>% fit(X_train, Y_train, callbacks = list(csv_logger))"},{"path":[]},{"path":"https://keras.posit.co/reference/callback_early_stopping.html","id":null,"dir":"Reference","previous_headings":"","what":"Stop training when a monitored metric has stopped improving. — callback_early_stopping","title":"Stop training when a monitored metric has stopped improving. — callback_early_stopping","text":"Assuming goal training minimize loss. , metric monitored 'loss', mode 'min'. model$fit() training loop check end every epoch whether loss longer decreasing, considering min_delta patience applicable. found longer decreasing, model$stop_training marked TRUE training terminates. quantity monitored needs available logs list. make , pass loss metrics model$compile().","code":""},{"path":"https://keras.posit.co/reference/callback_early_stopping.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Stop training when a monitored metric has stopped improving. — callback_early_stopping","text":"","code":"callback_early_stopping(   monitor = \"val_loss\",   min_delta = 0L,   patience = 0L,   verbose = 0L,   mode = \"auto\",   baseline = NULL,   restore_best_weights = FALSE,   start_from_epoch = 0L )"},{"path":"https://keras.posit.co/reference/callback_early_stopping.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Stop training when a monitored metric has stopped improving. — callback_early_stopping","text":"monitor Quantity monitored. Defaults \"val_loss\". min_delta Minimum change monitored quantity qualify improvement, .e. absolute change less min_delta, count improvement. Defaults 0. patience Number epochs improvement training stopped. Defaults 0. verbose Verbosity mode, 0 1. Mode 0 silent, mode 1 displays messages callback takes action. Defaults 0. mode One {\"auto\", \"min\", \"max\"}. min mode, training stop quantity monitored stopped decreasing; \"max\" mode stop quantity monitored stopped increasing; \"auto\" mode, direction automatically inferred name monitored quantity. Defaults \"auto\". baseline Baseline value monitored quantity. NULL, training stop model show improvement baseline. Defaults NULL. restore_best_weights Whether restore model weights epoch best value monitored quantity. FALSE, model weights obtained last step training used. epoch restored regardless performance relative baseline. epoch improves baseline, training run patience epochs restore weights best epoch set. Defaults FALSE. start_from_epoch Number epochs wait starting monitor improvement. allows warm-period improvement expected thus training stopped. Defaults 0.","code":""},{"path":"https://keras.posit.co/reference/callback_early_stopping.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Stop training when a monitored metric has stopped improving. — callback_early_stopping","text":"","code":"callback <- callback_early_stopping(monitor = 'loss',                                    patience = 3) # This callback will stop the training when there is no improvement in # the loss for three consecutive epochs. model <- keras_model_sequential() %>%   layer_dense(10) model %>% compile(optimizer = optimizer_sgd(), loss = 'mse') history <- model %>% fit(x = op_ones(c(5, 20)),                          y = op_zeros(5),                          epochs = 10, batch_size = 1,                          callbacks = list(callback),                          verbose = 0) nrow(as.data.frame(history))  # Only 4 epochs are run. ## [1] 10"},{"path":[]},{"path":"https://keras.posit.co/reference/callback_lambda.html","id":null,"dir":"Reference","previous_headings":"","what":"Callback for creating simple, custom callbacks on-the-fly. — callback_lambda","title":"Callback for creating simple, custom callbacks on-the-fly. — callback_lambda","text":"callback constructed anonymous functions called appropriate time (Model.{fit | evaluate | predict}). Note callbacks expects positional arguments, : on_epoch_begin on_epoch_end expect two positional arguments: epoch, logs on_train_begin on_train_end expect one positional argument: logs on_train_batch_begin on_train_batch_end expect two positional arguments: batch, logs See Callback class definition full list functions expected arguments.","code":""},{"path":"https://keras.posit.co/reference/callback_lambda.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Callback for creating simple, custom callbacks on-the-fly. — callback_lambda","text":"","code":"callback_lambda(   on_epoch_begin = NULL,   on_epoch_end = NULL,   on_train_begin = NULL,   on_train_end = NULL,   on_train_batch_begin = NULL,   on_train_batch_end = NULL,   ... )"},{"path":"https://keras.posit.co/reference/callback_lambda.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Callback for creating simple, custom callbacks on-the-fly. — callback_lambda","text":"on_epoch_begin called beginning every epoch. on_epoch_end called end every epoch. on_train_begin called beginning model training. on_train_end called end model training. on_train_batch_begin called beginning every train batch. on_train_batch_end called end every train batch. ... function Callback want override passing function_name = function. example, callback_lambda(.., on_train_end = train_end_fn). custom function needs arguments ones defined Callback.","code":""},{"path":"https://keras.posit.co/reference/callback_lambda.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Callback for creating simple, custom callbacks on-the-fly. — callback_lambda","text":"","code":"# Print the batch number at the beginning of every batch. batch_print_callback <- callback_lambda(   on_train_batch_begin = function(batch, logs) {     print(batch)   } )  # Stream the epoch loss to a file in new-line delimited JSON format # (one valid JSON object per line) json_log <- file('loss_log.json', open = 'wt') json_logging_callback <- callback_lambda(   on_epoch_end = function(epoch, logs) {     jsonlite::write_json(       list(epoch = epoch, loss = logs$loss),       json_log,       append = TRUE     )   },   on_train_end = function(logs) {     close(json_log)   } )  # Terminate some processes after having finished model training. processes <- ... cleanup_callback <- callback_lambda(   on_train_end = function(logs) {     for (p in processes) {       if (is_alive(p)) {         terminate(p)       }     }   } )  model %>% fit(   ...,   callbacks = list(     batch_print_callback,     json_logging_callback,     cleanup_callback   ) )"},{"path":[]},{"path":"https://keras.posit.co/reference/callback_learning_rate_scheduler.html","id":null,"dir":"Reference","previous_headings":"","what":"Learning rate scheduler. — callback_learning_rate_scheduler","title":"Learning rate scheduler. — callback_learning_rate_scheduler","text":"beginning every epoch, callback gets updated learning rate value schedule function provided __init__, current epoch current learning rate, applies updated learning rate optimizer.","code":""},{"path":"https://keras.posit.co/reference/callback_learning_rate_scheduler.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Learning rate scheduler. — callback_learning_rate_scheduler","text":"","code":"callback_learning_rate_scheduler(schedule, verbose = 0L)"},{"path":"https://keras.posit.co/reference/callback_learning_rate_scheduler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Learning rate scheduler. — callback_learning_rate_scheduler","text":"schedule function takes epoch index (integer, indexed 0) current learning rate (float) inputs returns new learning rate output (float). verbose Integer. 0: quiet, 1: log update messages.","code":""},{"path":"https://keras.posit.co/reference/callback_learning_rate_scheduler.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Learning rate scheduler. — callback_learning_rate_scheduler","text":"","code":"# This function keeps the initial learning rate for the first ten epochs # and decreases it exponentially after that. scheduler <- function(epoch, lr) {   if (epoch < 10)     return(lr)   else     return(lr * exp(-0.1)) }  model <- keras_model_sequential() |> layer_dense(units = 10) model |> compile(optimizer = optimizer_sgd(), loss = 'mse') model$optimizer$learning_rate |> as.array() |> round(5) ## [1] 0.01 callback <- callback_learning_rate_scheduler(schedule = scheduler) history <- model |> fit(x = array(runif(100), c(5, 20)),                         y = array(0, c(5, 1)),                         epochs = 15, callbacks = list(callback), verbose = 0) model$optimizer$learning_rate |> as.array() |> round(5) ## [1] 0.00607"},{"path":[]},{"path":"https://keras.posit.co/reference/callback_model_checkpoint.html","id":null,"dir":"Reference","previous_headings":"","what":"Callback to save the Keras model or model weights at some frequency.\n@description\ncallback_model_checkpoint() is used in conjunction with training using\nmodel |> fit() to save a model or weights (in a checkpoint file) at some\ninterval, so the model or weights can be loaded later to continue the\ntraining from the state saved. — callback_model_checkpoint","title":"Callback to save the Keras model or model weights at some frequency.\n@description\ncallback_model_checkpoint() is used in conjunction with training using\nmodel |> fit() to save a model or weights (in a checkpoint file) at some\ninterval, so the model or weights can be loaded later to continue the\ntraining from the state saved. — callback_model_checkpoint","text":"options callback provides include:","code":""},{"path":"https://keras.posit.co/reference/callback_model_checkpoint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Callback to save the Keras model or model weights at some frequency.\n@description\ncallback_model_checkpoint() is used in conjunction with training using\nmodel |> fit() to save a model or weights (in a checkpoint file) at some\ninterval, so the model or weights can be loaded later to continue the\ntraining from the state saved. — callback_model_checkpoint","text":"","code":"callback_model_checkpoint(   filepath,   monitor = \"val_loss\",   verbose = 0L,   save_best_only = FALSE,   save_weights_only = FALSE,   mode = \"auto\",   save_freq = \"epoch\",   initial_value_threshold = NULL )"},{"path":"https://keras.posit.co/reference/callback_model_checkpoint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Callback to save the Keras model or model weights at some frequency.\n@description\ncallback_model_checkpoint() is used in conjunction with training using\nmodel |> fit() to save a model or weights (in a checkpoint file) at some\ninterval, so the model or weights can be loaded later to continue the\ntraining from the state saved. — callback_model_checkpoint","text":"filepath string, path save model file. filepath can contain named formatting options, filled value epoch keys logs (passed on_epoch_end). filepath name needs end \".weights.h5\" save_weights_only = TRUE end \".keras\" checkpoint saving whole model (default). example: filepath \"{epoch:02d}-{val_loss:.2f}.keras\", model checkpoints saved epoch number validation loss filename. directory filepath reused callbacks avoid conflicts. monitor metric name monitor. Typically metrics set model |> compile() method. Note: Prefix name \"val_\" monitor validation metrics. Use \"loss\" \"val_loss\" monitor model's total loss. specify metrics strings, like \"accuracy\", pass string (without \"val_\" prefix). pass Metric objects (created one metric_*()), monitor set metric$name. sure metric names can check contents history$metrics list returned history <- model |> fit() Multi-output models set additional prefixes metric names. verbose Verbosity mode, 0 1. Mode 0 silent, mode 1 displays messages callback takes action. save_best_only save_best_only = TRUE, saves model considered \"best\" latest best model according quantity monitored overwritten. filepath contain formatting options like {epoch} filepath overwritten new better model. save_weights_only TRUE, model's weights saved (model |> save_model_weights(filepath)), else full model saved (model |> save_model(filepath)). mode one \"auto\", \"min\", \"max\". save_best_only = TRUE, decision overwrite current save file made based either maximization minimization monitored quantity. val_acc, \"max\", val_loss \"min\", etc. \"auto\" mode, mode set \"max\" quantities monitored \"acc\" start \"fmeasure\" set \"min\" rest quantities. save_freq \"epoch\" integer. using \"epoch\", callback saves model epoch. using integer, callback saves model end many batches. Model compiled steps_per_execution = N, saving criteria checked every Nth batch. Note saving aligned epochs, monitored metric may potentially less reliable (reflect little 1 batch, since metrics get reset every epoch). Defaults \"epoch\". initial_value_threshold Floating point initial \"best\" value metric monitored. applies save_best_value = TRUE. overwrites model weights already saved performance current model better value.","code":""},{"path":"https://keras.posit.co/reference/callback_model_checkpoint.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Callback to save the Keras model or model weights at some frequency.\n@description\ncallback_model_checkpoint() is used in conjunction with training using\nmodel |> fit() to save a model or weights (in a checkpoint file) at some\ninterval, so the model or weights can be loaded later to continue the\ntraining from the state saved. — callback_model_checkpoint","text":"Whether keep model achieved \"best performance\" far, whether save model end every epoch regardless performance. Definition \"best\"; quantity monitor whether maximized minimized. frequency save . Currently, callback supports saving end every epoch, fixed number training batches. Whether weights saved, whole model saved.","code":""},{"path":"https://keras.posit.co/reference/callback_model_checkpoint.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Callback to save the Keras model or model weights at some frequency.\n@description\ncallback_model_checkpoint() is used in conjunction with training using\nmodel |> fit() to save a model or weights (in a checkpoint file) at some\ninterval, so the model or weights can be loaded later to continue the\ntraining from the state saved. — callback_model_checkpoint","text":"","code":"model <- keras_model_sequential(input_shape = c(10)) |>   layer_dense(1, activation = \"sigmoid\") |>   compile(loss = \"binary_crossentropy\", optimizer = \"adam\",           metrics = c('accuracy'))  EPOCHS <- 10 checkpoint_filepath <- tempfile('checkpoint-model-', fileext = \".keras\") model_checkpoint_callback <- callback_model_checkpoint(   filepath = checkpoint_filepath,   monitor = 'val_accuracy',   mode = 'max',   save_best_only = TRUE )  # Model is saved at the end of every epoch, if it's the best seen so far. model |> fit(x = random_uniform(c(2, 10)), y = op_ones(2, 1),              epochs = EPOCHS, validation_split = .5, verbose = 0,              callbacks = list(model_checkpoint_callback))  # The model (that are considered the best) can be loaded as - load_model(checkpoint_filepath) ## Model: \"sequential\" ## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓ ## ┃ Layer (type)                    ┃ Output Shape              ┃    Param # ┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩ ## │ dense (Dense)                   │ (None, 1)                 │         11 │ ## └─────────────────────────────────┴───────────────────────────┴────────────┘ ##  Total params: 35 (144.00 B) ##  Trainable params: 11 (44.00 B) ##  Non-trainable params: 0 (0.00 B) ##  Optimizer params: 24 (100.00 B) # Alternatively, one could checkpoint just the model weights as - checkpoint_filepath <- tempfile('checkpoint-', fileext = \".weights.h5\") model_checkpoint_callback <- callback_model_checkpoint(   filepath = checkpoint_filepath,   save_weights_only = TRUE,   monitor = 'val_accuracy',   mode = 'max',   save_best_only = TRUE )  # Model weights are saved at the end of every epoch, if it's the best seen # so far. # same as above model |> fit(x = random_uniform(c(2, 10)), y = op_ones(2, 1),              epochs = EPOCHS, validation_split = .5, verbose = 0,              callbacks = list(model_checkpoint_callback))  # The model weights (that are considered the best) can be loaded model |> load_model_weights(checkpoint_filepath)"},{"path":[]},{"path":"https://keras.posit.co/reference/callback_progbar_logger.html","id":null,"dir":"Reference","previous_headings":"","what":"Callback that prints metrics to stdout. — callback_progbar_logger","title":"Callback that prints metrics to stdout. — callback_progbar_logger","text":"Callback prints metrics stdout.","code":""},{"path":"https://keras.posit.co/reference/callback_progbar_logger.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Callback that prints metrics to stdout. — callback_progbar_logger","text":"","code":"callback_progbar_logger(count_mode = NULL)"},{"path":"https://keras.posit.co/reference/callback_progbar_logger.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Callback that prints metrics to stdout. — callback_progbar_logger","text":"count_mode One \"steps\" \"samples\". Whether progress bar count samples seen steps (batches) seen.","code":""},{"path":"https://keras.posit.co/reference/callback_progbar_logger.html","id":"raises","dir":"Reference","previous_headings":"","what":"Raises","title":"Callback that prints metrics to stdout. — callback_progbar_logger","text":"ValueError: case invalid count_mode.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/callback_reduce_lr_on_plateau.html","id":null,"dir":"Reference","previous_headings":"","what":"Reduce learning rate when a metric has stopped improving. — callback_reduce_lr_on_plateau","title":"Reduce learning rate when a metric has stopped improving. — callback_reduce_lr_on_plateau","text":"Models often benefit reducing learning rate factor 2-10 learning stagnates. callback monitors quantity improvement seen 'patience' number epochs, learning rate reduced.","code":""},{"path":"https://keras.posit.co/reference/callback_reduce_lr_on_plateau.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reduce learning rate when a metric has stopped improving. — callback_reduce_lr_on_plateau","text":"","code":"callback_reduce_lr_on_plateau(   monitor = \"val_loss\",   factor = 0.1,   patience = 10L,   verbose = 0L,   mode = \"auto\",   min_delta = 1e-04,   cooldown = 0L,   min_lr = 0L,   ... )"},{"path":"https://keras.posit.co/reference/callback_reduce_lr_on_plateau.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reduce learning rate when a metric has stopped improving. — callback_reduce_lr_on_plateau","text":"monitor String. Quantity monitored. factor Numeric. Factor learning rate reduced. new_lr = lr * factor. patience Integer. Number epochs improvement learning rate reduced. verbose Integer. 0: quiet, 1: update messages. mode String. One {'auto', 'min', 'max'}. 'min' mode, learning rate reduced quantity monitored stopped decreasing; 'max' mode reduced quantity monitored stopped increasing; 'auto' mode, direction automatically inferred name monitored quantity. min_delta Numeric. Threshold measuring new optimum, focus significant changes. cooldown Integer. Number epochs wait resuming normal operation learning rate reduced. min_lr Numeric. Lower bound learning rate. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/callback_reduce_lr_on_plateau.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reduce learning rate when a metric has stopped improving. — callback_reduce_lr_on_plateau","text":"","code":"reduce_lr <- callback_reduce_lr_on_plateau(monitor = 'val_loss', factor = 0.2,                                            patience = 5, min_lr = 0.001) model %>% fit(x_train, y_train, callbacks = list(reduce_lr))"},{"path":[]},{"path":"https://keras.posit.co/reference/callback_remote_monitor.html","id":null,"dir":"Reference","previous_headings":"","what":"Callback used to stream events to a server. — callback_remote_monitor","title":"Callback used to stream events to a server. — callback_remote_monitor","text":"Requires requests library. Events sent root + '/publish/epoch/end/' default. Calls HTTP POST, data argument JSON-encoded named list event data. send_as_json = TRUE, content type request \"application/json\". Otherwise serialized JSON sent within form.","code":""},{"path":"https://keras.posit.co/reference/callback_remote_monitor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Callback used to stream events to a server. — callback_remote_monitor","text":"","code":"callback_remote_monitor(   root = \"http://localhost:9000\",   path = \"/publish/epoch/end/\",   field = \"data\",   headers = NULL,   send_as_json = FALSE )"},{"path":"https://keras.posit.co/reference/callback_remote_monitor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Callback used to stream events to a server. — callback_remote_monitor","text":"root String; root url target server. path String; path relative root events sent. field String; JSON field data stored. field used payload sent within form (.e. send_as_json set FALSE). headers Named list; optional custom HTTP headers. send_as_json Boolean; whether request sent \"application/json\".","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/callback_tensorboard.html","id":null,"dir":"Reference","previous_headings":"","what":"Enable visualizations for TensorBoard. — callback_tensorboard","title":"Enable visualizations for TensorBoard. — callback_tensorboard","text":"TensorBoard visualization tool provided TensorFlow. TensorFlow installation required use callback. callback logs events TensorBoard, including: Metrics summary plots Training graph visualization Weight histograms Sampled profiling used model |> evaluate() regular validation addition epoch summaries, summary records evaluation metrics vs model$optimizer$iterations written. metric names prepended evaluation, model$optimizer$iterations step visualized TensorBoard. installed TensorFlow pip reticulate::py_install(), able launch TensorBoard command line:   R tensorflow::tensorboard(). can find information TensorBoard .","code":"tensorboard --logdir=path_to_your_logs"},{"path":"https://keras.posit.co/reference/callback_tensorboard.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Enable visualizations for TensorBoard. — callback_tensorboard","text":"","code":"callback_tensorboard(   log_dir = \"logs\",   histogram_freq = 0L,   write_graph = TRUE,   write_images = FALSE,   write_steps_per_second = FALSE,   update_freq = \"epoch\",   profile_batch = 0L,   embeddings_freq = 0L,   embeddings_metadata = NULL )"},{"path":"https://keras.posit.co/reference/callback_tensorboard.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Enable visualizations for TensorBoard. — callback_tensorboard","text":"log_dir path directory save log files parsed TensorBoard. e.g., log_dir = file.path(working_dir, 'logs'). directory reused callbacks. histogram_freq frequency (epochs) compute weight histograms layers model. set 0, histograms computed. Validation data (split) must specified histogram visualizations. write_graph (supported time) Whether visualize graph TensorBoard. Note log file can become quite large write_graph set TRUE. write_images whether write model weights visualize image TensorBoard. write_steps_per_second whether log training steps per second TensorBoard. supports epoch batch frequency logging. update_freq \"batch\" \"epoch\" integer. using \"epoch\", writes losses metrics TensorBoard every epoch. using integer, say 1000, metrics losses (including custom ones added Model.compile) logged TensorBoard every 1000 batches. \"batch\" synonym 1, meaning written every batch. Note however writing frequently TensorBoard can slow training, especially used distribution strategies incur additional synchronization overhead. Batch-level summary writing also available via train_step override. Please see TensorBoard Scalars tutorial  # noqa: E501 details. profile_batch (supported time) Profile batch(es) sample compute characteristics. profile_batch must non-negative integer tuple integers. pair positive integers signify range batches profile. default, profiling disabled. embeddings_freq frequency (epochs) embedding layers visualized. set 0, embeddings visualized. embeddings_metadata Named list maps embedding layer names filename file save metadata embedding layer. case metadata file used embedding layers, single filename can passed.","code":""},{"path":"https://keras.posit.co/reference/callback_tensorboard.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Enable visualizations for TensorBoard. — callback_tensorboard","text":"Basic usage:   Custom batch-level summaries subclassed Model:   Custom batch-level summaries Functional API Model:   Profiling:","code":"tensorboard_callback <- callback_tensorboard(log_dir = \"./logs\") model %>% fit(x_train, y_train, epochs = 2, callbacks = list(tensorboard_callback)) # Then run the tensorboard command to view the visualizations. MyModel <- new_model_class(\"MyModel\",   initialize = function() {     self$dense <- layer_dense(units = 10)   },   call = function(x) {     outputs <- x |> self$dense()     tf$summary$histogram('outputs', outputs)     outputs   } )  model <- MyModel() model |> compile(optimizer = 'sgd', loss = 'mse')  # Make sure to set `update_freq = N` to log a batch-level summary every N # batches. In addition to any `tf$summary` contained in `model$call()`, # metrics added in `model |>compile` will be logged every N batches. tb_callback <- callback_tensorboard(log_dir = './logs', update_freq = 1) model |> fit(x_train, y_train, callbacks = list(tb_callback)) my_summary <- function(x) {   tf$summary$histogram('x', x)   x }  inputs <- layer_input(10) outputs <- inputs |>   layer_dense(10) |>   layer_lambda(my_summary)  model <- keras_model(inputs, outputs) model |> compile(optimizer = 'sgd', loss = 'mse')  # Make sure to set `update_freq = N` to log a batch-level summary every N # batches. In addition to any `tf.summary` contained in `Model.call`, # metrics added in `Model.compile` will be logged every N batches. tb_callback <- callback_tensorboard(log_dir = './logs', update_freq = 1) model |> fit(x_train, y_train, callbacks = list(tb_callback)) # Profile a single batch, e.g. the 5th batch. tensorboard_callback <- callback_tensorboard(   log_dir = './logs', profile_batch = 5) model |> fit(x_train, y_train, epochs = 2,              callbacks = list(tensorboard_callback))  # Profile a range of batches, e.g. from 10 to 20. tensorboard_callback <- callback_tensorboard(   log_dir = './logs', profile_batch = c(10, 20)) model |> fit(x_train, y_train, epochs = 2,              callbacks = list(tensorboard_callback))"},{"path":[]},{"path":"https://keras.posit.co/reference/callback_terminate_on_nan.html","id":null,"dir":"Reference","previous_headings":"","what":"Callback that terminates training when a NaN loss is encountered. — callback_terminate_on_nan","title":"Callback that terminates training when a NaN loss is encountered. — callback_terminate_on_nan","text":"Callback terminates training NaN loss encountered.","code":""},{"path":"https://keras.posit.co/reference/callback_terminate_on_nan.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Callback that terminates training when a NaN loss is encountered. — callback_terminate_on_nan","text":"","code":"callback_terminate_on_nan()"},{"path":[]},{"path":"https://keras.posit.co/reference/clear_session.html","id":null,"dir":"Reference","previous_headings":"","what":"Resets all state generated by Keras. — clear_session","title":"Resets all state generated by Keras. — clear_session","text":"Keras manages global state, uses implement Functional model-building API uniquify autogenerated layer names. creating many models loop, global state consume increasing amount memory time, may want clear . Calling clear_session() releases global state: helps avoid clutter old models layers, especially memory limited. Example 1: calling clear_session() creating models loop   Example 2: resetting layer name generation counter","code":"for (i in 1:100) {   # Without `clear_session()`, each iteration of this loop will   # slightly increase the size of the global state managed by Keras   model <- keras_model_sequential()   for (j in 1:10) {     model <- model |> layer_dense(units = 10)   } }  for (i in 1:100) {   # With `clear_session()` called at the beginning,   # Keras starts with a blank state at each iteration   # and memory consumption is constant over time.   clear_session()   model <- keras_model_sequential()   for (j in 1:10) {     model <- model |> layer_dense(units = 10)   } } layers <- lapply(1:10, \\(i) layer_dense(units = 10))  new_layer <- layer_dense(units = 10) print(new_layer$name) ## [1] \"dense_10\" clear_session() new_layer <- layer_dense(units = 10) print(new_layer$name) ## [1] \"dense\""},{"path":"https://keras.posit.co/reference/clear_session.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Resets all state generated by Keras. — clear_session","text":"","code":"clear_session()"},{"path":[]},{"path":"https://keras.posit.co/reference/clone_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Clone a model instance. — clone_model","title":"Clone a model instance. — clone_model","text":"Model cloning similar calling model new inputs, except creates new layers (thus new weights) instead sharing weights existing layers.","code":""},{"path":"https://keras.posit.co/reference/clone_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clone a model instance. — clone_model","text":"","code":"clone_model(model, input_tensors = NULL, clone_function = NULL)"},{"path":"https://keras.posit.co/reference/clone_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clone a model instance. — clone_model","text":"model Instance Keras model (functional model Sequential model). input_tensors Optional list input tensors build model upon. provided, placeholders created. clone_function Callable used clone layer target model (except InputLayer instances). takes argument layer instance cloned, returns corresponding layer instance used model copy. unspecified, callable defaults following serialization/deserialization function: function(layer) layer$`__class__`$from_config(layer$get_config()) passing custom callable, can customize copy model, e.g. wrapping certain layers interest (might want replace LSTM instances equivalent Bidirectional(LSTM(...)) instances, example).","code":""},{"path":"https://keras.posit.co/reference/compile.keras.models.model.Model.html","id":null,"dir":"Reference","previous_headings":"","what":"Configure a model for training. — compile.keras.models.model.Model","title":"Configure a model for training. — compile.keras.models.model.Model","text":"Configure model training.","code":""},{"path":"https://keras.posit.co/reference/compile.keras.models.model.Model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Configure a model for training. — compile.keras.models.model.Model","text":"","code":"# S3 method for keras.models.model.Model compile(   object,   optimizer = \"rmsprop\",   loss = NULL,   metrics = NULL,   ...,   loss_weights = NULL,   weighted_metrics = NULL,   run_eagerly = FALSE,   steps_per_execution = 1L,   jit_compile = \"auto\",   auto_scale_loss = TRUE )"},{"path":"https://keras.posit.co/reference/compile.keras.models.model.Model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Configure a model for training. — compile.keras.models.model.Model","text":"object Keras model object optimizer String (name optimizer) optimizer instance. See optimizer_* family. loss Loss function. May string (name loss function), Loss instance. See loss_* family functions. loss function callable signature loss = fn(y_true, y_pred), y_true ground truth values, y_pred model's predictions. y_true shape (batch_size, d1, .. dN) (except case sparse loss functions sparse categorical crossentropy expects integer arrays shape (batch_size, d1, .. dN-1)). y_pred shape (batch_size, d1, .. dN). loss function return float tensor. metrics List metrics evaluated model training testing. can string (name built-function), function Metric() instance. See metric_* family functions. Typically use metrics = c('accuracy'). function callable signature result = fn(y_true, y_pred). specify different metrics different outputs multi-output model, also pass named list, metrics = list(= 'accuracy', b = c('accuracy', 'mse')). can also pass list specify metric list metrics output, metrics = list(c('accuracy'), c('accuracy', 'mse')) metrics = list('accuracy', c('accuracy', 'mse')). pass strings 'accuracy' 'acc', convert one metric_binary_accuracy(), metric_categorical_accuracy(), metric_sparse_categorical_accuracy() based shapes targets model output. similar conversion done strings \"crossentropy\" \"ce\" well. metrics passed evaluated without sample weighting; like sample weighting apply, can specify metrics via weighted_metrics argument instead. ... Additional arguments passed compile() model method. loss_weights Optional list (named unnamed) specifying scalar coefficients (R numerics) weight loss contributions different model outputs. loss value minimized model weighted sum individual losses, weighted loss_weights coefficients.  unnamed list, expected 1:1 mapping model's outputs. named list, expected map output names (strings) scalar coefficients. weighted_metrics List metrics evaluated weighted sample_weight class_weight training testing. run_eagerly Bool. TRUE, model's forward pass never compiled. recommended leave FALSE training (best performance), set TRUE debugging. steps_per_execution Int. number batches run single compiled function call. Running multiple batches inside single compiled function call can greatly improve performance TPUs small models large R/Python overhead. , one full epoch run execution. number larger size epoch passed, execution truncated size epoch. Note steps_per_execution set N, Callback$on_batch_begin Callback$on_batch_end methods called every N batches (.e. /compiled function execution). supported PyTorch backend. jit_compile Bool \"auto\". Whether use XLA compilation compiling model. jax tensorflow backends, jit_compile=\"auto\" enables XLA compilation model supports , disabled otherwise. torch backend, \"auto\" default eager execution jit_compile=True run torch.compile \"inductor\" backend. auto_scale_loss Bool. TRUE model dtype policy \"mixed_float16\", passed optimizer automatically wrapped LossScaleOptimizer, dynamically scale loss prevent underflow.","code":""},{"path":"https://keras.posit.co/reference/compile.keras.models.model.Model.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Configure a model for training. — compile.keras.models.model.Model","text":"","code":"model |> compile(     optimizer = optimizer_Adam(learning_rate=1e-3),     loss = loss_binary_crossentropy(),     metrics = c(         metric_binary_accuracy(),         metric_false_negatives()     ) )"},{"path":[]},{"path":"https://keras.posit.co/reference/config_backend.html","id":null,"dir":"Reference","previous_headings":"","what":"Publicly accessible method for determining the current backend. — config_backend","title":"Publicly accessible method for determining the current backend. — config_backend","text":"Publicly accessible method determining current backend.","code":""},{"path":"https://keras.posit.co/reference/config_backend.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Publicly accessible method for determining the current backend. — config_backend","text":"","code":"config_backend()  config_set_backend(value = c(\"tensorflow\", \"jax\", \"torch\"))"},{"path":"https://keras.posit.co/reference/config_backend.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Publicly accessible method for determining the current backend. — config_backend","text":"value string, one \"tensorflow\", \"jax\", \"torch\".","code":""},{"path":"https://keras.posit.co/reference/config_backend.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Publicly accessible method for determining the current backend. — config_backend","text":"String, name backend Keras currently using. One \"tensorflow\", \"torch\", \"jax\".","code":""},{"path":"https://keras.posit.co/reference/config_backend.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Publicly accessible method for determining the current backend. — config_backend","text":"","code":"keras3::config_backend() ## [1] \"tensorflow\" # 'tensorflow'"},{"path":[]},{"path":"https://keras.posit.co/reference/config_disable_interactive_logging.html","id":null,"dir":"Reference","previous_headings":"","what":"Turn off interactive logging. — config_disable_interactive_logging","title":"Turn off interactive logging. — config_disable_interactive_logging","text":"interactive logging disabled, Keras sends logs absl.logging. best option using Keras non-interactive way, running training inference job server.","code":""},{"path":"https://keras.posit.co/reference/config_disable_interactive_logging.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Turn off interactive logging. — config_disable_interactive_logging","text":"","code":"config_disable_interactive_logging()"},{"path":[]},{"path":"https://keras.posit.co/reference/config_disable_traceback_filtering.html","id":null,"dir":"Reference","previous_headings":"","what":"Turn off traceback filtering. — config_disable_traceback_filtering","title":"Turn off traceback filtering. — config_disable_traceback_filtering","text":"Raw Keras tracebacks (also known stack traces) involve many internal frames, can challenging read , actionable end users. default, Keras filters internal frames exceptions raises, keep traceback short, readable, focused actionable (code). See also keras.config.enable_traceback_filtering() keras.config.is_traceback_filtering_enabled(). previously disabled traceback filtering via keras.config.disable_traceback_filtering(), can re-enable via keras.config.enable_traceback_filtering().","code":""},{"path":"https://keras.posit.co/reference/config_disable_traceback_filtering.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Turn off traceback filtering. — config_disable_traceback_filtering","text":"","code":"config_disable_traceback_filtering()"},{"path":[]},{"path":"https://keras.posit.co/reference/config_enable_interactive_logging.html","id":null,"dir":"Reference","previous_headings":"","what":"Turn on interactive logging. — config_enable_interactive_logging","title":"Turn on interactive logging. — config_enable_interactive_logging","text":"interactive logging enabled, Keras displays logs via stdout. provides best experience using Keras interactive environment shell notebook.","code":""},{"path":"https://keras.posit.co/reference/config_enable_interactive_logging.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Turn on interactive logging. — config_enable_interactive_logging","text":"","code":"config_enable_interactive_logging()"},{"path":[]},{"path":"https://keras.posit.co/reference/config_enable_traceback_filtering.html","id":null,"dir":"Reference","previous_headings":"","what":"Turn on traceback filtering. — config_enable_traceback_filtering","title":"Turn on traceback filtering. — config_enable_traceback_filtering","text":"Raw Keras tracebacks (also known stack traces) involve many internal frames, can challenging read , actionable end users. default, Keras filters internal frames exceptions raises, keep traceback short, readable, focused actionable (code). See also keras.config.disable_traceback_filtering() keras.config.is_traceback_filtering_enabled(). previously disabled traceback filtering via keras.config.disable_traceback_filtering(), can re-enable via keras.config.enable_traceback_filtering().","code":""},{"path":"https://keras.posit.co/reference/config_enable_traceback_filtering.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Turn on traceback filtering. — config_enable_traceback_filtering","text":"","code":"config_enable_traceback_filtering()"},{"path":[]},{"path":"https://keras.posit.co/reference/config_enable_unsafe_deserialization.html","id":null,"dir":"Reference","previous_headings":"","what":"Disables safe mode globally, allowing deserialization of lambdas. — config_enable_unsafe_deserialization","title":"Disables safe mode globally, allowing deserialization of lambdas. — config_enable_unsafe_deserialization","text":"Disables safe mode globally, allowing deserialization lambdas.","code":""},{"path":"https://keras.posit.co/reference/config_enable_unsafe_deserialization.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Disables safe mode globally, allowing deserialization of lambdas. — config_enable_unsafe_deserialization","text":"","code":"config_enable_unsafe_deserialization()"},{"path":[]},{"path":"https://keras.posit.co/reference/config_epsilon.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the value of the fuzz factor used in numeric expressions. — config_epsilon","title":"Return the value of the fuzz factor used in numeric expressions. — config_epsilon","text":"Return value fuzz factor used numeric expressions.","code":""},{"path":"https://keras.posit.co/reference/config_epsilon.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the value of the fuzz factor used in numeric expressions. — config_epsilon","text":"","code":"config_epsilon()"},{"path":"https://keras.posit.co/reference/config_epsilon.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the value of the fuzz factor used in numeric expressions. — config_epsilon","text":"float.","code":""},{"path":"https://keras.posit.co/reference/config_epsilon.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Return the value of the fuzz factor used in numeric expressions. — config_epsilon","text":"","code":"keras3::config_epsilon() ## [1] 1e-07"},{"path":[]},{"path":"https://keras.posit.co/reference/config_floatx.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the default float type, as a string. — config_floatx","title":"Return the default float type, as a string. — config_floatx","text":"E.g. 'float16', 'float32', 'float64'.","code":""},{"path":"https://keras.posit.co/reference/config_floatx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the default float type, as a string. — config_floatx","text":"","code":"config_floatx()"},{"path":"https://keras.posit.co/reference/config_floatx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the default float type, as a string. — config_floatx","text":"String, current default float type.","code":""},{"path":"https://keras.posit.co/reference/config_floatx.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Return the default float type, as a string. — config_floatx","text":"","code":"keras3:::config_floatx() ## [1] \"float32\""},{"path":[]},{"path":"https://keras.posit.co/reference/config_image_data_format.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the default image data format convention. — config_image_data_format","title":"Return the default image data format convention. — config_image_data_format","text":"Return default image data format convention.","code":""},{"path":"https://keras.posit.co/reference/config_image_data_format.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the default image data format convention. — config_image_data_format","text":"","code":"config_image_data_format()"},{"path":"https://keras.posit.co/reference/config_image_data_format.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the default image data format convention. — config_image_data_format","text":"string, either 'channels_first' 'channels_last'.","code":""},{"path":"https://keras.posit.co/reference/config_image_data_format.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Return the default image data format convention. — config_image_data_format","text":"","code":"config_image_data_format() ## [1] \"channels_last\""},{"path":[]},{"path":"https://keras.posit.co/reference/config_is_interactive_logging_enabled.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if interactive logging is enabled. — config_is_interactive_logging_enabled","title":"Check if interactive logging is enabled. — config_is_interactive_logging_enabled","text":"switch writing logs stdout absl.logging, may use keras.config.enable_interactive_logging() keras.config.disable_interactie_logging().","code":""},{"path":"https://keras.posit.co/reference/config_is_interactive_logging_enabled.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if interactive logging is enabled. — config_is_interactive_logging_enabled","text":"","code":"config_is_interactive_logging_enabled()"},{"path":"https://keras.posit.co/reference/config_is_interactive_logging_enabled.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if interactive logging is enabled. — config_is_interactive_logging_enabled","text":"Boolean, TRUE interactive logging enabled, FALSE otherwise.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/config_is_traceback_filtering_enabled.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if traceback filtering is enabled. — config_is_traceback_filtering_enabled","title":"Check if traceback filtering is enabled. — config_is_traceback_filtering_enabled","text":"Raw Keras tracebacks (also known stack traces) involve many internal frames, can challenging read , actionable end users. default, Keras filters internal frames exceptions raises, keep traceback short, readable, focused actionable (code). See also keras.config.enable_traceback_filtering() keras.config.disable_traceback_filtering(). previously disabled traceback filtering via keras.config.disable_traceback_filtering(), can re-enable via keras.config.enable_traceback_filtering().","code":""},{"path":"https://keras.posit.co/reference/config_is_traceback_filtering_enabled.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if traceback filtering is enabled. — config_is_traceback_filtering_enabled","text":"","code":"config_is_traceback_filtering_enabled()"},{"path":"https://keras.posit.co/reference/config_is_traceback_filtering_enabled.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if traceback filtering is enabled. — config_is_traceback_filtering_enabled","text":"Boolean, TRUE traceback filtering enabled, FALSE otherwise.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/config_set_epsilon.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the value of the fuzz factor used in numeric expressions. — config_set_epsilon","title":"Set the value of the fuzz factor used in numeric expressions. — config_set_epsilon","text":"Set value fuzz factor used numeric expressions.","code":""},{"path":"https://keras.posit.co/reference/config_set_epsilon.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the value of the fuzz factor used in numeric expressions. — config_set_epsilon","text":"","code":"config_set_epsilon(value)"},{"path":"https://keras.posit.co/reference/config_set_epsilon.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the value of the fuzz factor used in numeric expressions. — config_set_epsilon","text":"value float. New value epsilon.","code":""},{"path":"https://keras.posit.co/reference/config_set_epsilon.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the value of the fuzz factor used in numeric expressions. — config_set_epsilon","text":"","code":"config_epsilon() ## [1] 1e-07 config_set_epsilon(1e-5) config_epsilon() ## [1] 1e-05 # Set it back to the default value. config_set_epsilon(1e-7)"},{"path":[]},{"path":"https://keras.posit.co/reference/config_set_floatx.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the default float dtype. — config_set_floatx","title":"Set the default float dtype. — config_set_floatx","text":"Set default float dtype.","code":""},{"path":"https://keras.posit.co/reference/config_set_floatx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the default float dtype. — config_set_floatx","text":"","code":"config_set_floatx(value)"},{"path":"https://keras.posit.co/reference/config_set_floatx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the default float dtype. — config_set_floatx","text":"value String; 'float16', 'float32', 'float64'.","code":""},{"path":"https://keras.posit.co/reference/config_set_floatx.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Set the default float dtype. — config_set_floatx","text":"recommended set \"float16\" training, likely cause numeric stability issues. Instead, mixed precision, leverages mix float16 float32. can configured calling keras3::keras$mixed_precision$set_dtype_policy('mixed_float16').","code":""},{"path":"https://keras.posit.co/reference/config_set_floatx.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the default float dtype. — config_set_floatx","text":"","code":"config_floatx() ## [1] \"float32\" config_set_floatx('float64') config_floatx() ## [1] \"float64\" # Set it back to float32 config_set_floatx('float32')"},{"path":"https://keras.posit.co/reference/config_set_floatx.html","id":"raises","dir":"Reference","previous_headings":"","what":"Raises","title":"Set the default float dtype. — config_set_floatx","text":"ValueError: case invalid value.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/config_set_image_data_format.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the value of the image data format convention. — config_set_image_data_format","title":"Set the value of the image data format convention. — config_set_image_data_format","text":"Set value image data format convention.","code":""},{"path":"https://keras.posit.co/reference/config_set_image_data_format.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the value of the image data format convention. — config_set_image_data_format","text":"","code":"config_set_image_data_format(data_format)"},{"path":"https://keras.posit.co/reference/config_set_image_data_format.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the value of the image data format convention. — config_set_image_data_format","text":"data_format string. 'channels_first' 'channels_last'.","code":""},{"path":"https://keras.posit.co/reference/config_set_image_data_format.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the value of the image data format convention. — config_set_image_data_format","text":"","code":"config_image_data_format() ## [1] \"channels_last\" # 'channels_last' keras3::config_set_image_data_format('channels_first') config_image_data_format() ## [1] \"channels_first\" # Set it back to `'channels_last'` keras3::config_set_image_data_format('channels_last')"},{"path":[]},{"path":"https://keras.posit.co/reference/constraint_maxnorm.html","id":null,"dir":"Reference","previous_headings":"","what":"MaxNorm weight constraint. — constraint_maxnorm","title":"MaxNorm weight constraint. — constraint_maxnorm","text":"Constrains weights incident hidden unit norm less equal desired value. Also available via shortcut function keras.constraints.max_norm.","code":""},{"path":"https://keras.posit.co/reference/constraint_maxnorm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"MaxNorm weight constraint. — constraint_maxnorm","text":"","code":"constraint_maxnorm(max_value = 2L, axis = 1L)"},{"path":"https://keras.posit.co/reference/constraint_maxnorm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"MaxNorm weight constraint. — constraint_maxnorm","text":"max_value maximum norm value incoming weights. axis integer, axis along calculate weight norms. instance, Dense layer weight matrix shape (input_dim, output_dim), set axis 0 constrain weight vector length (input_dim,). Conv2D layer data_format = \"channels_last\", weight tensor shape (rows, cols, input_depth, output_depth), set axis [0, 1, 2] constrain weights filter tensor size (rows, cols, input_depth).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/constraint_minmaxnorm.html","id":null,"dir":"Reference","previous_headings":"","what":"MinMaxNorm weight constraint. — constraint_minmaxnorm","title":"MinMaxNorm weight constraint. — constraint_minmaxnorm","text":"Constrains weights incident hidden unit norm lower bound upper bound.","code":""},{"path":"https://keras.posit.co/reference/constraint_minmaxnorm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"MinMaxNorm weight constraint. — constraint_minmaxnorm","text":"","code":"constraint_minmaxnorm(min_value = 0, max_value = 1, rate = 1, axis = 1L)"},{"path":"https://keras.posit.co/reference/constraint_minmaxnorm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"MinMaxNorm weight constraint. — constraint_minmaxnorm","text":"min_value minimum norm incoming weights. max_value maximum norm incoming weights. rate rate enforcing constraint: weights rescaled yield (1 - rate) * norm + rate * norm.clip(min_value, max_value). Effectively, means rate = 1.0 stands strict enforcement constraint, rate<1.0 means weights rescaled step slowly move towards value inside desired interval. axis integer, axis along calculate weight norms. instance, Dense layer weight matrix shape (input_dim, output_dim), set axis 0 constrain weight vector length (input_dim,). Conv2D layer data_format = \"channels_last\", weight tensor shape (rows, cols, input_depth, output_depth), set axis [0, 1, 2] constrain weights filter tensor size (rows, cols, input_depth).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/constraint_nonneg.html","id":null,"dir":"Reference","previous_headings":"","what":"Constrains the weights to be non-negative. — constraint_nonneg","title":"Constrains the weights to be non-negative. — constraint_nonneg","text":"Constrains weights non-negative.","code":""},{"path":"https://keras.posit.co/reference/constraint_nonneg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Constrains the weights to be non-negative. — constraint_nonneg","text":"","code":"constraint_nonneg()"},{"path":[]},{"path":"https://keras.posit.co/reference/constraint_unitnorm.html","id":null,"dir":"Reference","previous_headings":"","what":"Constrains the weights incident to each hidden unit to have unit norm. — constraint_unitnorm","title":"Constrains the weights incident to each hidden unit to have unit norm. — constraint_unitnorm","text":"Constrains weights incident hidden unit unit norm.","code":""},{"path":"https://keras.posit.co/reference/constraint_unitnorm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Constrains the weights incident to each hidden unit to have unit norm. — constraint_unitnorm","text":"","code":"constraint_unitnorm(axis = 1L)"},{"path":"https://keras.posit.co/reference/constraint_unitnorm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Constrains the weights incident to each hidden unit to have unit norm. — constraint_unitnorm","text":"axis integer, axis along calculate weight norms. instance, Dense layer weight matrix shape (input_dim, output_dim), set axis 0 constrain weight vector length (input_dim,). Conv2D layer data_format = \"channels_last\", weight tensor shape (rows, cols, input_depth, output_depth), set axis [0, 1, 2] constrain weights filter tensor size (rows, cols, input_depth).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/count_params.html","id":null,"dir":"Reference","previous_headings":"","what":"Count the total number of scalars composing the weights. — count_params","title":"Count the total number of scalars composing the weights. — count_params","text":"Count total number scalars composing weights.","code":""},{"path":"https://keras.posit.co/reference/count_params.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Count the total number of scalars composing the weights. — count_params","text":"","code":"count_params(object)"},{"path":"https://keras.posit.co/reference/count_params.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Count the total number of scalars composing the weights. — count_params","text":"object Layer model object","code":""},{"path":"https://keras.posit.co/reference/count_params.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Count the total number of scalars composing the weights. — count_params","text":"integer count","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/create_layer.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Keras Layer — create_layer","title":"Create a Keras Layer — create_layer","text":"Create Keras Layer","code":""},{"path":"https://keras.posit.co/reference/create_layer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Keras Layer — create_layer","text":"","code":"create_layer(layer_class, object, args = list())"},{"path":"https://keras.posit.co/reference/create_layer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Keras Layer — create_layer","text":"layer_class Python layer class R6 class type KerasLayer object Object compose layer . either keras_model_sequential() add layer , another Layer layer call. args List arguments layer constructor function","code":""},{"path":"https://keras.posit.co/reference/create_layer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Keras Layer — create_layer","text":"Keras layer","code":""},{"path":"https://keras.posit.co/reference/create_layer.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Create a Keras Layer — create_layer","text":"object parameter can missing, case layer created without connection existing graph.","code":""},{"path":"https://keras.posit.co/reference/create_layer_wrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Keras Layer wrapper — create_layer_wrapper","title":"Create a Keras Layer wrapper — create_layer_wrapper","text":"Create Keras Layer wrapper","code":""},{"path":"https://keras.posit.co/reference/create_layer_wrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Keras Layer wrapper — create_layer_wrapper","text":"","code":"create_layer_wrapper(Layer, modifiers = NULL, convert = TRUE)"},{"path":"https://keras.posit.co/reference/create_layer_wrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Keras Layer wrapper — create_layer_wrapper","text":"Layer R6 Python class generator inherits keras$layers$Layer modifiers named list functions modify user-supplied arguments passed class constructor. (e.g., list(units = .integer)) convert ignored. See guide 'making_new_layers_and_models_via_subclassing.Rmd' example usage.","code":""},{"path":"https://keras.posit.co/reference/create_layer_wrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Keras Layer wrapper — create_layer_wrapper","text":"R function behaves similarly builtin keras layer_* functions. called, create class instance, also optionally call supplied argument object present. enables keras layers compose nicely pipe (|>). R function arguments taken initialize (__init__) method Layer. Layer R6 object, delay initializing python session, safe use R package.","code":""},{"path":"https://keras.posit.co/reference/custom_metric.html","id":null,"dir":"Reference","previous_headings":"","what":"Custom metric function — custom_metric","title":"Custom metric function — custom_metric","text":"Custom metric function","code":""},{"path":"https://keras.posit.co/reference/custom_metric.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Custom metric function — custom_metric","text":"","code":"custom_metric(name, metric_fn)"},{"path":"https://keras.posit.co/reference/custom_metric.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Custom metric function — custom_metric","text":"name name used show training progress output metric_fn R function signature function(y_true, y_pred){} accepts tensors.","code":""},{"path":"https://keras.posit.co/reference/custom_metric.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Custom metric function — custom_metric","text":"can provide arbitrary R function custom metric. Note y_true y_pred parameters tensors, computations use backend tensor functions. Use custom_metric() function define custom metric. Note name ('mean_pred') provided custom metric function: name used within training progress output. want save load model custom metrics, also specify metric call load_model(). example: load_model(\"my_model.keras\", c('mean_pred' = metric_mean_pred)). Alternatively, can wrap code call with_custom_object_scope() allow refer metric name just like built keras metrics. Documentation available backend tensor functions can found https://tensorflow.rstudio.com/reference/keras/#backend. Alternative ways supplying custom metrics: custom_metric(): Arbitrary R function. metric_mean_wrapper(): Wrap arbitrary R function Metric instance. subclass keras$metrics$Metric: see ?Metric example.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/dataset_boston_housing.html","id":null,"dir":"Reference","previous_headings":"","what":"Boston housing price regression dataset — dataset_boston_housing","title":"Boston housing price regression dataset — dataset_boston_housing","text":"Dataset taken StatLib library maintained Carnegie Mellon University.","code":""},{"path":"https://keras.posit.co/reference/dataset_boston_housing.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Boston housing price regression dataset — dataset_boston_housing","text":"","code":"dataset_boston_housing(   path = \"boston_housing.npz\",   test_split = 0.2,   seed = 113L )"},{"path":"https://keras.posit.co/reference/dataset_boston_housing.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Boston housing price regression dataset — dataset_boston_housing","text":"path Path cache dataset locally (relative ~/.keras/datasets). test_split fraction data reserve test set. seed Random seed shuffling data computing test split.","code":""},{"path":"https://keras.posit.co/reference/dataset_boston_housing.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Boston housing price regression dataset — dataset_boston_housing","text":"Lists training test data: train$x, train$y, test$x, test$y. Samples contain 13 attributes houses different locations around Boston suburbs late 1970s. Targets median values houses location (k$).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/dataset_cifar10.html","id":null,"dir":"Reference","previous_headings":"","what":"CIFAR10 small image classification — dataset_cifar10","title":"CIFAR10 small image classification — dataset_cifar10","text":"Dataset 50,000 32x32 color training images, labeled 10 categories, 10,000 test images.","code":""},{"path":"https://keras.posit.co/reference/dataset_cifar10.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"CIFAR10 small image classification — dataset_cifar10","text":"","code":"dataset_cifar10()"},{"path":"https://keras.posit.co/reference/dataset_cifar10.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"CIFAR10 small image classification — dataset_cifar10","text":"Lists training test data: train$x, train$y, test$x, test$y. x data array RGB image data shape (num_samples, 3, 32, 32). y data array category labels (integers range 0-9) shape (num_samples).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/dataset_cifar100.html","id":null,"dir":"Reference","previous_headings":"","what":"CIFAR100 small image classification — dataset_cifar100","title":"CIFAR100 small image classification — dataset_cifar100","text":"Dataset 50,000 32x32 color training images, labeled 100 categories, 10,000 test images.","code":""},{"path":"https://keras.posit.co/reference/dataset_cifar100.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"CIFAR100 small image classification — dataset_cifar100","text":"","code":"dataset_cifar100(label_mode = c(\"fine\", \"coarse\"))"},{"path":"https://keras.posit.co/reference/dataset_cifar100.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"CIFAR100 small image classification — dataset_cifar100","text":"label_mode one \"fine\", \"coarse\".","code":""},{"path":"https://keras.posit.co/reference/dataset_cifar100.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"CIFAR100 small image classification — dataset_cifar100","text":"Lists training test data: train$x, train$y, test$x, test$y. x data array RGB image data shape (num_samples, 3, 32, 32). y data array category labels shape (num_samples).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/dataset_fashion_mnist.html","id":null,"dir":"Reference","previous_headings":"","what":"Fashion-MNIST database of fashion articles — dataset_fashion_mnist","title":"Fashion-MNIST database of fashion articles — dataset_fashion_mnist","text":"Dataset 60,000 28x28 grayscale images 10 fashion article classes, along test set 10,000 images. dataset can used drop-replacement MNIST. class labels encoded integers 0-9 correspond T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt,","code":""},{"path":"https://keras.posit.co/reference/dataset_fashion_mnist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fashion-MNIST database of fashion articles — dataset_fashion_mnist","text":"","code":"dataset_fashion_mnist()"},{"path":"https://keras.posit.co/reference/dataset_fashion_mnist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fashion-MNIST database of fashion articles — dataset_fashion_mnist","text":"Lists training test data: train$x, train$y, test$x, test$y, x array grayscale image data shape (num_samples, 28, 28) y array article labels (integers range 0-9) shape (num_samples).","code":""},{"path":"https://keras.posit.co/reference/dataset_fashion_mnist.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fashion-MNIST database of fashion articles — dataset_fashion_mnist","text":"Dataset 60,000 28x28 grayscale images 10 fashion categories, along test set 10,000 images. dataset can used drop-replacement MNIST. class labels : 0 - T-shirt/top 1 - Trouser 2 - Pullover 3 - Dress 4 - Coat 5 - Sandal 6 - Shirt 7 - Sneaker 8 - Bag 9 - Ankle boot","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/dataset_imdb.html","id":null,"dir":"Reference","previous_headings":"","what":"IMDB Movie reviews sentiment classification — dataset_imdb","title":"IMDB Movie reviews sentiment classification — dataset_imdb","text":"Dataset 25,000 movies reviews IMDB, labeled sentiment (positive/negative). Reviews preprocessed, review encoded sequence word indexes (integers). convenience, words indexed overall frequency dataset, instance integer \"3\" encodes 3rd frequent word data. allows quick filtering operations : \"consider top 10,000 common words, eliminate top 20 common words\".","code":""},{"path":"https://keras.posit.co/reference/dataset_imdb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"IMDB Movie reviews sentiment classification — dataset_imdb","text":"","code":"dataset_imdb(   path = \"imdb.npz\",   num_words = NULL,   skip_top = 0L,   maxlen = NULL,   seed = 113L,   start_char = 1L,   oov_char = 2L,   index_from = 3L )  dataset_imdb_word_index(path = \"imdb_word_index.json\")"},{"path":"https://keras.posit.co/reference/dataset_imdb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"IMDB Movie reviews sentiment classification — dataset_imdb","text":"path cache data (relative ~/.keras/dataset). num_words Max number words include. Words ranked often occur (training set) frequent words kept skip_top Skip top N frequently occuring words (may informative). maxlen sequences longer filtered . seed random seed sample shuffling. start_char start sequence marked character. Set 1 0 usually padding character. oov_char Words cut num_words skip_top limit replaced character. index_from Index actual words index higher.","code":""},{"path":"https://keras.posit.co/reference/dataset_imdb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"IMDB Movie reviews sentiment classification — dataset_imdb","text":"Lists training test data: train$x, train$y, test$x, test$y. x data includes integer sequences. num_words argument specific, maximum possible index value num_words-1. maxlen argument specified, largest possible sequence length maxlen. y data includes set integer labels (0 1). dataset_imdb_word_index() function returns list names words values integer.","code":""},{"path":"https://keras.posit.co/reference/dataset_imdb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"IMDB Movie reviews sentiment classification — dataset_imdb","text":"convention, \"0\" stand specific word, instead used encode unknown word.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/dataset_mnist.html","id":null,"dir":"Reference","previous_headings":"","what":"MNIST database of handwritten digits — dataset_mnist","title":"MNIST database of handwritten digits — dataset_mnist","text":"Dataset 60,000 28x28 grayscale images 10 digits, along test set 10,000 images.","code":""},{"path":"https://keras.posit.co/reference/dataset_mnist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"MNIST database of handwritten digits — dataset_mnist","text":"","code":"dataset_mnist(path = \"mnist.npz\")"},{"path":"https://keras.posit.co/reference/dataset_mnist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"MNIST database of handwritten digits — dataset_mnist","text":"path Path cache dataset locally (relative ~/.keras/datasets).","code":""},{"path":"https://keras.posit.co/reference/dataset_mnist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"MNIST database of handwritten digits — dataset_mnist","text":"Lists training test data: train$x, train$y, test$x, test$y, x array grayscale image data shape (num_samples, 28, 28) y array digit labels (integers range 0-9) shape (num_samples).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/dataset_reuters.html","id":null,"dir":"Reference","previous_headings":"","what":"Reuters newswire topics classification — dataset_reuters","title":"Reuters newswire topics classification — dataset_reuters","text":"Dataset 11,228 newswires Reuters, labeled 46 topics. dataset_imdb() , wire encoded sequence word indexes (conventions).","code":""},{"path":"https://keras.posit.co/reference/dataset_reuters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reuters newswire topics classification — dataset_reuters","text":"","code":"dataset_reuters(   path = \"reuters.npz\",   num_words = NULL,   skip_top = 0L,   maxlen = NULL,   test_split = 0.2,   seed = 113L,   start_char = 1L,   oov_char = 2L,   index_from = 3L )  dataset_reuters_word_index(path = \"reuters_word_index.pkl\")"},{"path":"https://keras.posit.co/reference/dataset_reuters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reuters newswire topics classification — dataset_reuters","text":"path cache data (relative ~/.keras/dataset). num_words Max number words include. Words ranked often occur (training set) frequent words kept skip_top Skip top N frequently occuring words (may informative). maxlen Truncate sequences length. test_split Fraction dataset used test data. seed Random seed sample shuffling. start_char start sequence marked character. Set 1 0 usually padding character. oov_char words cut num_words skip_top limit replaced character. index_from index actual words index higher.","code":""},{"path":"https://keras.posit.co/reference/dataset_reuters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reuters newswire topics classification — dataset_reuters","text":"Lists training test data: train$x, train$y, test$x, test$y format dataset_imdb(). dataset_reuters_word_index() function returns list names words values integer. e.g. word_index[[\"giraffe\"]] might return 1234.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/deserialize_keras_object.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve the object by deserializing the config dict. — deserialize_keras_object","title":"Retrieve the object by deserializing the config dict. — deserialize_keras_object","text":"config dict Python dictionary consists set key-value pairs, represents Keras object, Optimizer, Layer, Metrics, etc. saving loading library uses following keys record information Keras object: class_name: String. name class, exactly defined source code, \"LossesContainer\". config: Named List. Library-defined user-defined key-value pairs store configuration object, obtained object$get_config(). module: String. path python module. Built-Keras classes expect prefix keras. registered_name: String. key class registered via register_keras_serializable(package, name) API. key format '{package}>{name}', package name arguments passed register_keras_serializable(). name provided, uses class name. registered_name successfully resolves class (registered), class_name config values config dict used. registered_name used non-built-classes. example, following config list represents built-Adam optimizer relevant config:     class exported Keras namespace, library tracks module class_name. example:   following config represents user-customized MeanSquaredError loss:","code":"config = list(   class_name = \"Adam\",   config = list(     amsgrad = FALSE,     beta_1 = 0.8999999761581421,     beta_2 = 0.9990000128746033,     epsilon = 1e-07,     learning_rate = 0.0010000000474974513,     name = \"Adam\"   ),   module = \"keras.optimizers\",   registered_name = NULL ) # Returns an `Adam` instance identical to the original one. deserialize_keras_object(config) ## <keras.src.optimizers.adam.Adam object> config = list(   class_name = \"MetricsList\",   config =  list(     ...   ),   module = \"keras.trainers.compile_utils\",   registered_name = \"MetricsList\" )  # Returns a `MetricsList` instance identical to the original one. deserialize_keras_object(config) loss_modified_mse <- new_loss_class(   \"ModifiedMeanSquaredError\",   inherit = keras$losses$MeanSquaredError)  register_keras_serializable(loss_modified_mse, package='my_package')  config <- list(   class_name = \"ModifiedMeanSquaredError\",   config = list(     # These arguments will be passed to     # `loss_modified_mse$initialize()` when     # deserializing the object.     name = \"modified_mean_squared_error\",     reduction = \"sum_over_batch_size\"   ),   registered_name = \"my_package>ModifiedMeanSquaredError\" )  # Returns the `ModifiedMeanSquaredError` object deserialize_keras_object(config) ## <<r-namespace:keras3>.ModifiedMeanSquaredError object>"},{"path":"https://keras.posit.co/reference/deserialize_keras_object.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve the object by deserializing the config dict. — deserialize_keras_object","text":"","code":"deserialize_keras_object(config, custom_objects = NULL, safe_mode = TRUE, ...)"},{"path":"https://keras.posit.co/reference/deserialize_keras_object.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve the object by deserializing the config dict. — deserialize_keras_object","text":"config Named list describing object. custom_objects Named list containing mapping custom object names corresponding classes functions. safe_mode Boolean, whether disallow unsafe lambda deserialization. safe_mode=FALSE, loading object potential trigger arbitrary code execution. argument applicable Keras v3 model format. Defaults TRUE. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/deserialize_keras_object.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve the object by deserializing the config dict. — deserialize_keras_object","text":"object described config dictionary.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/evaluate.keras.models.model.Model.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluate a Keras Model — evaluate.keras.models.model.Model","title":"Evaluate a Keras Model — evaluate.keras.models.model.Model","text":"functions returns loss value metrics values model test mode. Computation done batches (see batch_size arg.)","code":""},{"path":"https://keras.posit.co/reference/evaluate.keras.models.model.Model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluate a Keras Model — evaluate.keras.models.model.Model","text":"","code":"# S3 method for keras.models.model.Model evaluate(   object,   x = NULL,   y = NULL,   ...,   batch_size = NULL,   verbose = getOption(\"keras.verbose\", default = \"auto\"),   sample_weight = NULL,   steps = NULL,   callbacks = NULL )"},{"path":"https://keras.posit.co/reference/evaluate.keras.models.model.Model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluate a Keras Model — evaluate.keras.models.model.Model","text":"object Keras model object x Input data. : R array (array-like), list arrays (case model multiple inputs). tensor, list tensors (case model multiple inputs). named list mapping input names corresponding array/tensors, model named inputs. tf.data.Dataset. return tuple either (inputs, targets) (inputs, targets, sample_weights). generator returning (inputs, targets) (inputs, targets, sample_weights). y Target data. Like input data x, either R array(s) backend-native tensor(s). x tf.data.Dataset generator function, y specified (since targets obtained iterator/dataset). ... forward/backward compatability. batch_size Integer NULL. Number samples per batch computation. unspecified, batch_size default 32. specify batch_size data form tf dataset generator (since generate batches). verbose \"auto\", 0, 1, 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. \"auto\" becomes 1 cases, 2 knitr render running distributed training server. Note progress bar particularly useful logged file, verbose=2 recommended running interactively (e.g. production environment). Defaults \"auto\". sample_weight Optional array weights test samples, used weighting loss function. can either pass flat (1D) R array length input samples (1:1 mapping weights samples), case temporal data, can pass 2D array shape (samples, sequence_length), apply different weight every timestep every sample. argument supported x tfdataset, instead pass sample weights third element x. steps Integer NULL. Total number steps (batches samples) declaring evaluation round finished. Ignored default value NULL. x tf.data.Dataset steps NULL, evaluation run dataset exhausted. callbacks List Callback instances. List callbacks apply evaluation.","code":""},{"path":"https://keras.posit.co/reference/evaluate.keras.models.model.Model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluate a Keras Model — evaluate.keras.models.model.Model","text":"Scalar test loss (model single output metrics) list scalars (model multiple outputs /metrics). attribute model$metrics_names give display labels scalar outputs.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/evaluate_generator.html","id":null,"dir":"Reference","previous_headings":"","what":"(Deprecated) Evaluates the model on a data generator. — evaluate_generator","title":"(Deprecated) Evaluates the model on a data generator. — evaluate_generator","text":"generator return kind data accepted test_on_batch().","code":""},{"path":"https://keras.posit.co/reference/evaluate_generator.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"(Deprecated) Evaluates the model on a data generator. — evaluate_generator","text":"","code":"evaluate_generator(   object,   generator,   steps,   max_queue_size = 10,   workers = 1,   callbacks = NULL )"},{"path":"https://keras.posit.co/reference/evaluate_generator.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"(Deprecated) Evaluates the model on a data generator. — evaluate_generator","text":"object Model object evaluate generator Generator yielding lists (inputs, targets) (inputs, targets, sample_weights) steps Total number steps (batches samples) yield generator stopping. max_queue_size Maximum size generator queue. unspecified, max_queue_size default 10. workers Maximum number threads use parallel processing. Note parallel processing performed native Keras generators (e.g. flow_images_from_directory()) R based generators must run main thread. callbacks List callbacks apply evaluation.","code":""},{"path":"https://keras.posit.co/reference/evaluate_generator.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"(Deprecated) Evaluates the model on a data generator. — evaluate_generator","text":"Named list model test loss (losses models multiple outputs) model metrics.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/export_savedmodel.keras.models.model.Model.html","id":null,"dir":"Reference","previous_headings":"","what":"[TF backend only] Create a TF SavedModel artifact for inference — export_savedmodel.keras.models.model.Model","title":"[TF backend only] Create a TF SavedModel artifact for inference — export_savedmodel.keras.models.model.Model","text":"(e.g. via TF-Serving). Note: can currently used TF backend. method lets export model lightweight SavedModel artifact contains model's forward pass (call() method) can served via e.g. TF-Serving. forward pass registered name serve() (see example ). original code model (including custom layers may used) longer necessary reload artifact -- entirely standalone.","code":""},{"path":"https://keras.posit.co/reference/export_savedmodel.keras.models.model.Model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"[TF backend only] Create a TF SavedModel artifact for inference — export_savedmodel.keras.models.model.Model","text":"","code":"# S3 method for keras.models.model.Model export_savedmodel(object, export_dir_base, ...)"},{"path":"https://keras.posit.co/reference/export_savedmodel.keras.models.model.Model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"[TF backend only] Create a TF SavedModel artifact for inference — export_savedmodel.keras.models.model.Model","text":"object keras model. export_dir_base string, file path save artifact. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/export_savedmodel.keras.models.model.Model.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"[TF backend only] Create a TF SavedModel artifact for inference — export_savedmodel.keras.models.model.Model","text":"","code":"# Create the artifact model |> tensorflow::export_savedmodel(\"path/to/location\")  # Later, in a different process / environment... library(tensorflow) reloaded_artifact <- tf$saved_model$load(\"path/to/location\") predictions <- reloaded_artifact$serve(input_data)  # see tfdeploy::serve_savedmodel() for serving a model over a local web api."},{"path":[]},{"path":"https://keras.posit.co/reference/fit.keras.models.model.Model.html","id":null,"dir":"Reference","previous_headings":"","what":"Train a model for a fixed number of epochs (dataset iterations). — fit.keras.models.model.Model","title":"Train a model for a fixed number of epochs (dataset iterations). — fit.keras.models.model.Model","text":"Train model fixed number epochs (dataset iterations).","code":""},{"path":"https://keras.posit.co/reference/fit.keras.models.model.Model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Train a model for a fixed number of epochs (dataset iterations). — fit.keras.models.model.Model","text":"","code":"# S3 method for keras.models.model.Model fit(   object,   x = NULL,   y = NULL,   ...,   batch_size = NULL,   epochs = 1L,   callbacks = NULL,   validation_split = 0,   validation_data = NULL,   shuffle = TRUE,   class_weight = NULL,   sample_weight = NULL,   initial_epoch = 1L,   steps_per_epoch = NULL,   validation_steps = NULL,   validation_batch_size = NULL,   validation_freq = 1L,   verbose = getOption(\"keras.verbose\", default = \"auto\"),   view_metrics = getOption(\"keras.view_metrics\", default = \"auto\") )"},{"path":"https://keras.posit.co/reference/fit.keras.models.model.Model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Train a model for a fixed number of epochs (dataset iterations). — fit.keras.models.model.Model","text":"object Keras model object x Input data. : array (array-like), list arrays (case model multiple inputs). tensor, list tensors (case model multiple inputs). named list mapping input names corresponding array/tensors, model named inputs. tf.data.Dataset. return tuple either (inputs, targets) (inputs, targets, sample_weights). generator returning (inputs, targets) (inputs, targets, sample_weights). y Target data. Like input data x, either array(s) backend-native tensor(s). x TF Dataset generator, y specified (since targets obtained x). ... Additional arguments passed model fit() method. batch_size Integer NULL. Number samples per gradient update. unspecified, batch_size default 32. specify batch_size data form TF Datasets generators, (since generate batches). epochs Integer. Number epochs train model. epoch iteration entire x y data provided (unless steps_per_epoch flag set something NULL). Note conjunction initial_epoch, epochs understood \"final epoch\". model trained number iterations given epochs, merely epoch index epochs reached. callbacks List Callback() instances. List callbacks apply training. See callback_*. validation_split Float 0 1. Fraction training data used validation data. model set apart fraction training data, train , evaluate loss model metrics data end epoch. validation data selected last samples x y data provided, shuffling. argument supported x TF Dataset generator. validation_data validation_split provided, validation_data override validation_split. validation_data Data evaluate loss model metrics end epoch. model trained data. Thus, note fact validation loss data provided using validation_split validation_data affected regularization layers like noise dropout. validation_data override validation_split. validation_data : tuple (x_val, y_val) arrays tensors. tuple (x_val, y_val, val_sample_weights) arrays. generator returning (inputs, targets) (inputs, targets, sample_weights). shuffle Boolean, whether shuffle training data epoch. argument ignored x generator TF Dataset. class_weight Optional named list mapping class indices (integers, 0-based) weight (float) value, used weighting loss function (training ). can useful tell model \"pay attention\" samples -represented class. class_weight specified targets rank 2 greater, either y must one-hot encoded, explicit final dimension 1 must included sparse class labels. sample_weight Optional array weights training samples, used weighting loss function (training ). can either pass flat (1D) array/vector length input samples (1:1 mapping weights samples), case temporal data, can pass 2D array (matrix) shape (samples, sequence_length), apply different weight every timestep every sample. argument supported x TF Dataset generator, instead provide sample_weights third element x. Note sample weighting apply metrics specified via metrics argument compile(). apply sample weighting metrics, can specify via weighted_metrics compile() instead. initial_epoch Integer. Epoch start training (useful resuming previous training run). steps_per_epoch Integer NULL. Total number steps (batches samples) declaring one epoch finished starting next epoch. training input tensors backend-native tensors, default NULL equal number samples dataset divided batch size, 1 determined. x TF Dataset, steps_per_epoch NULL, epoch run input dataset exhausted.  passing infinitely repeating dataset, must specify steps_per_epoch argument. steps_per_epoch = -1 training run indefinitely infinitely repeating dataset. validation_steps relevant validation_data provided. Total number steps (batches samples) draw stopping performing validation end every epoch. validation_steps NULL, validation run validation_data dataset exhausted. case infinitely repeated dataset, run infinite loop. validation_steps specified part dataset consumed, evaluation start beginning dataset epoch. ensures validation samples used every time. validation_batch_size Integer NULL. Number samples per validation batch. unspecified, default batch_size. specify validation_batch_size data form TF Datasets generator instances (since generate batches). validation_freq relevant validation data provided. Specifies many training epochs run new validation run performed, e.g. validation_freq=2 runs validation every 2 epochs. verbose \"auto\", 0, 1, 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. \"auto\" becomes 1 cases, 2 knitr render running distributed training server. Note progress bar particularly useful logged file, verbose=2 recommended running interactively (e.g., production environment). Defaults \"auto\". view_metrics View realtime plot training metrics (epoch). default (\"auto\") display plot running within RStudio, metrics specified model compile(), epochs > 1 verbose > 0. Use global keras.view_metrics option establish different default.","code":""},{"path":"https://keras.posit.co/reference/fit.keras.models.model.Model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Train a model for a fixed number of epochs (dataset iterations). — fit.keras.models.model.Model","text":"History object. History.history attribute record training loss values metrics values successive epochs, well validation loss values validation metrics values (applicable).","code":""},{"path":"https://keras.posit.co/reference/fit.keras.models.model.Model.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Train a model for a fixed number of epochs (dataset iterations). — fit.keras.models.model.Model","text":"Unpacking behavior iterator-like inputs: common pattern pass iterator like object tf.data.Dataset generator fit(), fact yield features (x) optionally targets (y) sample weights (sample_weight). Keras requires output iterator-likes unambiguous. iterator return tuple() length 1, 2, 3, optional second third elements used y sample_weight respectively. type provided wrapped length-one tuple(), effectively treating everything x. yielding named lists, still adhere top-level tuple structure, e.g. tuple(list(x0 = x0, x = x1), y). Keras attempt separate features, targets, weights keys single dict.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/fit_generator.html","id":null,"dir":"Reference","previous_headings":"","what":"(Deprecated) Fits the model on data yielded batch-by-batch by a generator. — fit_generator","title":"(Deprecated) Fits the model on data yielded batch-by-batch by a generator. — fit_generator","text":"generator run parallel model, efficiency. instance, allows real-time data augmentation images CPU parallel training model GPU.","code":""},{"path":"https://keras.posit.co/reference/fit_generator.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"(Deprecated) Fits the model on data yielded batch-by-batch by a generator. — fit_generator","text":"","code":"fit_generator(   object,   generator,   steps_per_epoch,   epochs = 1,   verbose = getOption(\"keras.fit_verbose\", default = 1),   callbacks = NULL,   view_metrics = getOption(\"keras.view_metrics\", default = \"auto\"),   validation_data = NULL,   validation_steps = NULL,   class_weight = NULL,   max_queue_size = 10,   workers = 1,   initial_epoch = 0 )"},{"path":"https://keras.posit.co/reference/fit_generator.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"(Deprecated) Fits the model on data yielded batch-by-batch by a generator. — fit_generator","text":"object Keras model object generator generator (e.g. like one provided flow_images_from_directory() custom R generator function). output generator must list one forms:   list (single output generator) makes single batch. Therefore, arrays list must length (equal size batch). Different batches may different sizes. example, last batch epoch commonly smaller others, size dataset divisible batch size. generator expected loop data indefinitely. epoch finishes steps_per_epoch batches seen model. steps_per_epoch Total number steps (batches samples) yield generator declaring one epoch finished starting next epoch. typically equal number samples dataset divided batch size. epochs Integer. Number epochs train model. epoch iteration entire data provided, defined steps_per_epoch. Note conjunction initial_epoch, epochs understood \"final epoch\". model trained number iterations given epochs, merely epoch index epochs reached. verbose Verbosity mode (0 = silent, 1 = progress bar, 2 = one line per epoch). Defaults 1 contexts, 2 knitr render running distributed training server. callbacks List callbacks apply training. view_metrics View realtime plot training metrics (epoch). default (\"auto\") display plot running within RStudio, metrics specified model compile(), epochs > 1 verbose > 0. Use global keras.view_metrics option establish different default. validation_data can either: generator validation data list (inputs, targets) list (inputs, targets, sample_weights). evaluate loss model metrics end epoch. model trained data. validation_steps relevant validation_data generator. Total number steps (batches samples) yield generator stopping end every epoch. typically equal number samples validation dataset divided batch size. class_weight Optional named list mapping class indices (integer) weight (float) value, used weighting loss function (training ). can useful tell model \"pay attention\" samples -represented class. max_queue_size Maximum size generator queue. unspecified, max_queue_size default 10. workers Maximum number threads use parallel processing. Note parallel processing performed native Keras generators (e.g. flow_images_from_directory()) R based generators must run main thread. initial_epoch epoch start training (useful resuming previous training run)","code":"- (inputs, targets)  - (inputs, targets, sample_weights)"},{"path":"https://keras.posit.co/reference/fit_generator.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"(Deprecated) Fits the model on data yielded batch-by-batch by a generator. — fit_generator","text":"Training history object (invisibly)","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/freeze_weights.html","id":null,"dir":"Reference","previous_headings":"","what":"Freeze and unfreeze weights — freeze_weights","title":"Freeze and unfreeze weights — freeze_weights","text":"Freeze weights model layer longer trainable.","code":""},{"path":"https://keras.posit.co/reference/freeze_weights.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Freeze and unfreeze weights — freeze_weights","text":"","code":"freeze_weights(object, from = NULL, to = NULL, which = NULL)  unfreeze_weights(object, from = NULL, to = NULL, which = NULL)"},{"path":"https://keras.posit.co/reference/freeze_weights.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Freeze and unfreeze weights — freeze_weights","text":"object Keras model layer object Layer instance, layer name, layer index within model Layer instance, layer name, layer index within model layer names, integer positions, layers, logical vector (length(object$layers)), function returning logical vector.","code":""},{"path":"https://keras.posit.co/reference/freeze_weights.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Freeze and unfreeze weights — freeze_weights","text":"layer arguments inclusive. applied model, freeze unfreeze global operation layers model (.e. layers within specified range set opposite value, e.g. unfrozen call freeze). Models must compiled weights frozen unfrozen.","code":""},{"path":"https://keras.posit.co/reference/freeze_weights.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Freeze and unfreeze weights — freeze_weights","text":"","code":"if (FALSE) { conv_base <- application_vgg16(   weights = \"imagenet\",   include_top = FALSE,   input_shape = c(150, 150, 3) )  # freeze it's weights freeze_weights(conv_base)  conv_base  # create a composite model that includes the base + more layers model <- keras_model_sequential() %>%   conv_base() %>%   layer_flatten() %>%   layer_dense(units = 256, activation = \"relu\") %>%   layer_dense(units = 1, activation = \"sigmoid\")  # compile model %>% compile(   loss = \"binary_crossentropy\",   optimizer = optimizer_rmsprop(lr = 2e-5),   metrics = c(\"accuracy\") )  model print(model, expand_nested = TRUE)    # unfreeze weights from \"block5_conv1\" on unfreeze_weights(conv_base, from = \"block5_conv1\")  # compile again since we froze or unfroze weights model %>% compile(   loss = \"binary_crossentropy\",   optimizer = optimizer_rmsprop(lr = 2e-5),   metrics = c(\"accuracy\") )  conv_base print(model, expand_nested = TRUE)  # freeze only the last 5 layers freeze_weights(conv_base, from = -5) conv_base # equivalently, also freeze only the last 5 layers unfreeze_weights(conv_base, to = -6) conv_base  # Freeze only layers of a certain type, e.g, BatchNorm layers batch_norm_layer_class_name <- class(layer_batch_normalization())[1] is_batch_norm_layer <- function(x) inherits(x, batch_norm_layer_class_name)  model <- application_efficientnet_b0() freeze_weights(model, which = is_batch_norm_layer) model # equivalent to: for(layer in model$layers) {   if(is_batch_norm_layer(layer))     layer$trainable <- FALSE   else     layer$trainable <- TRUE } }"},{"path":"https://keras.posit.co/reference/get_config.html","id":null,"dir":"Reference","previous_headings":"","what":"Layer/Model configuration — get_config","title":"Layer/Model configuration — get_config","text":"layer config object returned get_config() contains configuration layer model. layer model can reinstantiated later (without trained weights) configuration using from_config(). config include connectivity information, class name (handled externally).","code":""},{"path":"https://keras.posit.co/reference/get_config.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Layer/Model configuration — get_config","text":"","code":"get_config(object)  from_config(   config,   custom_objects = NULL,   class = attr(config, \"__class__\", TRUE) %||% keras$Model )"},{"path":"https://keras.posit.co/reference/get_config.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Layer/Model configuration — get_config","text":"object Layer model object config Object layer model configuration custom_objects list custom objects needed instantiate layer, e.g., custom layers defined new_layer_class() similar. class keras class restore.","code":""},{"path":"https://keras.posit.co/reference/get_config.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Layer/Model configuration — get_config","text":"get_config() returns object configuration, from_config() returns re-instantiation object.","code":""},{"path":"https://keras.posit.co/reference/get_config.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Layer/Model configuration — get_config","text":"Objects returned get_config() serializable. Therefore, want save restore model across sessions, can use model_to_json() function (model configuration , weights) save_model_tf() function save model configuration weights filesystem.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/get_custom_objects.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieves a live reference to the global list of custom objects (a Python dictionary). — clear_registered_custom_objects","title":"Retrieves a live reference to the global list of custom objects (a Python dictionary). — clear_registered_custom_objects","text":"Custom objects set using using custom_object_scope() added global list custom objects, appear returned list.","code":""},{"path":"https://keras.posit.co/reference/get_custom_objects.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieves a live reference to the global list of custom objects (a Python dictionary). — clear_registered_custom_objects","text":"","code":"clear_registered_custom_objects()  get_custom_objects()"},{"path":"https://keras.posit.co/reference/get_custom_objects.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieves a live reference to the global list of custom objects (a Python dictionary). — clear_registered_custom_objects","text":"Python dictionary, global dictionary mapping registered class names classes.","code":""},{"path":"https://keras.posit.co/reference/get_custom_objects.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieves a live reference to the global list of custom objects (a Python dictionary). — clear_registered_custom_objects","text":"","code":"get_custom_objects()$clear() get_custom_objects()$update(list(MyObject = MyObject))"},{"path":[]},{"path":"https://keras.posit.co/reference/get_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Downloads a file from a URL if it not already in the cache. — get_file","title":"Downloads a file from a URL if it not already in the cache. — get_file","text":"default file url origin downloaded cache_dir ~/.keras, placed cache_subdir datasets, given filename fname. final location file example.txt therefore ~/.keras/datasets/example.txt. Files .tar, .tar.gz, .tar.bz, .zip formats can also extracted. Passing hash verify file download. command line programs shasum sha256sum can compute hash.","code":""},{"path":"https://keras.posit.co/reference/get_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Downloads a file from a URL if it not already in the cache. — get_file","text":"","code":"get_file(   fname = NULL,   origin = NULL,   untar = FALSE,   md5_hash = NULL,   file_hash = NULL,   cache_subdir = \"datasets\",   hash_algorithm = \"auto\",   extract = FALSE,   archive_format = \"auto\",   cache_dir = NULL )"},{"path":"https://keras.posit.co/reference/get_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Downloads a file from a URL if it not already in the cache. — get_file","text":"fname Name file. absolute path, e.g. \"/path//file.txt\" specified, file saved location. NULL, name file origin used. origin Original URL file. untar Deprecated favor extract argument. boolean, whether file decompressed md5_hash Deprecated favor file_hash argument. md5 hash file verification file_hash expected hash string file download. sha256 md5 hash algorithms supported. cache_subdir Subdirectory Keras cache dir file saved. absolute path, e.g. \"/path//folder\" specified, file saved location. hash_algorithm Select hash algorithm verify file. options \"md5', \"sha256', \"auto'. default 'auto' detects hash algorithm use. extract TRUE tries extracting file Archive, like tar zip. archive_format Archive format try extracting file. Options \"auto', \"tar', \"zip', NULL. \"tar\" includes tar, tar.gz, tar.bz files. default \"auto\" corresponds c(\"tar\", \"zip\"). NULL empty list return matches found. cache_dir Location store cached files, NULL defaults Sys.getenv(\"KERAS_HOME\", '~/.keras/').","code":""},{"path":"https://keras.posit.co/reference/get_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Downloads a file from a URL if it not already in the cache. — get_file","text":"Path downloaded file. ** Warning malicious downloads ** Downloading something Internet carries risk. NEVER download file/archive trust source. recommend specify file_hash argument (hash source file known) make sure file getting one expect.","code":""},{"path":"https://keras.posit.co/reference/get_file.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Downloads a file from a URL if it not already in the cache. — get_file","text":"","code":"path_to_downloaded_file <- get_file(     origin = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\",     extract = TRUE )"},{"path":[]},{"path":"https://keras.posit.co/reference/get_input_at.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve tensors for layers with multiple nodes — get_input_at","title":"Retrieve tensors for layers with multiple nodes — get_input_at","text":"Whenever calling layer input, creating new tensor (output layer), adding \"node\" layer, linking input tensor output tensor. calling layer multiple times, layer owns multiple nodes indexed 1, 2, 3. functions enable retrieve various tensor properties layers multiple nodes.","code":""},{"path":"https://keras.posit.co/reference/get_input_at.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve tensors for layers with multiple nodes — get_input_at","text":"","code":"get_input_at(object, node_index)  get_output_at(object, node_index)  get_input_shape_at(object, node_index)  get_output_shape_at(object, node_index)  get_input_mask_at(object, node_index)  get_output_mask_at(object, node_index)"},{"path":"https://keras.posit.co/reference/get_input_at.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve tensors for layers with multiple nodes — get_input_at","text":"object Layer model object node_index Integer, index node retrieve attribute. E.g. node_index = 1 correspond first time layer called.","code":""},{"path":"https://keras.posit.co/reference/get_input_at.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve tensors for layers with multiple nodes — get_input_at","text":"tensor (list tensors layer multiple inputs/outputs).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/get_layer.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieves a layer based on either its name (unique) or index. — get_layer","title":"Retrieves a layer based on either its name (unique) or index. — get_layer","text":"Indices based order horizontal graph traversal (bottom-) 1-based. name index provided, index take precedence.","code":""},{"path":"https://keras.posit.co/reference/get_layer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieves a layer based on either its name (unique) or index. — get_layer","text":"","code":"get_layer(object, name = NULL, index = NULL)"},{"path":"https://keras.posit.co/reference/get_layer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieves a layer based on either its name (unique) or index. — get_layer","text":"object Keras model object name String, name layer. index Integer, index layer (1-based). Also valid negative values, count end model.","code":""},{"path":"https://keras.posit.co/reference/get_layer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieves a layer based on either its name (unique) or index. — get_layer","text":"layer instance.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/get_registered_name.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the name registered to an object within the Keras framework. — get_registered_name","title":"Returns the name registered to an object within the Keras framework. — get_registered_name","text":"function part Keras serialization deserialization framework. maps objects string names associated objects serialization/deserialization.","code":""},{"path":"https://keras.posit.co/reference/get_registered_name.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the name registered to an object within the Keras framework. — get_registered_name","text":"","code":"get_registered_name(obj)"},{"path":"https://keras.posit.co/reference/get_registered_name.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the name registered to an object within the Keras framework. — get_registered_name","text":"obj object look .","code":""},{"path":"https://keras.posit.co/reference/get_registered_name.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the name registered to an object within the Keras framework. — get_registered_name","text":"name associated object, default Python name object registered.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/get_registered_object.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the class associated with name if it is registered with Keras. — get_registered_object","title":"Returns the class associated with name if it is registered with Keras. — get_registered_object","text":"function part Keras serialization deserialization framework. maps strings objects associated serialization/deserialization.","code":""},{"path":"https://keras.posit.co/reference/get_registered_object.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the class associated with name if it is registered with Keras. — get_registered_object","text":"","code":"get_registered_object(name, custom_objects = NULL, module_objects = NULL)"},{"path":"https://keras.posit.co/reference/get_registered_object.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the class associated with name if it is registered with Keras. — get_registered_object","text":"name name look . custom_objects named list custom objects look name . Generally, custom_objects provided user. module_objects named list custom objects look name . Generally, module_objects provided midlevel library implementers.","code":""},{"path":"https://keras.posit.co/reference/get_registered_object.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the class associated with name if it is registered with Keras. — get_registered_object","text":"instantiable class associated name, NULL class exists.","code":""},{"path":"https://keras.posit.co/reference/get_registered_object.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Returns the class associated with name if it is registered with Keras. — get_registered_object","text":"","code":"from_config <- function(cls, config, custom_objects = NULL) {   if ('my_custom_object_name' %in% names(config)) {     config$hidden_cls <- get_registered_object(       config$my_custom_object_name,       custom_objects = custom_objects)   } }"},{"path":[]},{"path":"https://keras.posit.co/reference/get_source_inputs.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the list of input tensors necessary to compute tensor. — get_source_inputs","title":"Returns the list of input tensors necessary to compute tensor. — get_source_inputs","text":"Output always list tensors (potentially 1 element).","code":""},{"path":"https://keras.posit.co/reference/get_source_inputs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the list of input tensors necessary to compute tensor. — get_source_inputs","text":"","code":"get_source_inputs(tensor)"},{"path":"https://keras.posit.co/reference/get_source_inputs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the list of input tensors necessary to compute tensor. — get_source_inputs","text":"tensor tensor start .","code":""},{"path":"https://keras.posit.co/reference/get_source_inputs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the list of input tensors necessary to compute tensor. — get_source_inputs","text":"","code":"List of input tensors."},{"path":[]},{"path":"https://keras.posit.co/reference/get_weights.html","id":null,"dir":"Reference","previous_headings":"","what":"Layer/Model weights as R arrays — get_weights","title":"Layer/Model weights as R arrays — get_weights","text":"Layer/Model weights R arrays","code":""},{"path":"https://keras.posit.co/reference/get_weights.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Layer/Model weights as R arrays — get_weights","text":"","code":"get_weights(object, trainable = NA)  set_weights(object, weights)"},{"path":"https://keras.posit.co/reference/get_weights.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Layer/Model weights as R arrays — get_weights","text":"object Layer model object trainable NA (default), weights returned. TRUE, weights Weights R array","code":""},{"path":"https://keras.posit.co/reference/get_weights.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Layer/Model weights as R arrays — get_weights","text":"can access Layer/Model tf.Tensors tf.Variables object$weights, object$trainable_weights, object$non_trainable_weights","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/grapes-py_class-grapes.html","id":null,"dir":"Reference","previous_headings":"","what":"Make a python class constructor — %py_class%","title":"Make a python class constructor — %py_class%","text":"Make python class constructor","code":""},{"path":"https://keras.posit.co/reference/grapes-py_class-grapes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make a python class constructor — %py_class%","text":"","code":"spec %py_class% body"},{"path":"https://keras.posit.co/reference/grapes-py_class-grapes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make a python class constructor — %py_class%","text":"spec bare symbol MyClassName, call MyClassName(SuperClass) body expression can evaluated construct class methods.","code":""},{"path":"https://keras.posit.co/reference/grapes-py_class-grapes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make a python class constructor — %py_class%","text":"python class constructor, invisibly. Note, constructor also assigned parent frame.","code":""},{"path":"https://keras.posit.co/reference/grapes-py_class-grapes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make a python class constructor — %py_class%","text":"","code":"if (FALSE) { MyClass %py_class% {   initialize <- function(x) {     print(\"Hi from MyClass$initialize()!\")     self$x <- x   }   my_method <- function() {     self$x   } }  my_class_instance <- MyClass(42) my_class_instance$my_method()  MyClass2(MyClass) %py_class% {   \"This will be a __doc__ string for MyClass2\"    initialize <- function(...) {     \"This will be the __doc__ string for the MyClass2.__init__() method\"     print(\"Hi from MyClass2$initialize()!\")     super$initialize(...)   } }  my_class_instance2 <- MyClass2(42) my_class_instance2$my_method()  reticulate::py_help(MyClass2) # see the __doc__ strings and more!  # In addition to `self`, there is also `private` available. # This is an R environment unique to each class instance, where you can # store objects that you don't want converted to Python, but still want # available from methods. You can also assign methods to private, and # `self` and `private` will be available in private methods.  MyClass %py_class% {    initialize <- function(x) {     print(\"Hi from MyClass$initialize()!\")     private$y <- paste(\"A Private field:\", x)   }    get_private_field <- function() {     private$y   }    private$a_private_method <- function() {     cat(\"a_private_method() was called.\\n\")     cat(\"private$y is \", sQuote(private$y), \"\\n\")   }    call_private_method <- function()     private$a_private_method()    # equivalent of @property decorator in python   an_active_property %<-active% function(x = NULL) {     if(!is.null(x)) {       cat(\"`an_active_property` was assigned\", x, \"\\n\")       return(x)     } else {       cat(\"`an_active_property` was accessed\\n\")       return(42)     }   } }  inst1 <- MyClass(1) inst2 <- MyClass(2) inst1$get_private_field() inst2$get_private_field() inst1$call_private_method() inst2$call_private_method() inst1$an_active_property inst1$an_active_property <- 11 }"},{"path":"https://keras.posit.co/reference/grapes-set-active-grapes.html","id":null,"dir":"Reference","previous_headings":"","what":"Make an Active Binding — %<-active%","title":"Make an Active Binding — %<-active%","text":"Make Active Binding","code":""},{"path":"https://keras.posit.co/reference/grapes-set-active-grapes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make an Active Binding — %<-active%","text":"","code":"sym %<-active% value"},{"path":"https://keras.posit.co/reference/grapes-set-active-grapes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make an Active Binding — %<-active%","text":"sym symbol bind value function call value sym accessed.","code":""},{"path":"https://keras.posit.co/reference/grapes-set-active-grapes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make an Active Binding — %<-active%","text":"value, invisibly","code":""},{"path":"https://keras.posit.co/reference/grapes-set-active-grapes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Make an Active Binding — %<-active%","text":"Active bindings defined %py_class% converted @property decorated methods.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/grapes-set-active-grapes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make an Active Binding — %<-active%","text":"","code":"set.seed(1234) x %<-active% function(value) {   message(\"Evaluating function of active binding\")   if(missing(value))     runif(1)   else    message(\"Received: \", value) } x #> Evaluating function of active binding #> [1] 0.1137034 x #> Evaluating function of active binding #> [1] 0.6222994 x <- \"foo\" #> Evaluating function of active binding #> Received: foo x <- \"foo\" #> Evaluating function of active binding #> Received: foo x #> Evaluating function of active binding #> [1] 0.6092747 rm(x) # cleanup"},{"path":"https://keras.posit.co/reference/image_array_save.html","id":null,"dir":"Reference","previous_headings":"","what":"Saves an image stored as a NumPy array to a path or file object. — image_array_save","title":"Saves an image stored as a NumPy array to a path or file object. — image_array_save","text":"Saves image stored NumPy array path file object.","code":""},{"path":"https://keras.posit.co/reference/image_array_save.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Saves an image stored as a NumPy array to a path or file object. — image_array_save","text":"","code":"image_array_save(   x,   path,   data_format = NULL,   file_format = NULL,   scale = TRUE,   ... )"},{"path":"https://keras.posit.co/reference/image_array_save.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Saves an image stored as a NumPy array to a path or file object. — image_array_save","text":"x NumPy array. path Path file object. data_format Image data format, either \"channels_first\" \"channels_last\". file_format Optional file format override. omitted, format use determined filename extension. file object used instead filename, parameter always used. scale Whether rescale image values within [0, 255]. ... Additional keyword arguments passed PIL.Image.save().","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/image_dataset_from_directory.html","id":null,"dir":"Reference","previous_headings":"","what":"Generates a tf.data.Dataset from image files in a directory. — image_dataset_from_directory","title":"Generates a tf.data.Dataset from image files in a directory. — image_dataset_from_directory","text":"directory structure :   calling image_dataset_from_directory(main_directory, labels = 'inferred') return tf.data.Dataset yields batches images subdirectories class_a class_b, together labels 0 1 (0 corresponding class_a 1 corresponding class_b). Supported image formats: .jpeg, .jpg, .png, .bmp, .gif. Animated gifs truncated first frame.","code":"main_directory/ ...class_a/ ......a_image_1.jpg ......a_image_2.jpg ...class_b/ ......b_image_1.jpg ......b_image_2.jpg"},{"path":"https://keras.posit.co/reference/image_dataset_from_directory.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generates a tf.data.Dataset from image files in a directory. — image_dataset_from_directory","text":"","code":"image_dataset_from_directory(   directory,   labels = \"inferred\",   label_mode = \"int\",   class_names = NULL,   color_mode = \"rgb\",   batch_size = 32L,   image_size = list(256L, 256L),   shuffle = TRUE,   seed = NULL,   validation_split = NULL,   subset = NULL,   interpolation = \"bilinear\",   follow_links = FALSE,   crop_to_aspect_ratio = FALSE,   data_format = NULL )"},{"path":"https://keras.posit.co/reference/image_dataset_from_directory.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generates a tf.data.Dataset from image files in a directory. — image_dataset_from_directory","text":"directory Directory data located. labels \"inferred\", contain subdirectories, containing images class. Otherwise, directory structure ignored. labels Either \"inferred\" (labels generated directory structure), NULL (labels), list/tuple integer labels size number image files found directory. Labels sorted according alphanumeric order image file paths (obtained via os.walk(directory) Python). label_mode String describing encoding labels. Options : \"int\": means labels encoded integers (e.g. sparse_categorical_crossentropy loss). \"categorical\" means labels encoded categorical vector (e.g. categorical_crossentropy loss). \"binary\" means labels (can 2) encoded float32 scalars values 0 1 (e.g. binary_crossentropy). NULL (labels). class_names valid labels \"inferred\". explicit list class names (must match names subdirectories). Used control order classes (otherwise alphanumerical order used). color_mode One \"grayscale\", \"rgb\", \"rgba\". Defaults \"rgb\". Whether images converted 1, 3, 4 channels. batch_size Size batches data. Defaults 32. NULL, data batched (dataset yield individual samples). image_size Size resize images read disk, specified (height, width). Defaults (256, 256). Since pipeline processes batches images must size, must provided. shuffle Whether shuffle data. Defaults TRUE. set FALSE, sorts data alphanumeric order. seed Optional random seed shuffling transformations. validation_split Optional float 0 1, fraction data reserve validation. subset Subset data return. One \"training\", \"validation\", \"\". used validation_split set. subset = \"\", utility returns tuple two datasets (training validation datasets respectively). interpolation String, interpolation method used resizing images. Defaults \"bilinear\". Supports \"bilinear\", \"nearest\", \"bicubic\", \"area\", \"lanczos3\", \"lanczos5\", \"gaussian\", \"mitchellcubic\". follow_links Whether visit subdirectories pointed symlinks. Defaults FALSE. crop_to_aspect_ratio TRUE, resize images without aspect ratio distortion. original aspect ratio differs target aspect ratio, output image cropped return largest possible window image (size image_size) matches target aspect ratio. default (crop_to_aspect_ratio = FALSE), aspect ratio may preserved. data_format NULL uses keras.config.image_data_format() otherwise either 'channel_last' 'channel_first'.","code":""},{"path":"https://keras.posit.co/reference/image_dataset_from_directory.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generates a tf.data.Dataset from image files in a directory. — image_dataset_from_directory","text":"tf.data.Dataset object. label_mode NULL, yields float32 tensors shape (batch_size, image_size[0], image_size[1], num_channels), encoding images (see rules regarding num_channels). Otherwise, yields tuple (images, labels), images shape (batch_size, image_size[0], image_size[1], num_channels), labels follows format described . Rules regarding labels format: label_mode \"int\", labels int32 tensor shape (batch_size,). label_mode \"binary\", labels float32 tensor 1s 0s shape (batch_size, 1). label_mode \"categorical\", labels float32 tensor shape (batch_size, num_classes), representing one-hot encoding class index. Rules regarding number channels yielded images: color_mode \"grayscale\", 1 channel image tensors. color_mode \"rgb\", 3 channels image tensors. color_mode \"rgba\", 4 channels image tensors.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/image_from_array.html","id":null,"dir":"Reference","previous_headings":"","what":"Converts a 3D array to a PIL Image instance. — image_from_array","title":"Converts a 3D array to a PIL Image instance. — image_from_array","text":"Converts 3D array PIL Image instance.","code":""},{"path":"https://keras.posit.co/reference/image_from_array.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Converts a 3D array to a PIL Image instance. — image_from_array","text":"","code":"image_from_array(x, data_format = NULL, scale = TRUE, dtype = NULL)"},{"path":"https://keras.posit.co/reference/image_from_array.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Converts a 3D array to a PIL Image instance. — image_from_array","text":"x Input data, form can converted array. data_format Image data format, can either \"channels_first\" \"channels_last\". Defaults NULL, case global setting config_image_data_format() used (unless changed , defaults \"channels_last\"). scale Whether rescale image minimum maximum values 0 255 respectively. Defaults TRUE. dtype Dtype use. NULL means global setting config_floatx() used (unless changed , defaults \"float32\"). Defaults NULL.","code":""},{"path":"https://keras.posit.co/reference/image_from_array.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Converts a 3D array to a PIL Image instance. — image_from_array","text":"","code":"A PIL Image instance."},{"path":"https://keras.posit.co/reference/image_from_array.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Converts a 3D array to a PIL Image instance. — image_from_array","text":"","code":"img <- array(runif(30000), dim = c(100, 100, 3)) pil_img <- image_from_array(img) pil_img ## <PIL.Image.Image image mode=RGB size=100x100>"},{"path":[]},{"path":"https://keras.posit.co/reference/image_load.html","id":null,"dir":"Reference","previous_headings":"","what":"Loads an image into PIL format. — image_load","title":"Loads an image into PIL format. — image_load","text":"Loads image PIL format.","code":""},{"path":"https://keras.posit.co/reference/image_load.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Loads an image into PIL format. — image_load","text":"","code":"image_load(   path,   color_mode = \"rgb\",   target_size = NULL,   interpolation = \"nearest\",   keep_aspect_ratio = FALSE )"},{"path":"https://keras.posit.co/reference/image_load.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Loads an image into PIL format. — image_load","text":"path Path image file. color_mode One \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\". desired image format. target_size Either NULL (default original size) tuple ints (img_height, img_width). interpolation Interpolation method used resample image target size different loaded image. Supported methods \"nearest\", \"bilinear\", \"bicubic\". PIL version 1.1.3 newer installed, \"lanczos\" also supported. PIL version 3.4.0 newer installed, \"box\" \"hamming\" also supported. default, \"nearest\" used. keep_aspect_ratio Boolean, whether resize images target size without aspect ratio distortion. image cropped center target aspect ratio resizing.","code":""},{"path":"https://keras.posit.co/reference/image_load.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Loads an image into PIL format. — image_load","text":"","code":"A PIL Image instance."},{"path":"https://keras.posit.co/reference/image_load.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Loads an image into PIL format. — image_load","text":"","code":"image_path <- get_file(origin = \"https://www.r-project.org/logo/Rlogo.png\") (image <- image_load(image_path)) ## <PIL.Image.Image image mode=RGB size=724x561> input_arr <- image_to_array(image) str(input_arr) ##  num [1:561, 1:724, 1:3] 0 0 0 0 0 0 0 0 0 0 ... input_arr %<>% array_reshape(dim = c(1, dim(input_arr))) # Convert single image to a batch. model |> predict(input_arr)"},{"path":[]},{"path":"https://keras.posit.co/reference/image_smart_resize.html","id":null,"dir":"Reference","previous_headings":"","what":"Resize images to a target size without aspect ratio distortion. — image_smart_resize","title":"Resize images to a target size without aspect ratio distortion. — image_smart_resize","text":"Image datasets typically yield images different size. However, images need batched can processed Keras layers. batched, images need share height width. simply , TF (JAX equivalent):   However, , distort aspect ratio images, since general aspect ratio size. fine many cases, always (e.g. image generation models can problem). Note passing argument preserve_aspect_ratio = TRUE tf$image$resize() preserve aspect ratio, cost longer respecting provided target size. calls :   output images actually (200, 200), distorted. Instead, parts image fit within target size get cropped . resizing process : Take largest centered crop image aspect ratio target size. instance, size = c(200, 200) input image size (340, 500), take crop (340, 340) centered along width. Resize cropped image target size. example , resize (340, 340) crop (200, 200).","code":"size <- c(200, 200) ds <- ds$map(\\(img) tf$image$resize(img, size)) size <- c(200, 200) ds <- ds$map(\\(img) image_smart_resize(img, size))"},{"path":"https://keras.posit.co/reference/image_smart_resize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Resize images to a target size without aspect ratio distortion. — image_smart_resize","text":"","code":"image_smart_resize(   x,   size,   interpolation = \"bilinear\",   data_format = \"channels_last\",   backend_module = NULL )"},{"path":"https://keras.posit.co/reference/image_smart_resize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Resize images to a target size without aspect ratio distortion. — image_smart_resize","text":"x Input image batch images (tensor array). Must format (height, width, channels) (batch_size, height, width, channels). size Tuple (height, width) integer. Target size. interpolation String, interpolation use resizing. Defaults 'bilinear'. Supports bilinear, nearest, bicubic, lanczos3, lanczos5. data_format \"channels_last\" \"channels_first\". backend_module Backend module use (different default backend).","code":""},{"path":"https://keras.posit.co/reference/image_smart_resize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Resize images to a target size without aspect ratio distortion. — image_smart_resize","text":"Array shape (size[1], size[2], channels). input image array, output array, backend-native tensor, output backend-native tensor.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/image_to_array.html","id":null,"dir":"Reference","previous_headings":"","what":"Converts a PIL Image instance to a matrix. — image_to_array","title":"Converts a PIL Image instance to a matrix. — image_to_array","text":"Converts PIL Image instance matrix.","code":""},{"path":"https://keras.posit.co/reference/image_to_array.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Converts a PIL Image instance to a matrix. — image_to_array","text":"","code":"image_to_array(img, data_format = NULL, dtype = NULL)"},{"path":"https://keras.posit.co/reference/image_to_array.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Converts a PIL Image instance to a matrix. — image_to_array","text":"img Input PIL Image instance. data_format Image data format, can either \"channels_first\" \"channels_last\". Defaults NULL, case global setting config_image_data_format() used (unless changed , defaults \"channels_last\"). dtype Dtype use. NULL means global setting config_floatx() used (unless changed , defaults \"float32\").","code":""},{"path":"https://keras.posit.co/reference/image_to_array.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Converts a PIL Image instance to a matrix. — image_to_array","text":"","code":"A 3D array."},{"path":"https://keras.posit.co/reference/image_to_array.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Converts a PIL Image instance to a matrix. — image_to_array","text":"","code":"image_path <- get_file(origin = \"https://www.r-project.org/logo/Rlogo.png\") (img <- image_load(image_path)) ## <PIL.Image.Image image mode=RGB size=724x561> array <- image_to_array(img) str(array) ##  num [1:561, 1:724, 1:3] 0 0 0 0 0 0 0 0 0 0 ..."},{"path":[]},{"path":"https://keras.posit.co/reference/implementation.html","id":null,"dir":"Reference","previous_headings":"","what":"Keras implementation — implementation","title":"Keras implementation — implementation","text":"Obtain reference Python module used implementation Keras.","code":""},{"path":"https://keras.posit.co/reference/implementation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Keras implementation — implementation","text":"","code":"implementation()"},{"path":"https://keras.posit.co/reference/implementation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Keras implementation — implementation","text":"Reference Python module used implementation Keras.","code":""},{"path":"https://keras.posit.co/reference/implementation.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Keras implementation — implementation","text":"available Python modules implement Keras: keras tensorflow.keras (\"tensorflow\") keras_core (\"core\") function returns reference implementation currently used keras package. default implementation \"keras\". can override setting KERAS_IMPLEMENTATION environment variable \"tensorflow\".","code":""},{"path":"https://keras.posit.co/reference/initializer_constant.html","id":null,"dir":"Reference","previous_headings":"","what":"Initializer that generates tensors with constant values. — initializer_constant","title":"Initializer that generates tensors with constant values. — initializer_constant","text":"scalar values allowed. constant value provided must convertible dtype requested calling initializer.","code":""},{"path":"https://keras.posit.co/reference/initializer_constant.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initializer that generates tensors with constant values. — initializer_constant","text":"","code":"initializer_constant(value = 0)"},{"path":"https://keras.posit.co/reference/initializer_constant.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Initializer that generates tensors with constant values. — initializer_constant","text":"value numeric scalar.","code":""},{"path":"https://keras.posit.co/reference/initializer_constant.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Initializer that generates tensors with constant values. — initializer_constant","text":"","code":"# Standalone usage: initializer <- initializer_constant(10) values <- initializer(shape = c(2, 2)) # Usage in a Keras layer: initializer <- initializer_constant(10) layer <- layer_dense(units = 3, kernel_initializer = initializer)"},{"path":[]},{"path":"https://keras.posit.co/reference/initializer_glorot_normal.html","id":null,"dir":"Reference","previous_headings":"","what":"The Glorot normal initializer, also called Xavier normal initializer. — initializer_glorot_normal","title":"The Glorot normal initializer, also called Xavier normal initializer. — initializer_glorot_normal","text":"Draws samples truncated normal distribution centered 0 stddev = sqrt(2 / (fan_in + fan_out)) fan_in number input units weight tensor fan_out number output units weight tensor.","code":""},{"path":"https://keras.posit.co/reference/initializer_glorot_normal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The Glorot normal initializer, also called Xavier normal initializer. — initializer_glorot_normal","text":"","code":"initializer_glorot_normal(seed = NULL)"},{"path":"https://keras.posit.co/reference/initializer_glorot_normal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Glorot normal initializer, also called Xavier normal initializer. — initializer_glorot_normal","text":"seed integer instance random_seed_generator(). Used make behavior initializer deterministic. Note initializer seeded integer NULL (unseeded) produce random values across multiple calls. get different random values across multiple calls, use seed instance random_seed_generator().","code":""},{"path":"https://keras.posit.co/reference/initializer_glorot_normal.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"The Glorot normal initializer, also called Xavier normal initializer. — initializer_glorot_normal","text":"","code":"# Standalone usage: initializer <- initializer_glorot_normal() values <- initializer(shape = c(2, 2)) # Usage in a Keras layer: initializer <- initializer_glorot_normal() layer <- layer_dense(units = 3, kernel_initializer = initializer)"},{"path":"https://keras.posit.co/reference/initializer_glorot_normal.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"The Glorot normal initializer, also called Xavier normal initializer. — initializer_glorot_normal","text":"Glorot et al., 2010","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/initializer_glorot_uniform.html","id":null,"dir":"Reference","previous_headings":"","what":"The Glorot uniform initializer, also called Xavier uniform initializer. — initializer_glorot_uniform","title":"The Glorot uniform initializer, also called Xavier uniform initializer. — initializer_glorot_uniform","text":"Draws samples uniform distribution within [-limit, limit], limit = sqrt(6 / (fan_in + fan_out)) (fan_in number input units weight tensor fan_out number output units).","code":""},{"path":"https://keras.posit.co/reference/initializer_glorot_uniform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The Glorot uniform initializer, also called Xavier uniform initializer. — initializer_glorot_uniform","text":"","code":"initializer_glorot_uniform(seed = NULL)"},{"path":"https://keras.posit.co/reference/initializer_glorot_uniform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Glorot uniform initializer, also called Xavier uniform initializer. — initializer_glorot_uniform","text":"seed integer instance random_seed_generator(). Used make behavior initializer deterministic. Note initializer seeded integer NULL (unseeded) produce random values across multiple calls. get different random values across multiple calls, use seed instance random_seed_generator().","code":""},{"path":"https://keras.posit.co/reference/initializer_glorot_uniform.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"The Glorot uniform initializer, also called Xavier uniform initializer. — initializer_glorot_uniform","text":"","code":"# Standalone usage: initializer <- initializer_glorot_uniform() values <- initializer(shape = c(2, 2)) # Usage in a Keras layer: initializer <- initializer_glorot_uniform() layer <- layer_dense(units = 3, kernel_initializer = initializer)"},{"path":"https://keras.posit.co/reference/initializer_glorot_uniform.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"The Glorot uniform initializer, also called Xavier uniform initializer. — initializer_glorot_uniform","text":"Glorot et al., 2010","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/initializer_he_normal.html","id":null,"dir":"Reference","previous_headings":"","what":"He normal initializer. — initializer_he_normal","title":"He normal initializer. — initializer_he_normal","text":"draws samples truncated normal distribution centered 0 stddev = sqrt(2 / fan_in) fan_in number input units weight tensor.","code":""},{"path":"https://keras.posit.co/reference/initializer_he_normal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"He normal initializer. — initializer_he_normal","text":"","code":"initializer_he_normal(seed = NULL)"},{"path":"https://keras.posit.co/reference/initializer_he_normal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"He normal initializer. — initializer_he_normal","text":"seed integer instance random_seed_generator(). Used make behavior initializer deterministic. Note initializer seeded integer NULL (unseeded) produce random values across multiple calls. get different random values across multiple calls, use seed instance random_seed_generator().","code":""},{"path":"https://keras.posit.co/reference/initializer_he_normal.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"He normal initializer. — initializer_he_normal","text":"","code":"# Standalone usage: initializer <- initializer_he_normal() values <- initializer(shape = c(2, 2)) # Usage in a Keras layer: initializer <- initializer_he_normal() layer <- layer_dense(units = 3, kernel_initializer = initializer)"},{"path":"https://keras.posit.co/reference/initializer_he_normal.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"He normal initializer. — initializer_he_normal","text":"et al., 2015","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/initializer_he_uniform.html","id":null,"dir":"Reference","previous_headings":"","what":"He uniform variance scaling initializer. — initializer_he_uniform","title":"He uniform variance scaling initializer. — initializer_he_uniform","text":"Draws samples uniform distribution within [-limit, limit], limit = sqrt(6 / fan_in) (fan_in number input units weight tensor).","code":""},{"path":"https://keras.posit.co/reference/initializer_he_uniform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"He uniform variance scaling initializer. — initializer_he_uniform","text":"","code":"initializer_he_uniform(seed = NULL)"},{"path":"https://keras.posit.co/reference/initializer_he_uniform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"He uniform variance scaling initializer. — initializer_he_uniform","text":"seed integer instance random_seed_generator(). Used make behavior initializer deterministic. Note initializer seeded integer NULL (unseeded) produce random values across multiple calls. get different random values across multiple calls, use seed instance random_seed_generator().","code":""},{"path":"https://keras.posit.co/reference/initializer_he_uniform.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"He uniform variance scaling initializer. — initializer_he_uniform","text":"","code":"# Standalone usage: initializer <- initializer_he_uniform() values <- initializer(shape = c(2, 2)) # Usage in a Keras layer: initializer <- initializer_he_uniform() layer <- layer_dense(units = 3, kernel_initializer = initializer)"},{"path":"https://keras.posit.co/reference/initializer_he_uniform.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"He uniform variance scaling initializer. — initializer_he_uniform","text":"et al., 2015","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/initializer_identity.html","id":null,"dir":"Reference","previous_headings":"","what":"Initializer that generates the identity matrix. — initializer_identity","title":"Initializer that generates the identity matrix. — initializer_identity","text":"usable generating 2D matrices.","code":""},{"path":"https://keras.posit.co/reference/initializer_identity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initializer that generates the identity matrix. — initializer_identity","text":"","code":"initializer_identity(gain = 1)"},{"path":"https://keras.posit.co/reference/initializer_identity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Initializer that generates the identity matrix. — initializer_identity","text":"gain Multiplicative factor apply identity matrix.","code":""},{"path":"https://keras.posit.co/reference/initializer_identity.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Initializer that generates the identity matrix. — initializer_identity","text":"","code":"# Standalone usage: initializer <- initializer_identity() values <- initializer(shape = c(2, 2)) # Usage in a Keras layer: initializer <- initializer_identity() layer <- layer_dense(units = 3, kernel_initializer = initializer)"},{"path":[]},{"path":"https://keras.posit.co/reference/initializer_lecun_normal.html","id":null,"dir":"Reference","previous_headings":"","what":"Lecun normal initializer. — initializer_lecun_normal","title":"Lecun normal initializer. — initializer_lecun_normal","text":"Initializers allow pre-specify initialization strategy, encoded Initializer object, without knowing shape dtype variable initialized. Draws samples truncated normal distribution centered 0 stddev = sqrt(1 / fan_in) fan_in number input units weight tensor.","code":""},{"path":"https://keras.posit.co/reference/initializer_lecun_normal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Lecun normal initializer. — initializer_lecun_normal","text":"","code":"initializer_lecun_normal(seed = NULL)"},{"path":"https://keras.posit.co/reference/initializer_lecun_normal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Lecun normal initializer. — initializer_lecun_normal","text":"seed integer instance random_seed_generator(). Used make behavior initializer deterministic. Note initializer seeded integer NULL (unseeded) produce random values across multiple calls. get different random values across multiple calls, use seed instance random_seed_generator().","code":""},{"path":"https://keras.posit.co/reference/initializer_lecun_normal.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Lecun normal initializer. — initializer_lecun_normal","text":"","code":"# Standalone usage: initializer <- initializer_lecun_normal() values <- initializer(shape = c(2, 2)) # Usage in a Keras layer: initializer <- initializer_lecun_normal() layer <- layer_dense(units = 3, kernel_initializer = initializer)"},{"path":"https://keras.posit.co/reference/initializer_lecun_normal.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Lecun normal initializer. — initializer_lecun_normal","text":"Klambauer et al., 2017","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/initializer_lecun_uniform.html","id":null,"dir":"Reference","previous_headings":"","what":"Lecun uniform initializer. — initializer_lecun_uniform","title":"Lecun uniform initializer. — initializer_lecun_uniform","text":"Draws samples uniform distribution within [-limit, limit], limit = sqrt(3 / fan_in) (fan_in number input units weight tensor).","code":""},{"path":"https://keras.posit.co/reference/initializer_lecun_uniform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Lecun uniform initializer. — initializer_lecun_uniform","text":"","code":"initializer_lecun_uniform(seed = NULL)"},{"path":"https://keras.posit.co/reference/initializer_lecun_uniform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Lecun uniform initializer. — initializer_lecun_uniform","text":"seed integer instance random_seed_generator(). Used make behavior initializer deterministic. Note initializer seeded integer NULL (unseeded) produce random values across multiple calls. get different random values across multiple calls, use seed instance random_seed_generator().","code":""},{"path":"https://keras.posit.co/reference/initializer_lecun_uniform.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Lecun uniform initializer. — initializer_lecun_uniform","text":"","code":"# Standalone usage: initializer <- initializer_lecun_uniform() values <- initializer(shape = c(2, 2)) # Usage in a Keras layer: initializer <- initializer_lecun_uniform() layer <- layer_dense(units = 3, kernel_initializer = initializer)"},{"path":"https://keras.posit.co/reference/initializer_lecun_uniform.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Lecun uniform initializer. — initializer_lecun_uniform","text":"Klambauer et al., 2017","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/initializer_ones.html","id":null,"dir":"Reference","previous_headings":"","what":"Initializer that generates tensors initialized to 1. — initializer_ones","title":"Initializer that generates tensors initialized to 1. — initializer_ones","text":"Also available via shortcut function ones.","code":""},{"path":"https://keras.posit.co/reference/initializer_ones.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initializer that generates tensors initialized to 1. — initializer_ones","text":"","code":"initializer_ones()"},{"path":"https://keras.posit.co/reference/initializer_ones.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Initializer that generates tensors initialized to 1. — initializer_ones","text":"","code":"# Standalone usage: initializer <- initializer_ones() values <- initializer(shape = c(2, 2)) # Usage in a Keras layer: initializer <- initializer_ones() layer <- layer_dense(units = 3, kernel_initializer = initializer)"},{"path":[]},{"path":"https://keras.posit.co/reference/initializer_orthogonal.html","id":null,"dir":"Reference","previous_headings":"","what":"Initializer that generates an orthogonal matrix. — initializer_orthogonal","title":"Initializer that generates an orthogonal matrix. — initializer_orthogonal","text":"shape tensor initialize two-dimensional, initialized orthogonal matrix obtained QR decomposition matrix random numbers drawn normal distribution. matrix fewer rows columns output orthogonal rows. Otherwise, output orthogonal columns. shape tensor initialize two-dimensional, matrix shape (shape[1] * ... * shape[n - 1], shape[n]) initialized, n length shape vector. matrix subsequently reshaped give tensor desired shape.","code":""},{"path":"https://keras.posit.co/reference/initializer_orthogonal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initializer that generates an orthogonal matrix. — initializer_orthogonal","text":"","code":"initializer_orthogonal(gain = 1, seed = NULL)"},{"path":"https://keras.posit.co/reference/initializer_orthogonal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Initializer that generates an orthogonal matrix. — initializer_orthogonal","text":"gain Multiplicative factor apply orthogonal matrix. seed integer. Used make behavior initializer deterministic.","code":""},{"path":"https://keras.posit.co/reference/initializer_orthogonal.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Initializer that generates an orthogonal matrix. — initializer_orthogonal","text":"","code":"# Standalone usage: initializer <- initializer_orthogonal() values <- initializer(shape = c(2, 2)) # Usage in a Keras layer: initializer <- initializer_orthogonal() layer <- layer_dense(units = 3, kernel_initializer = initializer)"},{"path":"https://keras.posit.co/reference/initializer_orthogonal.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Initializer that generates an orthogonal matrix. — initializer_orthogonal","text":"Saxe et al., 2014","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/initializer_random_normal.html","id":null,"dir":"Reference","previous_headings":"","what":"Random normal initializer. — initializer_random_normal","title":"Random normal initializer. — initializer_random_normal","text":"Draws samples normal distribution given parameters.","code":""},{"path":"https://keras.posit.co/reference/initializer_random_normal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Random normal initializer. — initializer_random_normal","text":"","code":"initializer_random_normal(mean = 0, stddev = 0.05, seed = NULL)"},{"path":"https://keras.posit.co/reference/initializer_random_normal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Random normal initializer. — initializer_random_normal","text":"mean numeric scalar. Mean random values generate. stddev numeric scalar. Standard deviation random values generate. seed integer instance random_seed_generator(). Used make behavior initializer deterministic. Note initializer seeded integer NULL (unseeded) produce random values across multiple calls. get different random values across multiple calls, use seed instance random_seed_generator().","code":""},{"path":"https://keras.posit.co/reference/initializer_random_normal.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Random normal initializer. — initializer_random_normal","text":"","code":"# Standalone usage: initializer <- initializer_random_normal(mean = 0.0, stddev = 1.0) values <- initializer(shape = c(2, 2)) # Usage in a Keras layer: initializer <- initializer_random_normal(mean = 0.0, stddev = 1.0) layer <- layer_dense(units = 3, kernel_initializer = initializer)"},{"path":[]},{"path":"https://keras.posit.co/reference/initializer_random_uniform.html","id":null,"dir":"Reference","previous_headings":"","what":"Random uniform initializer. — initializer_random_uniform","title":"Random uniform initializer. — initializer_random_uniform","text":"Draws samples uniform distribution given parameters.","code":""},{"path":"https://keras.posit.co/reference/initializer_random_uniform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Random uniform initializer. — initializer_random_uniform","text":"","code":"initializer_random_uniform(minval = -0.05, maxval = 0.05, seed = NULL)"},{"path":"https://keras.posit.co/reference/initializer_random_uniform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Random uniform initializer. — initializer_random_uniform","text":"minval numeric scalar scalar keras tensor. Lower bound range random values generate (inclusive). maxval numeric scalar scalar keras tensor. Upper bound range random values generate (exclusive). seed integer instance random_seed_generator(). Used make behavior initializer deterministic. Note initializer seeded integer NULL (unseeded) produce random values across multiple calls. get different random values across multiple calls, use seed instance random_seed_generator().","code":""},{"path":"https://keras.posit.co/reference/initializer_random_uniform.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Random uniform initializer. — initializer_random_uniform","text":"","code":"# Standalone usage: initializer <- initializer_random_uniform(minval = 0.0, maxval = 1.0) values <- initializer(shape = c(2, 2)) # Usage in a Keras layer: initializer <- initializer_random_uniform(minval = 0.0, maxval = 1.0) layer <- layer_dense(units = 3, kernel_initializer = initializer)"},{"path":[]},{"path":"https://keras.posit.co/reference/initializer_truncated_normal.html","id":null,"dir":"Reference","previous_headings":"","what":"Initializer that generates a truncated normal distribution. — initializer_truncated_normal","title":"Initializer that generates a truncated normal distribution. — initializer_truncated_normal","text":"values generated similar values RandomNormal initializer, except values two standard deviations mean discarded re-drawn.","code":""},{"path":"https://keras.posit.co/reference/initializer_truncated_normal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initializer that generates a truncated normal distribution. — initializer_truncated_normal","text":"","code":"initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)"},{"path":"https://keras.posit.co/reference/initializer_truncated_normal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Initializer that generates a truncated normal distribution. — initializer_truncated_normal","text":"mean numeric scalar. Mean random values generate. stddev numeric scalar. Standard deviation random values generate. seed integer instance random_seed_generator(). Used make behavior initializer deterministic. Note initializer seeded integer NULL (unseeded) produce random values across multiple calls. get different random values across multiple calls, use seed instance random_seed_generator().","code":""},{"path":"https://keras.posit.co/reference/initializer_truncated_normal.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Initializer that generates a truncated normal distribution. — initializer_truncated_normal","text":"","code":"# Standalone usage: initializer <- initializer_truncated_normal(mean = 0, stddev = 1) values <- initializer(shape = c(2, 2)) # Usage in a Keras layer: initializer <- initializer_truncated_normal(mean = 0, stddev = 1) layer <- layer_dense(units = 3, kernel_initializer = initializer)"},{"path":[]},{"path":"https://keras.posit.co/reference/initializer_variance_scaling.html","id":null,"dir":"Reference","previous_headings":"","what":"Initializer that adapts its scale to the shape of its input tensors. — initializer_variance_scaling","title":"Initializer that adapts its scale to the shape of its input tensors. — initializer_variance_scaling","text":"distribution = \"truncated_normal\" \"untruncated_normal\", samples drawn truncated/untruncated normal distribution mean zero standard deviation (truncation, used) stddev = sqrt(scale / n), n : number input units weight tensor, mode = \"fan_in\" number output units, mode = \"fan_out\" average numbers input output units, mode = \"fan_avg\" distribution = \"uniform\", samples drawn uniform distribution within [-limit, limit], limit = sqrt(3 * scale / n).","code":""},{"path":"https://keras.posit.co/reference/initializer_variance_scaling.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initializer that adapts its scale to the shape of its input tensors. — initializer_variance_scaling","text":"","code":"initializer_variance_scaling(   scale = 1,   mode = \"fan_in\",   distribution = \"truncated_normal\",   seed = NULL )"},{"path":"https://keras.posit.co/reference/initializer_variance_scaling.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Initializer that adapts its scale to the shape of its input tensors. — initializer_variance_scaling","text":"scale Scaling factor (positive float). mode One \"fan_in\", \"fan_out\", \"fan_avg\". distribution Random distribution use. One \"truncated_normal\", \"untruncated_normal\", \"uniform\". seed integer instance random_seed_generator(). Used make behavior initializer deterministic. Note initializer seeded integer NULL (unseeded) produce random values across multiple calls. get different random values across multiple calls, use seed instance random_seed_generator().","code":""},{"path":"https://keras.posit.co/reference/initializer_variance_scaling.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Initializer that adapts its scale to the shape of its input tensors. — initializer_variance_scaling","text":"","code":"# Standalone usage: initializer <- initializer_variance_scaling(scale = 0.1, mode = 'fan_in',                                             distribution = 'uniform') values <- initializer(shape = c(2, 2)) # Usage in a Keras layer: initializer <- initializer_variance_scaling(scale = 0.1, mode = 'fan_in',                                             distribution = 'uniform') layer <- layer_dense(units = 3, kernel_initializer = initializer)"},{"path":[]},{"path":"https://keras.posit.co/reference/initializer_zeros.html","id":null,"dir":"Reference","previous_headings":"","what":"Initializer that generates tensors initialized to 0. — initializer_zeros","title":"Initializer that generates tensors initialized to 0. — initializer_zeros","text":"Initializer generates tensors initialized 0.","code":""},{"path":"https://keras.posit.co/reference/initializer_zeros.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initializer that generates tensors initialized to 0. — initializer_zeros","text":"","code":"initializer_zeros()"},{"path":"https://keras.posit.co/reference/initializer_zeros.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Initializer that generates tensors initialized to 0. — initializer_zeros","text":"","code":"# Standalone usage: initializer <- initializer_zeros() values <- initializer(shape = c(2, 2)) # Usage in a Keras layer: initializer <- initializer_zeros() layer <- layer_dense(units = 3, kernel_initializer = initializer)"},{"path":[]},{"path":"https://keras.posit.co/reference/install_keras.html","id":null,"dir":"Reference","previous_headings":"","what":"Install Keras — install_keras","title":"Install Keras — install_keras","text":"function install Keras along selected backend, including Python dependencies.","code":""},{"path":"https://keras.posit.co/reference/install_keras.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Install Keras — install_keras","text":"","code":"install_keras(   envname = \"r-keras\",   ...,   extra_packages = c(\"scipy\", \"pandas\", \"Pillow\", \"pydot\", \"ipython\",     \"tensorflow_datasets\"),   python_version = \">=3.9,<=3.11\",   backend = \"tf-nightly\",   gpu = NA,   restart_session = TRUE )"},{"path":"https://keras.posit.co/reference/install_keras.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Install Keras — install_keras","text":"envname Name path Python virtual environment ... reserved future compatability. extra_packages Additional Python packages install alongside Keras python_version Passed reticulate::virtualenv_starter() backend backend. Accepted values include  \"tensorflow\", \"jax\" \"pytorch\" gpu whether install GPU capable version backend. restart_session Whether restart R session installing (note occur within RStudio).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/is_keras_available.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if Keras is Available — is_keras_available","title":"Check if Keras is Available — is_keras_available","text":"Probe see whether Keras Python package available current system environment.","code":""},{"path":"https://keras.posit.co/reference/is_keras_available.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if Keras is Available — is_keras_available","text":"","code":"is_keras_available(version = NULL)"},{"path":"https://keras.posit.co/reference/is_keras_available.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if Keras is Available — is_keras_available","text":"version Minimum required version Keras (defaults NULL, required version).","code":""},{"path":"https://keras.posit.co/reference/is_keras_available.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if Keras is Available — is_keras_available","text":"Logical indicating whether Keras (specified minimum version Keras) available.","code":""},{"path":"https://keras.posit.co/reference/is_keras_available.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check if Keras is Available — is_keras_available","text":"","code":"if (FALSE) { # testthat utilty for skipping tests when Keras isn't available skip_if_no_keras <- function(version = NULL) {   if (!is_keras_available(version))     skip(\"Required keras version not available for testing\") }  # use the function within a test test_that(\"keras function works correctly\", {   skip_if_no_keras()   # test code here }) }"},{"path":"https://keras.posit.co/reference/keras.html","id":null,"dir":"Reference","previous_headings":"","what":"Main Keras module — keras","title":"Main Keras module — keras","text":"keras module object equivalent retirculate::import(\"keras\") provided mainly convenience.","code":""},{"path":"https://keras.posit.co/reference/keras.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Main Keras module — keras","text":"object class python.builtin.module","code":""},{"path":"https://keras.posit.co/reference/keras.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Main Keras module — keras","text":"keras Python module","code":""},{"path":"https://keras.posit.co/reference/keras3-package.html","id":null,"dir":"Reference","previous_headings":"","what":"keras3: R Interface to 'Keras' — keras3-package","title":"keras3: R Interface to 'Keras' — keras3-package","text":"Interface 'Keras' https://keras.io, high-level neural networks 'API'. 'Keras' developed focus enabling fast experimentation, supports convolution based networks recurrent networks (well combinations two), runs seamlessly 'CPU' 'GPU' devices. Keras high-level neural networks API, developed focus enabling fast experimentation. Keras following key features:","code":""},{"path":"https://keras.posit.co/reference/keras3-package.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"keras3: R Interface to 'Keras' — keras3-package","text":"Allows code run CPU GPU, seamlessly. User-friendly API makes easy quickly prototype deep learning models. Built-support convolutional networks (computer vision), recurrent networks (sequence processing), combination . Supports arbitrary network architectures: multi-input multi-output models, layer sharing, model sharing, etc. means Keras appropriate building essentially deep learning model, memory network neural Turing machine. capable running top multiple back-ends including TensorFlow, Jax, PyTorch. See package website https://keras.rstudio.com complete documentation.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/keras3-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"keras3: R Interface to 'Keras' — keras3-package","text":"Maintainer: Tomasz Kalinowski tomasz@posit.co [copyright holder] Authors: JJ Allaire [copyright holder] François Chollet [copyright holder] contributors: Daniel Falbel daniel@posit.co [contributor, copyright holder] Posit Software, PBC [copyright holder, funder] Google [copyright holder, funder] Yuan Tang terrytangyuan@gmail.com (ORCID) [contributor, copyright holder] Wouter Van Der Bijl [contributor, copyright holder] Martin Studer [contributor, copyright holder] Sigrid Keydana [contributor]","code":""},{"path":"https://keras.posit.co/reference/keras_array.html","id":null,"dir":"Reference","previous_headings":"","what":"Keras array object — keras_array","title":"Keras array object — keras_array","text":"Convert R vector, matrix, array object array optimal -memory layout floating point data type current Keras backend.","code":""},{"path":"https://keras.posit.co/reference/keras_array.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Keras array object — keras_array","text":"","code":"keras_array(x, dtype = NULL)"},{"path":"https://keras.posit.co/reference/keras_array.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Keras array object — keras_array","text":"x Object list objects convert dtype NumPy data type (e.g. float32, float64). unspecified R doubles converted default floating point type current Keras backend.","code":""},{"path":"https://keras.posit.co/reference/keras_array.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Keras array object — keras_array","text":"NumPy array specified dtype (list NumPy arrays list passed x).","code":""},{"path":"https://keras.posit.co/reference/keras_array.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Keras array object — keras_array","text":"Keras frequent row-oriented access arrays (shuffling drawing batches) order arrays created function always row-oriented (\"C\" opposed \"Fortran\" ordering, default R arrays). passed array already NumPy array desired dtype \"C\" order returned unmodified (additional copies made).","code":""},{"path":"https://keras.posit.co/reference/keras_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Keras Model — keras_model","title":"Keras Model — keras_model","text":"model directed acyclic graph layers.","code":""},{"path":"https://keras.posit.co/reference/keras_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Keras Model — keras_model","text":"","code":"keras_model(inputs, outputs = NULL, ...)"},{"path":"https://keras.posit.co/reference/keras_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Keras Model — keras_model","text":"inputs Input layer outputs Output layer ... additional arguments","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/keras_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Keras Model — keras_model","text":"","code":"if (FALSE) { library(keras3)  # input layer inputs <- layer_input(shape = c(784))  # outputs compose input + dense layers predictions <- inputs %>%   layer_dense(units = 64, activation = 'relu') %>%   layer_dense(units = 64, activation = 'relu') %>%   layer_dense(units = 10, activation = 'softmax')  # create and compile model model <- keras_model(inputs = inputs, outputs = predictions) model %>% compile(   optimizer = 'rmsprop',   loss = 'categorical_crossentropy',   metrics = c('accuracy') ) }"},{"path":"https://keras.posit.co/reference/keras_model_custom.html","id":null,"dir":"Reference","previous_headings":"","what":"(Deprecated) Create a Keras custom model — keras_model_custom","title":"(Deprecated) Create a Keras custom model — keras_model_custom","text":"keras_model_custom() soft-deprecated. Please define custom models subclassing keras$Model directly using %py_class% R6::R6Class(), calling new_model_class().","code":""},{"path":"https://keras.posit.co/reference/keras_model_custom.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"(Deprecated) Create a Keras custom model — keras_model_custom","text":"","code":"keras_model_custom(model_fn, name = NULL)"},{"path":"https://keras.posit.co/reference/keras_model_custom.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"(Deprecated) Create a Keras custom model — keras_model_custom","text":"model_fn Function returns R custom model name Optional name model","code":""},{"path":"https://keras.posit.co/reference/keras_model_custom.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"(Deprecated) Create a Keras custom model — keras_model_custom","text":"Keras model","code":""},{"path":"https://keras.posit.co/reference/keras_model_sequential.html","id":null,"dir":"Reference","previous_headings":"","what":"Keras Model composed of a linear stack of layers — keras_model_sequential","title":"Keras Model composed of a linear stack of layers — keras_model_sequential","text":"Keras Model composed linear stack layers","code":""},{"path":"https://keras.posit.co/reference/keras_model_sequential.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Keras Model composed of a linear stack of layers — keras_model_sequential","text":"","code":"keras_model_sequential(layers = NULL, name = NULL, ...)"},{"path":"https://keras.posit.co/reference/keras_model_sequential.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Keras Model composed of a linear stack of layers — keras_model_sequential","text":"layers List layers add model name Name model ... Arguments passed sequential_model_input_layer input_shape integer vector dimensions (including batch axis), tf$TensorShape instance (also including batch axis). batch_size Optional input batch size (integer NULL). dtype Optional datatype input. provided, Keras default float type used. input_tensor Optional tensor use layer input. set, layer use tf$TypeSpec tensor rather creating new placeholder tensor. sparse Boolean, whether placeholder created meant sparse. Default FALSE. ragged Boolean, whether placeholder created meant ragged. case, values 'NULL' 'shape' argument represent ragged dimensions. information RaggedTensors, see guide. Default FALSE. type_spec tf$TypeSpec object create Input . tf$TypeSpec represents entire batch. provided, args except name must NULL. input_layer_name,name Optional name input layer (string).","code":""},{"path":"https://keras.posit.co/reference/keras_model_sequential.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Keras Model composed of a linear stack of layers — keras_model_sequential","text":"arguments provided ..., sequential model initialized InputLayer instance. , first layer passed Sequential model defined input shape. means received input_shape batch_input_shape argument, type layers (recurrent, Dense...) input_dim argument.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/keras_model_sequential.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Keras Model composed of a linear stack of layers — keras_model_sequential","text":"","code":"if (FALSE) {  library(keras3)  model <- keras_model_sequential() model %>%   layer_dense(units = 32, input_shape = c(784)) %>%   layer_activation('relu') %>%   layer_dense(units = 10) %>%   layer_activation('softmax')  model %>% compile(   optimizer = 'rmsprop',   loss = 'categorical_crossentropy',   metrics = c('accuracy') )  # alternative way to provide input shape model <- keras_model_sequential(input_shape = c(784)) %>%   layer_dense(units = 32) %>%   layer_activation('relu') %>%   layer_dense(units = 10) %>%   layer_activation('softmax')  }"},{"path":"https://keras.posit.co/reference/layer_activation.html","id":null,"dir":"Reference","previous_headings":"","what":"Applies an activation function to an output. — layer_activation","title":"Applies an activation function to an output. — layer_activation","text":"Applies activation function output.","code":""},{"path":"https://keras.posit.co/reference/layer_activation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Applies an activation function to an output. — layer_activation","text":"","code":"layer_activation(object, activation, ...)"},{"path":"https://keras.posit.co/reference/layer_activation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Applies an activation function to an output. — layer_activation","text":"object Object compose layer . tensor, array, sequential model. activation Activation function. callable, name activation keras3::activation_* namespace. ... Base layer keyword arguments, name dtype.","code":""},{"path":"https://keras.posit.co/reference/layer_activation.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Applies an activation function to an output. — layer_activation","text":"","code":"x <- array(c(-3, -1, 0, 2)) layer <- layer_activation(activation = 'relu') layer(x) ## tf.Tensor([0. 0. 0. 2.], shape=(4), dtype=float32) layer <- layer_activation(activation = activation_relu) layer(x) ## tf.Tensor([0. 0. 0. 2.], shape=(4), dtype=float32) layer <- layer_activation(activation = op_relu) layer(x) ## tf.Tensor([0. 0. 0. 2.], shape=(4), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_activation_elu.html","id":null,"dir":"Reference","previous_headings":"","what":"Applies an Exponential Linear Unit function to an output. — layer_activation_elu","title":"Applies an Exponential Linear Unit function to an output. — layer_activation_elu","text":"Formula:","code":"f(x) = alpha * (exp(x) - 1.) for x < 0 f(x) = x for x >= 0"},{"path":"https://keras.posit.co/reference/layer_activation_elu.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Applies an Exponential Linear Unit function to an output. — layer_activation_elu","text":"","code":"layer_activation_elu(object, alpha = 1, ...)"},{"path":"https://keras.posit.co/reference/layer_activation_elu.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Applies an Exponential Linear Unit function to an output. — layer_activation_elu","text":"object Object compose layer . tensor, array, sequential model. alpha float, slope negative section. Defaults 1.0. ... Base layer keyword arguments, name dtype.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_activation_leaky_relu.html","id":null,"dir":"Reference","previous_headings":"","what":"Leaky version of a Rectified Linear Unit activation layer. — layer_activation_leaky_relu","title":"Leaky version of a Rectified Linear Unit activation layer. — layer_activation_leaky_relu","text":"layer allows small gradient unit active. Formula:","code":"f(x) = alpha * x if x < 0 f(x) = x if x >= 0"},{"path":"https://keras.posit.co/reference/layer_activation_leaky_relu.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Leaky version of a Rectified Linear Unit activation layer. — layer_activation_leaky_relu","text":"","code":"layer_activation_leaky_relu(object, negative_slope = 0.3, ...)"},{"path":"https://keras.posit.co/reference/layer_activation_leaky_relu.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Leaky version of a Rectified Linear Unit activation layer. — layer_activation_leaky_relu","text":"object Object compose layer . tensor, array, sequential model. negative_slope Float >= 0.0. Negative slope coefficient. Defaults 0.3. ... Base layer keyword arguments, name dtype.","code":""},{"path":"https://keras.posit.co/reference/layer_activation_leaky_relu.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Leaky version of a Rectified Linear Unit activation layer. — layer_activation_leaky_relu","text":"","code":"leaky_relu_layer = LeakyReLU(negative_slope=0.5) input = np.array([-10, -5, 0.0, 5, 10]) result = leaky_relu_layer(input) # result = [-5. , -2.5,  0. ,  5. , 10.]"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_activation_parametric_relu.html","id":null,"dir":"Reference","previous_headings":"","what":"Parametric Rectified Linear Unit activation layer. — layer_activation_parametric_relu","title":"Parametric Rectified Linear Unit activation layer. — layer_activation_parametric_relu","text":"Formula:   alpha learned array shape x.","code":"f(x) = alpha * x for x < 0 f(x) = x for x >= 0"},{"path":"https://keras.posit.co/reference/layer_activation_parametric_relu.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parametric Rectified Linear Unit activation layer. — layer_activation_parametric_relu","text":"","code":"layer_activation_parametric_relu(   object,   alpha_initializer = \"Zeros\",   alpha_regularizer = NULL,   alpha_constraint = NULL,   shared_axes = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_activation_parametric_relu.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parametric Rectified Linear Unit activation layer. — layer_activation_parametric_relu","text":"object Object compose layer . tensor, array, sequential model. alpha_initializer Initializer function weights. alpha_regularizer Regularizer weights. alpha_constraint Constraint weights. shared_axes axes along share learnable parameters activation function. example, incoming feature maps 2D convolution output shape (batch, height, width, channels), wish share parameters across space filter one set parameters, set shared_axes=[1, 2]. ... Base layer keyword arguments, name dtype.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_activation_relu.html","id":null,"dir":"Reference","previous_headings":"","what":"Rectified Linear Unit activation function layer. — layer_activation_relu","title":"Rectified Linear Unit activation function layer. — layer_activation_relu","text":"Formula:","code":"f(x) = max(x,0) f(x) = max_value if x >= max_value f(x) = x if threshold <= x < max_value f(x) = negative_slope * (x - threshold) otherwise"},{"path":"https://keras.posit.co/reference/layer_activation_relu.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rectified Linear Unit activation function layer. — layer_activation_relu","text":"","code":"layer_activation_relu(   object,   max_value = NULL,   negative_slope = 0,   threshold = 0,   ... )"},{"path":"https://keras.posit.co/reference/layer_activation_relu.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rectified Linear Unit activation function layer. — layer_activation_relu","text":"object Object compose layer . tensor, array, sequential model. max_value Float >= 0. Maximum activation value. NULL means unlimited. Defaults NULL. negative_slope Float >= 0. Negative slope coefficient. Defaults 0.0. threshold Float >= 0. Threshold value thresholded activation. Defaults 0.0. ... Base layer keyword arguments, name dtype.","code":""},{"path":"https://keras.posit.co/reference/layer_activation_relu.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rectified Linear Unit activation function layer. — layer_activation_relu","text":"","code":"relu_layer = keras.layers.activations.ReLU(     max_value=10,     negative_slope=0.5,     threshold=0, ) input = np.array([-10, -5, 0.0, 5, 10]) result = relu_layer(input) # result = [-5. , -2.5,  0. ,  5. , 10.]"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_activation_softmax.html","id":null,"dir":"Reference","previous_headings":"","what":"Softmax activation layer. — layer_activation_softmax","title":"Softmax activation layer. — layer_activation_softmax","text":"Formula:","code":"exp_x <- exp(x - max(x)) f(x) = exp_x / sum(exp_x)"},{"path":"https://keras.posit.co/reference/layer_activation_softmax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Softmax activation layer. — layer_activation_softmax","text":"","code":"layer_activation_softmax(object, axis = -1L, ...)"},{"path":"https://keras.posit.co/reference/layer_activation_softmax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Softmax activation layer. — layer_activation_softmax","text":"object Object compose layer . tensor, array, sequential model. axis Integer, list Integers, axis along softmax normalization applied. ... Base layer keyword arguments, name dtype.","code":""},{"path":"https://keras.posit.co/reference/layer_activation_softmax.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Softmax activation layer. — layer_activation_softmax","text":"","code":"Softmaxed output with the same shape as `inputs`."},{"path":"https://keras.posit.co/reference/layer_activation_softmax.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Softmax activation layer. — layer_activation_softmax","text":"","code":"softmax_layer <- layer_activation_softmax() input <- op_array(c(1, 2, 1)) softmax_layer(input) ## tf.Tensor([0.21194157 0.5761169  0.21194157], shape=(3), dtype=float32)"},{"path":"https://keras.posit.co/reference/layer_activation_softmax.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"Softmax activation layer. — layer_activation_softmax","text":"inputs: inputs (logits) softmax layer. mask: boolean mask shape inputs. mask specifies 1 keep 0 mask. Defaults NULL.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_activity_regularization.html","id":null,"dir":"Reference","previous_headings":"","what":"Layer that applies an update to the cost function based input activity. — layer_activity_regularization","title":"Layer that applies an update to the cost function based input activity. — layer_activity_regularization","text":"Layer applies update cost function based input activity.","code":""},{"path":"https://keras.posit.co/reference/layer_activity_regularization.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Layer that applies an update to the cost function based input activity. — layer_activity_regularization","text":"","code":"layer_activity_regularization(object, l1 = 0, l2 = 0, ...)"},{"path":"https://keras.posit.co/reference/layer_activity_regularization.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Layer that applies an update to the cost function based input activity. — layer_activity_regularization","text":"object Object compose layer . tensor, array, sequential model. l1 L1 regularization factor (positive float). l2 L2 regularization factor (positive float). ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_activity_regularization.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Layer that applies an update to the cost function based input activity. — layer_activity_regularization","text":"Arbitrary. Use keyword argument input_shape (tuple integers, include samples axis) using layer first layer model.","code":""},{"path":"https://keras.posit.co/reference/layer_activity_regularization.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Layer that applies an update to the cost function based input activity. — layer_activity_regularization","text":"shape input.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_add.html","id":null,"dir":"Reference","previous_headings":"","what":"Performs elementwise addition operation. — layer_add","title":"Performs elementwise addition operation. — layer_add","text":"takes input list tensors, shape, returns single tensor (also shape).","code":""},{"path":"https://keras.posit.co/reference/layer_add.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Performs elementwise addition operation. — layer_add","text":"","code":"layer_add(inputs, ...)"},{"path":"https://keras.posit.co/reference/layer_add.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Performs elementwise addition operation. — layer_add","text":"inputs layers combine ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_add.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Performs elementwise addition operation. — layer_add","text":"Usage Keras model:","code":"input_shape <- c(1, 2, 3) x1 <- op_ones(input_shape) x2 <- op_ones(input_shape) layer_add(x1, x2) ## tf.Tensor( ## [[[2. 2. 2.] ##   [2. 2. 2.]]], shape=(1, 2, 3), dtype=float32) input1 <- layer_input(shape = c(16)) x1 <- input1 |> layer_dense(8, activation = 'relu')  input2 <- layer_input(shape = c(32)) x2 <- input2 |> layer_dense(8, activation = 'relu')  # equivalent to `added = layer_add([x1, x2))` added <- layer_add(x1, x2) output <- added |> layer_dense(4)  model <- keras_model(inputs = c(input1, input2), outputs = output)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_additive_attention.html","id":null,"dir":"Reference","previous_headings":"","what":"Additive attention layer, a.k.a. Bahdanau-style attention. — layer_additive_attention","title":"Additive attention layer, a.k.a. Bahdanau-style attention. — layer_additive_attention","text":"Inputs list 2 3 elements: query tensor shape (batch_size, Tq, dim). value tensor shape (batch_size, Tv, dim). optional key tensor shape (batch_size, Tv, dim). none supplied, value used key. calculation follows steps: Calculate attention scores using query key shape (batch_size, Tq, Tv) non-linear sum scores = reduce_sum(tanh(query + key), axis=-1). Use scores calculate softmax distribution shape (batch_size, Tq, Tv). Use softmax distribution create linear combination value shape (batch_size, Tq, dim).","code":""},{"path":"https://keras.posit.co/reference/layer_additive_attention.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Additive attention layer, a.k.a. Bahdanau-style attention. — layer_additive_attention","text":"","code":"layer_additive_attention(object, use_scale = TRUE, dropout = 0, ...)"},{"path":"https://keras.posit.co/reference/layer_additive_attention.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Additive attention layer, a.k.a. Bahdanau-style attention. — layer_additive_attention","text":"object Object compose layer . tensor, array, sequential model. use_scale TRUE, create scalar variable scale attention scores. dropout Float 0 1. Fraction units drop attention scores. Defaults 0.0. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_additive_attention.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"Additive attention layer, a.k.a. Bahdanau-style attention. — layer_additive_attention","text":"inputs: List following tensors: query: Query tensor shape (batch_size, Tq, dim). value: Value tensor shape (batch_size, Tv, dim). key: Optional key tensor shape (batch_size, Tv, dim). given, use value key value, common case. mask: List following tensors: query_mask: boolean mask tensor shape (batch_size, Tq). given, output zero positions mask==FALSE. value_mask: boolean mask tensor shape (batch_size, Tv). given, apply mask values positions mask==FALSE contribute result. return_attention_scores: bool, TRUE, returns attention scores (masking softmax) additional output argument. training: Python boolean indicating whether layer behave training mode (adding dropout) inference mode (dropout). use_causal_mask: Boolean. Set TRUE decoder self-attention. Adds mask position attend positions j > . prevents flow information future towards past. Defaults FALSE.","code":""},{"path":"https://keras.posit.co/reference/layer_additive_attention.html","id":"output","dir":"Reference","previous_headings":"","what":"Output","title":"Additive attention layer, a.k.a. Bahdanau-style attention. — layer_additive_attention","text":"Attention outputs shape (batch_size, Tq, dim). (Optional) Attention scores masking softmax shape (batch_size, Tq, Tv).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_attention.html","id":null,"dir":"Reference","previous_headings":"","what":"Dot-product attention layer, a.k.a. Luong-style attention. — layer_attention","title":"Dot-product attention layer, a.k.a. Luong-style attention. — layer_attention","text":"Inputs list 2 3 elements: query tensor shape (batch_size, Tq, dim). value tensor shape (batch_size, Tv, dim). optional key tensor shape (batch_size, Tv, dim). none supplied, value used key. calculation follows steps: Calculate attention scores using query key shape (batch_size, Tq, Tv). Use scores calculate softmax distribution shape (batch_size, Tq, Tv). Use softmax distribution create linear combination value shape (batch_size, Tq, dim).","code":""},{"path":"https://keras.posit.co/reference/layer_attention.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dot-product attention layer, a.k.a. Luong-style attention. — layer_attention","text":"","code":"layer_attention(   object,   use_scale = FALSE,   score_mode = \"dot\",   dropout = 0,   seed = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_attention.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dot-product attention layer, a.k.a. Luong-style attention. — layer_attention","text":"object Object compose layer . tensor, array, sequential model. use_scale TRUE, create scalar variable scale attention scores. score_mode Function use compute attention scores, one {\"dot\", \"concat\"}. \"dot\" refers dot product query key vectors. \"concat\" refers hyperbolic tangent concatenation query key vectors. dropout Float 0 1. Fraction units drop attention scores. Defaults 0.0. seed integer use random seed incase dropout. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_attention.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"Dot-product attention layer, a.k.a. Luong-style attention. — layer_attention","text":"inputs: List following tensors: query: Query tensor shape (batch_size, Tq, dim). value: Value tensor shape (batch_size, Tv, dim). key: Optional key tensor shape (batch_size, Tv, dim). given, use value key value, common case. mask: List following tensors: query_mask: boolean mask tensor shape (batch_size, Tq). given, output zero positions mask==FALSE. value_mask: boolean mask tensor shape (batch_size, Tv). given, apply mask values positions mask==FALSE contribute result. return_attention_scores: bool, TRUE, returns attention scores (masking softmax) additional output argument. training: Python boolean indicating whether layer behave training mode (adding dropout) inference mode (dropout). use_causal_mask: Boolean. Set TRUE decoder self-attention. Adds mask position attend positions j > . prevents flow information future towards past. Defaults FALSE.","code":""},{"path":"https://keras.posit.co/reference/layer_attention.html","id":"output","dir":"Reference","previous_headings":"","what":"Output","title":"Dot-product attention layer, a.k.a. Luong-style attention. — layer_attention","text":"Attention outputs shape (batch_size, Tq, dim). (Optional) Attention scores masking softmax shape (batch_size, Tq, Tv).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_average.html","id":null,"dir":"Reference","previous_headings":"","what":"Averages a list of inputs element-wise.. — layer_average","title":"Averages a list of inputs element-wise.. — layer_average","text":"takes input list tensors, shape, returns single tensor (also shape).","code":""},{"path":"https://keras.posit.co/reference/layer_average.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Averages a list of inputs element-wise.. — layer_average","text":"","code":"layer_average(inputs, ...)"},{"path":"https://keras.posit.co/reference/layer_average.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Averages a list of inputs element-wise.. — layer_average","text":"inputs layers combine ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_average.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Averages a list of inputs element-wise.. — layer_average","text":"Usage Keras model:","code":"input_shape <- c(1, 2, 3) x1 <- op_ones(input_shape) x2 <- op_zeros(input_shape) layer_average(x1, x2) ## tf.Tensor( ## [[[0.5 0.5 0.5] ##   [0.5 0.5 0.5]]], shape=(1, 2, 3), dtype=float32) input1 <- layer_input(shape = c(16)) x1 <- input1 |> layer_dense(8, activation = 'relu')  input2 <- layer_input(shape = c(32)) x2 <- input2 |> layer_dense(8, activation = 'relu')  added <- layer_average(x1, x2) output <- added |> layer_dense(4)  model <- keras_model(inputs = c(input1, input2), outputs = output)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_average_pooling_1d.html","id":null,"dir":"Reference","previous_headings":"","what":"Average pooling for temporal data. — layer_average_pooling_1d","title":"Average pooling for temporal data. — layer_average_pooling_1d","text":"Downsamples input representation taking average value window defined pool_size. window shifted strides.  resulting output using \"valid\" padding option shape : output_shape = (input_shape - pool_size + 1) / strides) resulting output shape using \"\" padding option : output_shape = input_shape / strides","code":""},{"path":"https://keras.posit.co/reference/layer_average_pooling_1d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Average pooling for temporal data. — layer_average_pooling_1d","text":"","code":"layer_average_pooling_1d(   object,   pool_size,   strides = NULL,   padding = \"valid\",   data_format = NULL,   name = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_average_pooling_1d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Average pooling for temporal data. — layer_average_pooling_1d","text":"object Object compose layer . tensor, array, sequential model. pool_size int, size max pooling window. strides int NULL. Specifies much pooling window moves pooling step. NULL, default pool_size. padding string, either \"valid\" \"\" (case-insensitive). \"valid\" means padding. \"\" results padding evenly left/right /input output height/width dimension input. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, steps, features) \"channels_first\" corresponds inputs shape (batch, features, steps). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". name String, name object ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_average_pooling_1d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Average pooling for temporal data. — layer_average_pooling_1d","text":"data_format=\"channels_last\": 3D tensor shape (batch_size, steps, features). data_format=\"channels_first\": 3D tensor shape (batch_size, features, steps).","code":""},{"path":"https://keras.posit.co/reference/layer_average_pooling_1d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Average pooling for temporal data. — layer_average_pooling_1d","text":"data_format=\"channels_last\": 3D tensor shape (batch_size, downsampled_steps, features). data_format=\"channels_first\": 3D tensor shape (batch_size, features, downsampled_steps).","code":""},{"path":"https://keras.posit.co/reference/layer_average_pooling_1d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Average pooling for temporal data. — layer_average_pooling_1d","text":"strides=1 padding=\"valid\":     strides=2 padding=\"valid\":     strides=1 padding=\"\":","code":"x <- op_array(c(1., 2., 3., 4., 5.)) |> op_reshape(c(1, 5, 1)) output <- x |>   layer_average_pooling_1d(pool_size = 2,                            strides = 1,                            padding = \"valid\") output ## tf.Tensor( ## [[[1.5] ##   [2.5] ##   [3.5] ##   [4.5]]], shape=(1, 4, 1), dtype=float32) x <- op_array(c(1., 2., 3., 4., 5.)) |> op_reshape(c(1, 5, 1)) output <- x |>   layer_average_pooling_1d(pool_size = 2,                            strides = 2,                            padding = \"valid\") output ## tf.Tensor( ## [[[1.5] ##   [3.5]]], shape=(1, 2, 1), dtype=float32) x <- op_array(c(1., 2., 3., 4., 5.)) |> op_reshape(c(1, 5, 1)) output <- x |>   layer_average_pooling_1d(pool_size = 2,                            strides = 1,                            padding = \"same\") output ## tf.Tensor( ## [[[1.5] ##   [2.5] ##   [3.5] ##   [4.5] ##   [5. ]]], shape=(1, 5, 1), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_average_pooling_2d.html","id":null,"dir":"Reference","previous_headings":"","what":"Average pooling operation for 2D spatial data. — layer_average_pooling_2d","title":"Average pooling operation for 2D spatial data. — layer_average_pooling_2d","text":"Downsamples input along spatial dimensions (height width) taking average value input window (size defined pool_size) channel input. window shifted strides along dimension. resulting output using \"valid\" padding option spatial shape (number rows columns) : output_shape = math.floor((input_shape - pool_size) / strides) + 1 (input_shape >= pool_size) resulting output shape using \"\" padding option : output_shape = math.floor((input_shape - 1) / strides) + 1","code":""},{"path":"https://keras.posit.co/reference/layer_average_pooling_2d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Average pooling operation for 2D spatial data. — layer_average_pooling_2d","text":"","code":"layer_average_pooling_2d(   object,   pool_size,   strides = NULL,   padding = \"valid\",   data_format = \"channels_last\",   name = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_average_pooling_2d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Average pooling operation for 2D spatial data. — layer_average_pooling_2d","text":"object Object compose layer . tensor, array, sequential model. pool_size int list 2 integers, factors downscale (dim1, dim2). one integer specified, window length used dimensions. strides int list 2 integers, NULL. Strides values. NULL, default pool_size. one int specified, stride size used dimensions. padding string, either \"valid\" \"\" (case-insensitive). \"valid\" means padding. \"\" results padding evenly left/right /input output height/width dimension input. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, height, width, channels) \"channels_first\" corresponds inputs shape (batch, channels, height, width). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". name String, name object ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_average_pooling_2d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Average pooling operation for 2D spatial data. — layer_average_pooling_2d","text":"data_format=\"channels_last\": 4D tensor shape (batch_size, height, width, channels). data_format=\"channels_first\": 4D tensor shape (batch_size, channels, height, width).","code":""},{"path":"https://keras.posit.co/reference/layer_average_pooling_2d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Average pooling operation for 2D spatial data. — layer_average_pooling_2d","text":"data_format=\"channels_last\": 4D tensor shape (batch_size, pooled_height, pooled_width, channels). data_format=\"channels_first\": 4D tensor shape (batch_size, channels, pooled_height, pooled_width).","code":""},{"path":"https://keras.posit.co/reference/layer_average_pooling_2d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Average pooling operation for 2D spatial data. — layer_average_pooling_2d","text":"strides=(1, 1) padding=\"valid\":     strides=(2, 2) padding=\"valid\":     stride=(1, 1) padding=\"\":","code":"x <- op_array(1:9, \"float32\") |> op_reshape(c(1, 3, 3, 1)) output <- x |>   layer_average_pooling_2d(pool_size = c(2, 2),                            strides = c(1, 1),                            padding = \"valid\") output ## tf.Tensor( ## [[[[3.] ##    [4.]] ## ##   [[6.] ##    [7.]]]], shape=(1, 2, 2, 1), dtype=float32) x <- op_array(1:12, \"float32\") |> op_reshape(c(1, 3, 4, 1)) output <- x |>   layer_average_pooling_2d(pool_size = c(2, 2),                            strides = c(2, 2),                            padding = \"valid\") output ## tf.Tensor( ## [[[[3.5] ##    [5.5]]]], shape=(1, 1, 2, 1), dtype=float32) x <- op_array(1:9, \"float32\") |> op_reshape(c(1, 3, 3, 1)) output <- x |>   layer_average_pooling_2d(pool_size = c(2, 2),                            strides = c(1, 1),                            padding = \"same\") output ## tf.Tensor( ## [[[[3. ] ##    [4. ] ##    [4.5]] ## ##   [[6. ] ##    [7. ] ##    [7.5]] ## ##   [[7.5] ##    [8.5] ##    [9. ]]]], shape=(1, 3, 3, 1), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_average_pooling_3d.html","id":null,"dir":"Reference","previous_headings":"","what":"Average pooling operation for 3D data (spatial or spatio-temporal). — layer_average_pooling_3d","title":"Average pooling operation for 3D data (spatial or spatio-temporal). — layer_average_pooling_3d","text":"Downsamples input along spatial dimensions (depth, height, width) taking average value input window (size defined pool_size) channel input. window shifted strides along dimension.","code":""},{"path":"https://keras.posit.co/reference/layer_average_pooling_3d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Average pooling operation for 3D data (spatial or spatio-temporal). — layer_average_pooling_3d","text":"","code":"layer_average_pooling_3d(   object,   pool_size,   strides = NULL,   padding = \"valid\",   data_format = \"channels_last\",   name = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_average_pooling_3d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Average pooling operation for 3D data (spatial or spatio-temporal). — layer_average_pooling_3d","text":"object Object compose layer . tensor, array, sequential model. pool_size int list 3 integers, factors downscale (dim1, dim2, dim3). one integer specified, window length used dimensions. strides int list 3 integers, NULL. Strides values. NULL, default pool_size. one int specified, stride size used dimensions. padding string, either \"valid\" \"\" (case-insensitive). \"valid\" means padding. \"\" results padding evenly left/right /input output height/width dimension input. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) \"channels_first\" corresponds inputs shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". name String, name object ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_average_pooling_3d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Average pooling operation for 3D data (spatial or spatio-temporal). — layer_average_pooling_3d","text":"data_format=\"channels_last\": 5D tensor shape: (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) data_format=\"channels_first\": 5D tensor shape: (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)","code":""},{"path":"https://keras.posit.co/reference/layer_average_pooling_3d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Average pooling operation for 3D data (spatial or spatio-temporal). — layer_average_pooling_3d","text":"data_format=\"channels_last\": 5D tensor shape: (batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels) data_format=\"channels_first\": 5D tensor shape: (batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3)","code":""},{"path":"https://keras.posit.co/reference/layer_average_pooling_3d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Average pooling operation for 3D data (spatial or spatio-temporal). — layer_average_pooling_3d","text":"","code":"depth <- height <- width <- 30 channels <- 3  inputs <- layer_input(shape = c(depth, height, width, channels)) outputs <- inputs |> layer_average_pooling_3d(pool_size = 3) outputs # Shape: (batch_size, 10, 10, 10, 3) ## <KerasTensor shape=(None, 10, 10, 10, 3), dtype=float32, sparse=False, name=keras_tensor_1>"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_batch_normalization.html","id":null,"dir":"Reference","previous_headings":"","what":"Layer that normalizes its inputs. — layer_batch_normalization","title":"Layer that normalizes its inputs. — layer_batch_normalization","text":"Batch normalization applies transformation maintains mean output close 0 output standard deviation close 1. Importantly, batch normalization works differently training inference. training (.e. using fit() calling layer/model argument training=TRUE), layer normalizes output using mean standard deviation current batch inputs. say, channel normalized, layer returns gamma * (batch - mean(batch)) / sqrt(var(batch) + epsilon) + beta, : epsilon small constant (configurable part constructor arguments) gamma learned scaling factor (initialized 1), can disabled passing scale=FALSE constructor. beta learned offset factor (initialized 0), can disabled passing center=FALSE constructor. inference (.e. using evaluate() predict() calling layer/model argument training=FALSE (default), layer normalizes output using moving average mean standard deviation batches seen training. say, returns gamma * (batch - self$moving_mean) / sqrt(self$moving_var+epsilon) + beta. self$moving_mean self$moving_var non-trainable variables updated time layer called training mode, : moving_mean = moving_mean * momentum + mean(batch) * (1 - momentum) moving_var = moving_var * momentum + var(batch) * (1 - momentum) , layer normalize inputs inference trained data similar statistics inference data. setting layer.trainable = FALSE BatchNormalization layer: meaning setting layer.trainable = FALSE freeze layer, .e. internal state change training: trainable weights updated fit() train_on_batch(), state updates run. Usually, necessarily mean layer run inference mode (normally controlled training argument can passed calling layer). \"Frozen state\" \"inference mode\" two separate concepts. However, case BatchNormalization layer, setting trainable = FALSE layer means layer subsequently run inference mode (meaning use moving mean moving variance normalize current batch, rather using mean variance current batch). Note : Setting trainable model containing layers recursively set trainable value inner layers. value trainable attribute changed calling compile() model, new value take effect model compile() called .","code":""},{"path":"https://keras.posit.co/reference/layer_batch_normalization.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Layer that normalizes its inputs. — layer_batch_normalization","text":"","code":"layer_batch_normalization(   object,   axis = -1L,   momentum = 0.99,   epsilon = 0.001,   center = TRUE,   scale = TRUE,   beta_initializer = \"zeros\",   gamma_initializer = \"ones\",   moving_mean_initializer = \"zeros\",   moving_variance_initializer = \"ones\",   beta_regularizer = NULL,   gamma_regularizer = NULL,   beta_constraint = NULL,   gamma_constraint = NULL,   synchronized = FALSE,   ... )"},{"path":"https://keras.posit.co/reference/layer_batch_normalization.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Layer that normalizes its inputs. — layer_batch_normalization","text":"object Object compose layer . tensor, array, sequential model. axis Integer, axis normalized (typically features axis). instance, Conv2D layer data_format=\"channels_first\", use axis=1. momentum Momentum moving average. epsilon Small float added variance avoid dividing zero. center TRUE, add offset beta normalized tensor. FALSE, beta ignored. scale TRUE, multiply gamma. FALSE, gamma used. next layer linear can disabled since scaling done next layer. beta_initializer Initializer beta weight. gamma_initializer Initializer gamma weight. moving_mean_initializer Initializer moving mean. moving_variance_initializer Initializer moving variance. beta_regularizer Optional regularizer beta weight. gamma_regularizer Optional regularizer gamma weight. beta_constraint Optional constraint beta weight. gamma_constraint Optional constraint gamma weight. synchronized applicable TensorFlow backend. TRUE, synchronizes global batch statistics (mean variance) layer across devices training step distributed training strategy. FALSE, replica uses local batch statistics. ... Base layer keyword arguments (e.g. name dtype).","code":""},{"path":"https://keras.posit.co/reference/layer_batch_normalization.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"Layer that normalizes its inputs. — layer_batch_normalization","text":"inputs: Input tensor (rank). training: Python boolean indicating whether layer behave training mode inference mode. training=TRUE: layer normalize inputs using mean variance current batch inputs. training=FALSE: layer normalize inputs using mean variance moving statistics, learned training.","code":""},{"path":"https://keras.posit.co/reference/layer_batch_normalization.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Layer that normalizes its inputs. — layer_batch_normalization","text":"Ioffe Szegedy, 2015.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_bidirectional.html","id":null,"dir":"Reference","previous_headings":"","what":"Bidirectional wrapper for RNNs. — layer_bidirectional","title":"Bidirectional wrapper for RNNs. — layer_bidirectional","text":"Bidirectional wrapper RNNs.","code":""},{"path":"https://keras.posit.co/reference/layer_bidirectional.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bidirectional wrapper for RNNs. — layer_bidirectional","text":"","code":"layer_bidirectional(   object,   layer,   merge_mode = \"concat\",   weights = NULL,   backward_layer = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_bidirectional.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bidirectional wrapper for RNNs. — layer_bidirectional","text":"object Object compose layer . tensor, array, sequential model. layer keras.layers.RNN instance, keras.layers.LSTM keras.layers.GRU. also keras.layers.Layer instance meets following criteria: 1. sequence-processing layer (accepts 3D+ inputs). 2. go_backwards, return_sequences return_state attribute (semantics RNN class). 3. input_spec attribute. 4. Implement serialization via get_config() from_config(). Note recommended way create new RNN layers write custom RNN cell use keras.layers.RNN, instead subclassing keras.layers.Layer directly. return_sequences True, output masked timestep zero regardless layer's original zero_output_for_mask value. merge_mode Mode outputs forward backward RNNs combined. One {\"sum\", \"mul\", \"concat\", \"ave\", None}. None, outputs combined, returned list. Defaults \"concat\". weights see description backward_layer Optional keras.layers.RNN, keras.layers.Layer instance used handle backwards input processing. backward_layer provided, layer instance passed layer argument used generate backward layer automatically. Note provided backward_layer layer properties matching layer argument, particular values stateful, return_states, return_sequences, etc. addition, backward_layer layer different go_backwards argument values. ValueError raised requirements met. ... Passed Python callable","code":""},{"path":"https://keras.posit.co/reference/layer_bidirectional.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"Bidirectional wrapper for RNNs. — layer_bidirectional","text":"call arguments layer wrapped RNN layer. Beware passing initial_state argument call layer, first half list elements initial_state list passed forward RNN call last half list elements passed backward RNN call.","code":""},{"path":"https://keras.posit.co/reference/layer_bidirectional.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Bidirectional wrapper for RNNs. — layer_bidirectional","text":"instantiating Bidirectional layer existing RNN layer instance reuse weights state RNN layer instance -- Bidirectional layer freshly initialized weights.","code":""},{"path":"https://keras.posit.co/reference/layer_bidirectional.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bidirectional wrapper for RNNs. — layer_bidirectional","text":"","code":"model = Sequential([     Input(shape=(5, 10)),     Bidirectional(LSTM(10, return_sequences=True),     Bidirectional(LSTM(10)),     Dense(5, activation=\"softmax\"), ]) model.compile(loss='categorical_crossentropy', optimizer='rmsprop')  # With custom backward layer forward_layer = LSTM(10, return_sequences=True) backward_layer = LSTM(10, activation='relu', return_sequences=True,                       go_backwards=True) model = Sequential([     Input(shape=(5, 10)),     Bidirectional(forward_layer, backward_layer=backward_layer),     Dense(5, activation=\"softmax\"), ]) model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_category_encoding.html","id":null,"dir":"Reference","previous_headings":"","what":"A preprocessing layer which encodes integer features. — layer_category_encoding","title":"A preprocessing layer which encodes integer features. — layer_category_encoding","text":"layer provides options condensing data categorical encoding total number tokens known advance. accepts integer values inputs, outputs dense sparse representation inputs. integer inputs total number tokens known, use layer_integer_lookup() instead. Note: layer safe use inside tf.data pipeline (independently backend using).","code":""},{"path":"https://keras.posit.co/reference/layer_category_encoding.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A preprocessing layer which encodes integer features. — layer_category_encoding","text":"","code":"layer_category_encoding(   object,   num_tokens = NULL,   output_mode = \"multi_hot\",   ... )"},{"path":"https://keras.posit.co/reference/layer_category_encoding.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A preprocessing layer which encodes integer features. — layer_category_encoding","text":"object Object compose layer . tensor, array, sequential model. num_tokens total number tokens layer support. inputs layer must integers range 0 <= value < num_tokens, error thrown. output_mode Specification output layer. Values can \"one_hot\", \"multi_hot\" \"count\", configuring layer follows: - \"one_hot\": Encodes individual element input array num_tokens size, containing 1 element index. last dimension size 1, encode dimension. last dimension size 1, append new dimension encoded output. - \"multi_hot\": Encodes sample input single array num_tokens size, containing 1 vocabulary term present sample. Treats last dimension sample dimension, input shape (..., sample_length), output shape (..., num_tokens). - \"count\": Like \"multi_hot\", int array contains count number times token index appeared sample. output modes, currently output rank 2 supported. Defaults \"multi_hot\". ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_category_encoding.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A preprocessing layer which encodes integer features. — layer_category_encoding","text":"One-hot encoding data     Multi-hot encoding data     Using weighted inputs \"count\" mode","code":"layer <- layer_category_encoding(num_tokens = 4, output_mode = \"one_hot\") x <- op_array(c(3, 2, 0, 1), \"int32\") layer(x) ## tf.Tensor( ## [[0. 0. 0. 1.] ##  [0. 0. 1. 0.] ##  [1. 0. 0. 0.] ##  [0. 1. 0. 0.]], shape=(4, 4), dtype=float32) layer <- layer_category_encoding(num_tokens = 4, output_mode = \"multi_hot\") x <- op_array(rbind(c(0, 1),                    c(0, 0),                    c(1, 2),                    c(3, 1)), \"int32\") layer(x) ## tf.Tensor( ## [[1. 1. 0. 0.] ##  [1. 0. 0. 0.] ##  [0. 1. 1. 0.] ##  [0. 1. 0. 1.]], shape=(4, 4), dtype=float32) layer <- layer_category_encoding(num_tokens = 4, output_mode = \"count\") count_weights <- op_array(rbind(c(.1, .2),                                c(.1, .1),                                c(.2, .3),                                c(.4, .2))) x <- op_array(rbind(c(0, 1),                    c(0, 0),                    c(1, 2),                    c(3, 1)), \"int32\") layer(x, count_weights = count_weights) #   array([[01, 02, 0. , 0. ], #          [02, 0. , 0. , 0. ], #          [0. , 02, 03, 0. ], #          [0. , 02, 0. , 04]]>"},{"path":"https://keras.posit.co/reference/layer_category_encoding.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"A preprocessing layer which encodes integer features. — layer_category_encoding","text":"inputs: 1D 2D tensor integer inputs. count_weights: tensor shape inputs indicating weight sample value summing count mode. used \"multi_hot\" \"one_hot\" modes.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_center_crop.html","id":null,"dir":"Reference","previous_headings":"","what":"A preprocessing layer which crops images. — layer_center_crop","title":"A preprocessing layer which crops images. — layer_center_crop","text":"layers crops central portion images target size. image smaller target size, resized cropped return largest possible window image matches target aspect ratio. Input pixel values can range (e.g. [0., 1.) [0, 255]).","code":""},{"path":"https://keras.posit.co/reference/layer_center_crop.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A preprocessing layer which crops images. — layer_center_crop","text":"","code":"layer_center_crop(object, height, width, data_format = NULL, ...)"},{"path":"https://keras.posit.co/reference/layer_center_crop.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A preprocessing layer which crops images. — layer_center_crop","text":"object Object compose layer . tensor, array, sequential model. height Integer, height output shape. width Integer, width output shape. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, height, width, channels) \"channels_first\" corresponds inputs shape (batch, channels, height, width). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_center_crop.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"A preprocessing layer which crops images. — layer_center_crop","text":"3D (unbatched) 4D (batched) tensor shape: (..., height, width, channels), \"channels_last\" format, (..., channels, height, width), \"channels_first\" format.","code":""},{"path":"https://keras.posit.co/reference/layer_center_crop.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"A preprocessing layer which crops images. — layer_center_crop","text":"3D (unbatched) 4D (batched) tensor shape: (..., target_height, target_width, channels), (..., channels, target_height, target_width), \"channels_first\" format. input height/width even target height/width odd (inversely), input image left-padded 1 pixel. Note: layer safe use inside tf.data pipeline (independently backend using).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_concatenate.html","id":null,"dir":"Reference","previous_headings":"","what":"Concatenates a list of inputs. — layer_concatenate","title":"Concatenates a list of inputs. — layer_concatenate","text":"takes input list tensors, shape except concatenation axis, returns single tensor concatenation inputs.","code":""},{"path":"https://keras.posit.co/reference/layer_concatenate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Concatenates a list of inputs. — layer_concatenate","text":"","code":"layer_concatenate(inputs, ..., axis = -1L)"},{"path":"https://keras.posit.co/reference/layer_concatenate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Concatenates a list of inputs. — layer_concatenate","text":"inputs layers combine ... Standard layer keyword arguments. axis Axis along concatenate.","code":""},{"path":"https://keras.posit.co/reference/layer_concatenate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Concatenates a list of inputs. — layer_concatenate","text":"","code":"A tensor, the concatenation of the inputs alongside axis `axis`."},{"path":"https://keras.posit.co/reference/layer_concatenate.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Concatenates a list of inputs. — layer_concatenate","text":"Usage Keras model:","code":"x <- op_arange(20) |> op_reshape(c(2, 2, 5)) y <- op_arange(20, 40) |> op_reshape(c(2, 2, 5)) layer_concatenate(x, y, axis = 2) ## tf.Tensor( ## [[[ 0.  1.  2.  3.  4.] ##   [ 5.  6.  7.  8.  9.] ##   [20. 21. 22. 23. 24.] ##   [25. 26. 27. 28. 29.]] ## ##  [[10. 11. 12. 13. 14.] ##   [15. 16. 17. 18. 19.] ##   [30. 31. 32. 33. 34.] ##   [35. 36. 37. 38. 39.]]], shape=(2, 4, 5), dtype=float32) x1 <- op_arange(10)     |> op_reshape(c(5, 2)) |> layer_dense(8) x2 <- op_arange(10, 20) |> op_reshape(c(5, 2)) |> layer_dense(8) y <- layer_concatenate(x1, x2)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_conv_1d.html","id":null,"dir":"Reference","previous_headings":"","what":"1D convolution layer (e.g. temporal convolution). — layer_conv_1d","title":"1D convolution layer (e.g. temporal convolution). — layer_conv_1d","text":"layer creates convolution kernel convolved layer input single spatial (temporal) dimension produce tensor outputs. use_bias TRUE, bias vector created added outputs. Finally, activation NULL, applied outputs well.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_1d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"1D convolution layer (e.g. temporal convolution). — layer_conv_1d","text":"","code":"layer_conv_1d(   object,   filters,   kernel_size,   strides = 1L,   padding = \"valid\",   data_format = NULL,   dilation_rate = 1L,   groups = 1L,   activation = NULL,   use_bias = TRUE,   kernel_initializer = \"glorot_uniform\",   bias_initializer = \"zeros\",   kernel_regularizer = NULL,   bias_regularizer = NULL,   activity_regularizer = NULL,   kernel_constraint = NULL,   bias_constraint = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_conv_1d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"1D convolution layer (e.g. temporal convolution). — layer_conv_1d","text":"object Object compose layer . tensor, array, sequential model. filters int, dimension output space (number filters convolution). kernel_size int list 1 integer, specifying size convolution window. strides int list 1 integer, specifying stride length convolution. strides > 1 incompatible dilation_rate > 1. padding string, \"valid\", \"\" \"causal\"(case-insensitive). \"valid\" means padding. \"\" results padding evenly left/right /input. padding=\"\" strides=1, output size input. \"causal\" results causal (dilated) convolutions, e.g. output[t] depend ontail(input, t+1). Useful modeling temporal data model violate temporal order. See WaveNet: Generative Model Raw Audio, section2.1. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, steps, features) \"channels_first\" corresponds inputs shape (batch, features, steps). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". dilation_rate int list 1 integers, specifying dilation rate use dilated convolution. groups positive int specifying number groups input split along channel axis. group convolved separately filters // groups filters. output concatenation groups results along channel axis. Input channels filters must divisible groups. activation Activation function. NULL, activation applied. use_bias bool, TRUE, bias added output. kernel_initializer Initializer convolution kernel. NULL, default initializer (\"glorot_uniform\") used. bias_initializer Initializer bias vector. NULL, default initializer (\"zeros\") used. kernel_regularizer Optional regularizer convolution kernel. bias_regularizer Optional regularizer bias vector. activity_regularizer Optional regularizer function output. kernel_constraint Optional projection function applied kernel updated Optimizer (e.g. used implement norm constraints value constraints layer weights). function must take input unprojected variable must return projected variable (must shape). Constraints safe use asynchronous distributed training. bias_constraint Optional projection function applied bias updated Optimizer. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_1d.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"1D convolution layer (e.g. temporal convolution). — layer_conv_1d","text":"3D tensor representing activation(conv1d(inputs, kernel) + bias).","code":""},{"path":"https://keras.posit.co/reference/layer_conv_1d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"1D convolution layer (e.g. temporal convolution). — layer_conv_1d","text":"data_format=\"channels_last\": 3D tensor shape: (batch_shape, steps, channels) data_format=\"channels_first\": 3D tensor shape: (batch_shape, channels, steps)","code":""},{"path":"https://keras.posit.co/reference/layer_conv_1d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"1D convolution layer (e.g. temporal convolution). — layer_conv_1d","text":"data_format=\"channels_last\": 3D tensor shape: (batch_shape, new_steps, filters) data_format=\"channels_first\": 3D tensor shape: (batch_shape, filters, new_steps)","code":""},{"path":"https://keras.posit.co/reference/layer_conv_1d.html","id":"raises","dir":"Reference","previous_headings":"","what":"Raises","title":"1D convolution layer (e.g. temporal convolution). — layer_conv_1d","text":"ValueError: strides > 1 dilation_rate > 1.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_1d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"1D convolution layer (e.g. temporal convolution). — layer_conv_1d","text":"","code":"# The inputs are 128-length vectors with 10 timesteps, and the # batch size is 4. x <- random_uniform(c(4, 10, 128)) y <- x |> layer_conv_1d(32, 3, activation='relu') shape(y) ## shape(4, 8, 32)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_conv_1d_transpose.html","id":null,"dir":"Reference","previous_headings":"","what":"1D transposed convolution layer. — layer_conv_1d_transpose","title":"1D transposed convolution layer. — layer_conv_1d_transpose","text":"need transposed convolutions generally arise desire use transformation going opposite direction normal convolution, .e., something shape output convolution something shape input maintaining connectivity pattern compatible said convolution.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_1d_transpose.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"1D transposed convolution layer. — layer_conv_1d_transpose","text":"","code":"layer_conv_1d_transpose(   object,   filters,   kernel_size,   strides = 1L,   padding = \"valid\",   data_format = NULL,   dilation_rate = 1L,   activation = NULL,   use_bias = TRUE,   kernel_initializer = \"glorot_uniform\",   bias_initializer = \"zeros\",   kernel_regularizer = NULL,   bias_regularizer = NULL,   activity_regularizer = NULL,   kernel_constraint = NULL,   bias_constraint = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_conv_1d_transpose.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"1D transposed convolution layer. — layer_conv_1d_transpose","text":"object Object compose layer . tensor, array, sequential model. filters int, dimension output space (number filters transpose convolution). kernel_size int list 1 integer, specifying size transposed convolution window. strides int list 1 integer, specifying stride length transposed convolution. strides > 1 incompatible dilation_rate > 1. padding string, either \"valid\" \"\" (case-insensitive). \"valid\" means padding. \"\" results padding evenly left/right /input output height/width dimension input. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, steps, features) \"channels_first\" corresponds inputs shape (batch, features, steps). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". dilation_rate int list 1 integers, specifying dilation rate use dilated transposed convolution. activation Activation function. NULL, activation applied. use_bias bool, TRUE, bias added output. kernel_initializer Initializer convolution kernel. NULL, default initializer (\"glorot_uniform\") used. bias_initializer Initializer bias vector. NULL, default initializer (\"zeros\") used. kernel_regularizer Optional regularizer convolution kernel. bias_regularizer Optional regularizer bias vector. activity_regularizer Optional regularizer function output. kernel_constraint Optional projection function applied kernel updated Optimizer (e.g. used implement norm constraints value constraints layer weights). function must take input unprojected variable must return projected variable (must shape). Constraints safe use asynchronous distributed training. bias_constraint Optional projection function applied bias updated Optimizer. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_1d_transpose.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"1D transposed convolution layer. — layer_conv_1d_transpose","text":"3D tensor representing activation(conv1d_transpose(inputs, kernel) + bias).","code":""},{"path":"https://keras.posit.co/reference/layer_conv_1d_transpose.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"1D transposed convolution layer. — layer_conv_1d_transpose","text":"data_format=\"channels_last\": 3D tensor shape: (batch_shape, steps, channels) data_format=\"channels_first\": 3D tensor shape: (batch_shape, channels, steps)","code":""},{"path":"https://keras.posit.co/reference/layer_conv_1d_transpose.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"1D transposed convolution layer. — layer_conv_1d_transpose","text":"data_format=\"channels_last\": 3D tensor shape: (batch_shape, new_steps, filters) data_format=\"channels_first\": 3D tensor shape: (batch_shape, filters, new_steps)","code":""},{"path":"https://keras.posit.co/reference/layer_conv_1d_transpose.html","id":"raises","dir":"Reference","previous_headings":"","what":"Raises","title":"1D transposed convolution layer. — layer_conv_1d_transpose","text":"ValueError: strides > 1 dilation_rate > 1.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_1d_transpose.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"1D transposed convolution layer. — layer_conv_1d_transpose","text":"guide convolution arithmetic deep learning Deconvolutional Networks","code":""},{"path":"https://keras.posit.co/reference/layer_conv_1d_transpose.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"1D transposed convolution layer. — layer_conv_1d_transpose","text":"","code":"x <- random_uniform(c(4, 10, 128)) y <- x |> layer_conv_1d_transpose(32, 3, 2, activation='relu') shape(y) ## shape(4, 21, 32)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_conv_2d.html","id":null,"dir":"Reference","previous_headings":"","what":"2D convolution layer. — layer_conv_2d","title":"2D convolution layer. — layer_conv_2d","text":"layer creates convolution kernel convolved layer input single spatial (temporal) dimension produce tensor outputs. use_bias TRUE, bias vector created added outputs. Finally, activation NULL, applied outputs well.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_2d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"2D convolution layer. — layer_conv_2d","text":"","code":"layer_conv_2d(   object,   filters,   kernel_size,   strides = list(1L, 1L),   padding = \"valid\",   data_format = NULL,   dilation_rate = list(1L, 1L),   groups = 1L,   activation = NULL,   use_bias = TRUE,   kernel_initializer = \"glorot_uniform\",   bias_initializer = \"zeros\",   kernel_regularizer = NULL,   bias_regularizer = NULL,   activity_regularizer = NULL,   kernel_constraint = NULL,   bias_constraint = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_conv_2d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"2D convolution layer. — layer_conv_2d","text":"object Object compose layer . tensor, array, sequential model. filters int, dimension output space (number filters convolution). kernel_size int list 2 integer, specifying size convolution window. strides int list 2 integer, specifying stride length convolution. strides > 1 incompatible dilation_rate > 1. padding string, either \"valid\" \"\" (case-insensitive). \"valid\" means padding. \"\" results padding evenly left/right /input. padding=\"\" strides=1, output size input. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch_size, channels, height, width) \"channels_first\" corresponds inputs shape (batch_size, channels, height, width). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". dilation_rate int list 2 integers, specifying dilation rate use dilated convolution. groups positive int specifying number groups input split along channel axis. group convolved separately filters // groups filters. output concatenation groups results along channel axis. Input channels filters must divisible groups. activation Activation function. NULL, activation applied. use_bias bool, TRUE, bias added output. kernel_initializer Initializer convolution kernel. NULL, default initializer (\"glorot_uniform\") used. bias_initializer Initializer bias vector. NULL, default initializer (\"zeros\") used. kernel_regularizer Optional regularizer convolution kernel. bias_regularizer Optional regularizer bias vector. activity_regularizer Optional regularizer function output. kernel_constraint Optional projection function applied kernel updated Optimizer (e.g. used implement norm constraints value constraints layer weights). function must take input unprojected variable must return projected variable (must shape). Constraints safe use asynchronous distributed training. bias_constraint Optional projection function applied bias updated Optimizer. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_2d.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"2D convolution layer. — layer_conv_2d","text":"4D tensor representing activation(conv2d(inputs, kernel) + bias).","code":""},{"path":"https://keras.posit.co/reference/layer_conv_2d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"2D convolution layer. — layer_conv_2d","text":"data_format=\"channels_last\": 4D tensor shape: (batch_size, height, width, channels) data_format=\"channels_first\": 4D tensor shape: (batch_size, channels, height, width)","code":""},{"path":"https://keras.posit.co/reference/layer_conv_2d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"2D convolution layer. — layer_conv_2d","text":"data_format=\"channels_last\": 4D tensor shape: (batch_size, new_height, new_width, filters) data_format=\"channels_first\": 4D tensor shape: (batch_size, filters, new_height, new_width)","code":""},{"path":"https://keras.posit.co/reference/layer_conv_2d.html","id":"raises","dir":"Reference","previous_headings":"","what":"Raises","title":"2D convolution layer. — layer_conv_2d","text":"ValueError: strides > 1 dilation_rate > 1.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_2d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"2D convolution layer. — layer_conv_2d","text":"","code":"x <- random_uniform(c(4, 10, 10, 128)) y <- x |> layer_conv_2d(32, 3, activation='relu') shape(y) ## shape(4, 8, 8, 32)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_conv_2d_transpose.html","id":null,"dir":"Reference","previous_headings":"","what":"2D transposed convolution layer. — layer_conv_2d_transpose","title":"2D transposed convolution layer. — layer_conv_2d_transpose","text":"need transposed convolutions generally arise desire use transformation going opposite direction normal convolution, .e., something shape output convolution something shape input maintaining connectivity pattern compatible said convolution.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_2d_transpose.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"2D transposed convolution layer. — layer_conv_2d_transpose","text":"","code":"layer_conv_2d_transpose(   object,   filters,   kernel_size,   strides = list(1L, 1L),   padding = \"valid\",   data_format = NULL,   dilation_rate = list(1L, 1L),   activation = NULL,   use_bias = TRUE,   kernel_initializer = \"glorot_uniform\",   bias_initializer = \"zeros\",   kernel_regularizer = NULL,   bias_regularizer = NULL,   activity_regularizer = NULL,   kernel_constraint = NULL,   bias_constraint = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_conv_2d_transpose.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"2D transposed convolution layer. — layer_conv_2d_transpose","text":"object Object compose layer . tensor, array, sequential model. filters int, dimension output space (number filters transposed convolution). kernel_size int list 1 integer, specifying size transposed convolution window. strides int list 1 integer, specifying stride length transposed convolution. strides > 1 incompatible dilation_rate > 1. padding string, either \"valid\" \"\" (case-insensitive). \"valid\" means padding. \"\" results padding evenly left/right /input. padding=\"\" strides=1, output size input. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch_size, channels, height, width) \"channels_first\" corresponds inputs shape (batch_size, channels, height, width). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". dilation_rate int list 1 integers, specifying dilation rate use dilated transposed convolution. activation Activation function. NULL, activation applied. use_bias bool, TRUE, bias added output. kernel_initializer Initializer convolution kernel. NULL, default initializer (\"glorot_uniform\") used. bias_initializer Initializer bias vector. NULL, default initializer (\"zeros\") used. kernel_regularizer Optional regularizer convolution kernel. bias_regularizer Optional regularizer bias vector. activity_regularizer Optional regularizer function output. kernel_constraint Optional projection function applied kernel updated Optimizer (e.g. used implement norm constraints value constraints layer weights). function must take input unprojected variable must return projected variable (must shape). Constraints safe use asynchronous distributed training. bias_constraint Optional projection function applied bias updated Optimizer. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_2d_transpose.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"2D transposed convolution layer. — layer_conv_2d_transpose","text":"4D tensor representing activation(conv2d_transpose(inputs, kernel) + bias).","code":""},{"path":"https://keras.posit.co/reference/layer_conv_2d_transpose.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"2D transposed convolution layer. — layer_conv_2d_transpose","text":"data_format=\"channels_last\": 4D tensor shape: (batch_size, height, width, channels) data_format=\"channels_first\": 4D tensor shape: (batch_size, channels, height, width)","code":""},{"path":"https://keras.posit.co/reference/layer_conv_2d_transpose.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"2D transposed convolution layer. — layer_conv_2d_transpose","text":"data_format=\"channels_last\": 4D tensor shape: (batch_size, new_height, new_width, filters) data_format=\"channels_first\": 4D tensor shape: (batch_size, filters, new_height, new_width)","code":""},{"path":"https://keras.posit.co/reference/layer_conv_2d_transpose.html","id":"raises","dir":"Reference","previous_headings":"","what":"Raises","title":"2D transposed convolution layer. — layer_conv_2d_transpose","text":"ValueError: strides > 1 dilation_rate > 1.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_2d_transpose.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"2D transposed convolution layer. — layer_conv_2d_transpose","text":"guide convolution arithmetic deep learning Deconvolutional Networks","code":""},{"path":"https://keras.posit.co/reference/layer_conv_2d_transpose.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"2D transposed convolution layer. — layer_conv_2d_transpose","text":"","code":"x <- random_uniform(c(4, 10, 8, 128)) y <- x |> layer_conv_2d_transpose(32, 2, 2, activation='relu') shape(y) ## shape(4, 20, 16, 32) # (4, 20, 16, 32)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_conv_3d.html","id":null,"dir":"Reference","previous_headings":"","what":"3D convolution layer. — layer_conv_3d","title":"3D convolution layer. — layer_conv_3d","text":"layer creates convolution kernel convolved layer input single spatial (temporal) dimension produce tensor outputs. use_bias TRUE, bias vector created added outputs. Finally, activation NULL, applied outputs well.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_3d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"3D convolution layer. — layer_conv_3d","text":"","code":"layer_conv_3d(   object,   filters,   kernel_size,   strides = list(1L, 1L, 1L),   padding = \"valid\",   data_format = NULL,   dilation_rate = list(1L, 1L, 1L),   groups = 1L,   activation = NULL,   use_bias = TRUE,   kernel_initializer = \"glorot_uniform\",   bias_initializer = \"zeros\",   kernel_regularizer = NULL,   bias_regularizer = NULL,   activity_regularizer = NULL,   kernel_constraint = NULL,   bias_constraint = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_conv_3d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"3D convolution layer. — layer_conv_3d","text":"object Object compose layer . tensor, array, sequential model. filters int, dimension output space (number filters convolution). kernel_size int list 3 integer, specifying size convolution window. strides int list 3 integer, specifying stride length convolution. strides > 1 incompatible dilation_rate > 1. padding string, either \"valid\" \"\" (case-insensitive). \"valid\" means padding. \"\" results padding evenly left/right /input. padding=\"\" strides=1, output size input. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) \"channels_first\" corresponds inputs shape (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". dilation_rate int list 3 integers, specifying dilation rate use dilated convolution. groups positive int specifying number groups input split along channel axis. group convolved separately filters %/% groups filters. output concatenation groups results along channel axis. Input channels filters must divisible groups. activation Activation function. NULL, activation applied. use_bias bool, TRUE, bias added output. kernel_initializer Initializer convolution kernel. NULL, default initializer (\"glorot_uniform\") used. bias_initializer Initializer bias vector. NULL, default initializer (\"zeros\") used. kernel_regularizer Optional regularizer convolution kernel. bias_regularizer Optional regularizer bias vector. activity_regularizer Optional regularizer function output. kernel_constraint Optional projection function applied kernel updated Optimizer (e.g. used implement norm constraints value constraints layer weights). function must take input unprojected variable must return projected variable (must shape). Constraints safe use asynchronous distributed training. bias_constraint Optional projection function applied bias updated Optimizer. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_3d.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"3D convolution layer. — layer_conv_3d","text":"5D tensor representing activation(conv3d(inputs, kernel) + bias).","code":""},{"path":"https://keras.posit.co/reference/layer_conv_3d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"3D convolution layer. — layer_conv_3d","text":"data_format=\"channels_last\": 5D tensor shape: (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) data_format=\"channels_first\": 5D tensor shape: (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)","code":""},{"path":"https://keras.posit.co/reference/layer_conv_3d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"3D convolution layer. — layer_conv_3d","text":"data_format=\"channels_last\": 5D tensor shape: (batch_size, new_spatial_dim1, new_spatial_dim2, new_spatial_dim3, filters) data_format=\"channels_first\": 5D tensor shape: (batch_size, filters, new_spatial_dim1, new_spatial_dim2, new_spatial_dim3)","code":""},{"path":"https://keras.posit.co/reference/layer_conv_3d.html","id":"raises","dir":"Reference","previous_headings":"","what":"Raises","title":"3D convolution layer. — layer_conv_3d","text":"ValueError: strides > 1 dilation_rate > 1.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_3d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"3D convolution layer. — layer_conv_3d","text":"","code":"x <- random_uniform(c(4, 10, 10, 10, 128)) y <- x |> layer_conv_3d(32, 3, activation = 'relu') shape(y) ## shape(4, 8, 8, 8, 32)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_conv_3d_transpose.html","id":null,"dir":"Reference","previous_headings":"","what":"3D transposed convolution layer. — layer_conv_3d_transpose","title":"3D transposed convolution layer. — layer_conv_3d_transpose","text":"need transposed convolutions generally arise desire use transformation going opposite direction normal convolution, .e., something shape output convolution something shape input maintaining connectivity pattern compatible said convolution.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_3d_transpose.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"3D transposed convolution layer. — layer_conv_3d_transpose","text":"","code":"layer_conv_3d_transpose(   object,   filters,   kernel_size,   strides = list(1L, 1L, 1L),   padding = \"valid\",   data_format = NULL,   dilation_rate = list(1L, 1L, 1L),   activation = NULL,   use_bias = TRUE,   kernel_initializer = \"glorot_uniform\",   bias_initializer = \"zeros\",   kernel_regularizer = NULL,   bias_regularizer = NULL,   activity_regularizer = NULL,   kernel_constraint = NULL,   bias_constraint = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_conv_3d_transpose.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"3D transposed convolution layer. — layer_conv_3d_transpose","text":"object Object compose layer . tensor, array, sequential model. filters int, dimension output space (number filters transposed convolution). kernel_size int list 1 integer, specifying size transposed convolution window. strides int list 1 integer, specifying stride length transposed convolution. strides > 1 incompatible dilation_rate > 1. padding string, either \"valid\" \"\" (case-insensitive). \"valid\" means padding. \"\" results padding evenly left/right /input. padding=\"\" strides=1, output size input. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) \"channels_first\" corresponds inputs shape (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". dilation_rate int list 1 integers, specifying dilation rate use dilated transposed convolution. activation Activation function. NULL, activation applied. use_bias bool, TRUE, bias added output. kernel_initializer Initializer convolution kernel. NULL, default initializer (\"glorot_uniform\") used. bias_initializer Initializer bias vector. NULL, default initializer (\"zeros\") used. kernel_regularizer Optional regularizer convolution kernel. bias_regularizer Optional regularizer bias vector. activity_regularizer Optional regularizer function output. kernel_constraint Optional projection function applied kernel updated Optimizer (e.g. used implement norm constraints value constraints layer weights). function must take input unprojected variable must return projected variable (must shape). Constraints safe use asynchronous distributed training. bias_constraint Optional projection function applied bias updated Optimizer. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_3d_transpose.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"3D transposed convolution layer. — layer_conv_3d_transpose","text":"5D tensor representing activation(conv3d(inputs, kernel) + bias).","code":""},{"path":"https://keras.posit.co/reference/layer_conv_3d_transpose.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"3D transposed convolution layer. — layer_conv_3d_transpose","text":"data_format=\"channels_last\": 5D tensor shape: (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) data_format=\"channels_first\": 5D tensor shape: (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)","code":""},{"path":"https://keras.posit.co/reference/layer_conv_3d_transpose.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"3D transposed convolution layer. — layer_conv_3d_transpose","text":"data_format=\"channels_last\": 5D tensor shape: (batch_size, new_spatial_dim1, new_spatial_dim2, new_spatial_dim3, filters) data_format=\"channels_first\": 5D tensor shape: (batch_size, filters, new_spatial_dim1, new_spatial_dim2, new_spatial_dim3)","code":""},{"path":"https://keras.posit.co/reference/layer_conv_3d_transpose.html","id":"raises","dir":"Reference","previous_headings":"","what":"Raises","title":"3D transposed convolution layer. — layer_conv_3d_transpose","text":"ValueError: strides > 1 dilation_rate > 1.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_3d_transpose.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"3D transposed convolution layer. — layer_conv_3d_transpose","text":"guide convolution arithmetic deep learning Deconvolutional Networks","code":""},{"path":"https://keras.posit.co/reference/layer_conv_3d_transpose.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"3D transposed convolution layer. — layer_conv_3d_transpose","text":"","code":"x <- random_uniform(c(4, 10, 8, 12, 128)) y <- x |> layer_conv_3d_transpose(32, 2, 2, activation = 'relu') shape(y) ## shape(4, 20, 16, 24, 32)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_conv_lstm_1d.html","id":null,"dir":"Reference","previous_headings":"","what":"1D Convolutional LSTM. — layer_conv_lstm_1d","title":"1D Convolutional LSTM. — layer_conv_lstm_1d","text":"Similar LSTM layer, input transformations recurrent transformations convolutional.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_lstm_1d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"1D Convolutional LSTM. — layer_conv_lstm_1d","text":"","code":"layer_conv_lstm_1d(   object,   filters,   kernel_size,   strides = 1L,   padding = \"valid\",   data_format = NULL,   dilation_rate = 1L,   activation = \"tanh\",   recurrent_activation = \"sigmoid\",   use_bias = TRUE,   kernel_initializer = \"glorot_uniform\",   recurrent_initializer = \"orthogonal\",   bias_initializer = \"zeros\",   unit_forget_bias = TRUE,   kernel_regularizer = NULL,   recurrent_regularizer = NULL,   bias_regularizer = NULL,   activity_regularizer = NULL,   kernel_constraint = NULL,   recurrent_constraint = NULL,   bias_constraint = NULL,   dropout = 0,   recurrent_dropout = 0,   seed = NULL,   return_sequences = FALSE,   return_state = FALSE,   go_backwards = FALSE,   stateful = FALSE,   ...,   unroll = NULL )"},{"path":"https://keras.posit.co/reference/layer_conv_lstm_1d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"1D Convolutional LSTM. — layer_conv_lstm_1d","text":"object Object compose layer . tensor, array, sequential model. filters int, dimension output space (number filters convolution). kernel_size int tuple/list 1 integer, specifying size convolution window. strides int tuple/list 1 integer, specifying stride length convolution. strides > 1 incompatible dilation_rate > 1. padding string, \"valid\" \"\" (case-insensitive). \"valid\" means padding. \"\" results padding evenly left/right /input output height/width dimension input. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, steps, features) \"channels_first\" corresponds inputs shape (batch, features, steps). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". dilation_rate int tuple/list 1 integers, specifying dilation rate use dilated convolution. activation Activation function use. default hyperbolic tangent activation function applied (tanh(x)). recurrent_activation Activation function use recurrent step. use_bias Boolean, whether layer uses bias vector. kernel_initializer Initializer kernel weights matrix, used linear transformation inputs. recurrent_initializer Initializer recurrent_kernel weights matrix, used linear transformation recurrent state. bias_initializer Initializer bias vector. unit_forget_bias Boolean. TRUE, add 1 bias forget gate initialization. Use combination bias_initializer=\"zeros\". recommended Jozefowicz et al., 2015 kernel_regularizer Regularizer function applied kernel weights matrix. recurrent_regularizer Regularizer function applied recurrent_kernel weights matrix. bias_regularizer Regularizer function applied bias vector. activity_regularizer Regularizer function applied . kernel_constraint Constraint function applied kernel weights matrix. recurrent_constraint Constraint function applied recurrent_kernel weights matrix. bias_constraint Constraint function applied bias vector. dropout Float 0 1. Fraction units drop linear transformation inputs. recurrent_dropout Float 0 1. Fraction units drop linear transformation recurrent state. seed Random seed dropout. return_sequences Boolean. Whether return last output output sequence, full sequence. Default: FALSE. return_state Boolean. Whether return last state addition output. Default: FALSE. go_backwards Boolean (default: FALSE). TRUE, process input sequence backwards return reversed sequence. stateful Boolean (default FALSE). TRUE, last state sample index batch used initial state sample index following batch. ... forward/backward compatability. unroll Boolean (default: FALSE). TRUE, network unrolled, else symbolic loop used. Unrolling can speed-RNN, although tends memory-intensive. Unrolling suitable short sequences.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_lstm_1d.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"1D Convolutional LSTM. — layer_conv_lstm_1d","text":"inputs: 4D tensor. initial_state: List initial state tensors passed first call cell. mask: Binary tensor shape (samples, timesteps) indicating whether given timestep masked. training: Python boolean indicating whether layer behave training mode inference mode. relevant dropout recurrent_dropout set.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_lstm_1d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"1D Convolutional LSTM. — layer_conv_lstm_1d","text":"data_format=\"channels_first\": 4D tensor shape: (samples, time, channels, rows) data_format=\"channels_last\": 4D tensor shape: (samples, time, rows, channels)","code":""},{"path":"https://keras.posit.co/reference/layer_conv_lstm_1d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"1D Convolutional LSTM. — layer_conv_lstm_1d","text":"return_state: list tensors. first tensor output. remaining tensors last states, 3D tensor shape: (samples, filters, new_rows) data_format='channels_first' shape: (samples, new_rows, filters) data_format='channels_last'. rows values might changed due padding. return_sequences: 4D tensor shape: (samples, timesteps, filters, new_rows) data_format='channels_first' shape: (samples, timesteps, new_rows, filters) data_format='channels_last'. Else, 3D tensor shape: (samples, filters, new_rows) data_format='channels_first' shape: (samples, new_rows, filters) data_format='channels_last'.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_lstm_1d.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"1D Convolutional LSTM. — layer_conv_lstm_1d","text":"Shi et al., 2015 (current implementation include feedback loop cells output).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_conv_lstm_2d.html","id":null,"dir":"Reference","previous_headings":"","what":"2D Convolutional LSTM. — layer_conv_lstm_2d","title":"2D Convolutional LSTM. — layer_conv_lstm_2d","text":"Similar LSTM layer, input transformations recurrent transformations convolutional.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_lstm_2d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"2D Convolutional LSTM. — layer_conv_lstm_2d","text":"","code":"layer_conv_lstm_2d(   object,   filters,   kernel_size,   strides = 1L,   padding = \"valid\",   data_format = NULL,   dilation_rate = 1L,   activation = \"tanh\",   recurrent_activation = \"sigmoid\",   use_bias = TRUE,   kernel_initializer = \"glorot_uniform\",   recurrent_initializer = \"orthogonal\",   bias_initializer = \"zeros\",   unit_forget_bias = TRUE,   kernel_regularizer = NULL,   recurrent_regularizer = NULL,   bias_regularizer = NULL,   activity_regularizer = NULL,   kernel_constraint = NULL,   recurrent_constraint = NULL,   bias_constraint = NULL,   dropout = 0,   recurrent_dropout = 0,   seed = NULL,   return_sequences = FALSE,   return_state = FALSE,   go_backwards = FALSE,   stateful = FALSE,   ...,   unroll = NULL )"},{"path":"https://keras.posit.co/reference/layer_conv_lstm_2d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"2D Convolutional LSTM. — layer_conv_lstm_2d","text":"object Object compose layer . tensor, array, sequential model. filters int, dimension output space (number filters convolution). kernel_size int tuple/list 2 integers, specifying size convolution window. strides int tuple/list 2 integers, specifying stride length convolution. strides > 1 incompatible dilation_rate > 1. padding string, \"valid\" \"\" (case-insensitive). \"valid\" means padding. \"\" results padding evenly left/right /input output height/width dimension input. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, steps, features) \"channels_first\" corresponds inputs shape (batch, features, steps). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". dilation_rate int tuple/list 2 integers, specifying dilation rate use dilated convolution. activation Activation function use. default hyperbolic tangent activation function applied (tanh(x)). recurrent_activation Activation function use recurrent step. use_bias Boolean, whether layer uses bias vector. kernel_initializer Initializer kernel weights matrix, used linear transformation inputs. recurrent_initializer Initializer recurrent_kernel weights matrix, used linear transformation recurrent state. bias_initializer Initializer bias vector. unit_forget_bias Boolean. TRUE, add 1 bias forget gate initialization. Use combination bias_initializer=\"zeros\". recommended Jozefowicz et al., 2015 kernel_regularizer Regularizer function applied kernel weights matrix. recurrent_regularizer Regularizer function applied recurrent_kernel weights matrix. bias_regularizer Regularizer function applied bias vector. activity_regularizer Regularizer function applied . kernel_constraint Constraint function applied kernel weights matrix. recurrent_constraint Constraint function applied recurrent_kernel weights matrix. bias_constraint Constraint function applied bias vector. dropout Float 0 1. Fraction units drop linear transformation inputs. recurrent_dropout Float 0 1. Fraction units drop linear transformation recurrent state. seed Random seed dropout. return_sequences Boolean. Whether return last output output sequence, full sequence. Default: FALSE. return_state Boolean. Whether return last state addition output. Default: FALSE. go_backwards Boolean (default: FALSE). TRUE, process input sequence backwards return reversed sequence. stateful Boolean (default FALSE). TRUE, last state sample index batch used initial state sample index following batch. ... forward/backward compatability. unroll Boolean (default: FALSE). TRUE, network unrolled, else symbolic loop used. Unrolling can speed-RNN, although tends memory-intensive. Unrolling suitable short sequences.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_lstm_2d.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"2D Convolutional LSTM. — layer_conv_lstm_2d","text":"inputs: 5D tensor. mask: Binary tensor shape (samples, timesteps) indicating whether given timestep masked. training: Python boolean indicating whether layer behave training mode inference mode. relevant dropout recurrent_dropout set. initial_state: List initial state tensors passed first call cell.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_lstm_2d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"2D Convolutional LSTM. — layer_conv_lstm_2d","text":"data_format='channels_first': 5D tensor shape: (samples, time, channels, rows, cols) data_format='channels_last': 5D tensor shape: (samples, time, rows, cols, channels)","code":""},{"path":"https://keras.posit.co/reference/layer_conv_lstm_2d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"2D Convolutional LSTM. — layer_conv_lstm_2d","text":"return_state: list tensors. first tensor output. remaining tensors last states, 4D tensor shape: (samples, filters, new_rows, new_cols) data_format='channels_first' shape: (samples, new_rows, new_cols, filters) data_format='channels_last'. rows cols values might changed due padding. return_sequences: 5D tensor shape: (samples, timesteps, filters, new_rows, new_cols) data_format='channels_first' shape: (samples, timesteps, new_rows, new_cols, filters) data_format='channels_last'. Else, 4D tensor shape: (samples, filters, new_rows, new_cols) data_format='channels_first' shape: (samples, new_rows, new_cols, filters) data_format='channels_last'.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_lstm_2d.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"2D Convolutional LSTM. — layer_conv_lstm_2d","text":"Shi et al., 2015 (current implementation include feedback loop cells output).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_conv_lstm_3d.html","id":null,"dir":"Reference","previous_headings":"","what":"3D Convolutional LSTM. — layer_conv_lstm_3d","title":"3D Convolutional LSTM. — layer_conv_lstm_3d","text":"Similar LSTM layer, input transformations recurrent transformations convolutional.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_lstm_3d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"3D Convolutional LSTM. — layer_conv_lstm_3d","text":"","code":"layer_conv_lstm_3d(   object,   filters,   kernel_size,   strides = 1L,   padding = \"valid\",   data_format = NULL,   dilation_rate = 1L,   activation = \"tanh\",   recurrent_activation = \"sigmoid\",   use_bias = TRUE,   kernel_initializer = \"glorot_uniform\",   recurrent_initializer = \"orthogonal\",   bias_initializer = \"zeros\",   unit_forget_bias = TRUE,   kernel_regularizer = NULL,   recurrent_regularizer = NULL,   bias_regularizer = NULL,   activity_regularizer = NULL,   kernel_constraint = NULL,   recurrent_constraint = NULL,   bias_constraint = NULL,   dropout = 0,   recurrent_dropout = 0,   seed = NULL,   return_sequences = FALSE,   return_state = FALSE,   go_backwards = FALSE,   stateful = FALSE,   ...,   unroll = NULL )"},{"path":"https://keras.posit.co/reference/layer_conv_lstm_3d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"3D Convolutional LSTM. — layer_conv_lstm_3d","text":"object Object compose layer . tensor, array, sequential model. filters int, dimension output space (number filters convolution). kernel_size int tuple/list 3 integers, specifying size convolution window. strides int tuple/list 3 integers, specifying stride length convolution. strides > 1 incompatible dilation_rate > 1. padding string, \"valid\" \"\" (case-insensitive). \"valid\" means padding. \"\" results padding evenly left/right /input output height/width dimension input. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, steps, features) \"channels_first\" corresponds inputs shape (batch, features, steps). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". dilation_rate int tuple/list 3 integers, specifying dilation rate use dilated convolution. activation Activation function use. default hyperbolic tangent activation function applied (tanh(x)). recurrent_activation Activation function use recurrent step. use_bias Boolean, whether layer uses bias vector. kernel_initializer Initializer kernel weights matrix, used linear transformation inputs. recurrent_initializer Initializer recurrent_kernel weights matrix, used linear transformation recurrent state. bias_initializer Initializer bias vector. unit_forget_bias Boolean. TRUE, add 1 bias forget gate initialization. Use combination bias_initializer=\"zeros\". recommended Jozefowicz et al., 2015 kernel_regularizer Regularizer function applied kernel weights matrix. recurrent_regularizer Regularizer function applied recurrent_kernel weights matrix. bias_regularizer Regularizer function applied bias vector. activity_regularizer Regularizer function applied . kernel_constraint Constraint function applied kernel weights matrix. recurrent_constraint Constraint function applied recurrent_kernel weights matrix. bias_constraint Constraint function applied bias vector. dropout Float 0 1. Fraction units drop linear transformation inputs. recurrent_dropout Float 0 1. Fraction units drop linear transformation recurrent state. seed Random seed dropout. return_sequences Boolean. Whether return last output output sequence, full sequence. Default: FALSE. return_state Boolean. Whether return last state addition output. Default: FALSE. go_backwards Boolean (default: FALSE). TRUE, process input sequence backwards return reversed sequence. stateful Boolean (default FALSE). TRUE, last state sample index batch used initial state sample index following batch. ... forward/backward compatability. unroll Boolean (default: FALSE). TRUE, network unrolled, else symbolic loop used. Unrolling can speed-RNN, although tends memory-intensive. Unrolling suitable short sequences.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_lstm_3d.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"3D Convolutional LSTM. — layer_conv_lstm_3d","text":"inputs: 6D tensor. mask: Binary tensor shape (samples, timesteps) indicating whether given timestep masked. training: Python boolean indicating whether layer behave training mode inference mode. relevant dropout recurrent_dropout set. initial_state: List initial state tensors passed first call cell.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_lstm_3d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"3D Convolutional LSTM. — layer_conv_lstm_3d","text":"data_format='channels_first': 5D tensor shape: (samples, time, channels, *spatial_dims) data_format='channels_last': 5D tensor shape: (samples, time, *spatial_dims, channels)","code":""},{"path":"https://keras.posit.co/reference/layer_conv_lstm_3d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"3D Convolutional LSTM. — layer_conv_lstm_3d","text":"return_state: list tensors. first tensor output. remaining tensors last states, 4D tensor shape: (samples, filters, *spatial_dims) data_format='channels_first' shape: (samples, *spatial_dims, filters) data_format='channels_last'. return_sequences: 5D tensor shape: (samples, timesteps, filters, *spatial_dims) data_format='channels_first' shape: (samples, timesteps, *spatial_dims, filters) data_format='channels_last'. Else, 4D tensor shape: (samples, filters, *spatial_dims) data_format='channels_first' shape: (samples, *spatial_dims, filters) data_format='channels_last'.","code":""},{"path":"https://keras.posit.co/reference/layer_conv_lstm_3d.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"3D Convolutional LSTM. — layer_conv_lstm_3d","text":"Shi et al., 2015 (current implementation include feedback loop cells output).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_cropping_1d.html","id":null,"dir":"Reference","previous_headings":"","what":"Cropping layer for 1D input (e.g. temporal sequence). — layer_cropping_1d","title":"Cropping layer for 1D input (e.g. temporal sequence). — layer_cropping_1d","text":"crops along time dimension (axis 2).","code":""},{"path":"https://keras.posit.co/reference/layer_cropping_1d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cropping layer for 1D input (e.g. temporal sequence). — layer_cropping_1d","text":"","code":"layer_cropping_1d(object, cropping = list(1L, 1L), ...)"},{"path":"https://keras.posit.co/reference/layer_cropping_1d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cropping layer for 1D input (e.g. temporal sequence). — layer_cropping_1d","text":"object Object compose layer . tensor, array, sequential model. cropping Int, list int (length 2). int: many units trimmed beginning end cropping dimension (axis 1). list 2 ints: many units trimmed beginning end cropping dimension ((left_crop, right_crop)). ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_cropping_1d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cropping layer for 1D input (e.g. temporal sequence). — layer_cropping_1d","text":"","code":"input_shape <- c(2, 3, 2) x <- op_arange(prod(input_shape)) |> op_reshape(input_shape) x ## tf.Tensor( ## [[[ 0.  1.] ##   [ 2.  3.] ##   [ 4.  5.]] ## ##  [[ 6.  7.] ##   [ 8.  9.] ##   [10. 11.]]], shape=(2, 3, 2), dtype=float64) y <- x |> layer_cropping_1d(cropping = 1) y ## tf.Tensor( ## [[[2. 3.]] ## ##  [[8. 9.]]], shape=(2, 1, 2), dtype=float32)"},{"path":"https://keras.posit.co/reference/layer_cropping_1d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Cropping layer for 1D input (e.g. temporal sequence). — layer_cropping_1d","text":"3D tensor shape (batch_size, axis_to_crop, features)","code":""},{"path":"https://keras.posit.co/reference/layer_cropping_1d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Cropping layer for 1D input (e.g. temporal sequence). — layer_cropping_1d","text":"3D tensor shape (batch_size, cropped_axis, features)","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_cropping_2d.html","id":null,"dir":"Reference","previous_headings":"","what":"Cropping layer for 2D input (e.g. picture). — layer_cropping_2d","title":"Cropping layer for 2D input (e.g. picture). — layer_cropping_2d","text":"crops along spatial dimensions, .e. height width.","code":""},{"path":"https://keras.posit.co/reference/layer_cropping_2d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cropping layer for 2D input (e.g. picture). — layer_cropping_2d","text":"","code":"layer_cropping_2d(   object,   cropping = list(list(0L, 0L), list(0L, 0L)),   data_format = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_cropping_2d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cropping layer for 2D input (e.g. picture). — layer_cropping_2d","text":"object Object compose layer . tensor, array, sequential model. cropping Int, list 2 ints, list 2 lists 2 ints. int: symmetric cropping applied height width. list 2 ints: interpreted two different symmetric cropping values height width: (symmetric_height_crop, symmetric_width_crop). list 2 lists 2 ints: interpreted ((top_crop, bottom_crop), (left_crop, right_crop)). data_format string, one \"channels_last\" (default) \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch_size, height, width, channels) \"channels_first\" corresponds inputs shape (batch_size, channels, height, width). unspecified, uses image_data_format value found Keras config file ~/.keras/keras.json (exists). Defaults \"channels_last\". ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_cropping_2d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cropping layer for 2D input (e.g. picture). — layer_cropping_2d","text":"","code":"input_shape <- c(2, 28, 28, 3) x <- op_arange(prod(input_shape), dtype ='int32') |> op_reshape(input_shape) y <- x |> layer_cropping_2d(cropping=list(c(2, 2), c(4, 4))) shape(y) ## shape(2, 24, 20, 3)"},{"path":"https://keras.posit.co/reference/layer_cropping_2d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Cropping layer for 2D input (e.g. picture). — layer_cropping_2d","text":"4D tensor shape: data_format \"channels_last\": (batch_size, height, width, channels) data_format \"channels_first\": (batch_size, channels, height, width)","code":""},{"path":"https://keras.posit.co/reference/layer_cropping_2d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Cropping layer for 2D input (e.g. picture). — layer_cropping_2d","text":"4D tensor shape: data_format \"channels_last\": (batch_size, cropped_height, cropped_width, channels) data_format \"channels_first\": (batch_size, channels, cropped_height, cropped_width)","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_cropping_3d.html","id":null,"dir":"Reference","previous_headings":"","what":"Cropping layer for 3D data (e.g. spatial or spatio-temporal). — layer_cropping_3d","title":"Cropping layer for 3D data (e.g. spatial or spatio-temporal). — layer_cropping_3d","text":"Cropping layer 3D data (e.g. spatial spatio-temporal).","code":""},{"path":"https://keras.posit.co/reference/layer_cropping_3d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cropping layer for 3D data (e.g. spatial or spatio-temporal). — layer_cropping_3d","text":"","code":"layer_cropping_3d(   object,   cropping = list(list(1L, 1L), list(1L, 1L), list(1L, 1L)),   data_format = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_cropping_3d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cropping layer for 3D data (e.g. spatial or spatio-temporal). — layer_cropping_3d","text":"object Object compose layer . tensor, array, sequential model. cropping Int, list 3 ints, list 3 lists 2 ints. int: symmetric cropping applied depth, height, width. list 3 ints: interpreted three different symmetric cropping values depth, height, width: (symmetric_dim1_crop, symmetric_dim2_crop, symmetric_dim3_crop). list 3 lists 2 ints: interpreted ((left_dim1_crop, right_dim1_crop), (left_dim2_crop, right_dim2_crop), (left_dim3_crop, right_dim3_crop)). data_format string, one \"channels_last\" (default) \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) \"channels_first\" corresponds inputs shape (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3). unspecified, uses image_data_format value found Keras config file ~/.keras/keras.json (exists). Defaults \"channels_last\". ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_cropping_3d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cropping layer for 3D data (e.g. spatial or spatio-temporal). — layer_cropping_3d","text":"","code":"input_shape <- c(2, 28, 28, 10, 3) x <- input_shape %>% { op_reshape(seq(prod(.)), .) } y <- x |> layer_cropping_3d(cropping = c(2, 4, 2)) shape(y) ## shape(2, 24, 20, 6, 3)"},{"path":"https://keras.posit.co/reference/layer_cropping_3d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Cropping layer for 3D data (e.g. spatial or spatio-temporal). — layer_cropping_3d","text":"5D tensor shape: data_format \"channels_last\": (batch_size, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop, channels) data_format \"channels_first\": (batch_size, channels, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop)","code":""},{"path":"https://keras.posit.co/reference/layer_cropping_3d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Cropping layer for 3D data (e.g. spatial or spatio-temporal). — layer_cropping_3d","text":"5D tensor shape: data_format \"channels_last\": (batch_size, first_cropped_axis, second_cropped_axis, third_cropped_axis, channels) data_format \"channels_first\": (batch_size, channels, first_cropped_axis, second_cropped_axis, third_cropped_axis)","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_dense.html","id":null,"dir":"Reference","previous_headings":"","what":"Just your regular densely-connected NN layer. — layer_dense","title":"Just your regular densely-connected NN layer. — layer_dense","text":"Dense implements operation: output = activation(dot(input, kernel) + bias) activation element-wise activation function passed activation argument, kernel weights matrix created layer, bias bias vector created layer (applicable use_bias TRUE).","code":""},{"path":"https://keras.posit.co/reference/layer_dense.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Just your regular densely-connected NN layer. — layer_dense","text":"","code":"layer_dense(   object,   units,   activation = NULL,   use_bias = TRUE,   kernel_initializer = \"glorot_uniform\",   bias_initializer = \"zeros\",   kernel_regularizer = NULL,   bias_regularizer = NULL,   activity_regularizer = NULL,   kernel_constraint = NULL,   bias_constraint = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_dense.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Just your regular densely-connected NN layer. — layer_dense","text":"object Object compose layer . tensor, array, sequential model. units Positive integer, dimensionality output space. activation Activation function use. specify anything, activation applied (ie. \"linear\" activation: (x) = x). use_bias Boolean, whether layer uses bias vector. kernel_initializer Initializer kernel weights matrix. bias_initializer Initializer bias vector. kernel_regularizer Regularizer function applied kernel weights matrix. bias_regularizer Regularizer function applied bias vector. activity_regularizer Regularizer function applied output layer (\"activation\"). kernel_constraint Constraint function applied kernel weights matrix. bias_constraint Constraint function applied bias vector. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_dense.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Just your regular densely-connected NN layer. — layer_dense","text":"input layer rank greater 2, Dense computes dot product inputs kernel along last axis inputs axis 0 kernel (using tf.tensordot). example, input dimensions (batch_size, d0, d1), create kernel shape (d1, units), kernel operates along axis 2 input, every sub-tensor shape (1, 1, d1) (batch_size * d0 sub-tensors). output case shape (batch_size, d0, units).","code":""},{"path":"https://keras.posit.co/reference/layer_dense.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Just your regular densely-connected NN layer. — layer_dense","text":"N-D tensor shape: (batch_size, ..., input_dim). common situation 2D input shape (batch_size, input_dim).","code":""},{"path":"https://keras.posit.co/reference/layer_dense.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Just your regular densely-connected NN layer. — layer_dense","text":"N-D tensor shape: (batch_size, ..., units). instance, 2D input shape (batch_size, input_dim), output shape (batch_size, units).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_depthwise_conv_1d.html","id":null,"dir":"Reference","previous_headings":"","what":"1D depthwise convolution layer. — layer_depthwise_conv_1d","title":"1D depthwise convolution layer. — layer_depthwise_conv_1d","text":"Depthwise convolution type convolution input channel convolved different kernel (called depthwise kernel). can understand depthwise convolution first step depthwise separable convolution. implemented via following steps: Split input individual channels. Convolve channel individual depthwise kernel depth_multiplier output channels. Concatenate convolved outputs along channels axis. Unlike regular 1D convolution, depthwise convolution mix information across different input channels. depth_multiplier argument determines many filters applied one input channel. , controls amount output channels generated per input channel depthwise step.","code":""},{"path":"https://keras.posit.co/reference/layer_depthwise_conv_1d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"1D depthwise convolution layer. — layer_depthwise_conv_1d","text":"","code":"layer_depthwise_conv_1d(   object,   kernel_size,   strides = 1L,   padding = \"valid\",   depth_multiplier = 1L,   data_format = NULL,   dilation_rate = 1L,   activation = NULL,   use_bias = TRUE,   depthwise_initializer = \"glorot_uniform\",   bias_initializer = \"zeros\",   depthwise_regularizer = NULL,   bias_regularizer = NULL,   activity_regularizer = NULL,   depthwise_constraint = NULL,   bias_constraint = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_depthwise_conv_1d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"1D depthwise convolution layer. — layer_depthwise_conv_1d","text":"object Object compose layer . tensor, array, sequential model. kernel_size int list 1 integer, specifying size depthwise convolution window. strides int list 1 integer, specifying stride length convolution. strides > 1 incompatible dilation_rate > 1. padding string, either \"valid\" \"\" (case-insensitive). \"valid\" means padding. \"\" results padding evenly left/right /input. padding=\"\" strides=1, output size input. depth_multiplier number depthwise convolution output channels input channel. total number depthwise convolution output channels equal input_channel * depth_multiplier. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, steps, features) \"channels_first\" corresponds inputs shape (batch, features, steps). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". dilation_rate int list 1 integers, specifying dilation rate use dilated convolution. activation Activation function. NULL, activation applied. use_bias bool, TRUE, bias added output. depthwise_initializer Initializer convolution kernel. NULL, default initializer (\"glorot_uniform\") used. bias_initializer Initializer bias vector. NULL, default initializer (\"zeros\") used. depthwise_regularizer Optional regularizer convolution kernel. bias_regularizer Optional regularizer bias vector. activity_regularizer Optional regularizer function output. depthwise_constraint Optional projection function applied kernel updated Optimizer (e.g. used implement norm constraints value constraints layer weights). function must take input unprojected variable must return projected variable (must shape). Constraints safe use asynchronous distributed training. bias_constraint Optional projection function applied bias updated Optimizer. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_depthwise_conv_1d.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"1D depthwise convolution layer. — layer_depthwise_conv_1d","text":"3D tensor representing activation(depthwise_conv1d(inputs, kernel) + bias).","code":""},{"path":"https://keras.posit.co/reference/layer_depthwise_conv_1d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"1D depthwise convolution layer. — layer_depthwise_conv_1d","text":"data_format=\"channels_last\": 3D tensor shape: (batch_shape, steps, channels) data_format=\"channels_first\": 3D tensor shape: (batch_shape, channels, steps)","code":""},{"path":"https://keras.posit.co/reference/layer_depthwise_conv_1d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"1D depthwise convolution layer. — layer_depthwise_conv_1d","text":"data_format=\"channels_last\": 3D tensor shape: (batch_shape, new_steps, channels * depth_multiplier) data_format=\"channels_first\": 3D tensor shape: (batch_shape, channels * depth_multiplier, new_steps)","code":""},{"path":"https://keras.posit.co/reference/layer_depthwise_conv_1d.html","id":"raises","dir":"Reference","previous_headings":"","what":"Raises","title":"1D depthwise convolution layer. — layer_depthwise_conv_1d","text":"ValueError: strides > 1 dilation_rate > 1.","code":""},{"path":"https://keras.posit.co/reference/layer_depthwise_conv_1d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"1D depthwise convolution layer. — layer_depthwise_conv_1d","text":"","code":"x <- random_uniform(c(4, 10, 12)) y <- x |> layer_depthwise_conv_1d(   kernel_size = 3,   depth_multiplier = 3,   activation = 'relu' ) shape(y) ## shape(4, 8, 36)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_depthwise_conv_2d.html","id":null,"dir":"Reference","previous_headings":"","what":"2D depthwise convolution layer. — layer_depthwise_conv_2d","title":"2D depthwise convolution layer. — layer_depthwise_conv_2d","text":"Depthwise convolution type convolution input channel convolved different kernel (called depthwise kernel). can understand depthwise convolution first step depthwise separable convolution. implemented via following steps: Split input individual channels. Convolve channel individual depthwise kernel depth_multiplier output channels. Concatenate convolved outputs along channels axis. Unlike regular 2D convolution, depthwise convolution mix information across different input channels. depth_multiplier argument determines many filters applied one input channel. , controls amount output channels generated per input channel depthwise step.","code":""},{"path":"https://keras.posit.co/reference/layer_depthwise_conv_2d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"2D depthwise convolution layer. — layer_depthwise_conv_2d","text":"","code":"layer_depthwise_conv_2d(   object,   kernel_size,   strides = list(1L, 1L),   padding = \"valid\",   depth_multiplier = 1L,   data_format = NULL,   dilation_rate = list(1L, 1L),   activation = NULL,   use_bias = TRUE,   depthwise_initializer = \"glorot_uniform\",   bias_initializer = \"zeros\",   depthwise_regularizer = NULL,   bias_regularizer = NULL,   activity_regularizer = NULL,   depthwise_constraint = NULL,   bias_constraint = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_depthwise_conv_2d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"2D depthwise convolution layer. — layer_depthwise_conv_2d","text":"object Object compose layer . tensor, array, sequential model. kernel_size int list 2 integer, specifying size depthwise convolution window. strides int list 2 integer, specifying stride length depthwise convolution. strides > 1 incompatible dilation_rate > 1. padding string, either \"valid\" \"\" (case-insensitive). \"valid\" means padding. \"\" results padding evenly left/right /input. padding=\"\" strides=1, output size input. depth_multiplier number depthwise convolution output channels input channel. total number depthwise convolution output channels equal input_channel * depth_multiplier. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, steps, features) \"channels_first\" corresponds inputs shape (batch, features, steps). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". dilation_rate int list 2 integers, specifying dilation rate use dilated convolution. activation Activation function. NULL, activation applied. use_bias bool, TRUE, bias added output. depthwise_initializer Initializer convolution kernel. NULL, default initializer (\"glorot_uniform\") used. bias_initializer Initializer bias vector. NULL, default initializer (\"zeros\") used. depthwise_regularizer Optional regularizer convolution kernel. bias_regularizer Optional regularizer bias vector. activity_regularizer Optional regularizer function output. depthwise_constraint Optional projection function applied kernel updated Optimizer (e.g. used implement norm constraints value constraints layer weights). function must take input unprojected variable must return projected variable (must shape). Constraints safe use asynchronous distributed training. bias_constraint Optional projection function applied bias updated Optimizer. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_depthwise_conv_2d.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"2D depthwise convolution layer. — layer_depthwise_conv_2d","text":"4D tensor representing activation(depthwise_conv2d(inputs, kernel) + bias).","code":""},{"path":"https://keras.posit.co/reference/layer_depthwise_conv_2d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"2D depthwise convolution layer. — layer_depthwise_conv_2d","text":"data_format=\"channels_last\": 4D tensor shape: (batch_size, height, width, channels) data_format=\"channels_first\": 4D tensor shape: (batch_size, channels, height, width)","code":""},{"path":"https://keras.posit.co/reference/layer_depthwise_conv_2d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"2D depthwise convolution layer. — layer_depthwise_conv_2d","text":"data_format=\"channels_last\": 4D tensor shape: (batch_size, new_height, new_width, channels * depth_multiplier) data_format=\"channels_first\": 4D tensor shape: (batch_size, channels * depth_multiplier, new_height, new_width)","code":""},{"path":"https://keras.posit.co/reference/layer_depthwise_conv_2d.html","id":"raises","dir":"Reference","previous_headings":"","what":"Raises","title":"2D depthwise convolution layer. — layer_depthwise_conv_2d","text":"ValueError: strides > 1 dilation_rate > 1.","code":""},{"path":"https://keras.posit.co/reference/layer_depthwise_conv_2d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"2D depthwise convolution layer. — layer_depthwise_conv_2d","text":"","code":"x <- random_uniform(c(4, 10, 10, 12)) y <- x |> layer_depthwise_conv_2d(3, 3, activation = 'relu') shape(y) ## shape(4, 3, 3, 12)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_discretization.html","id":null,"dir":"Reference","previous_headings":"","what":"A preprocessing layer which buckets continuous features by ranges. — layer_discretization","title":"A preprocessing layer which buckets continuous features by ranges. — layer_discretization","text":"layer place element input data one several contiguous ranges output integer index indicating range element placed . Note: layer safe use inside tf.data pipeline (independently backend using).","code":""},{"path":"https://keras.posit.co/reference/layer_discretization.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A preprocessing layer which buckets continuous features by ranges. — layer_discretization","text":"","code":"layer_discretization(   object,   bin_boundaries = NULL,   num_bins = NULL,   epsilon = 0.01,   output_mode = \"int\",   sparse = FALSE,   dtype = NULL,   name = NULL )"},{"path":"https://keras.posit.co/reference/layer_discretization.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A preprocessing layer which buckets continuous features by ranges. — layer_discretization","text":"object Object compose layer . tensor, array, sequential model. bin_boundaries list bin boundaries. leftmost rightmost bins always extend -Inf Inf, bin_boundaries = c(0, 1, 2) generates bins (-Inf, 0), [0, 1), [1, 2), [2, +Inf). option set, adapt() called. num_bins integer number bins compute. option set, adapt() called learn bin boundaries. epsilon Error tolerance, typically small fraction close zero (e.g. 0.01). Higher values epsilon increase quantile approximation, hence result unequal buckets, improve performance resource consumption. output_mode Specification output layer. Values can \"int\", \"one_hot\", \"multi_hot\", \"count\" configuring layer follows: \"int\": Return discretized bin indices directly. \"one_hot\": Encodes individual element input array size num_bins, containing 1 input's bin index. last dimension size 1, encode dimension.  last dimension size 1, append new dimension encoded output. \"multi_hot\": Encodes sample input single array size num_bins, containing 1 bin index index present sample. Treats last dimension sample dimension, input shape (..., sample_length), output shape (..., num_tokens). \"count\": \"multi_hot\", int array contains count number times bin index appeared sample. Defaults \"int\". sparse Boolean. applicable \"one_hot\", \"multi_hot\", \"count\" output modes. supported TensorFlow backend. TRUE, returns SparseTensor instead dense Tensor. Defaults FALSE. dtype datatype (e.g., \"float32\"). name String, name object","code":""},{"path":"https://keras.posit.co/reference/layer_discretization.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"A preprocessing layer which buckets continuous features by ranges. — layer_discretization","text":"array dimension 2 higher.","code":""},{"path":"https://keras.posit.co/reference/layer_discretization.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"A preprocessing layer which buckets continuous features by ranges. — layer_discretization","text":"input shape.","code":""},{"path":"https://keras.posit.co/reference/layer_discretization.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A preprocessing layer which buckets continuous features by ranges. — layer_discretization","text":"Discretize float values based provided buckets.     Discretize float values based number buckets compute.","code":"input <- op_array(rbind(c(-1.5, 1, 3.4, 0.5),                        c(0, 3, 1.3, 0),                        c(-.5, 0, .5, 1),                        c(1.5, 2, 2.5, 3))) output <- input |> layer_discretization(bin_boundaries = c(0, 1, 2)) output ## tf.Tensor( ## [[0 2 3 1] ##  [1 3 2 1] ##  [0 1 1 2] ##  [2 3 3 3]], shape=(4, 4), dtype=int64) layer <- layer_discretization(num_bins = 4, epsilon = 0.01) layer |> adapt(input) layer(input) ## tf.Tensor( ## [[0 2 3 1] ##  [1 3 2 1] ##  [0 1 1 2] ##  [2 3 3 3]], shape=(4, 4), dtype=int64)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_dot.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes element-wise dot product of two tensors. — layer_dot","title":"Computes element-wise dot product of two tensors. — layer_dot","text":"takes list inputs size 2, axes corresponding input along dot product performed. say x y two input tensors shapes (2, 3, 5) (2, 10, 3). batch dimension size inputs, axes correspond dimensions size corresponding inputs. e.g. axes = c(1, 2), dot product x, y result tensor shape (2, 5, 10)","code":""},{"path":"https://keras.posit.co/reference/layer_dot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes element-wise dot product of two tensors. — layer_dot","text":"","code":"layer_dot(inputs, ..., axes, normalize = FALSE)"},{"path":"https://keras.posit.co/reference/layer_dot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes element-wise dot product of two tensors. — layer_dot","text":"inputs layers combine ... Standard layer keyword arguments. axes Integer list integers, axis axes along take dot product. list, two integers corresponding desired axis first input desired axis second input, respectively. Note size two selected axes must match. normalize Whether L2-normalize samples along dot product axis taking dot product. set TRUE, output dot product cosine proximity two samples.","code":""},{"path":"https://keras.posit.co/reference/layer_dot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes element-wise dot product of two tensors. — layer_dot","text":"","code":"A tensor, the dot product of the samples from the inputs."},{"path":"https://keras.posit.co/reference/layer_dot.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes element-wise dot product of two tensors. — layer_dot","text":"Usage Keras model:","code":"x <- op_reshape(0:9,   c(1, 5, 2)) y <- op_reshape(10:19, c(1, 2, 5)) layer_dot(x, y, axes=c(2, 3)) ## tf.Tensor( ## [[[260 360] ##   [320 445]]], shape=(1, 2, 2), dtype=int64) x1 <- op_reshape(0:9, c(5, 2)) |> layer_dense(8) x2 <- op_reshape(10:19, c(5, 2)) |> layer_dense(8) shape(x1) ## shape(5, 8) shape(x2) ## shape(5, 8) y <- layer_dot(x1, x2, axes=2) shape(y) ## shape(5, 1)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_dropout.html","id":null,"dir":"Reference","previous_headings":"","what":"Applies dropout to the input. — layer_dropout","title":"Applies dropout to the input. — layer_dropout","text":"Dropout layer randomly sets input units 0 frequency rate step training time, helps prevent overfitting. Inputs set 0 scaled 1 / (1 - rate) sum inputs unchanged. Note Dropout layer applies training set TRUE call(), values dropped inference. using model.fit, training appropriately set TRUE automatically. contexts, can set argument explicitly TRUE calling layer. (contrast setting trainable=FALSE Dropout layer. trainable affect layer's behavior, Dropout variables/weights can frozen training.)","code":""},{"path":"https://keras.posit.co/reference/layer_dropout.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Applies dropout to the input. — layer_dropout","text":"","code":"layer_dropout(object, rate, noise_shape = NULL, seed = NULL, ...)"},{"path":"https://keras.posit.co/reference/layer_dropout.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Applies dropout to the input. — layer_dropout","text":"object Object compose layer . tensor, array, sequential model. rate Float 0 1. Fraction input units drop. noise_shape 1D integer tensor representing shape binary dropout mask multiplied input. instance, inputs shape (batch_size, timesteps, features) want dropout mask timesteps, can use noise_shape=(batch_size, 1, features). seed Python integer use random seed. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_dropout.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"Applies dropout to the input. — layer_dropout","text":"inputs: Input tensor (rank). training: Python boolean indicating whether layer behave training mode (adding dropout) inference mode (nothing).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_einsum_dense.html","id":null,"dir":"Reference","previous_headings":"","what":"A layer that uses einsum as the backing computation. — layer_einsum_dense","title":"A layer that uses einsum as the backing computation. — layer_einsum_dense","text":"layer can perform einsum calculations arbitrary dimensionality.","code":""},{"path":"https://keras.posit.co/reference/layer_einsum_dense.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A layer that uses einsum as the backing computation. — layer_einsum_dense","text":"","code":"layer_einsum_dense(   object,   equation,   output_shape,   activation = NULL,   bias_axes = NULL,   kernel_initializer = \"glorot_uniform\",   bias_initializer = \"zeros\",   kernel_regularizer = NULL,   bias_regularizer = NULL,   kernel_constraint = NULL,   bias_constraint = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_einsum_dense.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A layer that uses einsum as the backing computation. — layer_einsum_dense","text":"object Object compose layer . tensor, array, sequential model. equation equation describing einsum perform. equation must valid einsum string form ab,bc->ac, ...ab,bc->...ac, ab...,bc->ac... 'ab', 'bc', 'ac' can valid einsum axis expression sequence. output_shape expected shape output tensor (excluding batch dimension dimensions represented ellipses). can specify NA NULL dimension unknown can inferred input shape. activation Activation function use. specify anything, activation applied (, \"linear\" activation: (x) = x). bias_axes string containing output dimension(s) apply bias . character bias_axes string correspond character output portion equation string. kernel_initializer Initializer kernel weights matrix. bias_initializer Initializer bias vector. kernel_regularizer Regularizer function applied kernel weights matrix. bias_regularizer Regularizer function applied bias vector. kernel_constraint Constraint function applied kernel weights matrix. bias_constraint Constraint function applied bias vector. ... Base layer keyword arguments, name dtype.","code":""},{"path":"https://keras.posit.co/reference/layer_einsum_dense.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A layer that uses einsum as the backing computation. — layer_einsum_dense","text":"Biased dense layer einsums example shows instantiate standard Keras dense layer using einsum operations. example equivalent layer_Dense(64, use_bias=TRUE).     Applying dense layer sequence example shows instantiate layer applies dense operation every element sequence. , output_shape two values (since two non-batch dimensions output); first dimension output_shape NA, sequence dimension b unknown shape.     Applying dense layer sequence using ellipses example shows instantiate layer applies dense operation every element sequence, uses ellipsis notation instead specifying batch sequence dimensions. using ellipsis notation specified one axis, output_shape arg single value. instantiated way, layer can handle number sequence dimensions - including case sequence dimension exists.","code":"input <- layer_input(shape = c(32)) output <- input |>   layer_einsum_dense(\"ab,bc->ac\",                      output_shape = 64,                      bias_axes = \"c\") output # shape(NA, 64) ## <KerasTensor shape=(None, 64), dtype=float32, sparse=False, name=keras_tensor_1> input <- layer_input(shape = c(32, 128)) output <- input |>   layer_einsum_dense(\"abc,cd->abd\",                      output_shape = c(NA, 64),                      bias_axes = \"d\") output  # shape(NA, 32, 64) ## <KerasTensor shape=(None, None, 64), dtype=float32, sparse=False, name=keras_tensor_3> input <- layer_input(shape = c(32, 128)) output <- input |>   layer_einsum_dense(\"...x,xy->...y\",                      output_shape = 64,                      bias_axes = \"y\")  output  # shape(NA, 32, 64) ## <KerasTensor shape=(None, 32, 64), dtype=float32, sparse=False, name=keras_tensor_5>"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_embedding.html","id":null,"dir":"Reference","previous_headings":"","what":"Turns positive integers (indexes) into dense vectors of fixed size. — layer_embedding","title":"Turns positive integers (indexes) into dense vectors of fixed size. — layer_embedding","text":"e.g. rbind(4L, 20L) \\(\\rightarrow\\) rbind(c(0.25, 0.1), c(0.6, -0.2)) layer can used positive integer inputs fixed range.","code":""},{"path":"https://keras.posit.co/reference/layer_embedding.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Turns positive integers (indexes) into dense vectors of fixed size. — layer_embedding","text":"","code":"layer_embedding(   object,   input_dim,   output_dim,   embeddings_initializer = \"uniform\",   embeddings_regularizer = NULL,   embeddings_constraint = NULL,   mask_zero = FALSE,   ... )"},{"path":"https://keras.posit.co/reference/layer_embedding.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Turns positive integers (indexes) into dense vectors of fixed size. — layer_embedding","text":"object Object compose layer . tensor, array, sequential model. input_dim Integer. Size vocabulary, .e. maximum integer index + 1. output_dim Integer. Dimension dense embedding. embeddings_initializer Initializer embeddings matrix (see keras3::initializer_*). embeddings_regularizer Regularizer function applied embeddings matrix (see keras3::regularizer_*). embeddings_constraint Constraint function applied embeddings matrix (see keras3::constraint_*). mask_zero Boolean, whether input value 0 special \"padding\" value masked . useful using recurrent layers may take variable length input. TRUE, subsequent layers model need support masking exception raised. mask_zero set TRUE, consequence, index 0 used vocabulary (input_dim equal size vocabulary + 1). ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_embedding.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Turns positive integers (indexes) into dense vectors of fixed size. — layer_embedding","text":"","code":"model <- keras_model_sequential() |>   layer_embedding(1000, 64)  # The model will take as input an integer matrix of size (batch,input_length), # and the largest integer (i.e. word index) in the input # should be no larger than 999 (vocabulary size). # Now model$output_shape is (NA, 10, 64), where `NA` is the batch # dimension.  input_array <- random_integer(shape = c(32, 10), minval = 0, maxval = 1000) model |> compile('rmsprop', 'mse') output_array <- model |> predict(input_array, verbose = 0) dim(output_array)    # (32, 10, 64) ## [1] 32 10 64"},{"path":"https://keras.posit.co/reference/layer_embedding.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Turns positive integers (indexes) into dense vectors of fixed size. — layer_embedding","text":"2D tensor shape: (batch_size, input_length).","code":""},{"path":"https://keras.posit.co/reference/layer_embedding.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Turns positive integers (indexes) into dense vectors of fixed size. — layer_embedding","text":"3D tensor shape: (batch_size, input_length, output_dim).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_feature_space.html","id":null,"dir":"Reference","previous_headings":"","what":"One-stop utility for preprocessing and encoding structured data. — layer_feature_space","title":"One-stop utility for preprocessing and encoding structured data. — layer_feature_space","text":"Available feature types: Note features can referred string name, e.g. \"integer_categorical\". using string name, default argument values used.","code":"# Plain float values. feature_float(name = NULL)  # Float values to be preprocessed via featurewise standardization # (i.e. via a `layer_normalization()` layer). feature_float_normalized(name = NULL)  # Float values to be preprocessed via linear rescaling # (i.e. via a `layer_rescaling` layer). feature_float_rescaled(scale = 1., offset = 0., name = NULL)  # Float values to be discretized. By default, the discrete # representation will then be one-hot encoded. feature_float_discretized(   num_bins,   bin_boundaries = NULL,   output_mode = \"one_hot\",   name = NULL )  # Integer values to be indexed. By default, the discrete # representation will then be one-hot encoded. feature_integer_categorical(   max_tokens = NULL,   num_oov_indices = 1,   output_mode = \"one_hot\",   name = NULL )  # String values to be indexed. By default, the discrete # representation will then be one-hot encoded. feature_string_categorical(   max_tokens = NULL,   num_oov_indices = 1,   output_mode = \"one_hot\",   name = NULL )  # Integer values to be hashed into a fixed number of bins. # By default, the discrete representation will then be one-hot encoded. feature_integer_hashed(num_bins, output_mode = \"one_hot\", name = NULL)  # String values to be hashed into a fixed number of bins. # By default, the discrete representation will then be one-hot encoded. feature_string_hashed(num_bins, output_mode = \"one_hot\", name = NULL)"},{"path":"https://keras.posit.co/reference/layer_feature_space.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"One-stop utility for preprocessing and encoding structured data. — layer_feature_space","text":"","code":"layer_feature_space(   object,   features,   output_mode = \"concat\",   crosses = NULL,   crossing_dim = 32L,   hashing_dim = 32L,   num_discretization_bins = 32L,   name = NULL,   feature_names = NULL )  feature_cross(feature_names, crossing_dim, output_mode = \"one_hot\")  feature_custom(dtype, preprocessor, output_mode)  feature_float(name = NULL)  feature_float_rescaled(scale = 1, offset = 0, name = NULL)  feature_float_normalized(name = NULL)  feature_float_discretized(   num_bins,   bin_boundaries = NULL,   output_mode = \"one_hot\",   name = NULL )  feature_integer_categorical(   max_tokens = NULL,   num_oov_indices = 1,   output_mode = \"one_hot\",   name = NULL )  feature_string_categorical(   max_tokens = NULL,   num_oov_indices = 1,   output_mode = \"one_hot\",   name = NULL )  feature_string_hashed(num_bins, output_mode = \"one_hot\", name = NULL)  feature_integer_hashed(num_bins, output_mode = \"one_hot\", name = NULL)"},{"path":"https://keras.posit.co/reference/layer_feature_space.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"One-stop utility for preprocessing and encoding structured data. — layer_feature_space","text":"object see description features see description output_mode string. layer_feature_space(), one \"concat\" \"dict\". concat mode, features get concatenated together single vector. dict mode, FeatureSpace returns named list individually encoded features (names input list names). feature_* functions, one : \"int\" \"one_hot\" \"float\". crosses List features crossed together, e.g. crosses=list(c(\"feature_1\", \"feature_2\")). features \"crossed\" hashing combined value fixed-length vector. crossing_dim Default vector size hashing crossed features. Defaults 32. hashing_dim Default vector size hashing features type \"integer_hashed\" \"string_hashed\". Defaults 32. num_discretization_bins Default number bins used discretizing features type \"float_discretized\". Defaults 32. name String, name object feature_names Named list mapping names features type specification, e.g. list(my_feature = \"integer_categorical\") list(my_feature = feature_integer_categorical()). complete list supported types, see \"Available feature types\" paragraph . dtype string, output dtype feature. E.g., \"float32\". preprocessor callable. scale, offset Passed layer_rescaling() num_bins, bin_boundaries Passed layer_discretization() max_tokens, num_oov_indices Passed layer_integer_lookup() feature_integer_categorical() layer_string_lookup() feature_string_categorical().","code":""},{"path":"https://keras.posit.co/reference/layer_feature_space.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"One-stop utility for preprocessing and encoding structured data. — layer_feature_space","text":"Basic usage named list input data:   Basic usage tf.data:   Basic usage Keras Functional API:           Customizing feature feature cross:   Returning dict (named list) integer-encoded features:   Specifying Keras preprocessing layer:   Retrieving underlying Keras preprocessing layers:   Saving reloading FeatureSpace:","code":"raw_data <- list(   float_values = c(0.0, 0.1, 0.2, 0.3),   string_values = c(\"zero\", \"one\", \"two\", \"three\"),   int_values = as.integer(c(0, 1, 2, 3)) )  dataset <- tfdatasets::tensor_slices_dataset(raw_data)  feature_space <- layer_feature_space(   features = list(     float_values = \"float_normalized\",     string_values = \"string_categorical\",     int_values = \"integer_categorical\"   ),   crosses = list(c(\"string_values\", \"int_values\")),   output_mode = \"concat\" )  # Before you start using the feature_space(), # you must `adapt()` it on some data. feature_space |> adapt(dataset)  # You can call the feature_space() on a named list of # data (batched or unbatched). output_vector <- feature_space(raw_data) library(tfdatasets) # Unlabeled data preprocessed_ds <- unlabeled_dataset |>   dataset_map(feature_space)  # Labeled data preprocessed_ds <- labeled_dataset |>   dataset_map(function(x, y) tuple(feature_space(x), y)) # Retrieve a named list of Keras layer_input() objects (inputs <- feature_space$get_inputs()) ## $float_values ## <KerasTensor shape=(None, 1), dtype=float32, sparse=None, name=float_values> ## ## $string_values ## <KerasTensor shape=(None, 1), dtype=string, sparse=None, name=string_values> ## ## $int_values ## <KerasTensor shape=(None, 1), dtype=int32, sparse=None, name=int_values> # Retrieve the corresponding encoded Keras tensors (encoded_features <- feature_space$get_encoded_features()) ## <KerasTensor shape=(None, 43), dtype=float32, sparse=False, name=keras_tensor_7> # Build a Functional model outputs <- encoded_features |> layer_dense(1, activation = \"sigmoid\") model <- keras_model(inputs, outputs) feature_space <- layer_feature_space(   features = list(     float_values = feature_float_normalized(),     string_values = feature_string_categorical(max_tokens = 10),     int_values = feature_integer_categorical(max_tokens = 10)   ),   crosses = list(     feature_cross(c(\"string_values\", \"int_values\"), crossing_dim = 32)   ),   output_mode = \"concat\" ) feature_space <- layer_feature_space(   features = list(     \"string_values\" = feature_string_categorical(output_mode = \"int\"),     \"int_values\" = feature_integer_categorical(output_mode = \"int\")   ),   crosses = list(     feature_cross(       feature_names = c(\"string_values\", \"int_values\"),       crossing_dim = 32,       output_mode = \"int\"     )   ),   output_mode = \"dict\" ) # Let's say that one of the features is a short text paragraph that # we want to encode as a vector (one vector per paragraph) via TF-IDF. data <- list(text = c(\"1st string\", \"2nd string\", \"3rd string\"))  # There's a Keras layer for this: layer_text_vectorization() custom_layer <- layer_text_vectorization(output_mode = \"tf_idf\")  # We can use feature_custom() to create a custom feature # that will use our preprocessing layer. feature_space <- layer_feature_space(   features = list(     text = feature_custom(preprocessor = custom_layer,                           dtype = \"string\",                           output_mode = \"float\"     )   ),   output_mode = \"concat\" ) feature_space |> adapt(tfdatasets::tensor_slices_dataset(data)) output_vector <- feature_space(data) # The preprocessing layer of each feature is available in `$preprocessors`. preprocessing_layer <- feature_space$preprocessors$feature1  # The crossing layer of each feature cross is available in `$crossers`. # It's an instance of layer_hashed_crossing() crossing_layer <- feature_space$crossers[[\"feature1_X_feature2\"]] feature_space$save(\"featurespace.keras\") reloaded_feature_space <- keras$models$load_model(\"featurespace.keras\")"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_flatten.html","id":null,"dir":"Reference","previous_headings":"","what":"Flattens the input. Does not affect the batch size. — layer_flatten","title":"Flattens the input. Does not affect the batch size. — layer_flatten","text":"Flattens input. affect batch size.","code":""},{"path":"https://keras.posit.co/reference/layer_flatten.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Flattens the input. Does not affect the batch size. — layer_flatten","text":"","code":"layer_flatten(object, data_format = NULL, ...)"},{"path":"https://keras.posit.co/reference/layer_flatten.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Flattens the input. Does not affect the batch size. — layer_flatten","text":"object Object compose layer . tensor, array, sequential model. data_format string, one \"channels_last\" (default) \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, ..., channels) \"channels_first\" corresponds inputs shape (batch, channels, ...). unspecified, uses image_data_format value found Keras config file ~/.keras/keras.json (exists). Defaults \"channels_last\". ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_flatten.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Flattens the input. Does not affect the batch size. — layer_flatten","text":"inputs shaped (batch) without feature axis, flattening adds extra channel dimension output shape (batch, 1).","code":""},{"path":"https://keras.posit.co/reference/layer_flatten.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Flattens the input. Does not affect the batch size. — layer_flatten","text":"","code":"x <- layer_input(shape=c(10, 64)) y <- x |> layer_flatten() shape(y) ## shape(NA, 640)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_gaussian_dropout.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply multiplicative 1-centered Gaussian noise. — layer_gaussian_dropout","title":"Apply multiplicative 1-centered Gaussian noise. — layer_gaussian_dropout","text":"regularization layer, active training time.","code":""},{"path":"https://keras.posit.co/reference/layer_gaussian_dropout.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply multiplicative 1-centered Gaussian noise. — layer_gaussian_dropout","text":"","code":"layer_gaussian_dropout(object, rate, seed = NULL, ...)"},{"path":"https://keras.posit.co/reference/layer_gaussian_dropout.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply multiplicative 1-centered Gaussian noise. — layer_gaussian_dropout","text":"object Object compose layer . tensor, array, sequential model. rate Float, drop probability (Dropout). multiplicative noise standard deviation sqrt(rate / (1 - rate)). seed Integer, optional random seed enable deterministic behavior. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_gaussian_dropout.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"Apply multiplicative 1-centered Gaussian noise. — layer_gaussian_dropout","text":"inputs: Input tensor (rank). training: Python boolean indicating whether layer behave training mode (adding dropout) inference mode (nothing).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_gaussian_noise.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply additive zero-centered Gaussian noise. — layer_gaussian_noise","title":"Apply additive zero-centered Gaussian noise. — layer_gaussian_noise","text":"useful mitigate overfitting (see form random data augmentation). Gaussian Noise (GS) natural choice corruption process real valued inputs. regularization layer, active training time.","code":""},{"path":"https://keras.posit.co/reference/layer_gaussian_noise.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply additive zero-centered Gaussian noise. — layer_gaussian_noise","text":"","code":"layer_gaussian_noise(object, stddev, seed = NULL, ...)"},{"path":"https://keras.posit.co/reference/layer_gaussian_noise.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply additive zero-centered Gaussian noise. — layer_gaussian_noise","text":"object Object compose layer . tensor, array, sequential model. stddev Float, standard deviation noise distribution. seed Integer, optional random seed enable deterministic behavior. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_gaussian_noise.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"Apply additive zero-centered Gaussian noise. — layer_gaussian_noise","text":"inputs: Input tensor (rank). training: Python boolean indicating whether layer behave training mode (adding noise) inference mode (nothing).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_global_average_pooling_1d.html","id":null,"dir":"Reference","previous_headings":"","what":"Global average pooling operation for temporal data. — layer_global_average_pooling_1d","title":"Global average pooling operation for temporal data. — layer_global_average_pooling_1d","text":"Global average pooling operation temporal data.","code":""},{"path":"https://keras.posit.co/reference/layer_global_average_pooling_1d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Global average pooling operation for temporal data. — layer_global_average_pooling_1d","text":"","code":"layer_global_average_pooling_1d(   object,   data_format = NULL,   keepdims = FALSE,   ... )"},{"path":"https://keras.posit.co/reference/layer_global_average_pooling_1d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Global average pooling operation for temporal data. — layer_global_average_pooling_1d","text":"object Object compose layer . tensor, array, sequential model. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, steps, features) \"channels_first\" corresponds inputs shape (batch, features, steps). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". keepdims boolean, whether keep temporal dimension . keepdims FALSE (default), rank tensor reduced spatial dimensions. keepdims TRUE, temporal dimension retained length 1. behavior tf$reduce_mean() op_mean(). ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_global_average_pooling_1d.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"Global average pooling operation for temporal data. — layer_global_average_pooling_1d","text":"inputs: 3D tensor. mask: Binary tensor shape (batch_size, steps) indicating whether given step masked (excluded average).","code":""},{"path":"https://keras.posit.co/reference/layer_global_average_pooling_1d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Global average pooling operation for temporal data. — layer_global_average_pooling_1d","text":"data_format='channels_last': 3D tensor shape: (batch_size, steps, features) data_format='channels_first': 3D tensor shape: (batch_size, features, steps)","code":""},{"path":"https://keras.posit.co/reference/layer_global_average_pooling_1d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Global average pooling operation for temporal data. — layer_global_average_pooling_1d","text":"keepdims=FALSE: 2D tensor shape (batch_size, features). keepdims=TRUE: data_format=\"channels_last\": 3D tensor shape (batch_size, 1, features) data_format=\"channels_first\": 3D tensor shape (batch_size, features, 1)","code":""},{"path":"https://keras.posit.co/reference/layer_global_average_pooling_1d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Global average pooling operation for temporal data. — layer_global_average_pooling_1d","text":"","code":"x <- random_uniform(c(2, 3, 4)) y <- x |> layer_global_average_pooling_1d() shape(y) ## shape(2, 4)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_global_average_pooling_2d.html","id":null,"dir":"Reference","previous_headings":"","what":"Global average pooling operation for 2D data. — layer_global_average_pooling_2d","title":"Global average pooling operation for 2D data. — layer_global_average_pooling_2d","text":"Global average pooling operation 2D data.","code":""},{"path":"https://keras.posit.co/reference/layer_global_average_pooling_2d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Global average pooling operation for 2D data. — layer_global_average_pooling_2d","text":"","code":"layer_global_average_pooling_2d(   object,   data_format = NULL,   keepdims = FALSE,   ... )"},{"path":"https://keras.posit.co/reference/layer_global_average_pooling_2d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Global average pooling operation for 2D data. — layer_global_average_pooling_2d","text":"object Object compose layer . tensor, array, sequential model. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, height, width, channels) \"channels_first\" corresponds inputs shape (batch, features, height, weight). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". keepdims boolean, whether keep temporal dimension . keepdims FALSE (default), rank tensor reduced spatial dimensions. keepdims TRUE, spatial dimension retained length 1. behavior tf$reduce_mean() op_mean(). ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_global_average_pooling_2d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Global average pooling operation for 2D data. — layer_global_average_pooling_2d","text":"data_format='channels_last': 4D tensor shape: (batch_size, height, width, channels) data_format='channels_first': 4D tensor shape: (batch_size, channels, height, width)","code":""},{"path":"https://keras.posit.co/reference/layer_global_average_pooling_2d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Global average pooling operation for 2D data. — layer_global_average_pooling_2d","text":"keepdims=FALSE: 2D tensor shape (batch_size, channels). keepdims=TRUE: data_format=\"channels_last\": 4D tensor shape (batch_size, 1, 1, channels) data_format=\"channels_first\": 4D tensor shape (batch_size, channels, 1, 1)","code":""},{"path":"https://keras.posit.co/reference/layer_global_average_pooling_2d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Global average pooling operation for 2D data. — layer_global_average_pooling_2d","text":"","code":"x <- random_uniform(c(2, 4, 5, 3)) y <- x |> layer_global_average_pooling_2d() shape(y) ## shape(2, 3)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_global_average_pooling_3d.html","id":null,"dir":"Reference","previous_headings":"","what":"Global average pooling operation for 3D data. — layer_global_average_pooling_3d","title":"Global average pooling operation for 3D data. — layer_global_average_pooling_3d","text":"Global average pooling operation 3D data.","code":""},{"path":"https://keras.posit.co/reference/layer_global_average_pooling_3d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Global average pooling operation for 3D data. — layer_global_average_pooling_3d","text":"","code":"layer_global_average_pooling_3d(   object,   data_format = NULL,   keepdims = FALSE,   ... )"},{"path":"https://keras.posit.co/reference/layer_global_average_pooling_3d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Global average pooling operation for 3D data. — layer_global_average_pooling_3d","text":"object Object compose layer . tensor, array, sequential model. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) \"channels_first\" corresponds inputs shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". keepdims boolean, whether keep temporal dimension . keepdims FALSE (default), rank tensor reduced spatial dimensions. keepdims TRUE, spatial dimension retained length 1. behavior tf$reduce_mean() op_mean(). ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_global_average_pooling_3d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Global average pooling operation for 3D data. — layer_global_average_pooling_3d","text":"data_format='channels_last': 5D tensor shape: (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) data_format='channels_first': 5D tensor shape: (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)","code":""},{"path":"https://keras.posit.co/reference/layer_global_average_pooling_3d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Global average pooling operation for 3D data. — layer_global_average_pooling_3d","text":"keepdims=FALSE: 2D tensor shape (batch_size, channels). keepdims=TRUE: data_format=\"channels_last\": 5D tensor shape (batch_size, 1, 1, 1, channels) data_format=\"channels_first\": 5D tensor shape (batch_size, channels, 1, 1, 1)","code":""},{"path":"https://keras.posit.co/reference/layer_global_average_pooling_3d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Global average pooling operation for 3D data. — layer_global_average_pooling_3d","text":"","code":"x <- random_uniform(c(2, 4, 5, 4, 3)) y <- x |> layer_global_average_pooling_3d() shape(y) ## shape(2, 3)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_global_max_pooling_1d.html","id":null,"dir":"Reference","previous_headings":"","what":"Global max pooling operation for temporal data. — layer_global_max_pooling_1d","title":"Global max pooling operation for temporal data. — layer_global_max_pooling_1d","text":"Global max pooling operation temporal data.","code":""},{"path":"https://keras.posit.co/reference/layer_global_max_pooling_1d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Global max pooling operation for temporal data. — layer_global_max_pooling_1d","text":"","code":"layer_global_max_pooling_1d(object, data_format = NULL, keepdims = FALSE, ...)"},{"path":"https://keras.posit.co/reference/layer_global_max_pooling_1d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Global max pooling operation for temporal data. — layer_global_max_pooling_1d","text":"object Object compose layer . tensor, array, sequential model. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, steps, features) \"channels_first\" corresponds inputs shape (batch, features, steps). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". keepdims boolean, whether keep temporal dimension . keepdims FALSE (default), rank tensor reduced spatial dimensions. keepdims TRUE, temporal dimension retained length 1. behavior tf$reduce_mean() op_mean(). ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_global_max_pooling_1d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Global max pooling operation for temporal data. — layer_global_max_pooling_1d","text":"data_format='channels_last': 3D tensor shape: (batch_size, steps, features) data_format='channels_first': 3D tensor shape: (batch_size, features, steps)","code":""},{"path":"https://keras.posit.co/reference/layer_global_max_pooling_1d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Global max pooling operation for temporal data. — layer_global_max_pooling_1d","text":"keepdims=FALSE: 2D tensor shape (batch_size, features). keepdims=TRUE: data_format=\"channels_last\": 3D tensor shape (batch_size, 1, features) data_format=\"channels_first\": 3D tensor shape (batch_size, features, 1)","code":""},{"path":"https://keras.posit.co/reference/layer_global_max_pooling_1d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Global max pooling operation for temporal data. — layer_global_max_pooling_1d","text":"","code":"x <- random_uniform(c(2, 3, 4)) y <- x |> layer_global_max_pooling_1d() shape(y) ## shape(2, 4)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_global_max_pooling_2d.html","id":null,"dir":"Reference","previous_headings":"","what":"Global max pooling operation for 2D data. — layer_global_max_pooling_2d","title":"Global max pooling operation for 2D data. — layer_global_max_pooling_2d","text":"Global max pooling operation 2D data.","code":""},{"path":"https://keras.posit.co/reference/layer_global_max_pooling_2d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Global max pooling operation for 2D data. — layer_global_max_pooling_2d","text":"","code":"layer_global_max_pooling_2d(object, data_format = NULL, keepdims = FALSE, ...)"},{"path":"https://keras.posit.co/reference/layer_global_max_pooling_2d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Global max pooling operation for 2D data. — layer_global_max_pooling_2d","text":"object Object compose layer . tensor, array, sequential model. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, height, width, channels) \"channels_first\" corresponds inputs shape (batch, features, height, weight). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". keepdims boolean, whether keep temporal dimension . keepdims FALSE (default), rank tensor reduced spatial dimensions. keepdims TRUE, spatial dimension retained length 1. behavior tf$reduce_mean() op_mean(). ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_global_max_pooling_2d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Global max pooling operation for 2D data. — layer_global_max_pooling_2d","text":"data_format='channels_last': 4D tensor shape: (batch_size, height, width, channels) data_format='channels_first': 4D tensor shape: (batch_size, channels, height, width)","code":""},{"path":"https://keras.posit.co/reference/layer_global_max_pooling_2d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Global max pooling operation for 2D data. — layer_global_max_pooling_2d","text":"keepdims=FALSE: 2D tensor shape (batch_size, channels). keepdims=TRUE: data_format=\"channels_last\": 4D tensor shape (batch_size, 1, 1, channels) data_format=\"channels_first\": 4D tensor shape (batch_size, channels, 1, 1)","code":""},{"path":"https://keras.posit.co/reference/layer_global_max_pooling_2d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Global max pooling operation for 2D data. — layer_global_max_pooling_2d","text":"","code":"x <- random_uniform(c(2, 4, 5, 3)) y <- x |> layer_global_max_pooling_2d() shape(y) ## shape(2, 3)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_global_max_pooling_3d.html","id":null,"dir":"Reference","previous_headings":"","what":"Global max pooling operation for 3D data. — layer_global_max_pooling_3d","title":"Global max pooling operation for 3D data. — layer_global_max_pooling_3d","text":"Global max pooling operation 3D data.","code":""},{"path":"https://keras.posit.co/reference/layer_global_max_pooling_3d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Global max pooling operation for 3D data. — layer_global_max_pooling_3d","text":"","code":"layer_global_max_pooling_3d(object, data_format = NULL, keepdims = FALSE, ...)"},{"path":"https://keras.posit.co/reference/layer_global_max_pooling_3d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Global max pooling operation for 3D data. — layer_global_max_pooling_3d","text":"object Object compose layer . tensor, array, sequential model. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) \"channels_first\" corresponds inputs shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". keepdims boolean, whether keep temporal dimension . keepdims FALSE (default), rank tensor reduced spatial dimensions. keepdims TRUE, spatial dimension retained length 1. behavior tf$reduce_mean() op_mean(). ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_global_max_pooling_3d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Global max pooling operation for 3D data. — layer_global_max_pooling_3d","text":"data_format='channels_last': 5D tensor shape: (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) data_format='channels_first': 5D tensor shape: (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)","code":""},{"path":"https://keras.posit.co/reference/layer_global_max_pooling_3d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Global max pooling operation for 3D data. — layer_global_max_pooling_3d","text":"keepdims=FALSE: 2D tensor shape (batch_size, channels). keepdims=TRUE: data_format=\"channels_last\": 5D tensor shape (batch_size, 1, 1, 1, channels) data_format=\"channels_first\": 5D tensor shape (batch_size, channels, 1, 1, 1)","code":""},{"path":"https://keras.posit.co/reference/layer_global_max_pooling_3d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Global max pooling operation for 3D data. — layer_global_max_pooling_3d","text":"","code":"x <- random_uniform(c(2, 4, 5, 4, 3)) y <- x |> layer_global_max_pooling_3d() shape(y) ## shape(2, 3)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_group_normalization.html","id":null,"dir":"Reference","previous_headings":"","what":"Group normalization layer. — layer_group_normalization","title":"Group normalization layer. — layer_group_normalization","text":"Group Normalization divides channels groups computes within group mean variance normalization. Empirically, accuracy stable batch norm wide range small batch sizes, learning rate adjusted linearly batch sizes. Relation Layer Normalization: number groups set 1, operation becomes nearly identical Layer Normalization (see Layer Normalization docs details). Relation Instance Normalization: number groups set input dimension (number groups equal number channels), operation becomes identical Instance Normalization. can achieve via groups=-1.","code":""},{"path":"https://keras.posit.co/reference/layer_group_normalization.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Group normalization layer. — layer_group_normalization","text":"","code":"layer_group_normalization(   object,   groups = 32L,   axis = -1L,   epsilon = 0.001,   center = TRUE,   scale = TRUE,   beta_initializer = \"zeros\",   gamma_initializer = \"ones\",   beta_regularizer = NULL,   gamma_regularizer = NULL,   beta_constraint = NULL,   gamma_constraint = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_group_normalization.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Group normalization layer. — layer_group_normalization","text":"object Object compose layer . tensor, array, sequential model. groups Integer, number groups Group Normalization. Can range [1, N] N input dimension. input dimension must divisible number groups. Defaults 32. axis Integer List/Tuple. axis axes normalize across. Typically, features axis/axes. left-axes typically batch axis/axes. -1 last dimension input. Defaults -1. epsilon Small float added variance avoid dividing zero. Defaults 1e-3. center TRUE, add offset beta normalized tensor. FALSE, beta ignored. Defaults TRUE. scale TRUE, multiply gamma. FALSE, gamma used. next layer linear (also e.g. relu), can disabled since scaling done next layer. Defaults TRUE. beta_initializer Initializer beta weight. Defaults zeros. gamma_initializer Initializer gamma weight. Defaults ones. beta_regularizer Optional regularizer beta weight. NULL default. gamma_regularizer Optional regularizer gamma weight. NULL default. beta_constraint Optional constraint beta weight. NULL default. gamma_constraint Optional constraint gamma weight. NULL default. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_group_normalization.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Group normalization layer. — layer_group_normalization","text":"Arbitrary. Use keyword argument input_shape (tuple integers, include samples axis) using layer first layer model.","code":""},{"path":"https://keras.posit.co/reference/layer_group_normalization.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Group normalization layer. — layer_group_normalization","text":"shape input. **kwargs: Base layer keyword arguments (e.g. name dtype).","code":""},{"path":"https://keras.posit.co/reference/layer_group_normalization.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Group normalization layer. — layer_group_normalization","text":"Yuxin Wu & Kaiming , 2018","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_group_query_attention.html","id":null,"dir":"Reference","previous_headings":"","what":"Grouped Query Attention layer. — layer_group_query_attention","title":"Grouped Query Attention layer. — layer_group_query_attention","text":"implementation grouped-query attention introduced Ainslie et al., 2023. num_key_value_heads denotes number groups, setting num_key_value_heads 1 equivalent multi-query attention, num_key_value_heads equal num_query_heads equivalent multi-head attention. layer first projects query, key, value tensors. , key value repeated match number heads query. , query scaled dot-producted key tensors. softmaxed obtain attention probabilities. value tensors interpolated probabilities concatenated back single tensor.","code":""},{"path":"https://keras.posit.co/reference/layer_group_query_attention.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Grouped Query Attention layer. — layer_group_query_attention","text":"","code":"layer_group_query_attention(   object,   head_dim,   num_query_heads,   num_key_value_heads,   dropout = 0,   use_bias = TRUE,   kernel_initializer = \"glorot_uniform\",   bias_initializer = \"zeros\",   kernel_regularizer = NULL,   bias_regularizer = NULL,   activity_regularizer = NULL,   kernel_constraint = NULL,   bias_constraint = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_group_query_attention.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Grouped Query Attention layer. — layer_group_query_attention","text":"object Object compose layer . tensor, array, sequential model. head_dim Size attention head. num_query_heads Number query attention heads. num_key_value_heads Number key value attention heads. dropout Dropout probability. use_bias Boolean, whether dense layers use bias vectors/matrices. kernel_initializer Initializer dense layer kernels. bias_initializer Initializer dense layer biases. kernel_regularizer Regularizer dense layer kernels. bias_regularizer Regularizer dense layer biases. activity_regularizer Regularizer dense layer activity. kernel_constraint Constraint dense layer kernels. bias_constraint Constraint dense layer kernels. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_group_query_attention.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Grouped Query Attention layer. — layer_group_query_attention","text":"attention_output: Result computation, shape (batch_dim, target_seq_len, feature_dim), target_seq_len target sequence length feature_dim query input last dim. attention_scores: (Optional) attention coefficients shape (batch_dim, num_query_heads, target_seq_len, source_seq_len).","code":""},{"path":"https://keras.posit.co/reference/layer_group_query_attention.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"Grouped Query Attention layer. — layer_group_query_attention","text":"query: Query tensor shape (batch_dim, target_seq_len, feature_dim), batch_dim batch size, target_seq_len length target sequence, feature_dim dimension feature. value: Value tensor shape (batch_dim, source_seq_len, feature_dim), batch_dim batch size, source_seq_len length source sequence, feature_dim dimension feature. key: Optional key tensor shape (batch_dim, source_seq_len, feature_dim). given, use value key value, common case. attention_mask: boolean mask shape (batch_dim, target_seq_len, source_seq_len), prevents attention certain positions. boolean mask specifies query elements can attend key elements, 1 indicates attention 0 indicates attention. Broadcasting can happen missing batch dimensions head dimension. return_attention_scores: boolean indicate whether output (attention_output, attention_scores) TRUE, attention_output FALSE. Defaults FALSE. training: Python boolean indicating whether layer behave training mode (adding dropout) inference mode (dropout). go either using training mode parent layer/model FALSE (inference) parent layer. use_causal_mask: boolean indicate whether apply causal mask prevent tokens attending future tokens (e.g., used decoder Transformer).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_gru.html","id":null,"dir":"Reference","previous_headings":"","what":"Gated Recurrent Unit - Cho et al. 2014. — layer_gru","title":"Gated Recurrent Unit - Cho et al. 2014. — layer_gru","text":"Based available runtime hardware constraints, layer choose different implementations (cuDNN-based backend-native) maximize performance. GPU available arguments layer meet requirement cuDNN kernel (see details), layer use fast cuDNN implementation using TensorFlow backend. requirements use cuDNN implementation : activation == tanh recurrent_activation == sigmoid dropout == 0 recurrent_dropout == 0 unroll FALSE use_bias TRUE reset_after TRUE Inputs, use masking, strictly right-padded. Eager execution enabled outermost context. two variants GRU implementation. default one based v3 reset gate applied hidden state matrix multiplication. one based original order reversed. second variant compatible CuDNNGRU (GPU-) allows inference CPU. Thus separate biases kernel recurrent_kernel. use variant, set reset_after=TRUE recurrent_activation='sigmoid'. example:","code":"inputs <- random_uniform(c(32, 10, 8)) outputs <- inputs |> layer_gru(4) shape(outputs) ## shape(32, 4) # (32, 4) gru <- layer_gru(, 4, return_sequences = TRUE, return_state = TRUE) c(whole_sequence_output, final_state) %<-% gru(inputs) shape(whole_sequence_output) ## shape(32, 10, 4) shape(final_state) ## shape(32, 4)"},{"path":"https://keras.posit.co/reference/layer_gru.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gated Recurrent Unit - Cho et al. 2014. — layer_gru","text":"","code":"layer_gru(   object,   units,   activation = \"tanh\",   recurrent_activation = \"sigmoid\",   use_bias = TRUE,   kernel_initializer = \"glorot_uniform\",   recurrent_initializer = \"orthogonal\",   bias_initializer = \"zeros\",   kernel_regularizer = NULL,   recurrent_regularizer = NULL,   bias_regularizer = NULL,   activity_regularizer = NULL,   kernel_constraint = NULL,   recurrent_constraint = NULL,   bias_constraint = NULL,   dropout = 0,   recurrent_dropout = 0,   seed = NULL,   return_sequences = FALSE,   return_state = FALSE,   go_backwards = FALSE,   stateful = FALSE,   unroll = FALSE,   reset_after = TRUE,   ... )"},{"path":"https://keras.posit.co/reference/layer_gru.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gated Recurrent Unit - Cho et al. 2014. — layer_gru","text":"object Object compose layer . tensor, array, sequential model. units Positive integer, dimensionality output space. activation Activation function use. Default: hyperbolic tangent (tanh). pass NULL, activation applied (ie. \"linear\" activation: (x) = x). recurrent_activation Activation function use recurrent step. Default: sigmoid (sigmoid). pass NULL, activation applied (ie. \"linear\" activation: (x) = x). use_bias Boolean, (default TRUE), whether layer use bias vector. kernel_initializer Initializer kernel weights matrix, used linear transformation inputs. Default: \"glorot_uniform\". recurrent_initializer Initializer recurrent_kernel weights matrix, used linear transformation recurrent state. Default: \"orthogonal\". bias_initializer Initializer bias vector. Default: \"zeros\". kernel_regularizer Regularizer function applied kernel weights matrix. Default: NULL. recurrent_regularizer Regularizer function applied recurrent_kernel weights matrix. Default: NULL. bias_regularizer Regularizer function applied bias vector. Default: NULL. activity_regularizer Regularizer function applied output layer (\"activation\"). Default: NULL. kernel_constraint Constraint function applied kernel weights matrix. Default: NULL. recurrent_constraint Constraint function applied recurrent_kernel weights matrix. Default: NULL. bias_constraint Constraint function applied bias vector. Default: NULL. dropout Float 0 1. Fraction units drop linear transformation inputs. Default: 0. recurrent_dropout Float 0 1. Fraction units drop linear transformation recurrent state. Default: 0. seed Random seed dropout. return_sequences Boolean. Whether return last output output sequence, full sequence. Default: FALSE. return_state Boolean. Whether return last state addition output. Default: FALSE. go_backwards Boolean (default FALSE). TRUE, process input sequence backwards return reversed sequence. stateful Boolean (default: FALSE). TRUE, last state sample index batch used initial state sample index following batch. unroll Boolean (default: FALSE). TRUE, network unrolled, else symbolic loop used. Unrolling can speed-RNN, although tends memory-intensive. Unrolling suitable short sequences. reset_after GRU convention (whether apply reset gate matrix multiplication). FALSE \"\", TRUE \"\" (default cuDNN compatible). ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_gru.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"Gated Recurrent Unit - Cho et al. 2014. — layer_gru","text":"inputs: 3D tensor, shape (batch, timesteps, feature). mask: Binary tensor shape (samples, timesteps) indicating whether given timestep masked  (optional). individual TRUE entry indicates corresponding timestep utilized, FALSE entry indicates corresponding timestep ignored. Defaults NULL. training: Python boolean indicating whether layer behave training mode inference mode. argument passed cell calling . relevant dropout recurrent_dropout used  (optional). Defaults NULL. initial_state: List initial state tensors passed first call cell (optional, NULL causes creation zero-filled initial state tensors). Defaults NULL.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_gru_cell.html","id":null,"dir":"Reference","previous_headings":"","what":"Cell class for the GRU layer. — layer_gru_cell","title":"Cell class for the GRU layer. — layer_gru_cell","text":"class processes one step within whole time sequence input, whereas keras.layer.GRU processes whole sequence.","code":""},{"path":"https://keras.posit.co/reference/layer_gru_cell.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cell class for the GRU layer. — layer_gru_cell","text":"","code":"layer_gru_cell(   units,   activation = \"tanh\",   recurrent_activation = \"sigmoid\",   use_bias = TRUE,   kernel_initializer = \"glorot_uniform\",   recurrent_initializer = \"orthogonal\",   bias_initializer = \"zeros\",   kernel_regularizer = NULL,   recurrent_regularizer = NULL,   bias_regularizer = NULL,   kernel_constraint = NULL,   recurrent_constraint = NULL,   bias_constraint = NULL,   dropout = 0,   recurrent_dropout = 0,   reset_after = TRUE,   seed = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_gru_cell.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cell class for the GRU layer. — layer_gru_cell","text":"units Positive integer, dimensionality output space. activation Activation function use. Default: hyperbolic tangent (tanh). pass NULL, activation applied (ie. \"linear\" activation: (x) = x). recurrent_activation Activation function use recurrent step. Default: sigmoid (sigmoid). pass NULL, activation applied (ie. \"linear\" activation: (x) = x). use_bias Boolean, (default TRUE), whether layer use bias vector. kernel_initializer Initializer kernel weights matrix, used linear transformation inputs. Default: \"glorot_uniform\". recurrent_initializer Initializer recurrent_kernel weights matrix, used linear transformation recurrent state. Default: \"orthogonal\". bias_initializer Initializer bias vector. Default: \"zeros\". kernel_regularizer Regularizer function applied kernel weights matrix. Default: NULL. recurrent_regularizer Regularizer function applied recurrent_kernel weights matrix. Default: NULL. bias_regularizer Regularizer function applied bias vector. Default: NULL. kernel_constraint Constraint function applied kernel weights matrix. Default: NULL. recurrent_constraint Constraint function applied recurrent_kernel weights matrix. Default: NULL. bias_constraint Constraint function applied bias vector. Default: NULL. dropout Float 0 1. Fraction units drop linear transformation inputs. Default: 0. recurrent_dropout Float 0 1. Fraction units drop linear transformation recurrent state. Default: 0. reset_after GRU convention (whether apply reset gate matrix multiplication). FALSE = \"\", TRUE = \"\" (default cuDNN compatible). seed Random seed dropout. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_gru_cell.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"Cell class for the GRU layer. — layer_gru_cell","text":"inputs: 2D tensor, shape (batch, features). states: 2D tensor shape (batch, units), state previous time step. training: Python boolean indicating whether layer behave training mode inference mode. relevant dropout recurrent_dropout used.","code":""},{"path":"https://keras.posit.co/reference/layer_gru_cell.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cell class for the GRU layer. — layer_gru_cell","text":"","code":"inputs <- random_uniform(c(32, 10, 8)) outputs <- inputs |> layer_rnn(layer_gru_cell(4)) shape(outputs) ## shape(32, 4) rnn <- layer_rnn(    cell = layer_gru_cell(4),    return_sequences=TRUE,    return_state=TRUE) c(whole_sequence_output, final_state) %<-% rnn(inputs) shape(whole_sequence_output) ## shape(32, 10, 4) shape(final_state) ## shape(32, 4)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_hashed_crossing.html","id":null,"dir":"Reference","previous_headings":"","what":"A preprocessing layer which crosses features using the ","title":"A preprocessing layer which crosses features using the ","text":"layer performs crosses categorical features using \"hashing trick\". Conceptually, transformation can thought : hash(concatenate(features)) %% num_bins. layer currently performs crosses scalar inputs batches scalar inputs. Valid input shapes (batch_size, 1), (batch_size) (). Note: layer wraps tf.keras.layers.HashedCrossing. used part compiled computation graph model backend TensorFlow. can however used backend running eagerly. can also always used part input preprocessing pipeline backend (outside model ), recommend use layer. Note: layer safe use inside tfdatasets pipeline (independently backend using).","code":""},{"path":"https://keras.posit.co/reference/layer_hashed_crossing.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A preprocessing layer which crosses features using the ","text":"","code":"layer_hashed_crossing(   object,   num_bins,   output_mode = \"int\",   sparse = FALSE,   name = NULL,   dtype = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_hashed_crossing.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A preprocessing layer which crosses features using the ","text":"object Object compose layer . tensor, array, sequential model. num_bins Number hash bins. output_mode Specification output layer. Values can \"int\", \"one_hot\" configuring layer follows: \"int\": Return integer bin indices directly. \"one_hot\": Encodes individual element input array size num_bins, containing 1 input's bin index. Defaults \"int\". sparse Boolean. applicable \"one_hot\" mode valid using TensorFlow backend. TRUE, returns SparseTensor instead dense Tensor. Defaults FALSE. name String, name object dtype datatype (e.g., \"float32\"). ... Keyword arguments construct layer.","code":""},{"path":"https://keras.posit.co/reference/layer_hashed_crossing.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A preprocessing layer which crosses features using the ","text":"Crossing two scalar features.     Crossing one-hotting two scalar features.","code":"feat1 <- c('A', 'B', 'A', 'B', 'A') |> as.array() feat2 <- c(101, 101, 101, 102, 102) |> as.integer() |> as.array() layer <- layer_hashed_crossing(num_bins = 5) layer(list(feat1, feat2)) ## tf.Tensor([1 4 1 1 3], shape=(5), dtype=int64) layer <- layer_hashed_crossing(num_bins = 5, output_mode = 'one_hot') layer(list(feat1, feat2)) ## tf.Tensor( ## [[0. 1. 0. 0. 0.] ##  [0. 0. 0. 0. 1.] ##  [0. 1. 0. 0. 0.] ##  [0. 1. 0. 0. 0.] ##  [0. 0. 0. 1. 0.]], shape=(5, 5), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_hashing.html","id":null,"dir":"Reference","previous_headings":"","what":"A preprocessing layer which hashes and bins categorical features. — layer_hashing","title":"A preprocessing layer which hashes and bins categorical features. — layer_hashing","text":"layer transforms categorical inputs hashed output. element-wise converts ints strings ints fixed range. stable hash function uses tensorflow::ops::Fingerprint produce output consistently across platforms. layer uses FarmHash64 default, provides consistent hashed output across different platforms stable across invocations, regardless device context, mixing input bits thoroughly. want obfuscate hashed output, can also pass random salt argument constructor. case, layer use SipHash64 hash function, salt value serving additional input hash function. Note: layer internally uses TensorFlow. used part compiled computation graph model backend TensorFlow. can however used backend running eagerly. can also always used part input preprocessing pipeline backend (outside model ), recommend use layer. Note: layer safe use inside tf.data pipeline (independently backend using). Example (FarmHash64)     Example (FarmHash64) mask value     Example (SipHash64)     Example (Siphash64 single integer, salt=[133, 133])","code":"layer <- layer_hashing(num_bins = 3) inp <- c('A', 'B', 'C', 'D', 'E') |> array(dim = c(5, 1)) layer(inp) ## tf.Tensor( ## [[1] ##  [0] ##  [1] ##  [1] ##  [2]], shape=(5, 1), dtype=int64) layer <- layer_hashing(num_bins=3, mask_value='') inp <- c('A', 'B', '', 'C', 'D') |> array(dim = c(5, 1)) layer(inp) ## tf.Tensor( ## [[1] ##  [1] ##  [0] ##  [2] ##  [2]], shape=(5, 1), dtype=int64) layer <- layer_hashing(num_bins=3, salt=c(133, 137)) inp <- c('A', 'B', 'C', 'D', 'E') |> array(dim = c(5, 1)) layer(inp) ## tf.Tensor( ## [[1] ##  [2] ##  [1] ##  [0] ##  [2]], shape=(5, 1), dtype=int64) layer <- layer_hashing(num_bins=3, salt=133) inp <- c('A', 'B', 'C', 'D', 'E') |> array(dim = c(5, 1)) layer(inp) ## tf.Tensor( ## [[0] ##  [0] ##  [2] ##  [1] ##  [0]], shape=(5, 1), dtype=int64)"},{"path":"https://keras.posit.co/reference/layer_hashing.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A preprocessing layer which hashes and bins categorical features. — layer_hashing","text":"","code":"layer_hashing(   object,   num_bins,   mask_value = NULL,   salt = NULL,   output_mode = \"int\",   sparse = FALSE,   ... )"},{"path":"https://keras.posit.co/reference/layer_hashing.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A preprocessing layer which hashes and bins categorical features. — layer_hashing","text":"object Object compose layer . tensor, array, sequential model. num_bins Number hash bins. Note includes mask_value bin, effective number bins (num_bins - 1) mask_value set. mask_value value represents masked inputs, mapped index 0. NULL means mask term added hashing start index 0. Defaults NULL. salt single unsigned integer NULL. passed, hash function used SipHash64, values used additional input (known \"salt\" cryptography). non-zero. NULL, uses FarmHash64 hash function. also supports list 2 unsigned integer numbers, see reference paper details. Defaults NULL. output_mode Specification output layer. Values can \"int\", \"one_hot\", \"multi_hot\", \"count\" configuring layer follows: \"int\": Return integer bin indices directly. \"one_hot\": Encodes individual element input array size num_bins, containing 1 input's bin index. last dimension size 1, encode dimension. last dimension size 1, append new dimension encoded output. \"multi_hot\": Encodes sample input single array size num_bins, containing 1 bin index index present sample. Treats last dimension sample dimension, input shape (..., sample_length), output shape (..., num_tokens). \"count\": \"multi_hot\", int array contains count number times bin index appeared sample. Defaults \"int\". sparse Boolean. applicable \"one_hot\", \"multi_hot\", \"count\" output modes. supported TensorFlow backend. TRUE, returns SparseTensor instead dense Tensor. Defaults FALSE. ... Keyword arguments construct layer.","code":""},{"path":"https://keras.posit.co/reference/layer_hashing.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"A preprocessing layer which hashes and bins categorical features. — layer_hashing","text":"single string, list strings, int32 int64 tensor shape (batch_size, ...,).","code":""},{"path":"https://keras.posit.co/reference/layer_hashing.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"A preprocessing layer which hashes and bins categorical features. — layer_hashing","text":"int32 tensor shape (batch_size, ...).","code":""},{"path":"https://keras.posit.co/reference/layer_hashing.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"A preprocessing layer which hashes and bins categorical features. — layer_hashing","text":"SipHash salt","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_identity.html","id":null,"dir":"Reference","previous_headings":"","what":"Identity layer. — layer_identity","title":"Identity layer. — layer_identity","text":"layer used placeholder operation performed. layer just returns inputs argument output.","code":""},{"path":"https://keras.posit.co/reference/layer_identity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Identity layer. — layer_identity","text":"","code":"layer_identity(object, ...)"},{"path":"https://keras.posit.co/reference/layer_identity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Identity layer. — layer_identity","text":"object Object compose layer . tensor, array, sequential model. ... forward/backward compatability.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_input.html","id":null,"dir":"Reference","previous_headings":"","what":"Used to instantiate a Keras tensor. — layer_input","title":"Used to instantiate a Keras tensor. — layer_input","text":"Keras tensor symbolic tensor-like object, augment certain attributes allow us build Keras model just knowing inputs outputs model. instance, , b c Keras tensors, becomes possible : model <- keras_model(input = c(, b), output = c)","code":""},{"path":"https://keras.posit.co/reference/layer_input.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Used to instantiate a Keras tensor. — layer_input","text":"","code":"layer_input(   shape = NULL,   batch_size = NULL,   dtype = NULL,   sparse = NULL,   batch_shape = NULL,   name = NULL,   tensor = NULL )"},{"path":"https://keras.posit.co/reference/layer_input.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Used to instantiate a Keras tensor. — layer_input","text":"shape shape list (list integers NULL objects), including batch size. instance, shape = c(32) indicates expected input batches 32-dimensional vectors. Elements list can NULL NA; NULL/NA elements represent dimensions shape known may vary (e.g. sequence length). batch_size Optional static batch size (integer). dtype data type expected input, string (e.g. \"float32\", \"int32\"...) sparse boolean specifying whether expected input sparse tensors. Note , sparse FALSE, sparse tensors can still passed input - densified default value 0. feature supported TensorFlow backend. Defaults FALSE. batch_shape Shape, including batch dim. name Optional name string layer. unique model (reuse name twice). autogenerated provided. tensor Optional existing tensor wrap Input layer. set, layer use tensor rather creating new placeholder tensor.","code":""},{"path":"https://keras.posit.co/reference/layer_input.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Used to instantiate a Keras tensor. — layer_input","text":"Keras tensor.","code":""},{"path":"https://keras.posit.co/reference/layer_input.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Used to instantiate a Keras tensor. — layer_input","text":"","code":"# This is a logistic regression in Keras input <- layer_input(shape=c(32)) output <- input |> layer_dense(16, activation='softmax') model <- keras_model(input, output)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_integer_lookup.html","id":null,"dir":"Reference","previous_headings":"","what":"A preprocessing layer that maps integers to (possibly encoded) indices. — layer_integer_lookup","title":"A preprocessing layer that maps integers to (possibly encoded) indices. — layer_integer_lookup","text":"layer maps set arbitrary integer input tokens indexed integer output via table-based vocabulary lookup. layer's output indices contiguously arranged maximum vocab size, even input tokens non-continguous unbounded. layer supports multiple options encoding output via output_mode, optional support --vocabulary (OOV) tokens masking. vocabulary layer must either supplied construction learned via adapt(). adapt(), layer analyze data set, determine frequency individual integer tokens, create vocabulary . vocabulary capped size, frequent tokens used create vocabulary others treated OOV. two possible output modes layer.  output_mode \"int\", input integers converted index vocabulary (integer).  output_mode \"multi_hot\", \"count\", \"tf_idf\", input integers encoded array dimension corresponds element vocabulary. vocabulary can optionally contain mask token well OOV token (can optionally occupy multiple indices vocabulary, set num_oov_indices). position tokens vocabulary fixed. output_mode \"int\", vocabulary begin mask token index 0, followed OOV indices, followed rest vocabulary. output_mode \"multi_hot\", \"count\", \"tf_idf\" vocabulary begin OOV indices instances mask token dropped. Note: layer uses TensorFlow internally. used part compiled computation graph model backend TensorFlow. can however used backend running eagerly. can also always used part input preprocessing pipeline backend (outside model ), recommend use layer. Note: layer safe use inside tf.data pipeline (independently backend using).","code":""},{"path":"https://keras.posit.co/reference/layer_integer_lookup.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A preprocessing layer that maps integers to (possibly encoded) indices. — layer_integer_lookup","text":"","code":"layer_integer_lookup(   object,   max_tokens = NULL,   num_oov_indices = 1L,   mask_token = NULL,   oov_token = -1L,   vocabulary = NULL,   vocabulary_dtype = \"int64\",   idf_weights = NULL,   invert = FALSE,   output_mode = \"int\",   sparse = FALSE,   pad_to_max_tokens = FALSE,   name = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_integer_lookup.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A preprocessing layer that maps integers to (possibly encoded) indices. — layer_integer_lookup","text":"object Object compose layer . tensor, array, sequential model. max_tokens Maximum size vocabulary layer. specified adapting vocabulary setting pad_to_max_tokens=TRUE. NULL, cap size vocabulary. Note size includes OOV mask tokens. Defaults NULL. num_oov_indices number --vocabulary tokens use. value 1, OOV inputs modulated determine OOV value. value 0, OOV inputs cause error calling layer. Defaults 1. mask_token integer token represents masked inputs. output_mode \"int\", token included vocabulary mapped index 0. output modes, token appear vocabulary instances mask token input dropped. set NULL, mask term added. Defaults NULL. oov_token used invert TRUE. token return OOV indices. Defaults -1. vocabulary Optional. Either array integers string path text file. passing array, can pass list, list, 1D NumPy array, 1D tensor containing integer vocbulary terms. passing file path, file contain one line per term vocabulary. argument set, need adapt() layer. vocabulary_dtype dtype vocabulary terms, example \"int64\" \"int32\". Defaults \"int64\". idf_weights valid output_mode \"tf_idf\". list, list, 1D NumPy array, 1D tensor length vocabulary, containing floating point inverse document frequency weights, multiplied per sample term counts final TF-IDF weight. vocabulary argument set, output_mode \"tf_idf\", argument must supplied. invert valid output_mode \"int\". TRUE, layer map indices vocabulary items instead mapping vocabulary items indices. Defaults FALSE. output_mode Specification output layer. Values can \"int\", \"one_hot\", \"multi_hot\", \"count\", \"tf_idf\" configuring layer follows: \"int\": Return vocabulary indices input tokens. \"one_hot\": Encodes individual element input array size vocabulary, containing 1 element index. last dimension size 1, encode dimension. last dimension size 1, append new dimension encoded output. \"multi_hot\": Encodes sample input single array size vocabulary, containing 1 vocabulary term present sample. Treats last dimension sample dimension, input shape (..., sample_length), output shape (..., num_tokens). \"count\": \"multi_hot\", int array contains count number times token index appeared sample. \"tf_idf\": \"multi_hot\", TF-IDF algorithm applied find value token slot. \"int\" output, shape input output supported. output modes, currently output rank 2 supported. Defaults \"int\". sparse Boolean. applicable \"multi_hot\", \"count\", \"tf_idf\" output modes. supported TensorFlow backend. TRUE, returns SparseTensor instead dense Tensor. Defaults FALSE. pad_to_max_tokens applicable output_mode \"multi_hot\", \"count\", \"tf_idf\". TRUE, output feature axis padded max_tokens even number unique tokens vocabulary less max_tokens, resulting tensor shape (batch_size, max_tokens) regardless vocabulary size. Defaults FALSE. name String, name object ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_integer_lookup.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A preprocessing layer that maps integers to (possibly encoded) indices. — layer_integer_lookup","text":"Creating lookup layer known vocabulary example creates lookup layer pre-existing vocabulary.     Creating lookup layer adapted vocabulary example creates lookup layer generates vocabulary analyzing dataset.     Note OOV token -1 added vocabulary. remaining tokens sorted frequency (42, 2 occurrences, first) inverse sort order.     Lookups multiple OOV indices example demonstrates use lookup layer multiple OOV indices.  layer created one OOV index, OOV tokens hashed number OOV buckets, distributing OOV tokens deterministic fashion across set.     Note output OOV token 37 1, output OOV token 1000 0. -vocab terms output index increased 1 earlier examples (12 maps 2, etc) order make space extra OOV token. One-hot output Configure layer output_mode='one_hot'. Note first num_oov_indices dimensions ont_hot encoding represent OOV values.     Multi-hot output Configure layer output_mode = 'multi_hot'. Note first num_oov_indices dimensions multi_hot encoding represent OOV tokens     Token count output Configure layer output_mode='count'. multi_hot output, first num_oov_indices dimensions output represent OOV tokens.     TF-IDF output Configure layer output_mode='tf_idf'. multi_hot output, first num_oov_indices dimensions output represent OOV tokens. token bin output token_count * idf_weight, idf weights inverse document frequency weights per token. provided along vocabulary. Note idf_weight OOV tokens default average idf weights passed .     specify idf weights oov tokens, need pass entire vocabularly including leading oov token.     adapting layer \"tf_idf\" mode, input sample considered document, IDF weight per token calculated : log(1 + num_documents / (1 + token_document_count)). Inverse lookup example demonstrates map indices tokens using layer. (can also use adapt() inverse = TRUE, simplicity pass vocab example.)     Note first index correspond oov token default. Forward inverse lookup pairs example demonstrates use vocabulary standard lookup layer create inverse lookup layer.     example, input token 1000 resulted output -1, since 1000 vocabulary - got represented OOV, OOV tokens returned -1 inverse layer. Also, note inverse work, must already set forward layer vocabulary either directly via adapt() calling get_vocabulary().","code":"vocab <- c(12, 36, 1138, 42) |> as.integer() data <- op_array(rbind(c(12, 1138, 42),                       c(42, 1000, 36)))  # Note OOV tokens out <- data |> layer_integer_lookup(vocabulary = vocab) out ## tf.Tensor( ## [[1 3 4] ##  [4 0 2]], shape=(2, 3), dtype=int64) data <- op_array(rbind(c(12, 1138, 42),                       c(42, 1000, 36)))  # Note OOV tokens layer <- layer_integer_lookup() layer |> adapt(data) layer |> get_vocabulary() |> str() ## List of 6 ##  $ : int -1 ##  $ : num 42 ##  $ : num 1138 ##  $ : num 1000 ##  $ : num 36 ##  $ : num 12 layer(data) ## tf.Tensor( ## [[5 2 1] ##  [1 3 4]], shape=(2, 3), dtype=int64) vocab <- c(12, 36, 1138, 42) |> as.integer() data <- op_array(rbind(c(12, 1138, 42),                       c(37, 1000, 36)))  # Note OOV tokens out <- data |>   layer_integer_lookup(vocabulary = vocab,                        num_oov_indices = 2) out ## tf.Tensor( ## [[2 4 5] ##  [1 0 3]], shape=(2, 3), dtype=int64) vocab <- c(12, 36, 1138, 42) |> as.integer() data <- op_array(c(12, 36, 1138, 42, 7), 'int32')  # Note OOV tokens layer <- layer_integer_lookup(vocabulary = vocab,                               output_mode = 'one_hot') layer(data) ## tf.Tensor( ## [[0 1 0 0 0] ##  [0 0 1 0 0] ##  [0 0 0 1 0] ##  [0 0 0 0 1] ##  [1 0 0 0 0]], shape=(5, 5), dtype=int64) vocab <- c(12, 36, 1138, 42) |> as.integer() data <- op_array(rbind(c(12, 1138, 42, 42),                       c(42,    7, 36,  7)), \"int64\")  # Note OOV tokens layer <- layer_integer_lookup(vocabulary = vocab,                               output_mode = 'multi_hot') layer(data) ## tf.Tensor( ## [[0 1 0 1 1] ##  [1 0 1 0 1]], shape=(2, 5), dtype=int64) vocab <- c(12, 36, 1138, 42) |> as.integer() data <- rbind(c(12, 1138, 42, 42),               c(42,    7, 36,  7)) |> op_array(\"int64\") layer <- layer_integer_lookup(vocabulary = vocab,                               output_mode = 'count') layer(data) ## tf.Tensor( ## [[0 1 0 1 2] ##  [2 0 1 0 1]], shape=(2, 5), dtype=int64) vocab <- c(12, 36, 1138, 42) |> as.integer() idf_weights <- c(0.25, 0.75, 0.6, 0.4) data <- rbind(c(12, 1138, 42, 42),               c(42,    7, 36,  7)) |> op_array(\"int64\") layer <- layer_integer_lookup(output_mode = 'tf_idf',                               vocabulary = vocab,                               idf_weights = idf_weights) layer(data) ## tf.Tensor( ## [[0.   0.25 0.   0.6  0.8 ] ##  [1.   0.   0.75 0.   0.4 ]], shape=(2, 5), dtype=float32) vocab <- c(-1, 12, 36, 1138, 42) |> as.integer() idf_weights <- c(0.9, 0.25, 0.75, 0.6, 0.4) data <- rbind(c(12, 1138, 42, 42),               c(42,    7, 36,  7)) |> op_array(\"int64\") layer <- layer_integer_lookup(output_mode = 'tf_idf',                               vocabulary = vocab,                               idf_weights = idf_weights) layer(data) ## tf.Tensor( ## [[0.   0.25 0.   0.6  0.8 ] ##  [1.8  0.   0.75 0.   0.4 ]], shape=(2, 5), dtype=float32) vocab <- c(12, 36, 1138, 42) |> as.integer() data <- op_array(c(1, 3, 4,                   4, 0, 2)) |> op_reshape(c(2,-1)) |> op_cast(\"int32\") layer <- layer_integer_lookup(vocabulary = vocab, invert = TRUE) layer(data) ## tf.Tensor( ## [[  12 1138   42] ##  [  42   -1   36]], shape=(2, 3), dtype=int64) vocab <- c(12, 36, 1138, 42) |> as.integer() data <- op_array(rbind(c(12, 1138, 42), c(42, 1000, 36)), \"int32\") layer <- layer_integer_lookup(vocabulary = vocab) i_layer <- layer_integer_lookup(vocabulary = get_vocabulary(layer),                                 invert = TRUE) int_data <- layer(data) i_layer(int_data) ## tf.Tensor( ## [[  12 1138   42] ##  [  42   -1   36]], shape=(2, 3), dtype=int64)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_lambda.html","id":null,"dir":"Reference","previous_headings":"","what":"Wraps arbitrary expressions as a Layer object. — layer_lambda","title":"Wraps arbitrary expressions as a Layer object. — layer_lambda","text":"layer_lambda() layer exists arbitrary expressions can used Layer constructing Sequential Functional API models. Lambda layers best suited simple operations quick experimentation. advanced use cases, prefer writing new subclasses Layer using new_layer_class().","code":""},{"path":"https://keras.posit.co/reference/layer_lambda.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wraps arbitrary expressions as a Layer object. — layer_lambda","text":"","code":"layer_lambda(   object,   f,   output_shape = NULL,   mask = NULL,   arguments = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_lambda.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wraps arbitrary expressions as a Layer object. — layer_lambda","text":"object Object compose layer . tensor, array, sequential model. f function evaluated. Takes input tensor first argument. output_shape Expected output shape function. argument can usually inferred explicitly provided. Can list function. list, specifies first dimension onward; sample dimension assumed either input: output_shape = c(input_shape[1], output_shape) , input NULL sample dimension also NULL: output_shape = c(NA, output_shape). function, specifies entire shape function input shape: output_shape = f(input_shape). mask Either NULL (indicating masking) callable signature compute_mask layer method, tensor returned output mask regardless input . arguments Optional named list arguments passed function. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_lambda.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Wraps arbitrary expressions as a Layer object. — layer_lambda","text":"","code":"# add a x -> x^2 layer model <- keras_model_sequential() model |> layer_lambda(\\(x) x^2)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_layer_normalization.html","id":null,"dir":"Reference","previous_headings":"","what":"Layer normalization layer (Ba et al., 2016). — layer_layer_normalization","title":"Layer normalization layer (Ba et al., 2016). — layer_layer_normalization","text":"Normalize activations previous layer given example batch independently, rather across batch like Batch Normalization. .e. applies transformation maintains mean activation within example close 0 activation standard deviation close 1. scale center enabled, layer scale normalized outputs broadcasting trainable variable gamma, center outputs broadcasting trainable variable beta. gamma default ones tensor beta default zeros tensor, centering scaling -ops training begun. , scaling centering enabled normalization equations follows: Let intermediate activations mini-batch inputs. sample x batch inputs, compute mean variance sample, normalize value sample (including small factor epsilon numerical stability), finally, transform normalized output gamma beta, learned parameters:   gamma beta span axes inputs specified axis, part inputs' shape must fully defined. example:         Note implementations layer normalization may choose define gamma beta separate set axes axes normalized across. example, Group Normalization (Wu et al. 2018) group size 1 corresponds layer_layer_normalization() normalizes across height, width, channel gamma beta span channel dimension. , layer_layer_normalization() implementation match layer_group_normalization() layer group size set 1.","code":"outputs <- inputs |> apply(1, function(x) {   x_normalized <- (x - mean(x)) /                   sqrt(var(x) + epsilon)   x_normalized * gamma + beta }) layer <- layer_layer_normalization(axis = c(2, 3, 4))  layer(op_ones(c(5, 20, 30, 40))) |> invisible() # build() shape(layer$beta) ## shape(20, 30, 40) shape(layer$gamma) ## shape(20, 30, 40)"},{"path":"https://keras.posit.co/reference/layer_layer_normalization.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Layer normalization layer (Ba et al., 2016). — layer_layer_normalization","text":"","code":"layer_layer_normalization(   object,   axis = -1L,   epsilon = 0.001,   center = TRUE,   scale = TRUE,   rms_scaling = FALSE,   beta_initializer = \"zeros\",   gamma_initializer = \"ones\",   beta_regularizer = NULL,   gamma_regularizer = NULL,   beta_constraint = NULL,   gamma_constraint = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_layer_normalization.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Layer normalization layer (Ba et al., 2016). — layer_layer_normalization","text":"object Object compose layer . tensor, array, sequential model. axis Integer list. axis axes normalize across. Typically, features axis/axes. left-axes typically batch axis/axes. -1 last dimension input. Defaults -1. epsilon Small float added variance avoid dividing zero. Defaults 1e-3. center TRUE, add offset beta normalized tensor. FALSE, beta ignored. Defaults TRUE. scale TRUE, multiply gamma. FALSE, gamma used. next layer linear (also e.g. layer_activation_relu()), can disabled since scaling done next layer. Defaults TRUE. rms_scaling TRUE, center scale ignored, inputs scaled gamma inverse square root square inputs. approximate faster approach avoids ever computing mean input. beta_initializer Initializer beta weight. Defaults zeros. gamma_initializer Initializer gamma weight. Defaults ones. beta_regularizer Optional regularizer beta weight. NULL default. gamma_regularizer Optional regularizer gamma weight. NULL default. beta_constraint Optional constraint beta weight. NULL default. gamma_constraint Optional constraint gamma weight. NULL default. ... Base layer keyword arguments (e.g. name dtype).","code":""},{"path":"https://keras.posit.co/reference/layer_layer_normalization.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Layer normalization layer (Ba et al., 2016). — layer_layer_normalization","text":"Lei Ba et al., 2016.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_lstm.html","id":null,"dir":"Reference","previous_headings":"","what":"Long Short-Term Memory layer - Hochreiter 1997. — layer_lstm","title":"Long Short-Term Memory layer - Hochreiter 1997. — layer_lstm","text":"Based available runtime hardware constraints, layer choose different implementations (cuDNN-based backend-native) maximize performance. GPU available arguments layer meet requirement cuDNN kernel (see details), layer use fast cuDNN implementation using TensorFlow backend. requirements use cuDNN implementation : activation == tanh recurrent_activation == sigmoid dropout == 0 recurrent_dropout == 0 unroll FALSE use_bias TRUE Inputs, use masking, strictly right-padded. Eager execution enabled outermost context. example:","code":"input <- random_uniform(c(32, 10, 8)) output <- input |> layer_lstm(4) shape(output) ## shape(32, 4) lstm <- layer_lstm(units = 4, return_sequences = TRUE, return_state = TRUE) c(whole_seq_output, final_memory_state, final_carry_state) %<-% lstm(input) shape(whole_seq_output) ## shape(32, 10, 4) shape(final_memory_state) ## shape(32, 4) shape(final_carry_state) ## shape(32, 4)"},{"path":"https://keras.posit.co/reference/layer_lstm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Long Short-Term Memory layer - Hochreiter 1997. — layer_lstm","text":"","code":"layer_lstm(   object,   units,   activation = \"tanh\",   recurrent_activation = \"sigmoid\",   use_bias = TRUE,   kernel_initializer = \"glorot_uniform\",   recurrent_initializer = \"orthogonal\",   bias_initializer = \"zeros\",   unit_forget_bias = TRUE,   kernel_regularizer = NULL,   recurrent_regularizer = NULL,   bias_regularizer = NULL,   activity_regularizer = NULL,   kernel_constraint = NULL,   recurrent_constraint = NULL,   bias_constraint = NULL,   dropout = 0,   recurrent_dropout = 0,   seed = NULL,   return_sequences = FALSE,   return_state = FALSE,   go_backwards = FALSE,   stateful = FALSE,   unroll = FALSE,   ... )"},{"path":"https://keras.posit.co/reference/layer_lstm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Long Short-Term Memory layer - Hochreiter 1997. — layer_lstm","text":"object Object compose layer . tensor, array, sequential model. units Positive integer, dimensionality output space. activation Activation function use. Default: hyperbolic tangent (tanh). pass NULL, activation applied (ie. \"linear\" activation: (x) = x). recurrent_activation Activation function use recurrent step. Default: sigmoid (sigmoid). pass NULL, activation applied (ie. \"linear\" activation: (x) = x). use_bias Boolean, (default TRUE), whether layer use bias vector. kernel_initializer Initializer kernel weights matrix, used linear transformation inputs. Default: \"glorot_uniform\". recurrent_initializer Initializer recurrent_kernel weights matrix, used linear transformation recurrent state. Default: \"orthogonal\". bias_initializer Initializer bias vector. Default: \"zeros\". unit_forget_bias Boolean (default TRUE). TRUE, add 1 bias forget gate initialization. Setting TRUE also force bias_initializer=\"zeros\". recommended Jozefowicz et al. kernel_regularizer Regularizer function applied kernel weights matrix. Default: NULL. recurrent_regularizer Regularizer function applied recurrent_kernel weights matrix. Default: NULL. bias_regularizer Regularizer function applied bias vector. Default: NULL. activity_regularizer Regularizer function applied output layer (\"activation\"). Default: NULL. kernel_constraint Constraint function applied kernel weights matrix. Default: NULL. recurrent_constraint Constraint function applied recurrent_kernel weights matrix. Default: NULL. bias_constraint Constraint function applied bias vector. Default: NULL. dropout Float 0 1. Fraction units drop linear transformation inputs. Default: 0. recurrent_dropout Float 0 1. Fraction units drop linear transformation recurrent state. Default: 0. seed Random seed dropout. return_sequences Boolean. Whether return last output output sequence, full sequence. Default: FALSE. return_state Boolean. Whether return last state addition output. Default: FALSE. go_backwards Boolean (default: FALSE). TRUE, process input sequence backwards return reversed sequence. stateful Boolean (default: FALSE). TRUE, last state sample index batch used initial state sample index following batch. unroll Boolean (default FALSE). TRUE, network unrolled, else symbolic loop used. Unrolling can speed-RNN, although tends memory-intensive. Unrolling suitable short sequences. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_lstm.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"Long Short-Term Memory layer - Hochreiter 1997. — layer_lstm","text":"inputs: 3D tensor, shape (batch, timesteps, feature). mask: Binary tensor shape (samples, timesteps) indicating whether given timestep masked  (optional). individual TRUE entry indicates corresponding timestep utilized, FALSE entry indicates corresponding timestep ignored. Defaults NULL. training: Boolean indicating whether layer behave training mode inference mode. argument passed cell calling . relevant dropout recurrent_dropout used  (optional). Defaults NULL. initial_state: List initial state tensors passed first call cell (optional, NULL causes creation zero-filled initial state tensors). Defaults NULL.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_lstm_cell.html","id":null,"dir":"Reference","previous_headings":"","what":"Cell class for the LSTM layer. — layer_lstm_cell","title":"Cell class for the LSTM layer. — layer_lstm_cell","text":"class processes one step within whole time sequence input, whereas keras.layer.LSTM processes whole sequence.","code":""},{"path":"https://keras.posit.co/reference/layer_lstm_cell.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cell class for the LSTM layer. — layer_lstm_cell","text":"","code":"layer_lstm_cell(   units,   activation = \"tanh\",   recurrent_activation = \"sigmoid\",   use_bias = TRUE,   kernel_initializer = \"glorot_uniform\",   recurrent_initializer = \"orthogonal\",   bias_initializer = \"zeros\",   unit_forget_bias = TRUE,   kernel_regularizer = NULL,   recurrent_regularizer = NULL,   bias_regularizer = NULL,   kernel_constraint = NULL,   recurrent_constraint = NULL,   bias_constraint = NULL,   dropout = 0,   recurrent_dropout = 0,   seed = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_lstm_cell.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cell class for the LSTM layer. — layer_lstm_cell","text":"units Positive integer, dimensionality output space. activation Activation function use. Default: hyperbolic tangent (tanh). pass NULL, activation applied (ie. \"linear\" activation: (x) = x). recurrent_activation Activation function use recurrent step. Default: sigmoid (sigmoid). pass NULL, activation applied (ie. \"linear\" activation: (x) = x). use_bias Boolean, (default TRUE), whether layer use bias vector. kernel_initializer Initializer kernel weights matrix, used linear transformation inputs. Default: \"glorot_uniform\". recurrent_initializer Initializer recurrent_kernel weights matrix, used linear transformation recurrent state. Default: \"orthogonal\". bias_initializer Initializer bias vector. Default: \"zeros\". unit_forget_bias Boolean (default TRUE). TRUE, add 1 bias forget gate initialization. Setting TRUE also force bias_initializer=\"zeros\". recommended Jozefowicz et al. kernel_regularizer Regularizer function applied kernel weights matrix. Default: NULL. recurrent_regularizer Regularizer function applied recurrent_kernel weights matrix. Default: NULL. bias_regularizer Regularizer function applied bias vector. Default: NULL. kernel_constraint Constraint function applied kernel weights matrix. Default: NULL. recurrent_constraint Constraint function applied recurrent_kernel weights matrix. Default: NULL. bias_constraint Constraint function applied bias vector. Default: NULL. dropout Float 0 1. Fraction units drop linear transformation inputs. Default: 0. recurrent_dropout Float 0 1. Fraction units drop linear transformation recurrent state. Default: 0. seed Random seed dropout. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_lstm_cell.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"Cell class for the LSTM layer. — layer_lstm_cell","text":"inputs: 2D tensor, shape (batch, features). states: 2D tensor shape (batch, units), state previous time step. training: Boolean indicating whether layer behave training mode inference mode. relevant dropout recurrent_dropout used.","code":""},{"path":"https://keras.posit.co/reference/layer_lstm_cell.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cell class for the LSTM layer. — layer_lstm_cell","text":"","code":"inputs <- random_uniform(c(32, 10, 8)) output <- inputs |>   layer_rnn(cell = layer_lstm_cell(4)) shape(output) ## shape(32, 4) rnn <- layer_rnn(cell = layer_lstm_cell(4),                  return_sequences = T,                  return_state = T) c(whole_sequence_output, ...final_state) %<-% rnn(inputs) str(whole_sequence_output) ## <tf.Tensor: shape=(32, 10, 4), dtype=float32, numpy=…> str(final_state) ## List of 2 ##  $ :<tf.Tensor: shape=(32, 4), dtype=float32, numpy=…> ##  $ :<tf.Tensor: shape=(32, 4), dtype=float32, numpy=…>"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_masking.html","id":null,"dir":"Reference","previous_headings":"","what":"Masks a sequence by using a mask value to skip timesteps. — layer_masking","title":"Masks a sequence by using a mask value to skip timesteps. — layer_masking","text":"timestep input tensor (dimension #1 tensor), values input tensor timestep equal mask_value, timestep masked (skipped) downstream layers (long support masking). downstream layer support masking yet receives input mask, exception raised.","code":""},{"path":"https://keras.posit.co/reference/layer_masking.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Masks a sequence by using a mask value to skip timesteps. — layer_masking","text":"","code":"layer_masking(object, mask_value = 0, ...)"},{"path":"https://keras.posit.co/reference/layer_masking.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Masks a sequence by using a mask value to skip timesteps. — layer_masking","text":"object Object compose layer . tensor, array, sequential model. mask_value see description ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_masking.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Masks a sequence by using a mask value to skip timesteps. — layer_masking","text":"Consider array x shape c(samples, timesteps, features), fed LSTM layer. want mask timestep #3 #5 lack data timesteps. can: Set x[, 3, ] <- 0. x[, 5, ] <- 0. Insert layer_masking() layer mask_value = 0. LSTM layer:","code":"c(samples, timesteps, features) %<-% c(32, 10, 8) inputs <- c(samples, timesteps, features) %>% { array(runif(prod(.)), dim = .) } inputs[, 3, ] <- 0 inputs[, 5, ] <- 0  model <- keras_model_sequential() %>%   layer_masking(mask_value = 0) %>%   layer_lstm(32)  output <- model(inputs) # The time step 3 and 5 will be skipped from LSTM calculation."},{"path":"https://keras.posit.co/reference/layer_masking.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Masks a sequence by using a mask value to skip timesteps. — layer_masking","text":"Keras masking convention, masked timestep denoted mask value FALSE, non-masked (.e. usable) timestep denoted mask value TRUE.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_max_pooling_1d.html","id":null,"dir":"Reference","previous_headings":"","what":"Max pooling operation for 1D temporal data. — layer_max_pooling_1d","title":"Max pooling operation for 1D temporal data. — layer_max_pooling_1d","text":"Downsamples input representation taking maximum value spatial window size pool_size. window shifted strides. resulting output using \"valid\" padding option shape : output_shape = (input_shape - pool_size + 1) / strides). resulting output shape using \"\" padding option : output_shape = input_shape / strides","code":""},{"path":"https://keras.posit.co/reference/layer_max_pooling_1d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Max pooling operation for 1D temporal data. — layer_max_pooling_1d","text":"","code":"layer_max_pooling_1d(   object,   pool_size = 2L,   strides = NULL,   padding = \"valid\",   data_format = NULL,   name = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_max_pooling_1d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Max pooling operation for 1D temporal data. — layer_max_pooling_1d","text":"object Object compose layer . tensor, array, sequential model. pool_size int, size max pooling window. strides int NULL. Specifies much pooling window moves pooling step. NULL, default pool_size. padding string, either \"valid\" \"\" (case-insensitive). \"valid\" means padding. \"\" results padding evenly left/right /input output height/width dimension input. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, steps, features) \"channels_first\" corresponds inputs shape (batch, features, steps). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". name String, name object ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_max_pooling_1d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Max pooling operation for 1D temporal data. — layer_max_pooling_1d","text":"data_format=\"channels_last\": 3D tensor shape (batch_size, steps, features). data_format=\"channels_first\": 3D tensor shape (batch_size, features, steps).","code":""},{"path":"https://keras.posit.co/reference/layer_max_pooling_1d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Max pooling operation for 1D temporal data. — layer_max_pooling_1d","text":"data_format=\"channels_last\": 3D tensor shape (batch_size, downsampled_steps, features). data_format=\"channels_first\": 3D tensor shape (batch_size, features, downsampled_steps).","code":""},{"path":"https://keras.posit.co/reference/layer_max_pooling_1d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Max pooling operation for 1D temporal data. — layer_max_pooling_1d","text":"strides=1 padding=\"valid\":     strides=2 padding=\"valid\":     strides=1 padding=\"\":","code":"x <- op_reshape(c(1, 2, 3, 4, 5),                c(1, 5, 1)) max_pool_1d <- layer_max_pooling_1d(pool_size = 2,                                     strides = 1,                                     padding = \"valid\") max_pool_1d(x) ## tf.Tensor( ## [[[2.] ##   [3.] ##   [4.] ##   [5.]]], shape=(1, 4, 1), dtype=float32) x <- op_reshape(c(1, 2, 3, 4, 5),                c(1, 5, 1)) max_pool_1d <- layer_max_pooling_1d(pool_size = 2,                                     strides = 2,                                     padding = \"valid\") max_pool_1d(x) ## tf.Tensor( ## [[[2.] ##   [4.]]], shape=(1, 2, 1), dtype=float32) x <- op_reshape(c(1, 2, 3, 4, 5),                c(1, 5, 1)) max_pool_1d <- layer_max_pooling_1d(pool_size = 2,                                     strides = 1,                                     padding = \"same\") max_pool_1d(x) ## tf.Tensor( ## [[[2.] ##   [3.] ##   [4.] ##   [5.] ##   [5.]]], shape=(1, 5, 1), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_max_pooling_2d.html","id":null,"dir":"Reference","previous_headings":"","what":"Max pooling operation for 2D spatial data. — layer_max_pooling_2d","title":"Max pooling operation for 2D spatial data. — layer_max_pooling_2d","text":"Downsamples input along spatial dimensions (height width) taking maximum value input window (size defined pool_size) channel input. window shifted strides along dimension. resulting output using \"valid\" padding option spatial shape (number rows columns) : output_shape = floor((input_shape - pool_size) / strides) + 1 (input_shape >= pool_size) resulting output shape using \"\" padding option : output_shape = floor((input_shape - 1) / strides) + 1","code":""},{"path":"https://keras.posit.co/reference/layer_max_pooling_2d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Max pooling operation for 2D spatial data. — layer_max_pooling_2d","text":"","code":"layer_max_pooling_2d(   object,   pool_size = list(2L, 2L),   strides = NULL,   padding = \"valid\",   data_format = NULL,   name = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_max_pooling_2d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Max pooling operation for 2D spatial data. — layer_max_pooling_2d","text":"object Object compose layer . tensor, array, sequential model. pool_size int list 2 integers, factors downscale (dim1, dim2). one integer specified, window length used dimensions. strides int list 2 integers, NULL. Strides values. NULL, default pool_size. one int specified, stride size used dimensions. padding string, either \"valid\" \"\" (case-insensitive). \"valid\" means padding. \"\" results padding evenly left/right /input output height/width dimension input. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, height, width, channels) \"channels_first\" corresponds inputs shape (batch, channels, height, width). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". name String, name object ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_max_pooling_2d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Max pooling operation for 2D spatial data. — layer_max_pooling_2d","text":"data_format=\"channels_last\": 4D tensor shape (batch_size, height, width, channels). data_format=\"channels_first\": 4D tensor shape (batch_size, channels, height, width).","code":""},{"path":"https://keras.posit.co/reference/layer_max_pooling_2d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Max pooling operation for 2D spatial data. — layer_max_pooling_2d","text":"data_format=\"channels_last\": 4D tensor shape (batch_size, pooled_height, pooled_width, channels). data_format=\"channels_first\": 4D tensor shape (batch_size, channels, pooled_height, pooled_width).","code":""},{"path":"https://keras.posit.co/reference/layer_max_pooling_2d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Max pooling operation for 2D spatial data. — layer_max_pooling_2d","text":"strides = (1, 1) padding = \"valid\":     strides = c(2, 2) padding = \"valid\":     stride = (1, 1) padding = \"\":","code":"x <- rbind(c(1., 2., 3.),            c(4., 5., 6.),            c(7., 8., 9.)) |> op_reshape(c(1, 3, 3, 1)) max_pool_2d <- layer_max_pooling_2d(pool_size = c(2, 2),                                     strides = c(1, 1),                                     padding = \"valid\") max_pool_2d(x) ## tf.Tensor( ## [[[[5.] ##    [6.]] ## ##   [[8.] ##    [9.]]]], shape=(1, 2, 2, 1), dtype=float32) x <- rbind(c(1., 2., 3., 4.),            c(5., 6., 7., 8.),            c(9., 10., 11., 12.)) |> op_reshape(c(1, 3, 4, 1)) max_pool_2d <- layer_max_pooling_2d(pool_size = c(2, 2),                                     strides = c(2, 2),                                     padding = \"valid\") max_pool_2d(x) ## tf.Tensor( ## [[[[6.] ##    [8.]]]], shape=(1, 1, 2, 1), dtype=float32) x <- rbind(c(1., 2., 3.),            c(4., 5., 6.),            c(7., 8., 9.)) |> op_reshape(c(1, 3, 3, 1)) max_pool_2d <- layer_max_pooling_2d(pool_size = c(2, 2),                                     strides = c(1, 1),                                     padding = \"same\") max_pool_2d(x) ## tf.Tensor( ## [[[[5.] ##    [6.] ##    [6.]] ## ##   [[8.] ##    [9.] ##    [9.]] ## ##   [[8.] ##    [9.] ##    [9.]]]], shape=(1, 3, 3, 1), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_max_pooling_3d.html","id":null,"dir":"Reference","previous_headings":"","what":"Max pooling operation for 3D data (spatial or spatio-temporal). — layer_max_pooling_3d","title":"Max pooling operation for 3D data (spatial or spatio-temporal). — layer_max_pooling_3d","text":"Downsamples input along spatial dimensions (depth, height, width) taking maximum value input window (size defined pool_size) channel input. window shifted strides along dimension.","code":""},{"path":"https://keras.posit.co/reference/layer_max_pooling_3d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Max pooling operation for 3D data (spatial or spatio-temporal). — layer_max_pooling_3d","text":"","code":"layer_max_pooling_3d(   object,   pool_size = list(2L, 2L, 2L),   strides = NULL,   padding = \"valid\",   data_format = NULL,   name = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_max_pooling_3d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Max pooling operation for 3D data (spatial or spatio-temporal). — layer_max_pooling_3d","text":"object Object compose layer . tensor, array, sequential model. pool_size int list 3 integers, factors downscale (dim1, dim2, dim3). one integer specified, window length used dimensions. strides int list 3 integers, NULL. Strides values. NULL, default pool_size. one int specified, stride size used dimensions. padding string, either \"valid\" \"\" (case-insensitive). \"valid\" means padding. \"\" results padding evenly left/right /input output height/width dimension input. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) \"channels_first\" corresponds inputs shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". name String, name object ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_max_pooling_3d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Max pooling operation for 3D data (spatial or spatio-temporal). — layer_max_pooling_3d","text":"data_format=\"channels_last\": 5D tensor shape: (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) data_format=\"channels_first\": 5D tensor shape: (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)","code":""},{"path":"https://keras.posit.co/reference/layer_max_pooling_3d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Max pooling operation for 3D data (spatial or spatio-temporal). — layer_max_pooling_3d","text":"data_format=\"channels_last\": 5D tensor shape: (batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels) data_format=\"channels_first\": 5D tensor shape: (batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3)","code":""},{"path":"https://keras.posit.co/reference/layer_max_pooling_3d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Max pooling operation for 3D data (spatial or spatio-temporal). — layer_max_pooling_3d","text":"","code":"depth <- 30 height <- 30 width <- 30 channels <- 3  inputs <- layer_input(shape=c(depth, height, width, channels)) layer <- layer_max_pooling_3d(pool_size=3) outputs <- inputs |> layer() outputs ## <KerasTensor shape=(None, 10, 10, 10, 3), dtype=float32, sparse=False, name=keras_tensor_1>"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_maximum.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes element-wise maximum on a list of inputs. — layer_maximum","title":"Computes element-wise maximum on a list of inputs. — layer_maximum","text":"takes input list tensors, shape, returns single tensor (also shape).","code":""},{"path":"https://keras.posit.co/reference/layer_maximum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes element-wise maximum on a list of inputs. — layer_maximum","text":"","code":"layer_maximum(inputs, ...)"},{"path":"https://keras.posit.co/reference/layer_maximum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes element-wise maximum on a list of inputs. — layer_maximum","text":"inputs layers combine ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_maximum.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes element-wise maximum on a list of inputs. — layer_maximum","text":"Usage Keras model:","code":"input_shape <- c(2, 3, 4) x1 <- random_uniform(input_shape) x2 <- random_uniform(input_shape) y <- layer_maximum(x1, x2) input1 <- layer_input(shape = c(16)) x1 <- input1 |> layer_dense(8, activation = 'relu') input2 <- layer_input(shape = c(32)) x2 <- input2 |> layer_dense(8, activation = 'relu') # equivalent to `y <- layer_maximum(x1, x2)` y <- layer_maximum(x1, x2) out <- y |> layer_dense(4) model <- keras_model(inputs = c(input1, input2), outputs = out)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_minimum.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes elementwise minimum on a list of inputs. — layer_minimum","title":"Computes elementwise minimum on a list of inputs. — layer_minimum","text":"takes input list tensors, shape, returns single tensor (also shape).","code":""},{"path":"https://keras.posit.co/reference/layer_minimum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes elementwise minimum on a list of inputs. — layer_minimum","text":"","code":"layer_minimum(inputs, ...)"},{"path":"https://keras.posit.co/reference/layer_minimum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes elementwise minimum on a list of inputs. — layer_minimum","text":"inputs layers combine ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_minimum.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes elementwise minimum on a list of inputs. — layer_minimum","text":"Usage Keras model:","code":"input_shape <- c(2, 3, 4) x1 <- random_uniform(input_shape) x2 <- random_uniform(input_shape) y <- layer_minimum(x1, x2) input1 <- layer_input(shape = c(16)) x1 <- input1 |> layer_dense(8, activation = 'relu') input2 <- layer_input(shape = c(32)) x2 <- input2 |> layer_dense(8, activation = 'relu') # equivalent to `y <- layer_minimum(x1, x2)` y <- layer_minimum(x1, x2) out <- y |> layer_dense(4) model <- keras_model(inputs = c(input1, input2), outputs = out)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_multi_head_attention.html","id":null,"dir":"Reference","previous_headings":"","what":"MultiHeadAttention layer. — layer_multi_head_attention","title":"MultiHeadAttention layer. — layer_multi_head_attention","text":"implementation multi-headed attention described paper \"Attention Need\" Vaswani et al., 2017. query, key, value , self-attention. timestep query attends corresponding sequence key, returns fixed-width vector. layer first projects query, key value. (effectively) list tensors length num_attention_heads, corresponding shapes (batch_size, <query dimensions>, key_dim), (batch_size, <key/value dimensions>, key_dim), (batch_size, <key/value dimensions>, value_dim). , query key tensors dot-producted scaled. softmaxed obtain attention probabilities. value tensors interpolated probabilities, concatenated back single tensor. Finally, result tensor last dimension value_dim can take linear projection return.","code":""},{"path":"https://keras.posit.co/reference/layer_multi_head_attention.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"MultiHeadAttention layer. — layer_multi_head_attention","text":"","code":"layer_multi_head_attention(   inputs,   num_heads,   key_dim,   value_dim = NULL,   dropout = 0,   use_bias = TRUE,   output_shape = NULL,   attention_axes = NULL,   kernel_initializer = \"glorot_uniform\",   bias_initializer = \"zeros\",   kernel_regularizer = NULL,   bias_regularizer = NULL,   activity_regularizer = NULL,   kernel_constraint = NULL,   bias_constraint = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_multi_head_attention.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"MultiHeadAttention layer. — layer_multi_head_attention","text":"inputs see description num_heads Number attention heads. key_dim Size attention head query key. value_dim Size attention head value. dropout Dropout probability. use_bias Boolean, whether dense layers use bias vectors/matrices. output_shape expected shape output tensor, besides batch sequence dims. specified, projects back query feature dim (query input's last dimension). attention_axes axes attention applied. NULL means attention axes, batch, heads, features. kernel_initializer Initializer dense layer kernels. bias_initializer Initializer dense layer biases. kernel_regularizer Regularizer dense layer kernels. bias_regularizer Regularizer dense layer biases. activity_regularizer Regularizer dense layer activity. kernel_constraint Constraint dense layer kernels. bias_constraint Constraint dense layer kernels. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_multi_head_attention.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"MultiHeadAttention layer. — layer_multi_head_attention","text":"attention_output: result computation, shape (B, T, E), T target sequence shapes E query input last dimension output_shape NULL. Otherwise, multi-head outputs projected shape specified output_shape. attention_scores: (Optional) multi-head attention coefficients attention axes.","code":""},{"path":"https://keras.posit.co/reference/layer_multi_head_attention.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"MultiHeadAttention layer. — layer_multi_head_attention","text":"query: Query tensor shape (B, T, dim), B batch size, T target sequence length, dim feature dimension. value: Value tensor shape (B, S, dim), B batch size, S source sequence length, dim feature dimension. key: Optional key tensor shape (B, S, dim). given, use value key value, common case. attention_mask: boolean mask shape (B, T, S), prevents attention certain positions. boolean mask specifies query elements can attend key elements, 1 indicates attention 0 indicates attention. Broadcasting can happen missing batch dimensions head dimension. return_attention_scores: boolean indicate whether output (attention_output, attention_scores) TRUE, attention_output FALSE. Defaults FALSE. training: Python boolean indicating whether layer behave training mode (adding dropout) inference mode (dropout). go either using training mode parent layer/model, FALSE (inference) parent layer. use_causal_mask: boolean indicate whether apply causal mask prevent tokens attending future tokens (e.g., used decoder Transformer).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_multiply.html","id":null,"dir":"Reference","previous_headings":"","what":"Performs elementwise multiplication. — layer_multiply","title":"Performs elementwise multiplication. — layer_multiply","text":"takes input list tensors, shape, returns single tensor (also shape).","code":""},{"path":"https://keras.posit.co/reference/layer_multiply.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Performs elementwise multiplication. — layer_multiply","text":"","code":"layer_multiply(inputs, ...)"},{"path":"https://keras.posit.co/reference/layer_multiply.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Performs elementwise multiplication. — layer_multiply","text":"inputs layers combine ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_multiply.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Performs elementwise multiplication. — layer_multiply","text":"Usage Keras model:","code":"input_shape <- c(2, 3, 4) x1 <- random_uniform(input_shape) x2 <- random_uniform(input_shape) y <- layer_multiply(x1, x2) input1 <- layer_input(shape = c(16)) x1 <- input1 |> layer_dense(8, activation = 'relu') input2 <- layer_input(shape = c(32)) x2 <- input2 |> layer_dense(8, activation = 'relu') # equivalent to `y <- layer_multiply(x1, x2)` y <- layer_multiply(x1, x2) out <- y |> layer_dense(4) model <- keras_model(inputs = c(input1, input2), outputs = out)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_normalization.html","id":null,"dir":"Reference","previous_headings":"","what":"A preprocessing layer that normalizes continuous features. — layer_normalization","title":"A preprocessing layer that normalizes continuous features. — layer_normalization","text":"layer shift scale inputs distribution centered around 0 standard deviation 1. accomplishes precomputing mean variance data, calling (input - mean) / sqrt(var) runtime. mean variance values layer must either supplied construction learned via adapt(). adapt() compute mean variance data store layer's weights. adapt() called fit(), evaluate(), predict().","code":""},{"path":"https://keras.posit.co/reference/layer_normalization.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A preprocessing layer that normalizes continuous features. — layer_normalization","text":"","code":"layer_normalization(   object,   axis = -1L,   mean = NULL,   variance = NULL,   invert = FALSE,   ... )"},{"path":"https://keras.posit.co/reference/layer_normalization.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A preprocessing layer that normalizes continuous features. — layer_normalization","text":"object Object compose layer . tensor, array, sequential model. axis Integer, list integers, NULL. axis axes separate mean variance index shape. example, shape (NULL, 5) axis=1, layer track 5 separate mean variance values last axis. axis set NULL, layer normalize elements input scalar mean variance. -1, last axis input assumed feature dimension normalized per index. Note specific case batched scalar inputs axis batch axis, default normalize index batch separately. case, consider passing axis=NULL. Defaults -1. mean mean value(s) use normalization. passed value(s) broadcast shape kept axes ; value(s) broadcast, error raised layer's build() method called. variance variance value(s) use normalization. passed value(s) broadcast shape kept axes ; value(s) broadcast, error raised layer's build() method called. invert TRUE, layer apply inverse transformation inputs: turn normalized input back original form. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_normalization.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A preprocessing layer that normalizes continuous features. — layer_normalization","text":"Calculate global mean variance analyzing dataset adapt().     Calculate mean variance index last axis.     Pass mean variance directly.     Use layer de-normalize inputs (adapting layer).","code":"adapt_data <- op_array(c(1., 2., 3., 4., 5.), dtype='float32') input_data <- op_array(c(1., 2., 3.), dtype='float32') layer <- layer_normalization(axis = NULL) layer %>% adapt(adapt_data) layer(input_data) ## tf.Tensor([-1.4142135  -0.70710677  0.        ], shape=(3), dtype=float32) adapt_data <- op_array(rbind(c(0., 7., 4.),                        c(2., 9., 6.),                        c(0., 7., 4.),                        c(2., 9., 6.)), dtype='float32') input_data <- op_array(matrix(c(0., 7., 4.), nrow = 1), dtype='float32') layer <- layer_normalization(axis=-1) layer %>% adapt(adapt_data) layer(input_data) ## tf.Tensor([[-1. -1. -1.]], shape=(1, 3), dtype=float32) input_data <- op_array(rbind(1, 2, 3), dtype='float32') layer <- layer_normalization(mean=3., variance=2.) layer(input_data) ## tf.Tensor( ## [[-1.4142135 ] ##  [-0.70710677] ##  [ 0.        ]], shape=(3, 1), dtype=float32) adapt_data <- op_array(rbind(c(0., 7., 4.),                        c(2., 9., 6.),                        c(0., 7., 4.),                        c(2., 9., 6.)), dtype='float32') input_data <- op_array(c(1., 2., 3.), dtype='float32') layer <- layer_normalization(axis=-1, invert=TRUE) layer %>% adapt(adapt_data) layer(input_data) ## tf.Tensor([[ 2. 10.  8.]], shape=(1, 3), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_permute.html","id":null,"dir":"Reference","previous_headings":"","what":"Permutes the dimensions of the input according to a given pattern. — layer_permute","title":"Permutes the dimensions of the input according to a given pattern. — layer_permute","text":"Useful e.g. connecting RNNs convnets.","code":""},{"path":"https://keras.posit.co/reference/layer_permute.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Permutes the dimensions of the input according to a given pattern. — layer_permute","text":"","code":"layer_permute(object, dims, ...)"},{"path":"https://keras.posit.co/reference/layer_permute.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Permutes the dimensions of the input according to a given pattern. — layer_permute","text":"object Object compose layer . tensor, array, sequential model. dims List integers. Permutation pattern include batch dimension. Indexing starts 1. instance, c(2, 1) permutes first second dimensions input. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_permute.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Permutes the dimensions of the input according to a given pattern. — layer_permute","text":"Arbitrary.","code":""},{"path":"https://keras.posit.co/reference/layer_permute.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Permutes the dimensions of the input according to a given pattern. — layer_permute","text":"input shape, dimensions re-ordered according specified pattern.","code":""},{"path":"https://keras.posit.co/reference/layer_permute.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Permutes the dimensions of the input according to a given pattern. — layer_permute","text":"","code":"x <- layer_input(shape=c(10, 64)) y <- layer_permute(x, c(2, 1)) shape(y) ## shape(NA, 64, 10)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_random_brightness.html","id":null,"dir":"Reference","previous_headings":"","what":"A preprocessing layer which randomly adjusts brightness during training. — layer_random_brightness","title":"A preprocessing layer which randomly adjusts brightness during training. — layer_random_brightness","text":"layer randomly increase/reduce brightness input RGB images. inference time, output identical input. Call layer training=TRUE adjust brightness input. Note: layer safe use inside tf.data pipeline (independently backend using).","code":""},{"path":"https://keras.posit.co/reference/layer_random_brightness.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A preprocessing layer which randomly adjusts brightness during training. — layer_random_brightness","text":"","code":"layer_random_brightness(   object,   factor,   value_range = list(0L, 255L),   seed = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_random_brightness.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A preprocessing layer which randomly adjusts brightness during training. — layer_random_brightness","text":"object Object compose layer . tensor, array, sequential model. factor Float list 2 floats -1.0 1.0. factor used determine lower bound upper bound brightness adjustment. float value chosen randomly limits. -1.0 chosen, output image black, 1.0 chosen, image fully white. one float provided, eg, 0.2, -0.2 used lower bound 0.2 used upper bound. value_range Optional list 2 floats lower upper limit values input data. make change, use c(0.0, 1.0), e.g., image input scaled layer. Defaults c(0.0, 255.0). brightness adjustment scaled range, output values clipped range. seed optional integer, fixed RNG behavior. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_random_brightness.html","id":"inputs","dir":"Reference","previous_headings":"","what":"Inputs","title":"A preprocessing layer which randomly adjusts brightness during training. — layer_random_brightness","text":"3D (HWC) 4D (NHWC) tensor, float int dtype. Input pixel values can range (e.g. [0., 1.) [0, 255])","code":""},{"path":"https://keras.posit.co/reference/layer_random_brightness.html","id":"output","dir":"Reference","previous_headings":"","what":"Output","title":"A preprocessing layer which randomly adjusts brightness during training. — layer_random_brightness","text":"3D (HWC) 4D (NHWC) tensor brightness adjusted based factor. default, layer output floats. output value clipped range [0, 255], valid range RGB colors, rescaled based value_range needed. Sample usage:","code":"random_bright <- layer_random_brightness(factor=0.2, seed = 1)  # An image with shape [2, 2, 3] image <- array(1:12, dim=c(2, 2, 3))  # Assume we randomly select the factor to be 0.1, then it will apply # 0.1 * 255 to all the channel output <- random_bright(image, training=TRUE) output ## tf.Tensor( ## [[[39.605797 43.605797 47.605797] ##   [41.605797 45.605797 49.605797]] ## ##  [[40.605797 44.605797 48.605797] ##   [42.605797 46.605797 50.605797]]], shape=(2, 2, 3), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_random_contrast.html","id":null,"dir":"Reference","previous_headings":"","what":"A preprocessing layer which randomly adjusts contrast during training. — layer_random_contrast","title":"A preprocessing layer which randomly adjusts contrast during training. — layer_random_contrast","text":"layer randomly adjust contrast image images random factor. Contrast adjusted independently channel image training. channel, layer computes mean image pixels channel adjusts component x pixel (x - mean) * contrast_factor + mean. Input pixel values can range (e.g. [0., 1.) [0, 255]) integer floating point dtype. default, layer output floats. Note: layer safe use inside tf.data pipeline (independently backend using).","code":""},{"path":"https://keras.posit.co/reference/layer_random_contrast.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A preprocessing layer which randomly adjusts contrast during training. — layer_random_contrast","text":"","code":"layer_random_contrast(object, factor, seed = NULL, ...)"},{"path":"https://keras.posit.co/reference/layer_random_contrast.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A preprocessing layer which randomly adjusts contrast during training. — layer_random_contrast","text":"object Object compose layer . tensor, array, sequential model. factor positive float represented fraction value, tuple size 2 representing lower upper bound. represented single float, lower = upper. contrast factor randomly picked [1.0 - lower, 1.0 + upper]. pixel x channel, output (x - mean) * factor + mean mean mean value channel. seed Integer. Used create random seed. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_random_contrast.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"A preprocessing layer which randomly adjusts contrast during training. — layer_random_contrast","text":"3D (unbatched) 4D (batched) tensor shape: (..., height, width, channels), \"channels_last\" format.","code":""},{"path":"https://keras.posit.co/reference/layer_random_contrast.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"A preprocessing layer which randomly adjusts contrast during training. — layer_random_contrast","text":"3D (unbatched) 4D (batched) tensor shape: (..., height, width, channels), \"channels_last\" format.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_random_crop.html","id":null,"dir":"Reference","previous_headings":"","what":"A preprocessing layer which randomly crops images during training. — layer_random_crop","title":"A preprocessing layer which randomly crops images during training. — layer_random_crop","text":"training, layer randomly choose location crop images target size. layer crop images batch cropping location. inference time, training input image smaller target size, input resized cropped return largest possible window image matches target aspect ratio. need apply random cropping inference time, set training TRUE calling layer. Input pixel values can range (e.g. [0., 1.) [0, 255]) integer floating point dtype. default, layer output floats. Note: layer safe use inside tf.data pipeline (independently backend using).","code":""},{"path":"https://keras.posit.co/reference/layer_random_crop.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A preprocessing layer which randomly crops images during training. — layer_random_crop","text":"","code":"layer_random_crop(   object,   height,   width,   seed = NULL,   data_format = NULL,   name = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_random_crop.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A preprocessing layer which randomly crops images during training. — layer_random_crop","text":"object Object compose layer . tensor, array, sequential model. height Integer, height output shape. width Integer, width output shape. seed Integer. Used create random seed. data_format see description name String, name object ... Base layer keyword arguments, name dtype.","code":""},{"path":"https://keras.posit.co/reference/layer_random_crop.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"A preprocessing layer which randomly crops images during training. — layer_random_crop","text":"3D (unbatched) 4D (batched) tensor shape: (..., height, width, channels), \"channels_last\" format.","code":""},{"path":"https://keras.posit.co/reference/layer_random_crop.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"A preprocessing layer which randomly crops images during training. — layer_random_crop","text":"3D (unbatched) 4D (batched) tensor shape: (..., target_height, target_width, channels).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_random_flip.html","id":null,"dir":"Reference","previous_headings":"","what":"A preprocessing layer which randomly flips images during training. — layer_random_flip","title":"A preprocessing layer which randomly flips images during training. — layer_random_flip","text":"layer flip images horizontally vertically based mode attribute. inference time, output identical input. Call layer training=TRUE flip input. Input pixel values can range (e.g. [0., 1.) [0, 255]) integer floating point dtype. default, layer output floats. Note: layer safe use inside tf.data pipeline (independently backend using).","code":""},{"path":"https://keras.posit.co/reference/layer_random_flip.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A preprocessing layer which randomly flips images during training. — layer_random_flip","text":"","code":"layer_random_flip(object, mode = \"horizontal_and_vertical\", seed = NULL, ...)"},{"path":"https://keras.posit.co/reference/layer_random_flip.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A preprocessing layer which randomly flips images during training. — layer_random_flip","text":"object Object compose layer . tensor, array, sequential model. mode String indicating flip mode use. Can \"horizontal\", \"vertical\", \"horizontal_and_vertical\". \"horizontal\" left-right flip \"vertical\" top-bottom flip. Defaults \"horizontal_and_vertical\" seed Integer. Used create random seed. ... Base layer keyword arguments, name dtype.","code":""},{"path":"https://keras.posit.co/reference/layer_random_flip.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"A preprocessing layer which randomly flips images during training. — layer_random_flip","text":"3D (unbatched) 4D (batched) tensor shape: (..., height, width, channels), \"channels_last\" format.","code":""},{"path":"https://keras.posit.co/reference/layer_random_flip.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"A preprocessing layer which randomly flips images during training. — layer_random_flip","text":"3D (unbatched) 4D (batched) tensor shape: (..., height, width, channels), \"channels_last\" format.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_random_rotation.html","id":null,"dir":"Reference","previous_headings":"","what":"A preprocessing layer which randomly rotates images during training. — layer_random_rotation","title":"A preprocessing layer which randomly rotates images during training. — layer_random_rotation","text":"layer apply random rotations image, filling empty space according fill_mode. default, random rotations applied training. inference time, layer nothing. need apply random rotations inference time, set training TRUE calling layer. Input pixel values can range (e.g. [0., 1.) [0, 255]) integer floating point dtype. default, layer output floats. Note: layer safe use inside tf.data pipeline (independently backend using).","code":""},{"path":"https://keras.posit.co/reference/layer_random_rotation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A preprocessing layer which randomly rotates images during training. — layer_random_rotation","text":"","code":"layer_random_rotation(   object,   factor,   fill_mode = \"reflect\",   interpolation = \"bilinear\",   seed = NULL,   fill_value = 0,   value_range = list(0L, 255L),   data_format = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_random_rotation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A preprocessing layer which randomly rotates images during training. — layer_random_rotation","text":"object Object compose layer . tensor, array, sequential model. factor float represented fraction 2 Pi, tuple size 2 representing lower upper bound rotating clockwise counter-clockwise. positive values means rotating counter clock-wise, negative value means clock-wise. represented single float, value used upper lower bound. instance, factor=(-0.2, 0.3) results output rotation random amount range [-20% * 2pi, 30% * 2pi]. factor=0.2 results output rotating random amount range [-20% * 2pi, 20% * 2pi]. fill_mode Points outside boundaries input filled according given mode (one {\"constant\", \"reflect\", \"wrap\", \"nearest\"}). reflect: (d c b | b c d | d c b ) input extended reflecting edge last pixel. constant: (k k k k | b c d | k k k k) input extended filling values beyond edge constant value k = 0. wrap: (b c d | b c d | b c d) input extended wrapping around opposite edge. nearest: (| b c d | d d d d) input extended nearest pixel. interpolation Interpolation mode. Supported values: \"nearest\", \"bilinear\". seed Integer. Used create random seed. fill_value float represents value filled outside boundaries fill_mode=\"constant\". value_range see description data_format see description ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_random_rotation.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"A preprocessing layer which randomly rotates images during training. — layer_random_rotation","text":"3D (unbatched) 4D (batched) tensor shape: (..., height, width, channels), \"channels_last\" format","code":""},{"path":"https://keras.posit.co/reference/layer_random_rotation.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"A preprocessing layer which randomly rotates images during training. — layer_random_rotation","text":"3D (unbatched) 4D (batched) tensor shape: (..., height, width, channels), \"channels_last\" format","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_random_translation.html","id":null,"dir":"Reference","previous_headings":"","what":"A preprocessing layer which randomly translates images during training. — layer_random_translation","title":"A preprocessing layer which randomly translates images during training. — layer_random_translation","text":"layer apply random translations image training, filling empty space according fill_mode. Input pixel values can range (e.g. [0., 1.) [0, 255]) integer floating point dtype. default, layer output floats.","code":""},{"path":"https://keras.posit.co/reference/layer_random_translation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A preprocessing layer which randomly translates images during training. — layer_random_translation","text":"","code":"layer_random_translation(   object,   height_factor,   width_factor,   fill_mode = \"reflect\",   interpolation = \"bilinear\",   seed = NULL,   fill_value = 0,   data_format = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_random_translation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A preprocessing layer which randomly translates images during training. — layer_random_translation","text":"object Object compose layer . tensor, array, sequential model. height_factor float represented fraction value, tuple size 2 representing lower upper bound shifting vertically. negative value means shifting image , positive value means shifting image . represented single positive float, value used upper lower bound. instance, height_factor=(-0.2, 0.3) results output shifted random amount range [-20%, +30%]. height_factor=0.2 results output height shifted random amount range [-20%, +20%]. width_factor float represented fraction value, tuple size 2 representing lower upper bound shifting horizontally. negative value means shifting image left, positive value means shifting image right. represented single positive float, value used upper lower bound. instance, width_factor=(-0.2, 0.3) results output shifted left 20%, shifted right 30%. width_factor=0.2 results output height shifted left right 20%. fill_mode Points outside boundaries input filled according given mode. Available methods \"constant\", \"nearest\", \"wrap\" \"reflect\". Defaults \"constant\". \"reflect\": (d c b | b c d | d c b ) input extended reflecting edge last pixel. \"constant\": (k k k k | b c d | k k k k) input extended filling values beyond edge constant value k specified fill_value. \"wrap\": (b c d | b c d | b c d) input extended wrapping around opposite edge. \"nearest\": (| b c d | d d d d) input extended nearest pixel. Note using torch backend, \"reflect\" redirected \"mirror\" (c d c b | b c d | c b b) torch support \"reflect\". Note torch backend support \"wrap\". interpolation Interpolation mode. Supported values: \"nearest\", \"bilinear\". seed Integer. Used create random seed. fill_value float represents value filled outside boundaries fill_mode=\"constant\". data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, height, width, channels) \"channels_first\" corresponds inputs shape (batch, channels, height, width). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". ... Base layer keyword arguments, name dtype.","code":""},{"path":"https://keras.posit.co/reference/layer_random_translation.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"A preprocessing layer which randomly translates images during training. — layer_random_translation","text":"3D (unbatched) 4D (batched) tensor shape: (..., height, width, channels), \"channels_last\" format, (..., channels, height, width), \"channels_first\" format.","code":""},{"path":"https://keras.posit.co/reference/layer_random_translation.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"A preprocessing layer which randomly translates images during training. — layer_random_translation","text":"3D (unbatched) 4D (batched) tensor shape: (..., target_height, target_width, channels), (..., channels, target_height, target_width), \"channels_first\" format. Note: layer safe use inside tf.data pipeline (independently backend using).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_random_zoom.html","id":null,"dir":"Reference","previous_headings":"","what":"A preprocessing layer which randomly zooms images during training. — layer_random_zoom","title":"A preprocessing layer which randomly zooms images during training. — layer_random_zoom","text":"layer randomly zoom axis image independently, filling empty space according fill_mode. Input pixel values can range (e.g. [0., 1.) [0, 255]) integer floating point dtype. default, layer output floats.","code":""},{"path":"https://keras.posit.co/reference/layer_random_zoom.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A preprocessing layer which randomly zooms images during training. — layer_random_zoom","text":"","code":"layer_random_zoom(   object,   height_factor,   width_factor = NULL,   fill_mode = \"reflect\",   interpolation = \"bilinear\",   seed = NULL,   fill_value = 0,   data_format = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_random_zoom.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A preprocessing layer which randomly zooms images during training. — layer_random_zoom","text":"object Object compose layer . tensor, array, sequential model. height_factor float represented fraction value, list size 2 representing lower upper bound zooming vertically. represented single float, value used upper lower bound. positive value means zooming , negative value means zooming . instance, height_factor=c(0.2, 0.3) result output zoomed random amount range [+20%, +30%]. height_factor=c(-0.3, -0.2) result output zoomed random amount range [+20%, +30%]. width_factor float represented fraction value, list size 2 representing lower upper bound zooming horizontally. represented single float, value used upper lower bound. instance, width_factor=c(0.2, 0.3) result output zooming 20% 30%. width_factor=c(-0.3, -0.2) result output zooming 20% 30%. NULL means .e., zooming vertical horizontal directions preserving aspect ratio. Defaults NULL. fill_mode Points outside boundaries input filled according given mode. Available methods \"constant\", \"nearest\", \"wrap\" \"reflect\". Defaults \"constant\". \"reflect\": (d c b | b c d | d c b ) input extended reflecting edge last pixel. \"constant\": (k k k k | b c d | k k k k) input extended filling values beyond edge constant value k specified fill_value. \"wrap\": (b c d | b c d | b c d) input extended wrapping around opposite edge. \"nearest\": (| b c d | d d d d) input extended nearest pixel. Note using torch backend, \"reflect\" redirected \"mirror\" (c d c b | b c d | c b b) torch support \"reflect\". Note torch backend support \"wrap\". interpolation Interpolation mode. Supported values: \"nearest\", \"bilinear\". seed Integer. Used create random seed. fill_value float represents value filled outside boundaries fill_mode=\"constant\". data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, height, width, channels) \"channels_first\" corresponds inputs shape (batch, channels, height, width). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". ... Base layer keyword arguments, name dtype.","code":""},{"path":"https://keras.posit.co/reference/layer_random_zoom.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"A preprocessing layer which randomly zooms images during training. — layer_random_zoom","text":"3D (unbatched) 4D (batched) tensor shape: (..., height, width, channels), \"channels_last\" format, (..., channels, height, width), \"channels_first\" format.","code":""},{"path":"https://keras.posit.co/reference/layer_random_zoom.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"A preprocessing layer which randomly zooms images during training. — layer_random_zoom","text":"3D (unbatched) 4D (batched) tensor shape: (..., target_height, target_width, channels), (..., channels, target_height, target_width), \"channels_first\" format. Note: layer safe use inside tf.data pipeline (independently backend using).","code":""},{"path":"https://keras.posit.co/reference/layer_random_zoom.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A preprocessing layer which randomly zooms images during training. — layer_random_zoom","text":"","code":"input_img <- random_uniform(c(32, 224, 224, 3)) layer <- layer_random_zoom(height_factor = .5, width_factor = .2) out_img <- layer(input_img)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_repeat_vector.html","id":null,"dir":"Reference","previous_headings":"","what":"Repeats the input n times. — layer_repeat_vector","title":"Repeats the input n times. — layer_repeat_vector","text":"Repeats input n times.","code":""},{"path":"https://keras.posit.co/reference/layer_repeat_vector.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Repeats the input n times. — layer_repeat_vector","text":"","code":"layer_repeat_vector(object, n, ...)"},{"path":"https://keras.posit.co/reference/layer_repeat_vector.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Repeats the input n times. — layer_repeat_vector","text":"object Object compose layer . tensor, array, sequential model. n Integer, repetition factor. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_repeat_vector.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Repeats the input n times. — layer_repeat_vector","text":"","code":"x <- layer_input(shape = 32) y <- layer_repeat_vector(x, n = 3) shape(y) ## shape(NA, 3, 32)"},{"path":"https://keras.posit.co/reference/layer_repeat_vector.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Repeats the input n times. — layer_repeat_vector","text":"2D tensor shape (batch_size, features).","code":""},{"path":"https://keras.posit.co/reference/layer_repeat_vector.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Repeats the input n times. — layer_repeat_vector","text":"3D tensor shape (batch_size, n, features).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_rescaling.html","id":null,"dir":"Reference","previous_headings":"","what":"A preprocessing layer which rescales input values to a new range. — layer_rescaling","title":"A preprocessing layer which rescales input values to a new range. — layer_rescaling","text":"layer rescales every value input (often image) multiplying scale adding offset. instance: rescale input [0, 255] range [0, 1] range, pass scale=1./255. rescale input [0, 255] range [-1, 1] range, pass scale=1./127.5, offset=-1. rescaling applied training inference. Inputs can integer floating point dtype, default layer output floats. Note: layer safe use inside tf.data pipeline (independently backend using).","code":""},{"path":"https://keras.posit.co/reference/layer_rescaling.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A preprocessing layer which rescales input values to a new range. — layer_rescaling","text":"","code":"layer_rescaling(object, scale, offset = 0, ...)"},{"path":"https://keras.posit.co/reference/layer_rescaling.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A preprocessing layer which rescales input values to a new range. — layer_rescaling","text":"object Object compose layer . tensor, array, sequential model. scale Float, scale apply inputs. offset Float, offset apply inputs. ... Base layer keyword arguments, name dtype.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_reshape.html","id":null,"dir":"Reference","previous_headings":"","what":"Layer that reshapes inputs into the given shape. — layer_reshape","title":"Layer that reshapes inputs into the given shape. — layer_reshape","text":"Layer reshapes inputs given shape.","code":""},{"path":"https://keras.posit.co/reference/layer_reshape.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Layer that reshapes inputs into the given shape. — layer_reshape","text":"","code":"layer_reshape(object, target_shape, ...)"},{"path":"https://keras.posit.co/reference/layer_reshape.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Layer that reshapes inputs into the given shape. — layer_reshape","text":"object Object compose layer . tensor, array, sequential model. target_shape Target shape. List integers, include samples dimension (batch size). ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_reshape.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Layer that reshapes inputs into the given shape. — layer_reshape","text":"Arbitrary, although dimensions input shape must known/fixed. Use keyword argument input_shape (list integers, include samples/batch size axis) using layer first layer model.","code":""},{"path":"https://keras.posit.co/reference/layer_reshape.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Layer that reshapes inputs into the given shape. — layer_reshape","text":"(batch_size, *target_shape)","code":""},{"path":"https://keras.posit.co/reference/layer_reshape.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Layer that reshapes inputs into the given shape. — layer_reshape","text":"","code":"x <- layer_input(shape = 12) y <- layer_reshape(x, c(3, 4)) shape(y) ## shape(NA, 3, 4) # also supports shape inference using `-1` as dimension y <- layer_reshape(x, c(-1, 2, 2)) shape(y) ## shape(NA, 3, 2, 2)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_resizing.html","id":null,"dir":"Reference","previous_headings":"","what":"A preprocessing layer which resizes images. — layer_resizing","title":"A preprocessing layer which resizes images. — layer_resizing","text":"layer resizes image input target height width. input 4D (batched) 3D (unbatched) tensor \"channels_last\" format. Input pixel values can range (e.g. [0., 1.) [0, 255]).","code":""},{"path":"https://keras.posit.co/reference/layer_resizing.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A preprocessing layer which resizes images. — layer_resizing","text":"","code":"layer_resizing(   object,   height,   width,   interpolation = \"bilinear\",   crop_to_aspect_ratio = FALSE,   data_format = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_resizing.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A preprocessing layer which resizes images. — layer_resizing","text":"object Object compose layer . tensor, array, sequential model. height Integer, height output shape. width Integer, width output shape. interpolation String, interpolation method. Supports \"bilinear\", \"nearest\", \"bicubic\", \"lanczos3\", \"lanczos5\". Defaults \"bilinear\". crop_to_aspect_ratio TRUE, resize images without aspect ratio distortion. original aspect ratio differs target aspect ratio, output image cropped return largest possible window image (size (height, width)) matches target aspect ratio. default (crop_to_aspect_ratio=FALSE), aspect ratio may preserved. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, height, width, channels) \"channels_first\" corresponds inputs shape (batch, channels, height, width). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". ... Base layer keyword arguments, name dtype.","code":""},{"path":"https://keras.posit.co/reference/layer_resizing.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"A preprocessing layer which resizes images. — layer_resizing","text":"3D (unbatched) 4D (batched) tensor shape: (..., height, width, channels), \"channels_last\" format, (..., channels, height, width), \"channels_first\" format.","code":""},{"path":"https://keras.posit.co/reference/layer_resizing.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"A preprocessing layer which resizes images. — layer_resizing","text":"3D (unbatched) 4D (batched) tensor shape: (..., target_height, target_width, channels), (..., channels, target_height, target_width), \"channels_first\" format. Note: layer safe use inside tf.data pipeline (independently backend using).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_rnn.html","id":null,"dir":"Reference","previous_headings":"","what":"Base class for recurrent layer_ — layer_rnn","title":"Base class for recurrent layer_ — layer_rnn","text":"Base class recurrent layer_","code":""},{"path":"https://keras.posit.co/reference/layer_rnn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Base class for recurrent layer_ — layer_rnn","text":"","code":"layer_rnn(   object,   cell,   return_sequences = FALSE,   return_state = FALSE,   go_backwards = FALSE,   stateful = FALSE,   unroll = FALSE,   zero_output_for_mask = FALSE,   ... )"},{"path":"https://keras.posit.co/reference/layer_rnn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base class for recurrent layer_ — layer_rnn","text":"object Object compose layer . tensor, array, sequential model. cell RNN cell instance list RNN cell instances. RNN cell class : call(input_at_t, states_at_t) method, returning (output_at_t, states_at_t_plus_1). call method cell can also take optional argument constants, see section \"Note passing external constants\" . state_size attribute. can single integer (single state) case size recurrent state. can also list integers (one size per state). output_size attribute, single integer. get_initial_state(batch_size=NULL) method creates tensor meant fed call() initial state, user specify initial state via means. returned initial state shape (batch_size, cell.state_size). cell might choose create tensor full zeros, values based cell's implementation. inputs input tensor RNN layer, shape (batch_size, timesteps, features). method implemented cell, RNN layer create zero filled tensor shape (batch_size, cell$state_size). case cell list RNN cell instances, cells stacked top RNN, resulting efficient stacked RNN. return_sequences Boolean (default FALSE). Whether return last output output sequence, full sequence. return_state Boolean (default FALSE). Whether return last state addition output. go_backwards Boolean (default FALSE). TRUE, process input sequence backwards return reversed sequence. stateful Boolean (default FALSE). TRUE, last state sample index batch used initial state sample index following batch. unroll Boolean (default FALSE). TRUE, network unrolled, else symbolic loop used. Unrolling can speed-RNN, although tends memory-intensive. Unrolling suitable short sequences. zero_output_for_mask Boolean (default FALSE). Whether output use zeros masked timesteps. Note field used return_sequences TRUE mask provided. can useful want reuse raw output sequence RNN without interference masked timesteps, e.g., merging bidirectional RNNs. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_rnn.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"Base class for recurrent layer_ — layer_rnn","text":"inputs: Input tensor. initial_state: List initial state tensors passed first call cell. mask: Binary tensor shape [batch_size, timesteps] indicating whether given timestep masked. individual TRUE entry indicates corresponding timestep utilized, FALSE entry indicates corresponding timestep ignored. training: Python boolean indicating whether layer behave training mode inference mode. argument passed cell calling . use cells use dropout.","code":""},{"path":"https://keras.posit.co/reference/layer_rnn.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Base class for recurrent layer_ — layer_rnn","text":"3-D tensor shape (batch_size, timesteps, features).","code":""},{"path":"https://keras.posit.co/reference/layer_rnn.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Base class for recurrent layer_ — layer_rnn","text":"return_state: list tensors. first tensor output. remaining tensors last states, shape (batch_size, state_size), state_size high dimension tensor shape. return_sequences: 3D tensor shape (batch_size, timesteps, output_size).","code":""},{"path":"https://keras.posit.co/reference/layer_rnn.html","id":"masking-","dir":"Reference","previous_headings":"","what":"Masking:","title":"Base class for recurrent layer_ — layer_rnn","text":"layer supports masking input data variable number timesteps. introduce masks data, use layer_Embedding layer mask_zero parameter set TRUE. Note using statefulness RNNs: can set RNN layers 'stateful', means states computed samples one batch reused initial states samples next batch. assumes one--one mapping samples different successive batches. enable statefulness: Specify stateful=TRUE layer constructor. Specify fixed batch size model, passing sequential model: batch_input_shape=(...) first layer model. Else functional model 1 Input layers: batch_shape=(...) first layers model. expected shape inputs including batch size. list integers, e.g. (32, 10, 100). Specify shuffle=FALSE calling fit(). reset states model, call .reset_states() either specific layer, entire model. Note specifying initial state RNNs: can specify initial state RNN layers symbolically calling keyword argument initial_state. value initial_state tensor list tensors representing initial state RNN layer. can specify initial state RNN layers numerically calling reset_states keyword argument states. value states numpy array list numpy arrays representing initial state RNN layer.","code":""},{"path":"https://keras.posit.co/reference/layer_rnn.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Base class for recurrent layer_ — layer_rnn","text":"","code":"# First, let's define a RNN Cell, as a layer subclass. MinimalRNNCell(keras$layers$Layer) %py_class% {   initialize <- function(units, ...) {     super$initialize(...)     self$units <- as.integer(units)     self$state_size <- as.integer(units)   }    build <- function(input_shape) {     self$kernel <- self$add_weight(       shape = c(tail(input_shape, 1), self$units),       initializer = 'uniform',       name = 'kernel')     self$recurrent_kernel <- self$add_weight(       shape = c(self$units, self$units),       initializer = 'uniform',       name = 'recurrent_kernel')     self$built <- TRUE   }    call <- function(inputs, states) {     prev_output <- states[[1]]     h <- op_matmul(inputs, self$kernel)     output <- h + op_matmul(prev_output, self$recurrent_kernel)     list(output, list(output))   }  }  # Let's use this cell in a RNN layer:  cell <- MinimalRNNCell(units = 32) x <- layer_input(shape = shape(NULL, 5)) layer <- layer_rnn(cell = cell) y <- layer(x)  # Here's how to use the cell to build a stacked RNN:  cells <- list(MinimalRNNCell(units = 32), MinimalRNNCell(units = 4)) x <- layer_input(shape = shape(NULL, 5)) layer <- layer_rnn(cell = cells) y <- layer(x)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_separable_conv_1d.html","id":null,"dir":"Reference","previous_headings":"","what":"1D separable convolution layer. — layer_separable_conv_1d","title":"1D separable convolution layer. — layer_separable_conv_1d","text":"layer performs depthwise convolution acts separately channels, followed pointwise convolution mixes channels. use_bias TRUE bias initializer provided, adds bias vector output. optionally applies activation function produce final output.","code":""},{"path":"https://keras.posit.co/reference/layer_separable_conv_1d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"1D separable convolution layer. — layer_separable_conv_1d","text":"","code":"layer_separable_conv_1d(   object,   filters,   kernel_size,   strides = 1L,   padding = \"valid\",   data_format = NULL,   dilation_rate = 1L,   depth_multiplier = 1L,   activation = NULL,   use_bias = TRUE,   depthwise_initializer = \"glorot_uniform\",   pointwise_initializer = \"glorot_uniform\",   bias_initializer = \"zeros\",   depthwise_regularizer = NULL,   pointwise_regularizer = NULL,   bias_regularizer = NULL,   activity_regularizer = NULL,   depthwise_constraint = NULL,   pointwise_constraint = NULL,   bias_constraint = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_separable_conv_1d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"1D separable convolution layer. — layer_separable_conv_1d","text":"object Object compose layer . tensor, array, sequential model. filters int, dimensionality output space (.e. number filters pointwise convolution). kernel_size int list 1 integers, specifying size depthwise convolution window. strides int list 1 integers, specifying stride length depthwise convolution. one int specified, stride size used dimensions. strides > 1 incompatible dilation_rate > 1. padding string, either \"valid\" \"\" (case-insensitive). \"valid\" means padding. \"\" results padding evenly left/right /input. padding=\"\" strides=1, output size input. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, steps, features) \"channels_first\" corresponds inputs shape (batch, features, steps). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". dilation_rate int list 1 integers, specifying dilation rate use dilated convolution. one int specified, dilation rate used dimensions. depth_multiplier number depthwise convolution output channels input channel. total number depthwise convolution output channels equal input_channel * depth_multiplier. activation Activation function. NULL, activation applied. use_bias bool, TRUE, bias added output. depthwise_initializer initializer depthwise convolution kernel. NULL, default initializer (\"glorot_uniform\") used. pointwise_initializer initializer pointwise convolution kernel. NULL, default initializer (\"glorot_uniform\") used. bias_initializer initializer bias vector. NULL, default initializer ('\"zeros\"') used. depthwise_regularizer Optional regularizer depthwise convolution kernel. pointwise_regularizer Optional regularizer pointwise convolution kernel. bias_regularizer Optional regularizer bias vector. activity_regularizer Optional regularizer function output. depthwise_constraint Optional projection function applied depthwise kernel updated Optimizer (e.g. used norm constraints value constraints layer weights). function must take input unprojected variable must return projected variable (must shape). pointwise_constraint Optional projection function applied pointwise kernel updated Optimizer. bias_constraint Optional projection function applied bias updated Optimizer. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_separable_conv_1d.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"1D separable convolution layer. — layer_separable_conv_1d","text":"3D tensor representing activation(separable_conv1d(inputs, kernel) + bias).","code":""},{"path":"https://keras.posit.co/reference/layer_separable_conv_1d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"1D separable convolution layer. — layer_separable_conv_1d","text":"data_format=\"channels_last\": 3D tensor shape: (batch_shape, steps, channels) data_format=\"channels_first\": 3D tensor shape: (batch_shape, channels, steps)","code":""},{"path":"https://keras.posit.co/reference/layer_separable_conv_1d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"1D separable convolution layer. — layer_separable_conv_1d","text":"data_format=\"channels_last\": 3D tensor shape: (batch_shape, new_steps, filters) data_format=\"channels_first\": 3D tensor shape: (batch_shape, filters, new_steps)","code":""},{"path":"https://keras.posit.co/reference/layer_separable_conv_1d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"1D separable convolution layer. — layer_separable_conv_1d","text":"","code":"x <- random_uniform(c(4, 10, 12)) y <- layer_separable_conv_1d(x, 3, 2, 2, activation='relu') shape(y) ## shape(4, 5, 3)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_separable_conv_2d.html","id":null,"dir":"Reference","previous_headings":"","what":"2D separable convolution layer. — layer_separable_conv_2d","title":"2D separable convolution layer. — layer_separable_conv_2d","text":"layer performs depthwise convolution acts separately channels, followed pointwise convolution mixes channels. use_bias TRUE bias initializer provided, adds bias vector output. optionally applies activation function produce final output.","code":""},{"path":"https://keras.posit.co/reference/layer_separable_conv_2d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"2D separable convolution layer. — layer_separable_conv_2d","text":"","code":"layer_separable_conv_2d(   object,   filters,   kernel_size,   strides = list(1L, 1L),   padding = \"valid\",   data_format = NULL,   dilation_rate = list(1L, 1L),   depth_multiplier = 1L,   activation = NULL,   use_bias = TRUE,   depthwise_initializer = \"glorot_uniform\",   pointwise_initializer = \"glorot_uniform\",   bias_initializer = \"zeros\",   depthwise_regularizer = NULL,   pointwise_regularizer = NULL,   bias_regularizer = NULL,   activity_regularizer = NULL,   depthwise_constraint = NULL,   pointwise_constraint = NULL,   bias_constraint = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_separable_conv_2d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"2D separable convolution layer. — layer_separable_conv_2d","text":"object Object compose layer . tensor, array, sequential model. filters int, dimensionality output space (.e. number filters pointwise convolution). kernel_size int list 2 integers, specifying size depthwise convolution window. strides int list 2 integers, specifying stride length depthwise convolution. one int specified, stride size used dimensions. strides > 1 incompatible dilation_rate > 1. padding string, either \"valid\" \"\" (case-insensitive). \"valid\" means padding. \"\" results padding evenly left/right /input. padding=\"\" strides=1, output size input. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, steps, features) \"channels_first\" corresponds inputs shape (batch, features, steps). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". dilation_rate int list 2 integers, specifying dilation rate use dilated convolution. one int specified, dilation rate used dimensions. depth_multiplier number depthwise convolution output channels input channel. total number depthwise convolution output channels equal input_channel * depth_multiplier. activation Activation function. NULL, activation applied. use_bias bool, TRUE, bias added output. depthwise_initializer initializer depthwise convolution kernel. NULL, default initializer (\"glorot_uniform\") used. pointwise_initializer initializer pointwise convolution kernel. NULL, default initializer (\"glorot_uniform\") used. bias_initializer initializer bias vector. NULL, default initializer ('\"zeros\"') used. depthwise_regularizer Optional regularizer depthwise convolution kernel. pointwise_regularizer Optional regularizer pointwise convolution kernel. bias_regularizer Optional regularizer bias vector. activity_regularizer Optional regularizer function output. depthwise_constraint Optional projection function applied depthwise kernel updated Optimizer (e.g. used norm constraints value constraints layer weights). function must take input unprojected variable must return projected variable (must shape). pointwise_constraint Optional projection function applied pointwise kernel updated Optimizer. bias_constraint Optional projection function applied bias updated Optimizer. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_separable_conv_2d.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"2D separable convolution layer. — layer_separable_conv_2d","text":"4D tensor representing activation(separable_conv2d(inputs, kernel) + bias).","code":""},{"path":"https://keras.posit.co/reference/layer_separable_conv_2d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"2D separable convolution layer. — layer_separable_conv_2d","text":"data_format=\"channels_last\": 4D tensor shape: (batch_size, height, width, channels) data_format=\"channels_first\": 4D tensor shape: (batch_size, channels, height, width)","code":""},{"path":"https://keras.posit.co/reference/layer_separable_conv_2d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"2D separable convolution layer. — layer_separable_conv_2d","text":"data_format=\"channels_last\": 4D tensor shape: (batch_size, new_height, new_width, filters) data_format=\"channels_first\": 4D tensor shape: (batch_size, filters, new_height, new_width)","code":""},{"path":"https://keras.posit.co/reference/layer_separable_conv_2d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"2D separable convolution layer. — layer_separable_conv_2d","text":"","code":"x <- random_uniform(c(4, 10, 10, 12)) y <- layer_separable_conv_2d(x, 3, c(4, 3), 2, activation='relu') shape(y) ## shape(4, 4, 4, 3)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_simple_rnn.html","id":null,"dir":"Reference","previous_headings":"","what":"Fully-connected RNN where the output is to be fed back as the new input. — layer_simple_rnn","title":"Fully-connected RNN where the output is to be fed back as the new input. — layer_simple_rnn","text":"Fully-connected RNN output fed back new input.","code":""},{"path":"https://keras.posit.co/reference/layer_simple_rnn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fully-connected RNN where the output is to be fed back as the new input. — layer_simple_rnn","text":"","code":"layer_simple_rnn(   object,   units,   activation = \"tanh\",   use_bias = TRUE,   kernel_initializer = \"glorot_uniform\",   recurrent_initializer = \"orthogonal\",   bias_initializer = \"zeros\",   kernel_regularizer = NULL,   recurrent_regularizer = NULL,   bias_regularizer = NULL,   activity_regularizer = NULL,   kernel_constraint = NULL,   recurrent_constraint = NULL,   bias_constraint = NULL,   dropout = 0,   recurrent_dropout = 0,   return_sequences = FALSE,   return_state = FALSE,   go_backwards = FALSE,   stateful = FALSE,   unroll = FALSE,   seed = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_simple_rnn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fully-connected RNN where the output is to be fed back as the new input. — layer_simple_rnn","text":"object Object compose layer . tensor, array, sequential model. units Positive integer, dimensionality output space. activation Activation function use. Default: hyperbolic tangent (tanh). pass NULL, activation applied (ie. \"linear\" activation: (x) = x). use_bias Boolean, (default TRUE), whether layer uses bias vector. kernel_initializer Initializer kernel weights matrix, used linear transformation inputs. Default: \"glorot_uniform\". recurrent_initializer Initializer recurrent_kernel weights matrix, used linear transformation recurrent state.  Default: \"orthogonal\". bias_initializer Initializer bias vector. Default: \"zeros\". kernel_regularizer Regularizer function applied kernel weights matrix. Default: NULL. recurrent_regularizer Regularizer function applied recurrent_kernel weights matrix. Default: NULL. bias_regularizer Regularizer function applied bias vector. Default: NULL. activity_regularizer Regularizer function applied output layer (\"activation\"). Default: NULL. kernel_constraint Constraint function applied kernel weights matrix. Default: NULL. recurrent_constraint Constraint function applied recurrent_kernel weights matrix.  Default: NULL. bias_constraint Constraint function applied bias vector. Default: NULL. dropout Float 0 1. Fraction units drop linear transformation inputs. Default: 0. recurrent_dropout Float 0 1. Fraction units drop linear transformation recurrent state. Default: 0. return_sequences Boolean. Whether return last output output sequence, full sequence. Default: FALSE. return_state Boolean. Whether return last state addition output. Default: FALSE. go_backwards Boolean (default: FALSE). TRUE, process input sequence backwards return reversed sequence. stateful Boolean (default: FALSE). TRUE, last state sample index batch used initial state sample index following batch. unroll Boolean (default: FALSE). TRUE, network unrolled, else symbolic loop used. Unrolling can speed-RNN, although tends memory-intensive. Unrolling suitable short sequences. seed Initial seed random number generator ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_simple_rnn.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"Fully-connected RNN where the output is to be fed back as the new input. — layer_simple_rnn","text":"sequence: 3D tensor, shape [batch, timesteps, feature]. mask: Binary tensor shape [batch, timesteps] indicating whether given timestep masked. individual TRUE entry indicates corresponding timestep utilized, FALSE entry indicates corresponding timestep ignored. training: Python boolean indicating whether layer behave training mode inference mode. argument passed cell calling . relevant dropout recurrent_dropout used. initial_state: List initial state tensors passed first call cell.","code":""},{"path":"https://keras.posit.co/reference/layer_simple_rnn.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fully-connected RNN where the output is to be fed back as the new input. — layer_simple_rnn","text":"","code":"inputs <- random_uniform(c(32, 10, 8)) simple_rnn <- layer_simple_rnn(units = 4) output <- simple_rnn(inputs)  # The output has shape `(32, 4)`. simple_rnn <- layer_simple_rnn(     units = 4, return_sequences=TRUE, return_state=TRUE ) # whole_sequence_output has shape `(32, 10, 4)`. # final_state has shape `(32, 4)`. c(whole_sequence_output, final_state) %<-% simple_rnn(inputs)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_simple_rnn_cell.html","id":null,"dir":"Reference","previous_headings":"","what":"Cell class for SimpleRNN. — layer_simple_rnn_cell","title":"Cell class for SimpleRNN. — layer_simple_rnn_cell","text":"class processes one step within whole time sequence input, whereas keras.layer.SimpleRNN processes whole sequence.","code":""},{"path":"https://keras.posit.co/reference/layer_simple_rnn_cell.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cell class for SimpleRNN. — layer_simple_rnn_cell","text":"","code":"layer_simple_rnn_cell(   units,   activation = \"tanh\",   use_bias = TRUE,   kernel_initializer = \"glorot_uniform\",   recurrent_initializer = \"orthogonal\",   bias_initializer = \"zeros\",   kernel_regularizer = NULL,   recurrent_regularizer = NULL,   bias_regularizer = NULL,   kernel_constraint = NULL,   recurrent_constraint = NULL,   bias_constraint = NULL,   dropout = 0,   recurrent_dropout = 0,   seed = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_simple_rnn_cell.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cell class for SimpleRNN. — layer_simple_rnn_cell","text":"units Positive integer, dimensionality output space. activation Activation function use. Default: hyperbolic tangent (tanh). pass NULL, activation applied (ie. \"linear\" activation: (x) = x). use_bias Boolean, (default TRUE), whether layer use bias vector. kernel_initializer Initializer kernel weights matrix, used linear transformation inputs. Default: \"glorot_uniform\". recurrent_initializer Initializer recurrent_kernel weights matrix, used linear transformation recurrent state. Default: \"orthogonal\". bias_initializer Initializer bias vector. Default: \"zeros\". kernel_regularizer Regularizer function applied kernel weights matrix. Default: NULL. recurrent_regularizer Regularizer function applied recurrent_kernel weights matrix. Default: NULL. bias_regularizer Regularizer function applied bias vector. Default: NULL. kernel_constraint Constraint function applied kernel weights matrix. Default: NULL. recurrent_constraint Constraint function applied recurrent_kernel weights matrix. Default: NULL. bias_constraint Constraint function applied bias vector. Default: NULL. dropout Float 0 1. Fraction units drop linear transformation inputs. Default: 0. recurrent_dropout Float 0 1. Fraction units drop linear transformation recurrent state. Default: 0. seed Random seed dropout. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_simple_rnn_cell.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"Cell class for SimpleRNN. — layer_simple_rnn_cell","text":"sequence: 2D tensor, shape (batch, features). states: 2D tensor shape (batch, units), state previous time step. training: Python boolean indicating whether layer behave training mode inference mode. relevant dropout recurrent_dropout used.","code":""},{"path":"https://keras.posit.co/reference/layer_simple_rnn_cell.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cell class for SimpleRNN. — layer_simple_rnn_cell","text":"","code":"inputs <- random_uniform(c(32, 10, 8)) rnn <- layer_rnn(cell = layer_simple_rnn_cell(units = 4)) output <- rnn(inputs)  # The output has shape `(32, 4)`. rnn <- layer_rnn(     cell = layer_simple_rnn_cell(units = 4),     return_sequences=TRUE,     return_state=TRUE ) # whole_sequence_output has shape `(32, 10, 4)`. # final_state has shape `(32, 4)`. c(whole_sequence_output, final_state) %<-% rnn(inputs)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_spatial_dropout_1d.html","id":null,"dir":"Reference","previous_headings":"","what":"Spatial 1D version of Dropout. — layer_spatial_dropout_1d","title":"Spatial 1D version of Dropout. — layer_spatial_dropout_1d","text":"layer performs function Dropout, however, drops entire 1D feature maps instead individual elements. adjacent frames within feature maps strongly correlated (normally case early convolution layers) regular dropout regularize activations otherwise just result effective learning rate decrease. case, SpatialDropout1D help promote independence feature maps used instead.","code":""},{"path":"https://keras.posit.co/reference/layer_spatial_dropout_1d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Spatial 1D version of Dropout. — layer_spatial_dropout_1d","text":"","code":"layer_spatial_dropout_1d(object, rate, seed = NULL, name = NULL, dtype = NULL)"},{"path":"https://keras.posit.co/reference/layer_spatial_dropout_1d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Spatial 1D version of Dropout. — layer_spatial_dropout_1d","text":"object Object compose layer . tensor, array, sequential model. rate Float 0 1. Fraction input units drop. seed Initial seed random number generator name String, name object dtype datatype (e.g., \"float32\").","code":""},{"path":"https://keras.posit.co/reference/layer_spatial_dropout_1d.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"Spatial 1D version of Dropout. — layer_spatial_dropout_1d","text":"inputs: 3D tensor. training: Python boolean indicating whether layer behave training mode (applying dropout) inference mode (pass-).","code":""},{"path":"https://keras.posit.co/reference/layer_spatial_dropout_1d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Spatial 1D version of Dropout. — layer_spatial_dropout_1d","text":"3D tensor shape: (samples, timesteps, channels)","code":""},{"path":"https://keras.posit.co/reference/layer_spatial_dropout_1d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Spatial 1D version of Dropout. — layer_spatial_dropout_1d","text":"input.","code":""},{"path":"https://keras.posit.co/reference/layer_spatial_dropout_1d.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Spatial 1D version of Dropout. — layer_spatial_dropout_1d","text":"Tompson et al., 2014","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_spatial_dropout_2d.html","id":null,"dir":"Reference","previous_headings":"","what":"Spatial 2D version of Dropout. — layer_spatial_dropout_2d","title":"Spatial 2D version of Dropout. — layer_spatial_dropout_2d","text":"version performs function Dropout, however, drops entire 2D feature maps instead individual elements. adjacent pixels within feature maps strongly correlated (normally case early convolution layers) regular dropout regularize activations otherwise just result effective learning rate decrease. case, SpatialDropout2D help promote independence feature maps used instead.","code":""},{"path":"https://keras.posit.co/reference/layer_spatial_dropout_2d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Spatial 2D version of Dropout. — layer_spatial_dropout_2d","text":"","code":"layer_spatial_dropout_2d(   object,   rate,   data_format = NULL,   seed = NULL,   name = NULL,   dtype = NULL )"},{"path":"https://keras.posit.co/reference/layer_spatial_dropout_2d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Spatial 2D version of Dropout. — layer_spatial_dropout_2d","text":"object Object compose layer . tensor, array, sequential model. rate Float 0 1. Fraction input units drop. data_format \"channels_first\" \"channels_last\". \"channels_first\" mode, channels dimension (depth) index 1, \"channels_last\" mode index 3. defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". seed Initial seed random number generator name String, name object dtype datatype (e.g., \"float32\").","code":""},{"path":"https://keras.posit.co/reference/layer_spatial_dropout_2d.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"Spatial 2D version of Dropout. — layer_spatial_dropout_2d","text":"inputs: 4D tensor. training: Python boolean indicating whether layer behave training mode (applying dropout) inference mode (pass-).","code":""},{"path":"https://keras.posit.co/reference/layer_spatial_dropout_2d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Spatial 2D version of Dropout. — layer_spatial_dropout_2d","text":"4D tensor shape: (samples, channels, rows, cols) data_format='channels_first' 4D tensor shape: (samples, rows, cols, channels) data_format='channels_last'.","code":""},{"path":"https://keras.posit.co/reference/layer_spatial_dropout_2d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Spatial 2D version of Dropout. — layer_spatial_dropout_2d","text":"input.","code":""},{"path":"https://keras.posit.co/reference/layer_spatial_dropout_2d.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Spatial 2D version of Dropout. — layer_spatial_dropout_2d","text":"Tompson et al., 2014","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_spatial_dropout_3d.html","id":null,"dir":"Reference","previous_headings":"","what":"Spatial 3D version of Dropout. — layer_spatial_dropout_3d","title":"Spatial 3D version of Dropout. — layer_spatial_dropout_3d","text":"version performs function Dropout, however, drops entire 3D feature maps instead individual elements. adjacent voxels within feature maps strongly correlated (normally case early convolution layers) regular dropout regularize activations otherwise just result effective learning rate decrease. case, SpatialDropout3D help promote independence feature maps used instead.","code":""},{"path":"https://keras.posit.co/reference/layer_spatial_dropout_3d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Spatial 3D version of Dropout. — layer_spatial_dropout_3d","text":"","code":"layer_spatial_dropout_3d(   object,   rate,   data_format = NULL,   seed = NULL,   name = NULL,   dtype = NULL )"},{"path":"https://keras.posit.co/reference/layer_spatial_dropout_3d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Spatial 3D version of Dropout. — layer_spatial_dropout_3d","text":"object Object compose layer . tensor, array, sequential model. rate Float 0 1. Fraction input units drop. data_format \"channels_first\" \"channels_last\". \"channels_first\" mode, channels dimension (depth) index 1, \"channels_last\" mode index 4. defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\". seed Initial seed random number generator name String, name object dtype datatype (e.g., \"float32\").","code":""},{"path":"https://keras.posit.co/reference/layer_spatial_dropout_3d.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"Spatial 3D version of Dropout. — layer_spatial_dropout_3d","text":"inputs: 5D tensor. training: Python boolean indicating whether layer behave training mode (applying dropout) inference mode (pass-).","code":""},{"path":"https://keras.posit.co/reference/layer_spatial_dropout_3d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Spatial 3D version of Dropout. — layer_spatial_dropout_3d","text":"5D tensor shape: (samples, channels, dim1, dim2, dim3) data_format='channels_first' 5D tensor shape: (samples, dim1, dim2, dim3, channels) data_format='channels_last'.","code":""},{"path":"https://keras.posit.co/reference/layer_spatial_dropout_3d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Spatial 3D version of Dropout. — layer_spatial_dropout_3d","text":"input.","code":""},{"path":"https://keras.posit.co/reference/layer_spatial_dropout_3d.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Spatial 3D version of Dropout. — layer_spatial_dropout_3d","text":"Tompson et al., 2014","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_spectral_normalization.html","id":null,"dir":"Reference","previous_headings":"","what":"Performs spectral normalization on the weights of a target layer. — layer_spectral_normalization","title":"Performs spectral normalization on the weights of a target layer. — layer_spectral_normalization","text":"wrapper controls Lipschitz constant weights layer constraining spectral norm, can stabilize training GANs.","code":""},{"path":"https://keras.posit.co/reference/layer_spectral_normalization.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Performs spectral normalization on the weights of a target layer. — layer_spectral_normalization","text":"","code":"layer_spectral_normalization(object, layer, power_iterations = 1L, ...)"},{"path":"https://keras.posit.co/reference/layer_spectral_normalization.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Performs spectral normalization on the weights of a target layer. — layer_spectral_normalization","text":"object Object compose layer . tensor, array, sequential model. layer Layer instance either kernel (e.g. layer_conv_2d, layer_dense...) embeddings attribute (Embedding layer). power_iterations int, number iterations normalization. ... Base wrapper keyword arguments.","code":""},{"path":"https://keras.posit.co/reference/layer_spectral_normalization.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Performs spectral normalization on the weights of a target layer. — layer_spectral_normalization","text":"Wrap layer_conv_2d:     Wrap layer_Dense:","code":"x <- random_uniform(c(1, 10, 10, 1)) conv2d <- layer_spectral_normalization(   layer = layer_conv_2d(filters = 2, kernel_size = 2) ) y <- conv2d(x) shape(y) ## shape(1, 9, 9, 2) x <- random_uniform(c(1, 10, 10, 1)) dense <- layer_spectral_normalization(layer = layer_dense(units = 10)) y <- dense(x) shape(y) ## shape(1, 10, 10, 10)"},{"path":"https://keras.posit.co/reference/layer_spectral_normalization.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Performs spectral normalization on the weights of a target layer. — layer_spectral_normalization","text":"Spectral Normalization GAN.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_stacked_rnn_cells.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper allowing a stack of RNN cells to behave as a single cell. — layer_stacked_rnn_cells","title":"Wrapper allowing a stack of RNN cells to behave as a single cell. — layer_stacked_rnn_cells","text":"Used implement efficient stacked RNNs.","code":""},{"path":"https://keras.posit.co/reference/layer_stacked_rnn_cells.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper allowing a stack of RNN cells to behave as a single cell. — layer_stacked_rnn_cells","text":"","code":"layer_stacked_rnn_cells(cells, ...)"},{"path":"https://keras.posit.co/reference/layer_stacked_rnn_cells.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper allowing a stack of RNN cells to behave as a single cell. — layer_stacked_rnn_cells","text":"cells List RNN cell instances. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_stacked_rnn_cells.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Wrapper allowing a stack of RNN cells to behave as a single cell. — layer_stacked_rnn_cells","text":"","code":"batch_size <- 3 sentence_length <- 5 num_features <- 2 new_shape <- c(batch_size, sentence_length, num_features) x <- array(1:30, dim = new_shape)  rnn_cells <- lapply(1:2, function(x) layer_lstm_cell(units = 128)) stacked_lstm <- layer_stacked_rnn_cells(rnn_cells) lstm_layer <- layer_rnn(cell = stacked_lstm)  result <- lstm_layer(x)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_string_lookup.html","id":null,"dir":"Reference","previous_headings":"","what":"A preprocessing layer that maps strings to (possibly encoded) indices. — layer_string_lookup","title":"A preprocessing layer that maps strings to (possibly encoded) indices. — layer_string_lookup","text":"layer translates set arbitrary strings integer output via table-based vocabulary lookup. layer perform splitting transformation input strings. layer can split tokenize natural language, see layer_text_vectorization layer. vocabulary layer must either supplied construction learned via adapt(). adapt(), layer analyze data set, determine frequency individual strings tokens, create vocabulary . vocabulary capped size, frequent tokens used create vocabulary others treated --vocabulary (OOV). two possible output modes layer. output_mode \"int\", input strings converted index vocabulary (integer). output_mode \"multi_hot\", \"count\", \"tf_idf\", input strings encoded array dimension corresponds element vocabulary. vocabulary can optionally contain mask token well OOV token (can optionally occupy multiple indices vocabulary, set num_oov_indices). position tokens vocabulary fixed. output_mode \"int\", vocabulary begin mask token (set), followed OOV indices, followed rest vocabulary. output_mode \"multi_hot\", \"count\", \"tf_idf\" vocabulary begin OOV indices instances mask token dropped. Note: layer uses TensorFlow internally. used part compiled computation graph model backend TensorFlow. can however used backend running eagerly. can also always used part input preprocessing pipeline backend (outside model ), recommend use layer. Note: layer safe use inside tf.data pipeline (independently backend using).","code":""},{"path":"https://keras.posit.co/reference/layer_string_lookup.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A preprocessing layer that maps strings to (possibly encoded) indices. — layer_string_lookup","text":"","code":"layer_string_lookup(   object,   max_tokens = NULL,   num_oov_indices = 1L,   mask_token = NULL,   oov_token = \"[UNK]\",   vocabulary = NULL,   idf_weights = NULL,   invert = FALSE,   output_mode = \"int\",   pad_to_max_tokens = FALSE,   sparse = FALSE,   encoding = \"utf-8\",   name = NULL,   ...,   vocabulary_dtype = NULL )"},{"path":"https://keras.posit.co/reference/layer_string_lookup.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A preprocessing layer that maps strings to (possibly encoded) indices. — layer_string_lookup","text":"object Object compose layer . tensor, array, sequential model. max_tokens Maximum size vocabulary layer. specified adapting vocabulary setting pad_to_max_tokens=TRUE. NULL, cap size vocabulary. Note size includes OOV mask tokens. Defaults NULL. num_oov_indices number --vocabulary tokens use. value 1, OOV inputs modulated determine OOV value. value 0, OOV inputs cause error calling layer. Defaults 1. mask_token token represents masked inputs. output_mode \"int\", token included vocabulary mapped index 0. output modes, token appear vocabulary instances mask token input dropped. set NULL, mask term added. Defaults NULL. oov_token used invert TRUE. token return OOV indices. Defaults \"[UNK]\". vocabulary Optional. Either array integers string path text file. passing array, can pass list, list, 1D NumPy array, 1D tensor containing integer vocbulary terms. passing file path, file contain one line per term vocabulary. argument set, need adapt() layer. idf_weights valid output_mode \"tf_idf\". list, list, 1D NumPy array, 1D tensor length vocabulary, containing floating point inverse document frequency weights, multiplied per sample term counts final TF-IDF weight. vocabulary argument set, output_mode \"tf_idf\", argument must supplied. invert valid output_mode \"int\". TRUE, layer map indices vocabulary items instead mapping vocabulary items indices. Defaults FALSE. output_mode Specification output layer. Values can \"int\", \"one_hot\", \"multi_hot\", \"count\", \"tf_idf\" configuring layer follows: \"int\": Return vocabulary indices input tokens. \"one_hot\": Encodes individual element input array size vocabulary, containing 1 element index. last dimension size 1, encode dimension. last dimension size 1, append new dimension encoded output. \"multi_hot\": Encodes sample input single array size vocabulary, containing 1 vocabulary term present sample. Treats last dimension sample dimension, input shape (..., sample_length), output shape (..., num_tokens). \"count\": \"multi_hot\", int array contains count number times token index appeared sample. \"tf_idf\": \"multi_hot\", TF-IDF algorithm applied find value token slot. \"int\" output, shape input output supported. output modes, currently output rank 2 supported. Defaults \"int\". pad_to_max_tokens applicable output_mode \"multi_hot\", \"count\", \"tf_idf\". TRUE, output feature axis padded max_tokens even number unique tokens vocabulary less max_tokens, resulting tensor shape (batch_size, max_tokens) regardless vocabulary size. Defaults FALSE. sparse Boolean. applicable \"multi_hot\", \"count\", \"tf_idf\" output modes. supported TensorFlow backend. TRUE, returns SparseTensor instead dense Tensor. Defaults FALSE. encoding Optional. text encoding use interpret input strings. Defaults \"utf-8\". name String, name object ... forward/backward compatability. vocabulary_dtype dtype vocabulary terms, example \"int64\" \"int32\". Defaults \"int64\".","code":""},{"path":"https://keras.posit.co/reference/layer_string_lookup.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A preprocessing layer that maps strings to (possibly encoded) indices. — layer_string_lookup","text":"Creating lookup layer known vocabulary example creates lookup layer pre-existing vocabulary.     Creating lookup layer adapted vocabulary example creates lookup layer generates vocabulary analyzing dataset.     Note OOV token \"[UNK]\" added vocabulary. remaining tokens sorted frequency (\"d\", 2 occurrences, first) inverse sort order.     Lookups multiple OOV indices example demonstrates use lookup layer multiple OOV indices.  layer created one OOV index, OOV values hashed number OOV buckets, distributing OOV values deterministic fashion across set.     Note output OOV value 'm' 0, output OOV value \"z\" 1. -vocab terms output index increased 1 earlier examples (maps 2, etc) order make space extra OOV value. One-hot output Configure layer output_mode='one_hot'. Note first num_oov_indices dimensions ont_hot encoding represent OOV values.     Multi-hot output Configure layer output_mode='multi_hot'. Note first num_oov_indices dimensions multi_hot encoding represent OOV values.     Token count output Configure layer output_mode='count'. multi_hot output, first num_oov_indices dimensions output represent OOV values.     TF-IDF output Configure layer output_mode=\"tf_idf\". multi_hot output, first num_oov_indices dimensions output represent OOV values. token bin output token_count * idf_weight, idf weights inverse document frequency weights per token. provided along vocabulary. Note idf_weight OOV values default average idf weights passed .     specify idf weights oov values, need pass entire vocabularly including leading oov token.     adapting layer \"tf_idf\" mode, input sample considered document, IDF weight per token calculated log(1 + num_documents / (1 + token_document_count)). Inverse lookup example demonstrates map indices strings using layer. (can also use adapt() inverse=TRUE, simplicity pass vocab example.)     Note first index correspond oov token default. Forward inverse lookup pairs example demonstrates use vocabulary standard lookup layer create inverse lookup layer.     example, input value \"z\" resulted output \"[UNK]\", since 1000 vocabulary - got represented OOV, OOV values returned \"[UNK]\" inverse layer. Also, note inverse work, must already set forward layer vocabulary either directly via adapt() calling get_vocabulary().","code":"vocab <- c(\"a\", \"b\", \"c\", \"d\") data <- rbind(c(\"a\", \"c\", \"d\"), c(\"d\", \"z\", \"b\")) layer <- layer_string_lookup(vocabulary=vocab) layer(data) ## tf.Tensor( ## [[1 3 4] ##  [4 0 2]], shape=(2, 3), dtype=int64) data <- rbind(c(\"a\", \"c\", \"d\"), c(\"d\", \"z\", \"b\")) layer <- layer_string_lookup() layer %>% adapt(data) get_vocabulary(layer) ## [1] \"[UNK]\" \"d\"     \"z\"     \"c\"     \"b\"     \"a\" data <- rbind(c(\"a\", \"c\", \"d\"), c(\"d\", \"z\", \"b\")) layer <- layer_string_lookup() layer %>% adapt(data) layer(data) ## tf.Tensor( ## [[5 3 1] ##  [1 2 4]], shape=(2, 3), dtype=int64) vocab <- c(\"a\", \"b\", \"c\", \"d\") data <- rbind(c(\"a\", \"c\", \"d\"), c(\"m\", \"z\", \"b\")) layer <- layer_string_lookup(vocabulary = vocab, num_oov_indices = 2) layer(data) ## tf.Tensor( ## [[2 4 5] ##  [0 1 3]], shape=(2, 3), dtype=int64) vocab <- c(\"a\", \"b\", \"c\", \"d\") data <- c(\"a\", \"b\", \"c\", \"d\", \"z\") layer <- layer_string_lookup(vocabulary = vocab, output_mode = 'one_hot') layer(data) ## tf.Tensor( ## [[0 1 0 0 0] ##  [0 0 1 0 0] ##  [0 0 0 1 0] ##  [0 0 0 0 1] ##  [1 0 0 0 0]], shape=(5, 5), dtype=int64) vocab <- c(\"a\", \"b\", \"c\", \"d\") data <- rbind(c(\"a\", \"c\", \"d\", \"d\"), c(\"d\", \"z\", \"b\", \"z\")) layer <- layer_string_lookup(vocabulary = vocab, output_mode = 'multi_hot') layer(data) ## tf.Tensor( ## [[0 1 0 1 1] ##  [1 0 1 0 1]], shape=(2, 5), dtype=int64) vocab <- c(\"a\", \"b\", \"c\", \"d\") data <- rbind(c(\"a\", \"c\", \"d\", \"d\"), c(\"d\", \"z\", \"b\", \"z\")) layer <- layer_string_lookup(vocabulary = vocab, output_mode = 'count') layer(data) ## tf.Tensor( ## [[0 1 0 1 2] ##  [2 0 1 0 1]], shape=(2, 5), dtype=int64) vocab <- c(\"a\", \"b\", \"c\", \"d\") idf_weights <- c(0.25, 0.75, 0.6, 0.4) data <- rbind(c(\"a\", \"c\", \"d\", \"d\"), c(\"d\", \"z\", \"b\", \"z\")) layer <- layer_string_lookup(output_mode = \"tf_idf\") layer %>% set_vocabulary(vocab, idf_weights=idf_weights) layer(data) ## tf.Tensor( ## [[0.   0.25 0.   0.6  0.8 ] ##  [1.   0.   0.75 0.   0.4 ]], shape=(2, 5), dtype=float32) vocab <- c(\"[UNK]\", \"a\", \"b\", \"c\", \"d\") idf_weights <- c(0.9, 0.25, 0.75, 0.6, 0.4) data <- rbind(c(\"a\", \"c\", \"d\", \"d\"), c(\"d\", \"z\", \"b\", \"z\")) layer <- layer_string_lookup(output_mode = \"tf_idf\") layer %>% set_vocabulary(vocab, idf_weights=idf_weights) layer(data) ## tf.Tensor( ## [[0.   0.25 0.   0.6  0.8 ] ##  [1.8  0.   0.75 0.   0.4 ]], shape=(2, 5), dtype=float32) vocab <- c(\"a\", \"b\", \"c\", \"d\") data <- rbind(c(1, 3, 4), c(4, 0, 2)) layer <- layer_string_lookup(vocabulary = vocab, invert = TRUE) layer(data) ## tf.Tensor( ## [[b'a' b'c' b'd'] ##  [b'd' b'[UNK]' b'b']], shape=(2, 3), dtype=string) vocab <- c(\"a\", \"b\", \"c\", \"d\") data <- rbind(c(\"a\", \"c\", \"d\"), c(\"d\", \"z\", \"b\")) layer <- layer_string_lookup(vocabulary = vocab) i_layer <- layer_string_lookup(vocabulary = vocab, invert = TRUE) int_data <- layer(data) i_layer(int_data) ## tf.Tensor( ## [[b'a' b'c' b'd'] ##  [b'd' b'[UNK]' b'b']], shape=(2, 3), dtype=string)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_subtract.html","id":null,"dir":"Reference","previous_headings":"","what":"Performs elementwise subtraction. — layer_subtract","title":"Performs elementwise subtraction. — layer_subtract","text":"takes input list tensors size 2 shape, returns single tensor (inputs[0] - inputs[1)) shape.","code":""},{"path":"https://keras.posit.co/reference/layer_subtract.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Performs elementwise subtraction. — layer_subtract","text":"","code":"layer_subtract(inputs, ...)"},{"path":"https://keras.posit.co/reference/layer_subtract.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Performs elementwise subtraction. — layer_subtract","text":"inputs layers combine ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_subtract.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Performs elementwise subtraction. — layer_subtract","text":"Usage Keras model:","code":"input_shape <- c(2, 3, 4) x1 <- random_uniform(input_shape) x2 <- random_uniform(input_shape) y <- layer_subtract(list(x1, x2)) input1 <- layer_input(shape = 16) x1 <- layer_dense(input1, units = 8, activation = 'relu') input2 <- layer_input(shape = 32) x2 <- layer_dense(input2, units = 8, activation = 'relu') subtracted <- layer_subtract(list(x1, x2)) out <- layer_dense(subtracted, units = 4) model <- keras_model(inputs = list(input1, input2), outputs = out)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_text_vectorization.html","id":null,"dir":"Reference","previous_headings":"","what":"A preprocessing layer which maps text features to integer sequences. — get_vocabulary","title":"A preprocessing layer which maps text features to integer sequences. — get_vocabulary","text":"layer basic options managing text Keras model. transforms batch strings (one example = one string) either list token indices (one example = 1D tensor integer token indices) dense representation (one example = 1D tensor float values representing data example's tokens). layer meant handle natural language inputs. handle simple string inputs (categorical strings pre-tokenized strings) see layer_string_lookup(). vocabulary layer must either supplied construction learned via adapt(). layer adapted, analyze dataset, determine frequency individual string values, create vocabulary . vocabulary can unlimited size capped, depending configuration options layer; unique values input maximum vocabulary size, frequent terms used create vocabulary. processing example contains following steps: Standardize example (usually lowercasing + punctuation stripping) Split example substrings (usually words) Recombine substrings tokens (usually ngrams) Index tokens (associate unique int value token) Transform example using index, either vector ints dense float vector. notes passing callables customize splitting normalization layer: callable can passed Layer, want serialize object pass functions registered Keras serializables (see keras.saving.register_keras_serializable details). using custom callable standardize, data received callable exactly passed layer. callable return tensor shape input. using custom callable split, data received callable 1st dimension squeezed - instead list(\"string split\", \"another string split\"), Callable see c(\"string split\", \"another string split\"). callable return tf.Tensor dtype string first dimension containing split tokens - example, see something like list(c(\"string\", \"\", \"split\"), c(\"another\", \"string\", \"\", \"split\")). Note: layer uses TensorFlow internally. used part compiled computation graph model backend TensorFlow. can however used backend running eagerly. can also always used part input preprocessing pipeline backend (outside model ), recommend use layer. Note: layer safe use inside tf.data pipeline (independently backend using).","code":""},{"path":"https://keras.posit.co/reference/layer_text_vectorization.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A preprocessing layer which maps text features to integer sequences. — get_vocabulary","text":"","code":"get_vocabulary(object, include_special_tokens = TRUE)  set_vocabulary(object, vocabulary, idf_weights = NULL, ...)  layer_text_vectorization(   object,   max_tokens = NULL,   standardize = \"lower_and_strip_punctuation\",   split = \"whitespace\",   ngrams = NULL,   output_mode = \"int\",   output_sequence_length = NULL,   pad_to_max_tokens = FALSE,   vocabulary = NULL,   idf_weights = NULL,   sparse = FALSE,   ragged = FALSE,   encoding = \"utf-8\",   name = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_text_vectorization.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A preprocessing layer which maps text features to integer sequences. — get_vocabulary","text":"object Object compose layer . tensor, array, sequential model. include_special_tokens TRUE, returned vocabulary include padding OOV tokens, term's index vocabulary equal term's index calling layer. FALSE, returned vocabulary include padding OOV tokens. vocabulary Optional. Either array strings string path text file. passing array, can pass list, list, 1D NumPy array, 1D tensor containing string vocabulary terms. passing file path, file contain one line per term vocabulary. argument set, need adapt() layer. idf_weights valid output_mode \"tf_idf\". list, list, 1D NumPy array, 1D tensor length vocabulary, containing floating point inverse document frequency weights, multiplied per sample term counts final tf_idf weight. vocabulary argument set, output_mode \"tf_idf\", argument must supplied. ... forward/backward compatability. max_tokens Maximum size vocabulary layer. specified adapting vocabulary setting pad_to_max_tokens=TRUE. Note vocabulary contains 1 OOV token, effective number tokens (max_tokens - 1 - (1 output_mode == \"int\" else 0)). standardize Optional specification standardization apply input text. Values can : NULL: standardization. \"lower_and_strip_punctuation\": Text lowercased punctuation removed. \"lower\": Text lowercased. \"strip_punctuation\": punctuation removed. Callable: Inputs passed callable function, standardized returned. split Optional specification splitting input text. Values can : NULL: splitting. \"whitespace\": Split whitespace. \"character\": Split unicode character. Callable: Standardized inputs passed callable function, split returned. ngrams Optional specification ngrams create possibly-split input text. Values can NULL, integer list integers; passing integer create ngrams integer, passing list integers create ngrams specified values list. Passing NULL means ngrams created. output_mode Optional specification output layer. Values can \"int\", \"multi_hot\", \"count\" \"tf_idf\", configuring layer follows: \"int\": Outputs integer indices, one integer index per split string token. output_mode == \"int\", 0 reserved masked locations; reduces vocab size max_tokens - 2 instead max_tokens - 1. \"multi_hot\": Outputs single int array per batch, either vocab_size max_tokens size, containing 1s elements token mapped index exists least batch item. \"count\": Like \"multi_hot\", int array contains count number times token index appeared batch item. \"tf_idf\": Like \"multi_hot\", TF-IDF algorithm applied find value token slot. \"int\" output, shape input output supported. output modes, currently rank 1 inputs (rank 2 outputs splitting) supported. output_sequence_length valid INT mode. set, output time dimension padded truncated exactly output_sequence_length values, resulting tensor shape (batch_size, output_sequence_length) regardless many tokens resulted splitting step. Defaults NULL. pad_to_max_tokens valid  \"multi_hot\", \"count\", \"tf_idf\" modes. TRUE, output feature axis padded max_tokens even number unique tokens vocabulary less max_tokens, resulting tensor shape (batch_size, max_tokens) regardless vocabulary size. Defaults FALSE. sparse Boolean. applicable \"multi_hot\", \"count\", \"tf_idf\" output modes. supported TensorFlow backend. TRUE, returns SparseTensor instead dense Tensor. Defaults FALSE. ragged Boolean. applicable \"int\" output mode. supported TensorFlow backend. TRUE, returns RaggedTensor instead dense Tensor, sequence may different length string splitting. Defaults FALSE. encoding Optional. text encoding use interpret input strings. Defaults \"utf-8\". name String, name object","code":""},{"path":"https://keras.posit.co/reference/layer_text_vectorization.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A preprocessing layer which maps text features to integer sequences. — get_vocabulary","text":"example instantiates TextVectorization layer lowercases text, splits whitespace, strips punctuation, outputs integer vocab indices.         example instantiates TextVectorization layer passing list vocabulary terms layer's __init__() method.","code":"max_tokens <- 5000  # Maximum vocab size. max_len <- 4  # Sequence length to pad the outputs to. # Create the layer. vectorize_layer <- layer_text_vectorization(     max_tokens = max_tokens,     output_mode = 'int',     output_sequence_length = max_len) # Now that the vocab layer has been created, call `adapt` on the # list of strings to create the vocabulary. vectorize_layer %>% adapt(c(\"foo bar\", \"bar baz\", \"baz bada boom\")) # Now, the layer can map strings to integers -- you can use an # embedding layer to map these integers to learned embeddings. input_data <- rbind(\"foo qux bar\", \"qux baz\") vectorize_layer(input_data) ## tf.Tensor( ## [[4 1 3 0] ##  [1 2 0 0]], shape=(2, 4), dtype=int64) vocab_data <- c(\"earth\", \"wind\", \"and\", \"fire\") max_len <- 4  # Sequence length to pad the outputs to. # Create the layer, passing the vocab directly. You can also pass the # vocabulary arg a path to a file containing one vocabulary word per # line. vectorize_layer <- layer_text_vectorization(     max_tokens = max_tokens,     output_mode = 'int',     output_sequence_length = max_len,     vocabulary = vocab_data) # Because we've passed the vocabulary directly, we don't need to adapt # the layer - the vocabulary is already set. The vocabulary contains the # padding token ('') and OOV token ('[UNK]') # as well as the passed tokens. vectorize_layer %>% get_vocabulary() ## [1] \"\"      \"[UNK]\" \"earth\" \"wind\"  \"and\"   \"fire\" # ['', '[UNK]', 'earth', 'wind', 'and', 'fire']"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_tfsm.html","id":null,"dir":"Reference","previous_headings":"","what":"Reload a Keras model/layer that was saved via SavedModel / ExportArchive. — layer_tfsm","title":"Reload a Keras model/layer that was saved via SavedModel / ExportArchive. — layer_tfsm","text":"Reload Keras model/layer saved via SavedModel / ExportArchive.","code":""},{"path":"https://keras.posit.co/reference/layer_tfsm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reload a Keras model/layer that was saved via SavedModel / ExportArchive. — layer_tfsm","text":"","code":"layer_tfsm(   object,   filepath,   call_endpoint = \"serve\",   call_training_endpoint = NULL,   trainable = TRUE,   name = NULL,   dtype = NULL )"},{"path":"https://keras.posit.co/reference/layer_tfsm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reload a Keras model/layer that was saved via SavedModel / ExportArchive. — layer_tfsm","text":"object Object compose layer . tensor, array, sequential model. filepath str. path SavedModel. call_endpoint Name endpoint use call() method reloaded layer. SavedModel created via export_savedmodel(), default endpoint name 'serve'. cases may named 'serving_default'. call_training_endpoint see description trainable see description name String, name object dtype datatype (e.g., \"float32\").","code":""},{"path":"https://keras.posit.co/reference/layer_tfsm.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reload a Keras model/layer that was saved via SavedModel / ExportArchive. — layer_tfsm","text":"reloaded object can used like regular Keras layer, supports training/fine-tuning trainable weights. Note reloaded object retains none internal structure custom methods original object -- brand new layer created around saved function. Limitations: call endpoints single inputs tensor argument (may optionally named list/list tensors) supported. endpoints multiple separate input tensor arguments, consider subclassing layer_tfsm implementing call() method custom signature. need training-time behavior differ inference-time behavior (.e. need reloaded object support training=TRUE argument __call__()), make sure training-time call function saved standalone endpoint artifact, provide name layer_tfsm via call_training_endpoint argument.","code":"model %>% export_savedmodel(\"path/to/artifact\") reloaded_layer <- layer_tfsm(\"path/to/artifact\") outputs <- reloaded_layer(inputs)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_time_distributed.html","id":null,"dir":"Reference","previous_headings":"","what":"This wrapper allows to apply a layer to every temporal slice of an input. — layer_time_distributed","title":"This wrapper allows to apply a layer to every temporal slice of an input. — layer_time_distributed","text":"Every input least 3D, dimension index one first input considered temporal dimension. Consider batch 32 video samples, sample 128x128 RGB image channels_last data format, across 10 timesteps. batch input shape (32, 10, 128, 128, 3). can use TimeDistributed apply Conv2D layer 10 timesteps, independently:     layer_time_distributed applies instance layer_conv2d timestamps, set weights used timestamp.","code":"inputs <- layer_input(shape = c(10, 128, 128, 3), batch_size = 32) conv_2d_layer <- layer_conv_2d(filters = 64, kernel_size = c(3, 3)) outputs <- layer_time_distributed(inputs, layer = conv_2d_layer) shape(outputs) ## shape(32, 10, 126, 126, 64)"},{"path":"https://keras.posit.co/reference/layer_time_distributed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"This wrapper allows to apply a layer to every temporal slice of an input. — layer_time_distributed","text":"","code":"layer_time_distributed(object, layer, ...)"},{"path":"https://keras.posit.co/reference/layer_time_distributed.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"This wrapper allows to apply a layer to every temporal slice of an input. — layer_time_distributed","text":"object Object compose layer . tensor, array, sequential model. layer layer_Layer instance. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_time_distributed.html","id":"call-arguments","dir":"Reference","previous_headings":"","what":"Call Arguments","title":"This wrapper allows to apply a layer to every temporal slice of an input. — layer_time_distributed","text":"inputs: Input tensor shape (batch, time, ...) nested tensors, shape (batch, time, ...). training: Boolean indicating whether layer behave training mode inference mode. argument passed wrapped layer (layer supports argument). mask: Binary tensor shape (samples, timesteps) indicating whether given timestep masked. argument passed wrapped layer (layer supports argument).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_torch_module_wrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Torch module wrapper layer. — layer_torch_module_wrapper","title":"Torch module wrapper layer. — layer_torch_module_wrapper","text":"layer_torch_module_wrapper wrapper class can turn torch.nn.Module Keras layer, particular making parameters trackable Keras.","code":""},{"path":"https://keras.posit.co/reference/layer_torch_module_wrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Torch module wrapper layer. — layer_torch_module_wrapper","text":"","code":"layer_torch_module_wrapper(object, module, name = NULL, ...)"},{"path":"https://keras.posit.co/reference/layer_torch_module_wrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Torch module wrapper layer. — layer_torch_module_wrapper","text":"object Object compose layer . tensor, array, sequential model. module torch.nn.Module instance. LazyModule instance, parameters must initialized passing instance layer_torch_module_wrapper (e.g. calling ). name name layer (string). ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_torch_module_wrapper.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Torch module wrapper layer. — layer_torch_module_wrapper","text":"example layer_torch_module_wrapper can used vanilla PyTorch modules.","code":"torch <- import(\"torch\") nn <- import(\"torch.nn\") nnf <- import(\"torch.nn.functional\")  Classifier(keras$Model) %py_class% {    initialize <- function(...) {     super$initialize(...)      self$conv1 <- layer_torch_module_wrapper(module = nn$Conv2d(       in_channels=1L, out_channels=32L, kernel_size=c(3L, 3L)     ))     self$conv2 <- layer_torch_module_wrapper(module = nn$Conv2d(       in_channels=32L, out_channels=64L, kernel_size=c(3L, 3L)     ))     self$pool <- nn$MaxPool2d(kernel_size=c(2L, 2L))     self$flatten <- nn$Flatten()     self$dropout <- nn$Dropout(p=0.5)     self$fc <- layer_torch_module_wrapper(module = nn$Linear(       1600L, 10L     ))   }    call <- function(inputs) {     x <- nnf$relu(self$conv1(inputs))     x <- self$pool(x)     x <- nnf$relu(self$conv2(x))     x <- self$pool(x)     x <- self$flatten(x)     x <- self$dropout(x)     x <- self$fc(x)     nnf$softmax(x, dim=1L)   }  }  model <- Classifier() model$build(shape(1, 28, 28)) print(\"Output shape:\", model(torch$ones(shape(1L, 1L, 28L, 28L))))  model %>% compile(     loss=\"sparse_categorical_crossentropy\",     optimizer=\"adam\",     metrics=\"accuracy\" ) model %>% fit(train_loader, epochs=5)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_unit_normalization.html","id":null,"dir":"Reference","previous_headings":"","what":"Unit normalization layer. — layer_unit_normalization","title":"Unit normalization layer. — layer_unit_normalization","text":"Normalize batch inputs input batch L2 norm equal 1 (across axes specified axis).","code":""},{"path":"https://keras.posit.co/reference/layer_unit_normalization.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Unit normalization layer. — layer_unit_normalization","text":"","code":"layer_unit_normalization(object, axis = -1L, ...)"},{"path":"https://keras.posit.co/reference/layer_unit_normalization.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Unit normalization layer. — layer_unit_normalization","text":"object Object compose layer . tensor, array, sequential model. axis Integer list. axis axes normalize across. Typically, features axis axes. left-axes typically batch axis axes. -1 last dimension input. Defaults -1. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_unit_normalization.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Unit normalization layer. — layer_unit_normalization","text":"","code":"data <- op_reshape(1:6, new_shape = c(2, 3)) normalized_data <- layer_unit_normalization(data) op_sum(normalized_data[1,]^2) ## tf.Tensor(0.9999999, shape=(), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/layer_upsampling_1d.html","id":null,"dir":"Reference","previous_headings":"","what":"Upsampling layer for 1D inputs. — layer_upsampling_1d","title":"Upsampling layer for 1D inputs. — layer_upsampling_1d","text":"Repeats temporal step size times along time axis.","code":""},{"path":"https://keras.posit.co/reference/layer_upsampling_1d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upsampling layer for 1D inputs. — layer_upsampling_1d","text":"","code":"layer_upsampling_1d(object, size = 2L, ...)"},{"path":"https://keras.posit.co/reference/layer_upsampling_1d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upsampling layer for 1D inputs. — layer_upsampling_1d","text":"object Object compose layer . tensor, array, sequential model. size Integer. Upsampling factor. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_upsampling_1d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Upsampling layer for 1D inputs. — layer_upsampling_1d","text":"[[ 6.  7.  8.] [ 6.  7.  8.] [ 9. 10. 11.] [ 9. 10. 11.]]]","code":"input_shape <- c(2, 2, 3) x <- seq_len(prod(input_shape)) %>% op_reshape(input_shape) x ## tf.Tensor( ## [[[ 1  2  3] ##   [ 4  5  6]] ## ##  [[ 7  8  9] ##   [10 11 12]]], shape=(2, 2, 3), dtype=int64) y <- layer_upsampling_1d(x, size = 2) y ## tf.Tensor( ## [[[ 1  2  3] ##   [ 1  2  3] ##   [ 4  5  6] ##   [ 4  5  6]] ## ##  [[ 7  8  9] ##   [ 7  8  9] ##   [10 11 12] ##   [10 11 12]]], shape=(2, 4, 3), dtype=int64)"},{"path":"https://keras.posit.co/reference/layer_upsampling_1d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Upsampling layer for 1D inputs. — layer_upsampling_1d","text":"3D tensor shape: (batch_size, steps, features).","code":""},{"path":"https://keras.posit.co/reference/layer_upsampling_1d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Upsampling layer for 1D inputs. — layer_upsampling_1d","text":"3D tensor shape: (batch_size, upsampled_steps, features).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_upsampling_2d.html","id":null,"dir":"Reference","previous_headings":"","what":"Upsampling layer for 2D inputs. — layer_upsampling_2d","title":"Upsampling layer for 2D inputs. — layer_upsampling_2d","text":"implementation uses interpolative resizing, given resize method (specified interpolation argument). Use interpolation=nearest repeat rows columns data.","code":""},{"path":"https://keras.posit.co/reference/layer_upsampling_2d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upsampling layer for 2D inputs. — layer_upsampling_2d","text":"","code":"layer_upsampling_2d(   object,   size = list(2L, 2L),   data_format = NULL,   interpolation = \"nearest\",   ... )"},{"path":"https://keras.posit.co/reference/layer_upsampling_2d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upsampling layer for 2D inputs. — layer_upsampling_2d","text":"object Object compose layer . tensor, array, sequential model. size Int, list 2 integers. upsampling factors rows columns. data_format string, one \"channels_last\" (default) \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch_size, height, width, channels) \"channels_first\" corresponds inputs shape (batch_size, channels, height, width). unspecified, uses image_data_format value found Keras config file ~/.keras/keras.json (exists) else \"channels_last\". Defaults \"channels_last\". interpolation string, one \"bicubic\", \"bilinear\", \"lanczos3\", \"lanczos5\", \"nearest\". ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_upsampling_2d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Upsampling layer for 2D inputs. — layer_upsampling_2d","text":"","code":"input_shape <- c(2, 2, 1, 3) x <- op_reshape(seq_len(prod(input_shape)), input_shape) print(x) ## tf.Tensor( ## [[[[ 1  2  3]] ## ##   [[ 4  5  6]]] ## ## ##  [[[ 7  8  9]] ## ##   [[10 11 12]]]], shape=(2, 2, 1, 3), dtype=int64) y <- layer_upsampling_2d(x, size = c(1, 2)) print(y) ## tf.Tensor( ## [[[[ 1  2  3] ##    [ 1  2  3]] ## ##   [[ 4  5  6] ##    [ 4  5  6]]] ## ## ##  [[[ 7  8  9] ##    [ 7  8  9]] ## ##   [[10 11 12] ##    [10 11 12]]]], shape=(2, 2, 2, 3), dtype=int64)"},{"path":"https://keras.posit.co/reference/layer_upsampling_2d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Upsampling layer for 2D inputs. — layer_upsampling_2d","text":"4D tensor shape: data_format \"channels_last\": (batch_size, rows, cols, channels) data_format \"channels_first\": (batch_size, channels, rows, cols)","code":""},{"path":"https://keras.posit.co/reference/layer_upsampling_2d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Upsampling layer for 2D inputs. — layer_upsampling_2d","text":"4D tensor shape: data_format \"channels_last\": (batch_size, upsampled_rows, upsampled_cols, channels) data_format \"channels_first\": (batch_size, channels, upsampled_rows, upsampled_cols)","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_upsampling_3d.html","id":null,"dir":"Reference","previous_headings":"","what":"Upsampling layer for 3D inputs. — layer_upsampling_3d","title":"Upsampling layer for 3D inputs. — layer_upsampling_3d","text":"Repeats 1st, 2nd 3rd dimensions data size[0], size[1] size[2] respectively.","code":""},{"path":"https://keras.posit.co/reference/layer_upsampling_3d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upsampling layer for 3D inputs. — layer_upsampling_3d","text":"","code":"layer_upsampling_3d(object, size = list(2L, 2L, 2L), data_format = NULL, ...)"},{"path":"https://keras.posit.co/reference/layer_upsampling_3d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upsampling layer for 3D inputs. — layer_upsampling_3d","text":"object Object compose layer . tensor, array, sequential model. size Int, list 3 integers. upsampling factors dim1, dim2 dim3. data_format string, one \"channels_last\" (default) \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) \"channels_first\" corresponds inputs shape (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3). unspecified, uses image_data_format value found Keras config file ~/.keras/keras.json (exists) else \"channels_last\". Defaults \"channels_last\". ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_upsampling_3d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Upsampling layer for 3D inputs. — layer_upsampling_3d","text":"","code":"input_shape <- c(2, 1, 2, 1, 3) x <- array(1, dim = input_shape) y <- layer_upsampling_3d(x, size = c(2, 2, 2)) shape(y) ## shape(2, 2, 4, 2, 3)"},{"path":"https://keras.posit.co/reference/layer_upsampling_3d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Upsampling layer for 3D inputs. — layer_upsampling_3d","text":"5D tensor shape: data_format \"channels_last\": (batch_size, dim1, dim2, dim3, channels) data_format \"channels_first\": (batch_size, channels, dim1, dim2, dim3)","code":""},{"path":"https://keras.posit.co/reference/layer_upsampling_3d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Upsampling layer for 3D inputs. — layer_upsampling_3d","text":"5D tensor shape: data_format \"channels_last\": (batch_size, upsampled_dim1, upsampled_dim2, upsampled_dim3, channels) data_format \"channels_first\": (batch_size, channels, upsampled_dim1, upsampled_dim2, upsampled_dim3)","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_zero_padding_1d.html","id":null,"dir":"Reference","previous_headings":"","what":"Zero-padding layer for 1D input (e.g. temporal sequence). — layer_zero_padding_1d","title":"Zero-padding layer for 1D input (e.g. temporal sequence). — layer_zero_padding_1d","text":"Zero-padding layer 1D input (e.g. temporal sequence).","code":""},{"path":"https://keras.posit.co/reference/layer_zero_padding_1d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Zero-padding layer for 1D input (e.g. temporal sequence). — layer_zero_padding_1d","text":"","code":"layer_zero_padding_1d(object, padding = 1L, ...)"},{"path":"https://keras.posit.co/reference/layer_zero_padding_1d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Zero-padding layer for 1D input (e.g. temporal sequence). — layer_zero_padding_1d","text":"object Object compose layer . tensor, array, sequential model. padding Int, list int (length 2), named listionary. int: many zeros add beginning end padding dimension (axis 1). list 2 ints: many zeros add beginning end padding dimension ((left_pad, right_pad)). ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_zero_padding_1d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Zero-padding layer for 1D input (e.g. temporal sequence). — layer_zero_padding_1d","text":"","code":"input_shape <- c(2, 2, 3) x <- op_reshape(seq_len(prod(input_shape)), input_shape) x ## tf.Tensor( ## [[[ 1  2  3] ##   [ 4  5  6]] ## ##  [[ 7  8  9] ##   [10 11 12]]], shape=(2, 2, 3), dtype=int64) y <- layer_zero_padding_1d(x, padding = 2) y ## tf.Tensor( ## [[[ 0  0  0] ##   [ 0  0  0] ##   [ 1  2  3] ##   [ 4  5  6] ##   [ 0  0  0] ##   [ 0  0  0]] ## ##  [[ 0  0  0] ##   [ 0  0  0] ##   [ 7  8  9] ##   [10 11 12] ##   [ 0  0  0] ##   [ 0  0  0]]], shape=(2, 6, 3), dtype=int64)"},{"path":"https://keras.posit.co/reference/layer_zero_padding_1d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Zero-padding layer for 1D input (e.g. temporal sequence). — layer_zero_padding_1d","text":"3D tensor shape (batch_size, axis_to_pad, features)","code":""},{"path":"https://keras.posit.co/reference/layer_zero_padding_1d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Zero-padding layer for 1D input (e.g. temporal sequence). — layer_zero_padding_1d","text":"3D tensor shape (batch_size, padded_axis, features)","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_zero_padding_2d.html","id":null,"dir":"Reference","previous_headings":"","what":"Zero-padding layer for 2D input (e.g. picture). — layer_zero_padding_2d","title":"Zero-padding layer for 2D input (e.g. picture). — layer_zero_padding_2d","text":"layer can add rows columns zeros top, bottom, left right side image tensor.","code":""},{"path":"https://keras.posit.co/reference/layer_zero_padding_2d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Zero-padding layer for 2D input (e.g. picture). — layer_zero_padding_2d","text":"","code":"layer_zero_padding_2d(object, padding = list(1L, 1L), data_format = NULL, ...)"},{"path":"https://keras.posit.co/reference/layer_zero_padding_2d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Zero-padding layer for 2D input (e.g. picture). — layer_zero_padding_2d","text":"object Object compose layer . tensor, array, sequential model. padding Int, list 2 ints, list 2 lists 2 ints. int: symmetric padding applied height width. list 2 ints: interpreted two different symmetric padding values height width: (symmetric_height_pad, symmetric_width_pad). list 2 lists 2 ints: interpreted ((top_pad, bottom_pad), (left_pad, right_pad)). data_format string, one \"channels_last\" (default) \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch_size, height, width, channels) \"channels_first\" corresponds inputs shape (batch_size, channels, height, width). unspecified, uses image_data_format value found Keras config file ~/.keras/keras.json (exists). Defaults \"channels_last\". ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_zero_padding_2d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Zero-padding layer for 2D input (e.g. picture). — layer_zero_padding_2d","text":"","code":"input_shape <- c(1, 1, 2, 2) x <- op_reshape(seq_len(prod(input_shape)), input_shape) x ## tf.Tensor( ## [[[[1 2] ##    [3 4]]]], shape=(1, 1, 2, 2), dtype=int64) y <- layer_zero_padding_2d(x, padding = 1) y ## tf.Tensor( ## [[[[0 0] ##    [0 0] ##    [0 0] ##    [0 0]] ## ##   [[0 0] ##    [1 2] ##    [3 4] ##    [0 0]] ## ##   [[0 0] ##    [0 0] ##    [0 0] ##    [0 0]]]], shape=(1, 3, 4, 2), dtype=int64)"},{"path":"https://keras.posit.co/reference/layer_zero_padding_2d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Zero-padding layer for 2D input (e.g. picture). — layer_zero_padding_2d","text":"4D tensor shape: data_format \"channels_last\": (batch_size, height, width, channels) data_format \"channels_first\": (batch_size, channels, height, width)","code":""},{"path":"https://keras.posit.co/reference/layer_zero_padding_2d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Zero-padding layer for 2D input (e.g. picture). — layer_zero_padding_2d","text":"4D tensor shape: data_format \"channels_last\": (batch_size, padded_height, padded_width, channels) data_format \"channels_first\": (batch_size, channels, padded_height, padded_width)","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/layer_zero_padding_3d.html","id":null,"dir":"Reference","previous_headings":"","what":"Zero-padding layer for 3D data (spatial or spatio-temporal). — layer_zero_padding_3d","title":"Zero-padding layer for 3D data (spatial or spatio-temporal). — layer_zero_padding_3d","text":"Zero-padding layer 3D data (spatial spatio-temporal).","code":""},{"path":"https://keras.posit.co/reference/layer_zero_padding_3d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Zero-padding layer for 3D data (spatial or spatio-temporal). — layer_zero_padding_3d","text":"","code":"layer_zero_padding_3d(   object,   padding = list(list(1L, 1L), list(1L, 1L), list(1L, 1L)),   data_format = NULL,   ... )"},{"path":"https://keras.posit.co/reference/layer_zero_padding_3d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Zero-padding layer for 3D data (spatial or spatio-temporal). — layer_zero_padding_3d","text":"object Object compose layer . tensor, array, sequential model. padding Int, list 3 ints, list 3 lists 2 ints. int: symmetric padding applied depth, height, width. list 3 ints: interpreted three different symmetric padding values depth, height, width: (symmetric_dim1_pad, symmetric_dim2_pad, symmetric_dim3_pad). list 3 lists 2 ints: interpreted ((left_dim1_pad, right_dim1_pad), (left_dim2_pad, right_dim2_pad), (left_dim3_pad, right_dim3_pad)). data_format string, one \"channels_last\" (default) \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels) \"channels_first\" corresponds inputs shape (batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3). unspecified, uses image_data_format value found Keras config file ~/.keras/keras.json (exists). Defaults \"channels_last\". ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/layer_zero_padding_3d.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Zero-padding layer for 3D data (spatial or spatio-temporal). — layer_zero_padding_3d","text":"","code":"input_shape <- c(1, 1, 2, 2, 3) x <- op_reshape(seq_len(prod(input_shape)), input_shape) x ## tf.Tensor( ## [[[[[ 1  2  3] ##     [ 4  5  6]] ## ##    [[ 7  8  9] ##     [10 11 12]]]]], shape=(1, 1, 2, 2, 3), dtype=int64) y <- layer_zero_padding_3d(x, padding = 2) shape(y) ## shape(1, 5, 6, 6, 3)"},{"path":"https://keras.posit.co/reference/layer_zero_padding_3d.html","id":"input-shape","dir":"Reference","previous_headings":"","what":"Input Shape","title":"Zero-padding layer for 3D data (spatial or spatio-temporal). — layer_zero_padding_3d","text":"5D tensor shape: data_format \"channels_last\": (batch_size, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad, depth) data_format \"channels_first\": (batch_size, depth, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad)","code":""},{"path":"https://keras.posit.co/reference/layer_zero_padding_3d.html","id":"output-shape","dir":"Reference","previous_headings":"","what":"Output Shape","title":"Zero-padding layer for 3D data (spatial or spatio-temporal). — layer_zero_padding_3d","text":"5D tensor shape: data_format \"channels_last\": (batch_size, first_padded_axis, second_padded_axis, third_axis_to_pad, depth) data_format \"channels_first\": (batch_size, depth, first_padded_axis, second_padded_axis, third_axis_to_pad)","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/learning_rate_schedule_cosine_decay.html","id":null,"dir":"Reference","previous_headings":"","what":"A LearningRateSchedule that uses a cosine decay with optional warmup. — learning_rate_schedule_cosine_decay","title":"A LearningRateSchedule that uses a cosine decay with optional warmup. — learning_rate_schedule_cosine_decay","text":"See Loshchilov & Hutter, ICLR2016, SGDR: Stochastic Gradient Descent Warm Restarts. idea linear warmup learning rate, see Goyal et al.. begin training model, often want initial increase learning rate followed decay. warmup_target int, schedule applies linear increase per optimizer step learning rate initial_learning_rate warmup_target duration warmup_steps. Afterwards, applies cosine decay function taking learning rate warmup_target alpha duration decay_steps. warmup_target NULL skip warmup decay take learning rate initial_learning_rate alpha. requires step value  compute learning rate. can just pass backend variable increment training step. schedule 1-arg callable produces warmup followed decayed learning rate passed current optimizer step. can useful changing learning rate value across different invocations optimizer functions. warmup computed :   decay computed :   Example usage without warmup:   Example usage warmup:   can pass schedule directly optimizer learning rate. learning rate schedule also serializable deserializable using keras$optimizers$schedules$serialize keras$optimizers$schedules$deserialize.","code":"warmup_learning_rate <- function(step) {   completed_fraction <- step / warmup_steps   total_delta <- target_warmup - initial_learning_rate   completed_fraction * total_delta } if (is.null(warmup_target)) {   initial_decay_lr <- initial_learning_rate } else {   initial_decay_lr <- warmup_target }  decayed_learning_rate <- function(step) {   step <- min(step, decay_steps)   cosine_decay <- 0.5 * (1 + cos(pi * step / decay_steps))   decayed <- (1 - alpha) * cosine_decay + alpha   initial_decay_lr * decayed } decay_steps <- 1000 initial_learning_rate <- 0.1 lr_decayed_fn <- learning_rate_schedule_cosine_decay(     initial_learning_rate, decay_steps) decay_steps <- 1000 initial_learning_rate <- 0 warmup_steps <- 1000 target_learning_rate <- 0.1 lr_warmup_decayed_fn <- learning_rate_schedule_cosine_decay(     initial_learning_rate, decay_steps, warmup_target = target_learning_rate,     warmup_steps = warmup_steps )"},{"path":"https://keras.posit.co/reference/learning_rate_schedule_cosine_decay.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A LearningRateSchedule that uses a cosine decay with optional warmup. — learning_rate_schedule_cosine_decay","text":"","code":"learning_rate_schedule_cosine_decay(   initial_learning_rate,   decay_steps,   alpha = 0,   name = \"CosineDecay\",   warmup_target = NULL,   warmup_steps = 0L )"},{"path":"https://keras.posit.co/reference/learning_rate_schedule_cosine_decay.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A LearningRateSchedule that uses a cosine decay with optional warmup. — learning_rate_schedule_cosine_decay","text":"initial_learning_rate float. initial learning rate. decay_steps int. Number steps decay . alpha float. Minimum learning rate value decay fraction initial_learning_rate. name String. Optional name operation.  Defaults \"CosineDecay\". warmup_target float. target learning rate warmup phase. cast initial_learning_rate datatype. Setting NULL skip warmup begins decay phase initial_learning_rate. Otherwise scheduler warmup initial_learning_rate warmup_target. warmup_steps int. Number steps warmup .","code":""},{"path":"https://keras.posit.co/reference/learning_rate_schedule_cosine_decay.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A LearningRateSchedule that uses a cosine decay with optional warmup. — learning_rate_schedule_cosine_decay","text":"1-arg callable learning rate schedule takes current optimizer step outputs decayed learning rate, scalar tensor type initial_learning_rate.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/learning_rate_schedule_cosine_decay_restarts.html","id":null,"dir":"Reference","previous_headings":"","what":"A LearningRateSchedule that uses a cosine decay schedule with restarts. — learning_rate_schedule_cosine_decay_restarts","title":"A LearningRateSchedule that uses a cosine decay schedule with restarts. — learning_rate_schedule_cosine_decay_restarts","text":"See Loshchilov & Hutter, ICLR2016, SGDR: Stochastic Gradient Descent Warm Restarts. training model, often useful lower learning rate training progresses. schedule applies cosine decay function restarts optimizer step, given provided initial learning rate. requires step value compute decayed learning rate. can just pass backend variable increment training step. schedule 1-arg callable produces decayed learning rate passed current optimizer step. can useful changing learning rate value across different invocations optimizer functions. learning rate multiplier first decays 1 alpha first_decay_steps steps. , warm restart performed. new warm restart runs t_mul times steps m_mul times initial learning rate new learning rate. Example usage:   can pass schedule directly optimizer learning rate. learning rate schedule also serializable deserializable using keras$optimizers$schedules$serialize keras$optimizers$schedules$deserialize.","code":"first_decay_steps <- 1000 lr_decayed_fn <- learning_rate_schedule_cosine_decay_restarts(         0.001,         first_decay_steps)"},{"path":"https://keras.posit.co/reference/learning_rate_schedule_cosine_decay_restarts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A LearningRateSchedule that uses a cosine decay schedule with restarts. — learning_rate_schedule_cosine_decay_restarts","text":"","code":"learning_rate_schedule_cosine_decay_restarts(   initial_learning_rate,   first_decay_steps,   t_mul = 2,   m_mul = 1,   alpha = 0,   name = \"SGDRDecay\" )"},{"path":"https://keras.posit.co/reference/learning_rate_schedule_cosine_decay_restarts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A LearningRateSchedule that uses a cosine decay schedule with restarts. — learning_rate_schedule_cosine_decay_restarts","text":"initial_learning_rate float. initial learning rate. first_decay_steps integer. Number steps decay . t_mul float. Used derive number iterations -th period. m_mul float. Used derive initial learning rate -th period. alpha float. Minimum learning rate value fraction initial_learning_rate. name String. Optional name operation. Defaults \"SGDRDecay\".","code":""},{"path":"https://keras.posit.co/reference/learning_rate_schedule_cosine_decay_restarts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A LearningRateSchedule that uses a cosine decay schedule with restarts. — learning_rate_schedule_cosine_decay_restarts","text":"1-arg callable learning rate schedule takes current optimizer step outputs decayed learning rate, scalar tensor type initial_learning_rate.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/learning_rate_schedule_exponential_decay.html","id":null,"dir":"Reference","previous_headings":"","what":"A LearningRateSchedule that uses an exponential decay schedule. — learning_rate_schedule_exponential_decay","title":"A LearningRateSchedule that uses an exponential decay schedule. — learning_rate_schedule_exponential_decay","text":"training model, often useful lower learning rate training progresses. schedule applies exponential decay function optimizer step, given provided initial learning rate. schedule 1-arg callable produces decayed learning rate passed current optimizer step. can useful changing learning rate value across different invocations optimizer functions. computed :   argument staircase TRUE, step / decay_steps integer division decayed learning rate follows staircase function. can pass schedule directly optimizer learning rate.","code":"decayed_learning_rate <- function(step) {   initial_learning_rate * decay_rate ^ (step / decay_steps) }"},{"path":"https://keras.posit.co/reference/learning_rate_schedule_exponential_decay.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A LearningRateSchedule that uses an exponential decay schedule. — learning_rate_schedule_exponential_decay","text":"","code":"learning_rate_schedule_exponential_decay(   initial_learning_rate,   decay_steps,   decay_rate,   staircase = FALSE,   name = \"ExponentialDecay\" )"},{"path":"https://keras.posit.co/reference/learning_rate_schedule_exponential_decay.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A LearningRateSchedule that uses an exponential decay schedule. — learning_rate_schedule_exponential_decay","text":"initial_learning_rate float. initial learning rate. decay_steps integer. Must positive. See decay computation . decay_rate float. decay rate. staircase Boolean.  TRUE decay learning rate discrete intervals. name String.  Optional name operation.  Defaults \"ExponentialDecay\".","code":""},{"path":"https://keras.posit.co/reference/learning_rate_schedule_exponential_decay.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A LearningRateSchedule that uses an exponential decay schedule. — learning_rate_schedule_exponential_decay","text":"1-arg callable learning rate schedule takes current optimizer step outputs decayed learning rate, scalar tensor type initial_learning_rate.","code":""},{"path":"https://keras.posit.co/reference/learning_rate_schedule_exponential_decay.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A LearningRateSchedule that uses an exponential decay schedule. — learning_rate_schedule_exponential_decay","text":"fitting Keras model, decay every 100000 steps base 0.96:   learning rate schedule also serializable deserializable using keras$optimizers$schedules$serialize keras$optimizers$schedules$deserialize.","code":"initial_learning_rate <- 0.1 lr_schedule <- learning_rate_schedule_exponential_decay(     initial_learning_rate,     decay_steps=100000,     decay_rate=0.96,     staircase=TRUE)  model %>% compile(   optimizer = optimizer_sgd(learning_rate = lr_schedule),   loss = 'sparse_categorical_crossentropy',   metrics = c('accuracy'))  model %>% fit(data, labels, epochs=5)"},{"path":[]},{"path":"https://keras.posit.co/reference/learning_rate_schedule_inverse_time_decay.html","id":null,"dir":"Reference","previous_headings":"","what":"A LearningRateSchedule that uses an inverse time decay schedule. — learning_rate_schedule_inverse_time_decay","title":"A LearningRateSchedule that uses an inverse time decay schedule. — learning_rate_schedule_inverse_time_decay","text":"training model, often useful lower learning rate training progresses. schedule applies inverse decay function optimizer step, given provided initial learning rate. requires step value compute decayed learning rate. can just pass backend variable increment training step. schedule 1-arg callable produces decayed learning rate passed current optimizer step. can useful changing learning rate value across different invocations optimizer functions. computed :   , staircase TRUE, :   can pass schedule directly optimizer_* learning rate.","code":"decayed_learning_rate <- function(step) {   initial_learning_rate / (1 + decay_rate * step / decay_step) } decayed_learning_rate <- function(step) {   initial_learning_rate /            (1 + decay_rate * floor(step / decay_step)) }"},{"path":"https://keras.posit.co/reference/learning_rate_schedule_inverse_time_decay.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A LearningRateSchedule that uses an inverse time decay schedule. — learning_rate_schedule_inverse_time_decay","text":"","code":"learning_rate_schedule_inverse_time_decay(   initial_learning_rate,   decay_steps,   decay_rate,   staircase = FALSE,   name = \"InverseTimeDecay\" )"},{"path":"https://keras.posit.co/reference/learning_rate_schedule_inverse_time_decay.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A LearningRateSchedule that uses an inverse time decay schedule. — learning_rate_schedule_inverse_time_decay","text":"initial_learning_rate float. initial learning rate. decay_steps often apply decay. decay_rate number.  decay rate. staircase Whether apply decay discrete staircase, o pposed continuous, fashion. name String.  Optional name operation.  Defaults \"InverseTimeDecay\".","code":""},{"path":"https://keras.posit.co/reference/learning_rate_schedule_inverse_time_decay.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A LearningRateSchedule that uses an inverse time decay schedule. — learning_rate_schedule_inverse_time_decay","text":"1-arg callable learning rate schedule takes current optimizer step outputs decayed learning rate, scalar tensor type initial_learning_rate.","code":""},{"path":"https://keras.posit.co/reference/learning_rate_schedule_inverse_time_decay.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A LearningRateSchedule that uses an inverse time decay schedule. — learning_rate_schedule_inverse_time_decay","text":"Fit Keras model decaying 1/t rate 0.5:","code":"... initial_learning_rate <- 0.1 decay_steps <- 1.0 decay_rate <- 0.5 learning_rate_fn <- learning_rate_schedule_inverse_time_decay(     initial_learning_rate, decay_steps, decay_rate)  model %>% compile(   optimizer = optimizer_sgd(learning_rate=learning_rate_fn),   loss = 'sparse_categorical_crossentropy',   metrics = 'accuracy') )  model %>% fit(data, labels, epochs=5)"},{"path":[]},{"path":"https://keras.posit.co/reference/learning_rate_schedule_piecewise_constant_decay.html","id":null,"dir":"Reference","previous_headings":"","what":"A LearningRateSchedule that uses a piecewise constant decay schedule. — learning_rate_schedule_piecewise_constant_decay","title":"A LearningRateSchedule that uses a piecewise constant decay schedule. — learning_rate_schedule_piecewise_constant_decay","text":"function returns 1-arg callable compute piecewise constant passed current optimizer step. can useful changing learning rate value across different invocations optimizer functions.","code":""},{"path":"https://keras.posit.co/reference/learning_rate_schedule_piecewise_constant_decay.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A LearningRateSchedule that uses a piecewise constant decay schedule. — learning_rate_schedule_piecewise_constant_decay","text":"","code":"learning_rate_schedule_piecewise_constant_decay(   boundaries,   values,   name = \"PiecewiseConstant\" )"},{"path":"https://keras.posit.co/reference/learning_rate_schedule_piecewise_constant_decay.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A LearningRateSchedule that uses a piecewise constant decay schedule. — learning_rate_schedule_piecewise_constant_decay","text":"boundaries list Python numbers strictly increasing entries, elements type optimizer step. values list Python numbers specifies values intervals defined boundaries. one element boundaries, elements type. name string. Optional name operation. Defaults \"PiecewiseConstant\".","code":""},{"path":"https://keras.posit.co/reference/learning_rate_schedule_piecewise_constant_decay.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A LearningRateSchedule that uses a piecewise constant decay schedule. — learning_rate_schedule_piecewise_constant_decay","text":"1-arg callable learning rate schedule takes current optimizer step outputs decayed learning rate, scalar tensor type boundary tensors. output 1-arg function takes step values[0] step <= boundaries[0], values[1] step > boundaries[0] step <= boundaries[1], ..., values[-1] step > boundaries[-1].","code":""},{"path":"https://keras.posit.co/reference/learning_rate_schedule_piecewise_constant_decay.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A LearningRateSchedule that uses a piecewise constant decay schedule. — learning_rate_schedule_piecewise_constant_decay","text":"use learning rate 1.0 first 100001 steps, 0.5 next 10000 steps, 0.1 additional steps.   can pass schedule directly optimizer learning rate. learning rate schedule also serializable deserializable using keras$optimizers$schedules$serialize keras$optimizers$schedules$deserialize.","code":"step <- 0 boundaries <- c(100000, 110000) values <- c(1.0, 0.5, 0.1) learning_rate_fn <- learning_rate_schedule_piecewise_constant_decay(   boundaries, values)  # Later, whenever we perform an optimization step, we pass in the step. learning_rate <- learning_rate_fn(step)"},{"path":"https://keras.posit.co/reference/learning_rate_schedule_piecewise_constant_decay.html","id":"raises","dir":"Reference","previous_headings":"","what":"Raises","title":"A LearningRateSchedule that uses a piecewise constant decay schedule. — learning_rate_schedule_piecewise_constant_decay","text":"ValueError: number elements boundaries values lists match.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/learning_rate_schedule_polynomial_decay.html","id":null,"dir":"Reference","previous_headings":"","what":"A LearningRateSchedule that uses a polynomial decay schedule. — learning_rate_schedule_polynomial_decay","title":"A LearningRateSchedule that uses a polynomial decay schedule. — learning_rate_schedule_polynomial_decay","text":"commonly observed monotonically decreasing learning rate, whose degree change carefully chosen, results better performing model. schedule applies polynomial decay function optimizer step, given provided initial_learning_rate, reach end_learning_rate given decay_steps. requires step value compute decayed learning rate. can just pass backend variable increment training step. schedule 1-arg callable produces decayed learning rate passed current optimizer step. can useful changing learning rate value across different invocations optimizer functions. computed :   cycle TRUE multiple decay_steps used, first one bigger step.   can pass schedule directly keras.optimizers.Optimizer learning rate.","code":"decayed_learning_rate <- function(step) {   step = min(step, decay_steps)   ((initial_learning_rate - end_learning_rate) *     (1 - step / decay_steps) ^ (power)) +     end_learning_rate } decayed_learning_rate <- function(step) {   decay_steps = decay_steps * ceil(step / decay_steps)   ((initial_learning_rate - end_learning_rate) *       (1 - step / decay_steps) ^ (power)) +     end_learning_rate }"},{"path":"https://keras.posit.co/reference/learning_rate_schedule_polynomial_decay.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A LearningRateSchedule that uses a polynomial decay schedule. — learning_rate_schedule_polynomial_decay","text":"","code":"learning_rate_schedule_polynomial_decay(   initial_learning_rate,   decay_steps,   end_learning_rate = 1e-04,   power = 1,   cycle = FALSE,   name = \"PolynomialDecay\" )"},{"path":"https://keras.posit.co/reference/learning_rate_schedule_polynomial_decay.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A LearningRateSchedule that uses a polynomial decay schedule. — learning_rate_schedule_polynomial_decay","text":"initial_learning_rate float. initial learning rate. decay_steps integer. Must positive. See decay computation . end_learning_rate float. minimal end learning rate. power float. power polynomial. Defaults 1.0. cycle boolean, whether cycle beyond decay_steps. name String.  Optional name operation. Defaults \"PolynomialDecay\".","code":""},{"path":"https://keras.posit.co/reference/learning_rate_schedule_polynomial_decay.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A LearningRateSchedule that uses a polynomial decay schedule. — learning_rate_schedule_polynomial_decay","text":"1-arg callable learning rate schedule takes current optimizer step outputs decayed learning rate, scalar tensor type initial_learning_rate.","code":""},{"path":"https://keras.posit.co/reference/learning_rate_schedule_polynomial_decay.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A LearningRateSchedule that uses a polynomial decay schedule. — learning_rate_schedule_polynomial_decay","text":"Fit model decaying 0.1 0.01 10000 steps using sqrt (.e. power=0.5):   learning rate schedule also serializable deserializable using keras$optimizers$schedules$serialize keras$optimizers$schedules$deserialize.","code":"... starter_learning_rate <- 0.1 end_learning_rate <- 0.01 decay_steps <- 10000 learning_rate_fn <- learning_rate_schedule_polynomial_decay(     starter_learning_rate,     decay_steps,     end_learning_rate,     power=0.5)  model %>% compile(   optimizer = optimizer_sgd(learning_rate=learning_rate_fn),   loss = 'sparse_categorical_crossentropy',   metrics = 'accuracy' )  model %>% fit(data, labels, epochs=5)"},{"path":[]},{"path":"https://keras.posit.co/reference/load_keras_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Loads a model saved via model.save(). — load_keras_model","title":"Loads a model saved via model.save(). — load_keras_model","text":"Loads model saved via model.save().","code":""},{"path":"https://keras.posit.co/reference/load_keras_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Loads a model saved via model.save(). — load_keras_model","text":"","code":"load_keras_model(   filepath,   custom_objects = NULL,   compile = TRUE,   safe_mode = TRUE )"},{"path":"https://keras.posit.co/reference/load_keras_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Loads a model saved via model.save(). — load_keras_model","text":"filepath string, path saved model file. custom_objects Optional named list mapping names custom classes functions considered deserialization. compile Boolean, whether compile model loading. safe_mode Boolean, whether disallow unsafe lambda deserialization. safe_mode=FALSE, loading object potential trigger arbitrary code execution. argument applicable Keras v3 model format. Defaults TRUE.","code":""},{"path":"https://keras.posit.co/reference/load_keras_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Loads a model saved via model.save(). — load_keras_model","text":"Keras model instance. original model compiled, argument compile = TRUE set, returned model compiled. Otherwise, model left uncompiled.","code":""},{"path":"https://keras.posit.co/reference/load_keras_model.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Loads a model saved via model.save(). — load_keras_model","text":"Note model variables may different name values (var$name property, e.g. \"dense_1/kernel:0\") reloaded. recommended use layer attributes access specific variables, e.g. model |> get_layer(\"dense_1\") |> _$kernel.","code":"model <- keras_model_sequential(input_shape = c(3)) |>   layer_dense(5) |>   layer_activation_softmax()  model |> save_keras_model(\"model.keras\") loaded_model <- load_keras_model(\"model.keras\") x <- random_uniform(c(10, 3)) stopifnot(all.equal(   model |> predict(x),   loaded_model |> predict(x) ))"},{"path":[]},{"path":"https://keras.posit.co/reference/load_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Loads a model saved via model.save(). — load_model","title":"Loads a model saved via model.save(). — load_model","text":"Loads model saved via model.save().","code":""},{"path":"https://keras.posit.co/reference/load_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Loads a model saved via model.save(). — load_model","text":"","code":"load_model(model, custom_objects = NULL, compile = TRUE, safe_mode = TRUE)"},{"path":"https://keras.posit.co/reference/load_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Loads a model saved via model.save(). — load_model","text":"model string, path saved model file, raw vector, returned save_model(filepath = NULL) custom_objects Optional named list mapping names custom classes functions considered deserialization. compile Boolean, whether compile model loading. safe_mode Boolean, whether disallow unsafe lambda deserialization. safe_mode=FALSE, loading object potential trigger arbitrary code execution. argument applicable Keras v3 model format. Defaults TRUE.","code":""},{"path":"https://keras.posit.co/reference/load_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Loads a model saved via model.save(). — load_model","text":"Keras model instance. original model compiled, argument compile = TRUE set, returned model compiled. Otherwise, model left uncompiled.","code":""},{"path":"https://keras.posit.co/reference/load_model.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Loads a model saved via model.save(). — load_model","text":"Note model variables may different name values (var$name property, e.g. \"dense_1/kernel:0\") reloaded. recommended use layer attributes access specific variables, e.g. model |> get_layer(\"dense_1\") |> _$kernel.","code":"model <- keras_model_sequential(input_shape = c(3)) |>   layer_dense(5) |>   layer_activation_softmax()  model |> save_model(\"model.keras\") loaded_model <- load_model(\"model.keras\") x <- random_uniform(c(10, 3)) stopifnot(all.equal(   model |> predict(x),   loaded_model |> predict(x) ))"},{"path":[]},{"path":"https://keras.posit.co/reference/load_model_weights.html","id":null,"dir":"Reference","previous_headings":"","what":"Load weights from a file saved via save_model_weights(). — load_model_weights","title":"Load weights from a file saved via save_model_weights(). — load_model_weights","text":"Weights loaded based network's topology. means architecture weights saved. Note layers weights taken account topological ordering, adding removing layers fine long weights. Partial weight loading modified model, instance adding new layer (weights) changing shape weights layer, can choose ignore errors continue loading setting skip_mismatch=TRUE. case layer mismatching weights skipped. warning displayed skipped layer.","code":""},{"path":"https://keras.posit.co/reference/load_model_weights.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load weights from a file saved via save_model_weights(). — load_model_weights","text":"","code":"load_model_weights(model, filepath, skip_mismatch = FALSE, ...)"},{"path":"https://keras.posit.co/reference/load_model_weights.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load weights from a file saved via save_model_weights(). — load_model_weights","text":"model keras model. filepath String, path weights file load. can either .weights.h5 file legacy .h5 weights file. skip_mismatch Boolean, whether skip loading layers mismatch number weights, mismatch shape weights. ... forward/backward compatability.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/loss_binary_crossentropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the cross-entropy loss between true labels and predicted labels. — loss_binary_crossentropy","title":"Computes the cross-entropy loss between true labels and predicted labels. — loss_binary_crossentropy","text":"Use cross-entropy loss binary (0 1) classification applications. loss function requires following inputs: y_true (true label): either 0 1. y_pred (predicted value): model's prediction, .e, single floating-point value either represents logit, (.e, value [-inf, inf] from_logits=TRUE) probability (.e, value [0., 1.] from_logits=FALSE).","code":""},{"path":"https://keras.posit.co/reference/loss_binary_crossentropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the cross-entropy loss between true labels and predicted labels. — loss_binary_crossentropy","text":"","code":"loss_binary_crossentropy(   y_true,   y_pred,   from_logits = FALSE,   label_smoothing = 0,   axis = -1L,   ...,   reduction = \"sum_over_batch_size\",   name = \"binary_crossentropy\" )"},{"path":"https://keras.posit.co/reference/loss_binary_crossentropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the cross-entropy loss between true labels and predicted labels. — loss_binary_crossentropy","text":"y_true Ground truth values. shape = [batch_size, d0, .. dN]. y_pred predicted values. shape = [batch_size, d0, .. dN]. from_logits Whether interpret y_pred tensor logit values. default, assume y_pred probabilities (.e., values [0, 1)). label_smoothing Float range [0, 1]. 0, smoothing occurs. > 0, compute loss predicted labels smoothed version true labels, smoothing squeezes labels towards 0.5. Larger values label_smoothing correspond heavier smoothing. axis axis along compute crossentropy (features axis). Defaults -1. ... forward/backward compatability. reduction Type reduction apply loss. almost cases \"sum_over_batch_size\". Supported options \"sum\", \"sum_over_batch_size\" NULL. name Optional name loss instance.","code":""},{"path":"https://keras.posit.co/reference/loss_binary_crossentropy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the cross-entropy loss between true labels and predicted labels. — loss_binary_crossentropy","text":"Binary crossentropy loss value. shape = [batch_size, d0, .. dN-1].","code":""},{"path":"https://keras.posit.co/reference/loss_binary_crossentropy.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the cross-entropy loss between true labels and predicted labels. — loss_binary_crossentropy","text":"Recommended Usage: (set from_logits=TRUE) compile() API:   standalone function:                     Default Usage: (set from_logits=FALSE)","code":"y_true <- rbind(c(0, 1), c(0, 0)) y_pred <- rbind(c(0.6, 0.4), c(0.4, 0.6)) loss <- loss_binary_crossentropy(y_true, y_pred) loss ## tf.Tensor([0.91629073 0.71355818], shape=(2), dtype=float64) model %>% compile(     loss = loss_binary_crossentropy(from_logits=TRUE),     ... ) # Example 1: (batch_size = 1, number of samples = 4) y_true <- op_array(c(0, 1, 0, 0)) y_pred <- op_array(c(-18.6, 0.51, 2.94, -12.8)) bce <- loss_binary_crossentropy(from_logits = TRUE) bce(y_true, y_pred) ## tf.Tensor(0.865458, shape=(), dtype=float32) # Example 2: (batch_size = 2, number of samples = 4) y_true <- rbind(c(0, 1), c(0, 0)) y_pred <- rbind(c(-18.6, 0.51), c(2.94, -12.8)) # Using default 'auto'/'sum_over_batch_size' reduction type. bce <- loss_binary_crossentropy(from_logits = TRUE) bce(y_true, y_pred) ## tf.Tensor(0.865458, shape=(), dtype=float32) # Using 'sample_weight' attribute bce(y_true, y_pred, sample_weight = c(0.8, 0.2)) ## tf.Tensor(0.2436386, shape=(), dtype=float32) # 0.243 # Using 'sum' reduction` type. bce <- loss_binary_crossentropy(from_logits = TRUE, reduction = \"sum\") bce(y_true, y_pred) ## tf.Tensor(1.730916, shape=(), dtype=float32) # Using 'none' reduction type. bce <- loss_binary_crossentropy(from_logits = TRUE, reduction = NULL) bce(y_true, y_pred) ## tf.Tensor([0.23515666 1.4957594 ], shape=(2), dtype=float32) # Make the following updates to the above \"Recommended Usage\" section # 1. Set `from_logits=FALSE` loss_binary_crossentropy() # OR ...('from_logits=FALSE') ## <keras.src.losses.losses.BinaryCrossentropy object> # 2. Update `y_pred` to use probabilities instead of logits y_pred <- c(0.6, 0.3, 0.2, 0.8) # OR [[0.6, 0.3], [0.2, 0.8]]"},{"path":[]},{"path":"https://keras.posit.co/reference/loss_binary_focal_crossentropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes focal cross-entropy loss between true labels and predictions. — loss_binary_focal_crossentropy","title":"Computes focal cross-entropy loss between true labels and predictions. — loss_binary_focal_crossentropy","text":"According Lin et al., 2018, helps apply focal factor -weight easy examples focus hard examples. default, focal tensor computed follows: focal_factor = (1 - output)^gamma class 1 focal_factor = output^gamma class 0 gamma focusing parameter. gamma = 0, focal effect binary crossentropy loss. apply_class_balancing == TRUE, function also takes account weight balancing factor binary classes 0 1 follows: weight = alpha class 1 (target == 1) weight = 1 - alpha class 0 alpha float range [0, 1]. Binary cross-entropy loss often used binary (0 1) classification tasks. loss function requires following inputs: y_true (true label): either 0 1. y_pred (predicted value): model's prediction, .e, single floating-point value either represents logit, (.e, value [-inf, inf] from_logits=TRUE) probability (.e, value [0., 1.] from_logits=FALSE). According Lin et al., 2018, helps apply \"focal factor\" -weight easy examples focus hard examples. default, focal tensor computed follows: focal_factor = (1 - output) ** gamma class 1 focal_factor = output ** gamma class 0 gamma focusing parameter. gamma=0, function equivalent binary crossentropy loss.","code":""},{"path":"https://keras.posit.co/reference/loss_binary_focal_crossentropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes focal cross-entropy loss between true labels and predictions. — loss_binary_focal_crossentropy","text":"","code":"loss_binary_focal_crossentropy(   y_true,   y_pred,   apply_class_balancing = FALSE,   alpha = 0.25,   gamma = 2,   from_logits = FALSE,   label_smoothing = 0,   axis = -1L,   ...,   reduction = \"sum_over_batch_size\",   name = \"binary_focal_crossentropy\" )"},{"path":"https://keras.posit.co/reference/loss_binary_focal_crossentropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes focal cross-entropy loss between true labels and predictions. — loss_binary_focal_crossentropy","text":"y_true Ground truth values, shape (batch_size, d0, .. dN). y_pred predicted values, shape (batch_size, d0, .. dN). apply_class_balancing bool, whether apply weight balancing binary classes 0 1. alpha weight balancing factor class 1, default 0.25 mentioned reference Lin et al., 2018.  weight class 0 1.0 - alpha. gamma focusing parameter used compute focal factor, default 2.0 mentioned reference Lin et al., 2018. from_logits Whether interpret y_pred tensor logit values. default, assume y_pred probabilities (.e., values [0, 1]). label_smoothing Float [0, 1]. 0, smoothing occurs. > 0, compute loss predicted labels smoothed version true labels, smoothing squeezes labels towards 0.5. Larger values label_smoothing correspond heavier smoothing. axis axis along compute crossentropy (features axis). Defaults -1. ... forward/backward compatability. reduction Type reduction apply loss. almost cases \"sum_over_batch_size\". Supported options \"sum\", \"sum_over_batch_size\" NULL. name Optional name loss instance.","code":""},{"path":"https://keras.posit.co/reference/loss_binary_focal_crossentropy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes focal cross-entropy loss between true labels and predictions. — loss_binary_focal_crossentropy","text":"Binary focal crossentropy loss value shape = [batch_size, d0, .. dN-1].","code":""},{"path":"https://keras.posit.co/reference/loss_binary_focal_crossentropy.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes focal cross-entropy loss between true labels and predictions. — loss_binary_focal_crossentropy","text":"compile() API:   standalone function:","code":"y_true <- rbind(c(0, 1), c(0, 0)) y_pred <- rbind(c(0.6, 0.4), c(0.4, 0.6)) loss <- loss_binary_focal_crossentropy(y_true, y_pred, gamma = 2) loss ## tf.Tensor([0.32986466 0.20579838], shape=(2), dtype=float64) model %>% compile(     loss = loss_binary_focal_crossentropy(         gamma = 2.0, from_logits = TRUE),     ... ) # Example 1: (batch_size = 1, number of samples = 4) y_true <- op_array(c(0, 1, 0, 0)) y_pred <- op_array(c(-18.6, 0.51, 2.94, -12.8)) loss <- loss_binary_focal_crossentropy(gamma = 2, from_logits = TRUE) loss(y_true, y_pred) ## tf.Tensor(0.6912122, shape=(), dtype=float32) # Apply class weight loss <- loss_binary_focal_crossentropy(   apply_class_balancing = TRUE, gamma = 2, from_logits = TRUE) loss(y_true, y_pred) ## tf.Tensor(0.5101333, shape=(), dtype=float32) # Example 2: (batch_size = 2, number of samples = 4) y_true <- rbind(c(0, 1), c(0, 0)) y_pred <- rbind(c(-18.6, 0.51), c(2.94, -12.8)) # Using default 'auto'/'sum_over_batch_size' reduction type. loss <- loss_binary_focal_crossentropy(     gamma = 3, from_logits = TRUE) loss(y_true, y_pred) ## tf.Tensor(0.6469951, shape=(), dtype=float32) # Apply class weight loss <- loss_binary_focal_crossentropy(      apply_class_balancing = TRUE, gamma = 3, from_logits = TRUE) loss(y_true, y_pred) ## tf.Tensor(0.48214132, shape=(), dtype=float32) # Using 'sample_weight' attribute with focal effect loss <- loss_binary_focal_crossentropy(     gamma = 3, from_logits = TRUE) loss(y_true, y_pred, sample_weight = c(0.8, 0.2)) ## tf.Tensor(0.13312504, shape=(), dtype=float32) # Apply class weight loss <- loss_binary_focal_crossentropy(      apply_class_balancing = TRUE, gamma = 3, from_logits = TRUE) loss(y_true, y_pred, sample_weight = c(0.8, 0.2)) ## tf.Tensor(0.09735977, shape=(), dtype=float32) # Using 'sum' reduction` type. loss <- loss_binary_focal_crossentropy(     gamma = 4, from_logits = TRUE,     reduction = \"sum\") loss(y_true, y_pred) ## tf.Tensor(1.2218808, shape=(), dtype=float32) # Apply class weight loss <- loss_binary_focal_crossentropy(     apply_class_balancing = TRUE, gamma = 4, from_logits = TRUE,     reduction = \"sum\") loss(y_true, y_pred) ## tf.Tensor(0.9140807, shape=(), dtype=float32) # Using 'none' reduction type. loss <- loss_binary_focal_crossentropy(     gamma = 5, from_logits = TRUE,     reduction = NULL) loss(y_true, y_pred) ## tf.Tensor([0.00174837 1.1561027 ], shape=(2), dtype=float32) # Apply class weight loss <- loss_binary_focal_crossentropy(     apply_class_balancing = TRUE, gamma = 5, from_logits = TRUE,     reduction = NULL) loss(y_true, y_pred) ## tf.Tensor([4.3709317e-04 8.6707699e-01], shape=(2), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/loss_categorical_crossentropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the crossentropy loss between the labels and predictions. — loss_categorical_crossentropy","title":"Computes the crossentropy loss between the labels and predictions. — loss_categorical_crossentropy","text":"Use crossentropy loss function two label classes. expect labels provided one_hot representation. want provide labels integers, please use SparseCategoricalCrossentropy loss. num_classes floating point values per feature, .e., shape y_pred y_true [batch_size, num_classes].","code":""},{"path":"https://keras.posit.co/reference/loss_categorical_crossentropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the crossentropy loss between the labels and predictions. — loss_categorical_crossentropy","text":"","code":"loss_categorical_crossentropy(   y_true,   y_pred,   from_logits = FALSE,   label_smoothing = 0,   axis = -1L,   ...,   reduction = \"sum_over_batch_size\",   name = \"categorical_crossentropy\" )"},{"path":"https://keras.posit.co/reference/loss_categorical_crossentropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the crossentropy loss between the labels and predictions. — loss_categorical_crossentropy","text":"y_true Tensor one-hot true targets. y_pred Tensor predicted targets. from_logits Whether y_pred expected logits tensor. default, assume y_pred encodes probability distribution. label_smoothing Float [0, 1]. > 0, label values smoothed, meaning confidence label values relaxed. example, 0.1, use 0.1 / num_classes non-target labels 0.9 + 0.1 / num_classes target labels. axis axis along compute crossentropy (features axis). Defaults -1. ... forward/backward compatability. reduction Type reduction apply loss. almost cases \"sum_over_batch_size\". Supported options \"sum\", \"sum_over_batch_size\" NULL. name Optional name loss instance.","code":""},{"path":"https://keras.posit.co/reference/loss_categorical_crossentropy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the crossentropy loss between the labels and predictions. — loss_categorical_crossentropy","text":"Categorical crossentropy loss value.","code":""},{"path":"https://keras.posit.co/reference/loss_categorical_crossentropy.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the crossentropy loss between the labels and predictions. — loss_categorical_crossentropy","text":"Standalone usage:                 Usage compile() API:","code":"y_true <- rbind(c(0, 1, 0), c(0, 0, 1)) y_pred <- rbind(c(0.05, 0.95, 0), c(0.1, 0.8, 0.1)) loss <- loss_categorical_crossentropy(y_true, y_pred) loss ## tf.Tensor([0.05129329 2.30258509], shape=(2), dtype=float64) y_true <- rbind(c(0, 1, 0), c(0, 0, 1)) y_pred <- rbind(c(0.05, 0.95, 0), c(0.1, 0.8, 0.1)) # Using 'auto'/'sum_over_batch_size' reduction type. cce <- loss_categorical_crossentropy() cce(y_true, y_pred) ## tf.Tensor(1.1769392, shape=(), dtype=float32) # Calling with 'sample_weight'. cce(y_true, y_pred, sample_weight = op_array(c(0.3, 0.7))) ## tf.Tensor(0.8135988, shape=(), dtype=float32) # Using 'sum' reduction type. cce <- loss_categorical_crossentropy(reduction = \"sum\") cce(y_true, y_pred) ## tf.Tensor(2.3538785, shape=(), dtype=float32) # Using 'none' reduction type. cce <- loss_categorical_crossentropy(reduction = NULL) cce(y_true, y_pred) ## tf.Tensor([0.05129331 2.3025851 ], shape=(2), dtype=float32) model %>% compile(optimizer = 'sgd',               loss=loss_categorical_crossentropy())"},{"path":[]},{"path":"https://keras.posit.co/reference/loss_categorical_focal_crossentropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the alpha balanced focal crossentropy loss. — loss_categorical_focal_crossentropy","title":"Computes the alpha balanced focal crossentropy loss. — loss_categorical_focal_crossentropy","text":"Use crossentropy loss function two label classes want handle class imbalance without using class_weights. expect labels provided one_hot representation. According Lin et al., 2018, helps apply focal factor -weight easy examples focus hard examples. general formula focal loss (FL) follows: FL(p_t) = (1 - p_t)^gamma * log(p_t) p_t defined follows: p_t = output y_true == 1, else 1 - output (1 - p_t)^gamma modulating_factor, gamma focusing parameter. gamma = 0, focal effect cross entropy. gamma reduces importance given simple examples smooth manner. authors use alpha-balanced variant focal loss (FL) paper: FL(p_t) = -alpha * (1 - p_t)^gamma * log(p_t) alpha weight factor classes. alpha = 1, loss able handle class imbalance properly classes weight. can constant list constants. alpha list, must length number classes. formula can generalized : FL(p_t) = alpha * (1 - p_t)^gamma * CrossEntropy(y_true, y_pred) minus comes CrossEntropy(y_true, y_pred) (CE). Extending multi-class case straightforward: FL(p_t) = alpha * (1 - p_t) ** gamma * CategoricalCE(y_true, y_pred) snippet , num_classes floating pointing values per example. shape y_pred y_true (batch_size, num_classes).","code":""},{"path":"https://keras.posit.co/reference/loss_categorical_focal_crossentropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the alpha balanced focal crossentropy loss. — loss_categorical_focal_crossentropy","text":"","code":"loss_categorical_focal_crossentropy(   y_true,   y_pred,   alpha = 0.25,   gamma = 2,   from_logits = FALSE,   label_smoothing = 0,   axis = -1L,   ...,   reduction = \"sum_over_batch_size\",   name = \"categorical_focal_crossentropy\" )"},{"path":"https://keras.posit.co/reference/loss_categorical_focal_crossentropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the alpha balanced focal crossentropy loss. — loss_categorical_focal_crossentropy","text":"y_true Tensor one-hot true targets. y_pred Tensor predicted targets. alpha weight balancing factor classes, default 0.25 mentioned reference. can list floats scalar. multi-class case, alpha may set inverse class frequency using compute_class_weight sklearn.utils. gamma focusing parameter, default 2.0 mentioned reference. helps gradually reduce importance given simple examples smooth manner. gamma = 0, focal effect categorical crossentropy. from_logits Whether output expected logits tensor. default, consider output encodes probability distribution. label_smoothing Float [0, 1]. > 0, label values smoothed, meaning confidence label values relaxed. example, 0.1, use 0.1 / num_classes non-target labels 0.9 + 0.1 / num_classes target labels. axis axis along compute crossentropy (features axis). Defaults -1. ... forward/backward compatability. reduction Type reduction apply loss. almost cases \"sum_over_batch_size\". Supported options \"sum\", \"sum_over_batch_size\" NULL. name Optional name loss instance.","code":""},{"path":"https://keras.posit.co/reference/loss_categorical_focal_crossentropy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the alpha balanced focal crossentropy loss. — loss_categorical_focal_crossentropy","text":"Categorical focal crossentropy loss value.","code":""},{"path":"https://keras.posit.co/reference/loss_categorical_focal_crossentropy.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the alpha balanced focal crossentropy loss. — loss_categorical_focal_crossentropy","text":"Standalone usage:                 Usage compile() API:","code":"y_true <- rbind(c(0, 1, 0), c(0, 0, 1)) y_pred <- rbind(c(0.05, 0.95, 0), c(0.1, 0.8, 0.1)) loss <- loss_categorical_focal_crossentropy(y_true, y_pred) loss ## tf.Tensor([3.20583090e-05 4.66273481e-01], shape=(2), dtype=float64) y_true <- rbind(c(0, 1, 0), c(0, 0, 1)) y_pred <- rbind(c(0.05, 0.95, 0), c(0.1, 0.8, 0.1)) # Using 'auto'/'sum_over_batch_size' reduction type. cce <- loss_categorical_focal_crossentropy() cce(y_true, y_pred) ## tf.Tensor(0.23315276, shape=(), dtype=float32) # Calling with 'sample_weight'. cce(y_true, y_pred, sample_weight = op_array(c(0.3, 0.7))) ## tf.Tensor(0.16320053, shape=(), dtype=float32) # Using 'sum' reduction type. cce <- loss_categorical_focal_crossentropy(reduction = \"sum\") cce(y_true, y_pred) ## tf.Tensor(0.46630552, shape=(), dtype=float32) # Using 'none' reduction type. cce <- loss_categorical_focal_crossentropy(reduction = NULL) cce(y_true, y_pred) ## tf.Tensor([3.2058331e-05 4.6627346e-01], shape=(2), dtype=float32) model %>% compile(   optimizer = 'adam',   loss = loss_categorical_focal_crossentropy())"},{"path":[]},{"path":"https://keras.posit.co/reference/loss_categorical_hinge.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the categorical hinge loss between y_true & y_pred. — loss_categorical_hinge","title":"Computes the categorical hinge loss between y_true & y_pred. — loss_categorical_hinge","text":"Formula:   neg=maximum((1-y_true)*y_pred) pos=sum(y_true*y_pred)","code":"loss <- maximum(neg - pos + 1, 0)"},{"path":"https://keras.posit.co/reference/loss_categorical_hinge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the categorical hinge loss between y_true & y_pred. — loss_categorical_hinge","text":"","code":"loss_categorical_hinge(   y_true,   y_pred,   ...,   reduction = \"sum_over_batch_size\",   name = \"categorical_hinge\" )"},{"path":"https://keras.posit.co/reference/loss_categorical_hinge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the categorical hinge loss between y_true & y_pred. — loss_categorical_hinge","text":"y_true ground truth values. y_true values expected either {-1, +1} {0, 1} (.e. one-hot-encoded tensor) shape <- [batch_size, d0, .. dN]. y_pred predicted values shape = [batch_size, d0, .. dN]. ... forward/backward compatability. reduction Type reduction apply loss. almost cases \"sum_over_batch_size\". Supported options \"sum\", \"sum_over_batch_size\" NULL. name Optional name loss instance.","code":""},{"path":"https://keras.posit.co/reference/loss_categorical_hinge.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the categorical hinge loss between y_true & y_pred. — loss_categorical_hinge","text":"Categorical hinge loss values shape = [batch_size, d0, .. dN-1].","code":""},{"path":"https://keras.posit.co/reference/loss_categorical_hinge.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the categorical hinge loss between y_true & y_pred. — loss_categorical_hinge","text":"","code":"y_true <- rbind(c(0, 1), c(0, 0)) y_pred <- rbind(c(0.6, 0.4), c(0.4, 0.6)) loss <- loss_categorical_hinge(y_true, y_pred)"},{"path":[]},{"path":"https://keras.posit.co/reference/loss_cosine_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the cosine similarity between y_true & y_pred. — loss_cosine_similarity","title":"Computes the cosine similarity between y_true & y_pred. — loss_cosine_similarity","text":"Formula:   Note number -1 1. negative number -1 0, 0 indicates orthogonality values closer -1 indicate greater similarity. makes usable loss function setting try maximize proximity predictions targets. either y_true y_pred zero vector, cosine similarity 0 regardless proximity predictions targets.","code":"loss <- -sum(l2_norm(y_true) * l2_norm(y_pred))"},{"path":"https://keras.posit.co/reference/loss_cosine_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the cosine similarity between y_true & y_pred. — loss_cosine_similarity","text":"","code":"loss_cosine_similarity(   y_true,   y_pred,   axis = -1L,   ...,   reduction = \"sum_over_batch_size\",   name = \"cosine_similarity\" )"},{"path":"https://keras.posit.co/reference/loss_cosine_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the cosine similarity between y_true & y_pred. — loss_cosine_similarity","text":"y_true Tensor true targets. y_pred Tensor predicted targets. axis axis along cosine similarity computed (features axis). Defaults -1. ... forward/backward compatability. reduction Type reduction apply loss. almost cases \"sum_over_batch_size\". Supported options \"sum\", \"sum_over_batch_size\" NULL. name Optional name loss instance.","code":""},{"path":"https://keras.posit.co/reference/loss_cosine_similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the cosine similarity between y_true & y_pred. — loss_cosine_similarity","text":"Cosine similarity tensor.","code":""},{"path":"https://keras.posit.co/reference/loss_cosine_similarity.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the cosine similarity between y_true & y_pred. — loss_cosine_similarity","text":"","code":"y_true <- rbind(c(0., 1.), c(1., 1.), c(1., 1.)) y_pred <- rbind(c(1., 0.), c(1., 1.), c(-1., -1.)) loss <- loss_cosine_similarity(y_true, y_pred, axis=-1) loss ## tf.Tensor([-0. -1.  1.], shape=(3), dtype=float64)"},{"path":[]},{"path":"https://keras.posit.co/reference/loss_hinge.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the hinge loss between y_true & y_pred. — loss_hinge","title":"Computes the hinge loss between y_true & y_pred. — loss_hinge","text":"Formula:   y_true values expected -1 1. binary (0 1) labels provided convert -1 1.","code":"loss <- mean(maximum(1 - y_true * y_pred, 0), axis=-1)"},{"path":"https://keras.posit.co/reference/loss_hinge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the hinge loss between y_true & y_pred. — loss_hinge","text":"","code":"loss_hinge(   y_true,   y_pred,   ...,   reduction = \"sum_over_batch_size\",   name = \"hinge\" )"},{"path":"https://keras.posit.co/reference/loss_hinge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the hinge loss between y_true & y_pred. — loss_hinge","text":"y_true ground truth values. y_true values expected -1 1. binary (0 1) labels provided converted -1 1 shape = [batch_size, d0, .. dN]. y_pred predicted values shape = [batch_size, d0, .. dN]. ... forward/backward compatability. reduction Type reduction apply loss. almost cases \"sum_over_batch_size\". Supported options \"sum\", \"sum_over_batch_size\" NULL. name Optional name loss instance.","code":""},{"path":"https://keras.posit.co/reference/loss_hinge.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the hinge loss between y_true & y_pred. — loss_hinge","text":"Hinge loss values shape = [batch_size, d0, .. dN-1].","code":""},{"path":"https://keras.posit.co/reference/loss_hinge.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the hinge loss between y_true & y_pred. — loss_hinge","text":"","code":"y_true <- array(sample(c(-1,1), 6, replace = TRUE), dim = c(2, 3)) y_pred <- random_uniform(c(2, 3)) loss <- loss_hinge(y_true, y_pred) loss ## tf.Tensor([1.0610152  0.93285507], shape=(2), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/loss_huber.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the Huber loss between y_true & y_pred. — loss_huber","title":"Computes the Huber loss between y_true & y_pred. — loss_huber","text":"Formula:   See: Huber loss.","code":"for (x in error) {   if (abs(x) <= delta){     loss <- c(loss, (0.5 * x^2))   } else if (abs(x) > delta) {     loss <- c(loss, (delta * abs(x) - 0.5 * delta^2))   } } loss <- mean(loss)"},{"path":"https://keras.posit.co/reference/loss_huber.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the Huber loss between y_true & y_pred. — loss_huber","text":"","code":"loss_huber(   y_true,   y_pred,   delta = 1,   ...,   reduction = \"sum_over_batch_size\",   name = \"huber_loss\" )"},{"path":"https://keras.posit.co/reference/loss_huber.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the Huber loss between y_true & y_pred. — loss_huber","text":"y_true tensor true targets. y_pred tensor predicted targets. delta float, point Huber loss function changes quadratic linear. Defaults 1.0. ... forward/backward compatability. reduction Type reduction apply loss. Options \"sum\", \"sum_over_batch_size\" NULL. Defaults \"sum_over_batch_size\". name Optional name instance.","code":""},{"path":"https://keras.posit.co/reference/loss_huber.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the Huber loss between y_true & y_pred. — loss_huber","text":"","code":"Tensor with one scalar loss entry per sample."},{"path":"https://keras.posit.co/reference/loss_huber.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the Huber loss between y_true & y_pred. — loss_huber","text":"","code":"y_true <- rbind(c(0, 1), c(0, 0)) y_pred <- rbind(c(0.6, 0.4), c(0.4, 0.6)) loss <- loss_huber(y_true, y_pred)"},{"path":[]},{"path":"https://keras.posit.co/reference/loss_kl_divergence.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes Kullback-Leibler divergence loss between y_true & y_pred. — loss_kl_divergence","title":"Computes Kullback-Leibler divergence loss between y_true & y_pred. — loss_kl_divergence","text":"Formula:","code":"loss <- y_true * log(y_true / y_pred)"},{"path":"https://keras.posit.co/reference/loss_kl_divergence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes Kullback-Leibler divergence loss between y_true & y_pred. — loss_kl_divergence","text":"","code":"loss_kl_divergence(   y_true,   y_pred,   ...,   reduction = \"sum_over_batch_size\",   name = \"kl_divergence\" )"},{"path":"https://keras.posit.co/reference/loss_kl_divergence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes Kullback-Leibler divergence loss between y_true & y_pred. — loss_kl_divergence","text":"y_true Tensor true targets. y_pred Tensor predicted targets. ... forward/backward compatability. reduction Type reduction apply loss. almost cases \"sum_over_batch_size\". Supported options \"sum\", \"sum_over_batch_size\" NULL. name Optional name loss instance.","code":""},{"path":"https://keras.posit.co/reference/loss_kl_divergence.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes Kullback-Leibler divergence loss between y_true & y_pred. — loss_kl_divergence","text":"KL Divergence loss values shape = [batch_size, d0, .. dN-1].","code":""},{"path":"https://keras.posit.co/reference/loss_kl_divergence.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes Kullback-Leibler divergence loss between y_true & y_pred. — loss_kl_divergence","text":"","code":"y_true <- random_uniform(c(2, 3), 0, 2) y_pred <- random_uniform(c(2,3)) loss <- loss_kl_divergence(y_true, y_pred) loss ## tf.Tensor([3.5312676 0.2128672], shape=(2), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/loss_log_cosh.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the logarithm of the hyperbolic cosine of the prediction error. — loss_log_cosh","title":"Computes the logarithm of the hyperbolic cosine of the prediction error. — loss_log_cosh","text":"Formula:   Note log(cosh(x)) approximately equal (x ** 2) / 2 small x abs(x) - log(2) large x. means 'logcosh' works mostly like mean squared error, strongly affected occasional wildly incorrect prediction.","code":"loss <- mean(log(cosh(y_pred - y_true)), axis=-1)"},{"path":"https://keras.posit.co/reference/loss_log_cosh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the logarithm of the hyperbolic cosine of the prediction error. — loss_log_cosh","text":"","code":"loss_log_cosh(   y_true,   y_pred,   ...,   reduction = \"sum_over_batch_size\",   name = \"log_cosh\" )"},{"path":"https://keras.posit.co/reference/loss_log_cosh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the logarithm of the hyperbolic cosine of the prediction error. — loss_log_cosh","text":"y_true Ground truth values shape = [batch_size, d0, .. dN]. y_pred predicted values shape = [batch_size, d0, .. dN]. ... forward/backward compatability. reduction Type reduction apply loss. Options \"sum\", \"sum_over_batch_size\" NULL. Defaults \"sum_over_batch_size\". name Optional name instance.","code":""},{"path":"https://keras.posit.co/reference/loss_log_cosh.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the logarithm of the hyperbolic cosine of the prediction error. — loss_log_cosh","text":"","code":"Logcosh error values with shape = `[batch_size, d0, .. dN-1]`."},{"path":"https://keras.posit.co/reference/loss_log_cosh.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the logarithm of the hyperbolic cosine of the prediction error. — loss_log_cosh","text":"","code":"y_true <- rbind(c(0., 1.), c(0., 0.)) y_pred <- rbind(c(1., 1.), c(0., 0.)) loss <- loss_log_cosh(y_true, y_pred) # 0.108"},{"path":[]},{"path":"https://keras.posit.co/reference/loss_mean_absolute_error.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the mean of absolute difference between labels and predictions. — loss_mean_absolute_error","title":"Computes the mean of absolute difference between labels and predictions. — loss_mean_absolute_error","text":"Formula:","code":"loss <- mean(abs(y_true - y_pred))"},{"path":"https://keras.posit.co/reference/loss_mean_absolute_error.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the mean of absolute difference between labels and predictions. — loss_mean_absolute_error","text":"","code":"loss_mean_absolute_error(   y_true,   y_pred,   ...,   reduction = \"sum_over_batch_size\",   name = \"mean_absolute_error\" )"},{"path":"https://keras.posit.co/reference/loss_mean_absolute_error.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the mean of absolute difference between labels and predictions. — loss_mean_absolute_error","text":"y_true Ground truth values shape = [batch_size, d0, .. dN]. y_pred predicted values shape = [batch_size, d0, .. dN]. ... forward/backward compatability. reduction Type reduction apply loss. almost cases \"sum_over_batch_size\". Supported options \"sum\", \"sum_over_batch_size\" NULL. name Optional name loss instance.","code":""},{"path":"https://keras.posit.co/reference/loss_mean_absolute_error.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the mean of absolute difference between labels and predictions. — loss_mean_absolute_error","text":"Mean absolute error values shape = [batch_size, d0, .. dN-1].","code":""},{"path":"https://keras.posit.co/reference/loss_mean_absolute_error.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the mean of absolute difference between labels and predictions. — loss_mean_absolute_error","text":"","code":"y_true <- random_uniform(c(2, 3), 0, 2) y_pred <- random_uniform(c(2, 3)) loss <- loss_mean_absolute_error(y_true, y_pred)"},{"path":[]},{"path":"https://keras.posit.co/reference/loss_mean_absolute_percentage_error.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the mean absolute percentage error between y_true & y_pred. — loss_mean_absolute_percentage_error","title":"Computes the mean absolute percentage error between y_true & y_pred. — loss_mean_absolute_percentage_error","text":"Formula:   Division zero prevented dividing maximum(y_true, epsilon) epsilon = keras.backend.epsilon() (default 1e-7).","code":"loss <- 100 * mean(abs((y_true - y_pred) / y_true), axis=-1)"},{"path":"https://keras.posit.co/reference/loss_mean_absolute_percentage_error.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the mean absolute percentage error between y_true & y_pred. — loss_mean_absolute_percentage_error","text":"","code":"loss_mean_absolute_percentage_error(   y_true,   y_pred,   ...,   reduction = \"sum_over_batch_size\",   name = \"mean_absolute_percentage_error\" )"},{"path":"https://keras.posit.co/reference/loss_mean_absolute_percentage_error.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the mean absolute percentage error between y_true & y_pred. — loss_mean_absolute_percentage_error","text":"y_true Ground truth values shape = [batch_size, d0, .. dN]. y_pred predicted values shape = [batch_size, d0, .. dN]. ... forward/backward compatability. reduction Type reduction apply loss. almost cases \"sum_over_batch_size\". Supported options \"sum\", \"sum_over_batch_size\" NULL. name Optional name loss instance.","code":""},{"path":"https://keras.posit.co/reference/loss_mean_absolute_percentage_error.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the mean absolute percentage error between y_true & y_pred. — loss_mean_absolute_percentage_error","text":"Mean absolute percentage error values shape = [batch_size, d0, ..dN-1].","code":""},{"path":"https://keras.posit.co/reference/loss_mean_absolute_percentage_error.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the mean absolute percentage error between y_true & y_pred. — loss_mean_absolute_percentage_error","text":"","code":"y_true <- random_uniform(c(2, 3)) y_pred <- random_uniform(c(2, 3)) loss <- loss_mean_absolute_percentage_error(y_true, y_pred)"},{"path":[]},{"path":"https://keras.posit.co/reference/loss_mean_squared_error.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the mean of squares of errors between labels and predictions. — loss_mean_squared_error","title":"Computes the mean of squares of errors between labels and predictions. — loss_mean_squared_error","text":"Formula:","code":"loss <- mean(square(y_true - y_pred))"},{"path":"https://keras.posit.co/reference/loss_mean_squared_error.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the mean of squares of errors between labels and predictions. — loss_mean_squared_error","text":"","code":"loss_mean_squared_error(   y_true,   y_pred,   ...,   reduction = \"sum_over_batch_size\",   name = \"mean_squared_error\" )"},{"path":"https://keras.posit.co/reference/loss_mean_squared_error.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the mean of squares of errors between labels and predictions. — loss_mean_squared_error","text":"y_true Ground truth values shape = [batch_size, d0, .. dN]. y_pred predicted values shape = [batch_size, d0, .. dN]. ... forward/backward compatability. reduction Type reduction apply loss. almost cases \"sum_over_batch_size\". Supported options \"sum\", \"sum_over_batch_size\" NULL. name Optional name loss instance.","code":""},{"path":"https://keras.posit.co/reference/loss_mean_squared_error.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the mean of squares of errors between labels and predictions. — loss_mean_squared_error","text":"","code":"Mean squared error values with shape = `[batch_size, d0, .. dN-1]`."},{"path":"https://keras.posit.co/reference/loss_mean_squared_error.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the mean of squares of errors between labels and predictions. — loss_mean_squared_error","text":"","code":"y_true <- random_uniform(c(2, 3), 0, 2) y_pred <- random_uniform(c(2, 3)) loss <- loss_mean_squared_error(y_true, y_pred)"},{"path":[]},{"path":"https://keras.posit.co/reference/loss_mean_squared_logarithmic_error.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the mean squared logarithmic error between y_true & y_pred. — loss_mean_squared_logarithmic_error","title":"Computes the mean squared logarithmic error between y_true & y_pred. — loss_mean_squared_logarithmic_error","text":"Note y_pred y_true less equal 0. Negative values 0 values replaced keras.backend.epsilon() (default 1e-7). Formula:","code":"loss <- mean(square(log(y_true + 1) - log(y_pred + 1)))"},{"path":"https://keras.posit.co/reference/loss_mean_squared_logarithmic_error.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the mean squared logarithmic error between y_true & y_pred. — loss_mean_squared_logarithmic_error","text":"","code":"loss_mean_squared_logarithmic_error(   y_true,   y_pred,   ...,   reduction = \"sum_over_batch_size\",   name = \"mean_squared_logarithmic_error\" )"},{"path":"https://keras.posit.co/reference/loss_mean_squared_logarithmic_error.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the mean squared logarithmic error between y_true & y_pred. — loss_mean_squared_logarithmic_error","text":"y_true Ground truth values shape = [batch_size, d0, .. dN]. y_pred predicted values shape = [batch_size, d0, .. dN]. ... forward/backward compatability. reduction Type reduction apply loss. almost cases \"sum_over_batch_size\". Supported options \"sum\", \"sum_over_batch_size\" NULL. name Optional name loss instance.","code":""},{"path":"https://keras.posit.co/reference/loss_mean_squared_logarithmic_error.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the mean squared logarithmic error between y_true & y_pred. — loss_mean_squared_logarithmic_error","text":"Mean squared logarithmic error values shape = [batch_size, d0, .. dN-1].","code":""},{"path":"https://keras.posit.co/reference/loss_mean_squared_logarithmic_error.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the mean squared logarithmic error between y_true & y_pred. — loss_mean_squared_logarithmic_error","text":"","code":"y_true <- random_uniform(c(2, 3), 0, 2) y_pred <- random_uniform(c(2, 3)) loss <- loss_mean_squared_logarithmic_error(y_true, y_pred)"},{"path":[]},{"path":"https://keras.posit.co/reference/loss_poisson.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the Poisson loss between y_true & y_pred. — loss_poisson","title":"Computes the Poisson loss between y_true & y_pred. — loss_poisson","text":"Formula:","code":"loss <- y_pred - y_true * log(y_pred)"},{"path":"https://keras.posit.co/reference/loss_poisson.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the Poisson loss between y_true & y_pred. — loss_poisson","text":"","code":"loss_poisson(   y_true,   y_pred,   ...,   reduction = \"sum_over_batch_size\",   name = \"poisson\" )"},{"path":"https://keras.posit.co/reference/loss_poisson.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the Poisson loss between y_true & y_pred. — loss_poisson","text":"y_true Ground truth values. shape = [batch_size, d0, .. dN]. y_pred predicted values. shape = [batch_size, d0, .. dN]. ... forward/backward compatability. reduction Type reduction apply loss. almost cases \"sum_over_batch_size\". Supported options \"sum\", \"sum_over_batch_size\" NULL. name Optional name loss instance.","code":""},{"path":"https://keras.posit.co/reference/loss_poisson.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the Poisson loss between y_true & y_pred. — loss_poisson","text":"Poisson loss values shape = [batch_size, d0, .. dN-1].","code":""},{"path":"https://keras.posit.co/reference/loss_poisson.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the Poisson loss between y_true & y_pred. — loss_poisson","text":"","code":"y_true <- random_uniform(c(2, 3), 0, 2) y_pred <- random_uniform(c(2, 3)) loss <- loss_poisson(y_true, y_pred) loss ## tf.Tensor([2.5907533  0.66836613], shape=(2), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/loss_sparse_categorical_crossentropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the crossentropy loss between the labels and predictions. — loss_sparse_categorical_crossentropy","title":"Computes the crossentropy loss between the labels and predictions. — loss_sparse_categorical_crossentropy","text":"Use crossentropy loss function two label classes.  expect labels provided integers. want provide labels using one-hot representation, please use CategoricalCrossentropy loss.  # classes floating point values per feature y_pred single floating point value per feature y_true. snippet , single floating point value per example y_true num_classes floating pointing values per example y_pred. shape y_true [batch_size] shape y_pred [batch_size, num_classes].","code":""},{"path":"https://keras.posit.co/reference/loss_sparse_categorical_crossentropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the crossentropy loss between the labels and predictions. — loss_sparse_categorical_crossentropy","text":"","code":"loss_sparse_categorical_crossentropy(   y_true,   y_pred,   from_logits = FALSE,   ignore_class = NULL,   axis = -1L,   ...,   reduction = \"sum_over_batch_size\",   name = \"sparse_categorical_crossentropy\" )"},{"path":"https://keras.posit.co/reference/loss_sparse_categorical_crossentropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the crossentropy loss between the labels and predictions. — loss_sparse_categorical_crossentropy","text":"y_true Ground truth values. y_pred predicted values. from_logits Whether y_pred expected logits tensor. default, assume y_pred encodes probability distribution. ignore_class Optional integer. ID class ignored loss computation. useful, example, segmentation problems featuring \"void\" class (commonly -1 255) segmentation maps. default (ignore_class=NULL), classes considered. axis Defaults -1. dimension along entropy computed. ... forward/backward compatability. reduction Type reduction apply loss. almost cases \"sum_over_batch_size\". Supported options \"sum\", \"sum_over_batch_size\" NULL. name Optional name loss instance.","code":""},{"path":"https://keras.posit.co/reference/loss_sparse_categorical_crossentropy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the crossentropy loss between the labels and predictions. — loss_sparse_categorical_crossentropy","text":"Sparse categorical crossentropy loss value.","code":""},{"path":"https://keras.posit.co/reference/loss_sparse_categorical_crossentropy.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the crossentropy loss between the labels and predictions. — loss_sparse_categorical_crossentropy","text":"Usage compile() API:","code":"y_true <- c(1, 2) y_pred <- rbind(c(0.05, 0.95, 0), c(0.1, 0.8, 0.1)) loss <- loss_sparse_categorical_crossentropy(y_true, y_pred) loss ## tf.Tensor([0.05129339 2.30258509], shape=(2), dtype=float64) y_true <- c(1, 2) y_pred <- rbind(c(0.05, 0.95, 0), c(0.1, 0.8, 0.1)) # Using 'auto'/'sum_over_batch_size' reduction type. scce <- loss_sparse_categorical_crossentropy() scce(op_array(y_true), op_array(y_pred)) ## tf.Tensor(1.1769392, shape=(), dtype=float32) # 1.177 # Calling with 'sample_weight'. scce(op_array(y_true), op_array(y_pred), sample_weight = op_array(c(0.3, 0.7))) ## tf.Tensor(0.8135988, shape=(), dtype=float32) # Using 'sum' reduction type. scce <- loss_sparse_categorical_crossentropy(reduction=\"sum\") scce(op_array(y_true), op_array(y_pred)) ## tf.Tensor(2.3538785, shape=(), dtype=float32) # 2.354 # Using 'none' reduction type. scce <- loss_sparse_categorical_crossentropy(reduction=NULL) scce(op_array(y_true), op_array(y_pred)) ## tf.Tensor([0.05129344 2.3025851 ], shape=(2), dtype=float32) # array([0.0513, 2.303], dtype=float32) model %>% compile(optimizer = 'sgd',                   loss = loss_sparse_categorical_crossentropy())"},{"path":[]},{"path":"https://keras.posit.co/reference/loss_squared_hinge.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the squared hinge loss between y_true & y_pred. — loss_squared_hinge","title":"Computes the squared hinge loss between y_true & y_pred. — loss_squared_hinge","text":"Formula:   y_true values expected -1 1. binary (0 1) labels provided convert -1 1.","code":"loss <- square(maximum(1 - y_true * y_pred, 0))"},{"path":"https://keras.posit.co/reference/loss_squared_hinge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the squared hinge loss between y_true & y_pred. — loss_squared_hinge","text":"","code":"loss_squared_hinge(   y_true,   y_pred,   ...,   reduction = \"sum_over_batch_size\",   name = \"squared_hinge\" )"},{"path":"https://keras.posit.co/reference/loss_squared_hinge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the squared hinge loss between y_true & y_pred. — loss_squared_hinge","text":"y_true ground truth values. y_true values expected -1 1. binary (0 1) labels provided convert -1 1 shape = [batch_size, d0, .. dN]. y_pred predicted values shape = [batch_size, d0, .. dN]. ... forward/backward compatability. reduction Type reduction apply loss. almost cases \"sum_over_batch_size\". Supported options \"sum\", \"sum_over_batch_size\" NULL. name Optional name loss instance.","code":""},{"path":"https://keras.posit.co/reference/loss_squared_hinge.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the squared hinge loss between y_true & y_pred. — loss_squared_hinge","text":"Squared hinge loss values shape = [batch_size, d0, .. dN-1].","code":""},{"path":"https://keras.posit.co/reference/loss_squared_hinge.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the squared hinge loss between y_true & y_pred. — loss_squared_hinge","text":"","code":"y_true <- array(sample(c(-1,1), 6, replace = TRUE), dim = c(2, 3)) y_pred <- random_uniform(c(2, 3)) loss <- loss_squared_hinge(y_true, y_pred)"},{"path":[]},{"path":"https://keras.posit.co/reference/metric-or-Metric.html","id":null,"dir":"Reference","previous_headings":"","what":"metric-or-Metric — metric-or-Metric","title":"metric-or-Metric — metric-or-Metric","text":"metric--Metric","code":""},{"path":"https://keras.posit.co/reference/metric-or-Metric.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"metric-or-Metric — metric-or-Metric","text":"y_true Tensor true targets. y_pred Tensor predicted targets. ... Passed underlying metric. Used forwards backwards compatibility. axis (Optional) (1-based) Defaults -1. dimension along metric computed. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric-or-Metric.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"metric-or-Metric — metric-or-Metric","text":"y_true y_pred missing, (subclassed) Metric instance returned. Metric object can passed directly compile(metrics = ) used standalone object. See ?Metric example usage. Alternatively, called y_true y_pred arguments, computed case-wise values mini-batch returned directly.","code":""},{"path":"https://keras.posit.co/reference/metric_auc.html","id":null,"dir":"Reference","previous_headings":"","what":"Approximates the AUC (Area under the curve) of the ROC or PR curves. — metric_auc","title":"Approximates the AUC (Area under the curve) of the ROC or PR curves. — metric_auc","text":"AUC (Area curve) ROC (Receiver operating characteristic; default) PR (Precision Recall) curves quality measures binary classifiers. Unlike accuracy, like cross-entropy losses, ROC-AUC PR-AUC evaluate operational points model. class approximates AUCs using Riemann sum. metric accumulation phrase, predictions accumulated within predefined buckets value. AUC computed interpolating per-bucket averages. buckets define evaluated operational points. metric creates four local variables, true_positives, true_negatives, false_positives false_negatives used compute AUC.  discretize AUC curve, linearly spaced set thresholds used compute pairs recall precision values. area ROC-curve therefore computed using height recall values false positive rate, area PR-curve computed using height precision values recall. value ultimately returned auc, idempotent operation computes area discretized curve precision versus recall values (computed using aforementioned variables). num_thresholds variable controls degree discretization larger numbers thresholds closely approximating true AUC. quality approximation may vary dramatically depending num_thresholds. thresholds parameter can used manually specify thresholds split predictions evenly. best approximation real AUC, predictions distributed approximately uniformly range [0, 1] (from_logits=FALSE). quality AUC approximation may poor case. Setting summation_method 'minoring' 'majoring' can help quantify error approximation providing lower upper bound estimate AUC. sample_weight NULL, weights default 1. Use sample_weight 0 mask values.","code":""},{"path":"https://keras.posit.co/reference/metric_auc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Approximates the AUC (Area under the curve) of the ROC or PR curves. — metric_auc","text":"","code":"metric_auc(   ...,   num_thresholds = 200L,   curve = \"ROC\",   summation_method = \"interpolation\",   name = NULL,   dtype = NULL,   thresholds = NULL,   multi_label = FALSE,   num_labels = NULL,   label_weights = NULL,   from_logits = FALSE )"},{"path":"https://keras.posit.co/reference/metric_auc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Approximates the AUC (Area under the curve) of the ROC or PR curves. — metric_auc","text":"... forward/backward compatability. num_thresholds (Optional) number thresholds use discretizing roc curve. Values must > 1. Defaults 200. curve (Optional) Specifies name curve computed, 'ROC' (default) 'PR' Precision-Recall-curve. summation_method (Optional) Specifies Riemann summation method used. 'interpolation' (default) applies mid-point summation scheme ROC.  PR-AUC, interpolates (true/false) positives ratio precision (see Davis & Goadrich 2006 details); 'minoring' applies left summation increasing intervals right summation decreasing intervals; 'majoring' opposite. name (Optional) string name metric instance. dtype (Optional) data type metric result. thresholds (Optional) list floating point values use thresholds discretizing curve. set, num_thresholds parameter ignored. Values [0, 1]. Endpoint thresholds equal -epsilon, 1+epsilon small positive epsilon value automatically included correctly handle predictions equal exactly 0 1. multi_label boolean indicating whether multilabel data treated , wherein AUC computed separately label averaged across labels, (FALSE) data flattened single label AUC computation. latter case, multilabel data passed AUC, label-prediction pair treated individual data point. set FALSE multi-class data. num_labels (Optional) number labels, used multi_label TRUE. num_labels specified, state variables get created first call update_state. label_weights (Optional) list, array, tensor non-negative weights used compute AUCs multilabel data. multi_label TRUE, weights applied individual label AUCs averaged produce multi-label AUC. FALSE, used weight individual label predictions computing confusion matrix flattened data. Note unlike class_weights class_weights weights example depending value label, whereas label_weights depends index label flattening; therefore label_weights used multi-class data. from_logits boolean indicating whether predictions (y_pred update_state) probabilities sigmoid logits. rule thumb, using keras loss, from_logits constructor argument loss match AUC from_logits constructor argument.","code":""},{"path":"https://keras.posit.co/reference/metric_auc.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Approximates the AUC (Area under the curve) of the ROC or PR curves. — metric_auc","text":"Standalone usage:             Usage compile() API:","code":"m <- metric_auc(num_thresholds = 3) m$update_state(c(0, 0, 1, 1), c(0, 0.5, 0.3, 0.9)) # threshold values are [0 - 1e-7, 0.5, 1 + 1e-7] # tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2] # tp_rate = recall = [1, 0.5, 0], fp_rate = [1, 0, 0] # auc = ((((1 + 0.5) / 2) * (1 - 0)) + (((0.5 + 0) / 2) * (0 - 0))) #     = 0.75 m$result() ## tf.Tensor(0.75, shape=(), dtype=float32) # 0.75 m$reset_state() m$update_state(c(0, 0, 1, 1), c(0, 0.5, 0.3, 0.9),                sample_weight=c(1, 0, 0, 1)) m$result() ## tf.Tensor(1.0, shape=(), dtype=float32) # 1.0 # Reports the AUC of a model outputting a probability. model %>% compile(optimizer = 'sgd',                   loss = loss_binary_crossentropy(),                   metrics = list(metric_auc()))  # Reports the AUC of a model outputting a logit. model %>% compile(optimizer = 'sgd',                   loss = loss_binary_crossentropy(from_logits = TRUE),                   metrics = list(metric_auc(from_logits = TRUE)))"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_binary_accuracy.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculates how often predictions match binary labels. — metric_binary_accuracy","title":"Calculates how often predictions match binary labels. — metric_binary_accuracy","text":"metric creates two local variables, total count used compute frequency y_pred matches y_true. frequency ultimately returned binary accuracy: idempotent operation simply divides total count. sample_weight NULL, weights default 1. Use sample_weight 0 mask values.","code":""},{"path":"https://keras.posit.co/reference/metric_binary_accuracy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates how often predictions match binary labels. — metric_binary_accuracy","text":"","code":"metric_binary_accuracy(   y_true,   y_pred,   threshold = 0.5,   ...,   name = \"binary_accuracy\",   dtype = NULL )"},{"path":"https://keras.posit.co/reference/metric_binary_accuracy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculates how often predictions match binary labels. — metric_binary_accuracy","text":"y_true Tensor true targets. y_pred Tensor predicted targets. threshold (Optional) Float representing threshold deciding whether prediction values 1 0. ... forward/backward compatability. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_binary_accuracy.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates how often predictions match binary labels. — metric_binary_accuracy","text":"Standalone usage:             Usage compile() API:","code":"m <- metric_binary_accuracy() m$update_state(rbind(1, 1, 0, 0), rbind(0.98, 1, 0, 0.6)) m$result() ## tf.Tensor(0.75, shape=(), dtype=float32) # 0.75 m$reset_state() m$update_state(rbind(1, 1, 0, 0), rbind(0.98, 1, 0, 0.6),                sample_weight = c(1, 0, 0, 1)) m$result() ## tf.Tensor(0.5, shape=(), dtype=float32) # 0.5 model %>% compile(optimizer='sgd',                   loss='binary_crossentropy',                   metrics=list(metric_binary_accuracy()))"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_binary_crossentropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the crossentropy metric between the labels and predictions. — metric_binary_crossentropy","title":"Computes the crossentropy metric between the labels and predictions. — metric_binary_crossentropy","text":"crossentropy metric class used two label classes (0 1).","code":""},{"path":"https://keras.posit.co/reference/metric_binary_crossentropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the crossentropy metric between the labels and predictions. — metric_binary_crossentropy","text":"","code":"metric_binary_crossentropy(   y_true,   y_pred,   from_logits = FALSE,   label_smoothing = 0,   axis = -1L,   ...,   name = \"binary_crossentropy\",   dtype = NULL )"},{"path":"https://keras.posit.co/reference/metric_binary_crossentropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the crossentropy metric between the labels and predictions. — metric_binary_crossentropy","text":"y_true Ground truth values. shape = [batch_size, d0, .. dN]. y_pred predicted values. shape = [batch_size, d0, .. dN]. from_logits (Optional) Whether output expected logits tensor. default, consider output encodes probability distribution. label_smoothing (Optional) Float [0, 1]. > 0, label values smoothed, meaning confidence label values relaxed. e.g. label_smoothing=0.2 means use value 0.1 label \"0\" 0.9 label \"1\". axis axis along mean computed. Defaults -1. ... forward/backward compatability. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_binary_crossentropy.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the crossentropy metric between the labels and predictions. — metric_binary_crossentropy","text":"Standalone usage:         Usage compile() API:","code":"m <- metric_binary_crossentropy() m$update_state(rbind(c(0, 1), c(0, 0)), rbind(c(0.6, 0.4), c(0.4, 0.6))) m$result() ## tf.Tensor(0.8149245, shape=(), dtype=float32) m$reset_state() m$update_state(rbind(c(0, 1), c(0, 0)), rbind(c(0.6, 0.4), c(0.4, 0.6)),                sample_weight=c(1, 0)) m$result() ## tf.Tensor(0.91629076, shape=(), dtype=float32) model %>% compile(     optimizer = 'sgd',     loss = 'mse',     metrics = list(metric_binary_crossentropy()))"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_binary_focal_crossentropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the binary focal crossentropy loss. — metric_binary_focal_crossentropy","title":"Computes the binary focal crossentropy loss. — metric_binary_focal_crossentropy","text":"According Lin et al., 2018, helps apply focal factor -weight easy examples focus hard examples. default, focal tensor computed follows: focal_factor = (1 - output)^gamma class 1 focal_factor = output^gamma class 0 gamma focusing parameter. gamma = 0, focal effect binary crossentropy loss. apply_class_balancing == TRUE, function also takes account weight balancing factor binary classes 0 1 follows: weight = alpha class 1 (target == 1) weight = 1 - alpha class 0 alpha float range [0, 1].","code":""},{"path":"https://keras.posit.co/reference/metric_binary_focal_crossentropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the binary focal crossentropy loss. — metric_binary_focal_crossentropy","text":"","code":"metric_binary_focal_crossentropy(   y_true,   y_pred,   apply_class_balancing = FALSE,   alpha = 0.25,   gamma = 2,   from_logits = FALSE,   label_smoothing = 0,   axis = -1L )"},{"path":"https://keras.posit.co/reference/metric_binary_focal_crossentropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the binary focal crossentropy loss. — metric_binary_focal_crossentropy","text":"y_true Ground truth values, shape (batch_size, d0, .. dN). y_pred predicted values, shape (batch_size, d0, .. dN). apply_class_balancing bool, whether apply weight balancing binary classes 0 1. alpha weight balancing factor class 1, default 0.25 mentioned reference. weight class 0 1.0 - alpha. gamma focusing parameter, default 2.0 mentioned reference. from_logits Whether y_pred expected logits tensor. default, assume y_pred encodes probability distribution. label_smoothing Float [0, 1]. > 0 smooth labels squeezing towards 0.5, , using 1. - 0.5 * label_smoothing target class 0.5 * label_smoothing non-target class. axis axis along mean computed. Defaults -1.","code":""},{"path":"https://keras.posit.co/reference/metric_binary_focal_crossentropy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the binary focal crossentropy loss. — metric_binary_focal_crossentropy","text":"Binary focal crossentropy loss value shape = [batch_size, d0, .. dN-1].","code":""},{"path":"https://keras.posit.co/reference/metric_binary_focal_crossentropy.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the binary focal crossentropy loss. — metric_binary_focal_crossentropy","text":"","code":"y_true <- rbind(c(0, 1), c(0, 0)) y_pred <- rbind(c(0.6, 0.4), c(0.4, 0.6)) loss <- loss_binary_focal_crossentropy(y_true, y_pred, gamma=2) loss ## tf.Tensor([0.32986466 0.20579838], shape=(2), dtype=float64)"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_binary_iou.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the Intersection-Over-Union metric for class 0 and/or 1. — metric_binary_iou","title":"Computes the Intersection-Over-Union metric for class 0 and/or 1. — metric_binary_iou","text":"Formula:   Intersection--Union common evaluation metric semantic image segmentation. compute IoUs, predictions accumulated confusion matrix, weighted sample_weight metric calculated . sample_weight NULL, weights default 1. Use sample_weight 0 mask values. class can used compute IoUs binary classification task predictions provided logits. First threshold applied predicted values threshold converted class 0 threshold converted class 1. IoUs classes 0 1 computed, mean IoUs classes specified target_class_ids returned.","code":"iou <- true_positives / (true_positives + false_positives + false_negatives)"},{"path":"https://keras.posit.co/reference/metric_binary_iou.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the Intersection-Over-Union metric for class 0 and/or 1. — metric_binary_iou","text":"","code":"metric_binary_iou(   ...,   target_class_ids = list(0L, 1L),   threshold = 0.5,   name = NULL,   dtype = NULL )"},{"path":"https://keras.posit.co/reference/metric_binary_iou.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the Intersection-Over-Union metric for class 0 and/or 1. — metric_binary_iou","text":"... forward/backward compatability. target_class_ids list list target class ids metric returned. Options 0, 1, c(0, 1). 0 (1), IoU metric class 0 (class 1, respectively) returned. c(0, 1), mean IoUs two classes returned. threshold threshold applies prediction logits convert either predicted class 0 logit threshold predicted class 1 logit threshold. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_binary_iou.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Computes the Intersection-Over-Union metric for class 0 and/or 1. — metric_binary_iou","text":"threshold=0, metric behavior IoU.","code":""},{"path":"https://keras.posit.co/reference/metric_binary_iou.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the Intersection-Over-Union metric for class 0 and/or 1. — metric_binary_iou","text":"Standalone usage:         Usage compile() API:","code":"m <- metric_binary_iou(target_class_ids=c(0L, 1L), threshold = 0.3) m$update_state(c(0, 1, 0, 1), c(0.1, 0.2, 0.4, 0.7)) m$result() ## tf.Tensor(0.33333334, shape=(), dtype=float32) m$reset_state() m$update_state(c(0, 1, 0, 1), c(0.1, 0.2, 0.4, 0.7),                sample_weight = c(0.2, 0.3, 0.4, 0.1)) m$result() ## tf.Tensor(0.17361109, shape=(), dtype=float32) model %>% compile(     optimizer = 'sgd',     loss = 'mse',     metrics = list(metric_binary_iou(         target_class_ids = 0L,         threshold = 0.5     )) )"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_categorical_accuracy.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculates how often predictions match one-hot labels. — metric_categorical_accuracy","title":"Calculates how often predictions match one-hot labels. — metric_categorical_accuracy","text":"can provide logits classes y_pred, since argmax logits probabilities . metric creates two local variables, total count used compute frequency y_pred matches y_true. frequency ultimately returned categorical accuracy: idempotent operation simply divides total count. y_pred y_true passed vectors probabilities, rather labels. necessary, use op_one_hot expand y_true vector. sample_weight NULL, weights default 1. Use sample_weight 0 mask values.","code":""},{"path":"https://keras.posit.co/reference/metric_categorical_accuracy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates how often predictions match one-hot labels. — metric_categorical_accuracy","text":"","code":"metric_categorical_accuracy(   y_true,   y_pred,   ...,   name = \"categorical_accuracy\",   dtype = NULL )"},{"path":"https://keras.posit.co/reference/metric_categorical_accuracy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculates how often predictions match one-hot labels. — metric_categorical_accuracy","text":"y_true Tensor true targets. y_pred Tensor predicted targets. ... forward/backward compatability. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_categorical_accuracy.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates how often predictions match one-hot labels. — metric_categorical_accuracy","text":"Standalone usage:           Usage compile() API:","code":"m <- metric_categorical_accuracy() m$update_state(rbind(c(0, 0, 1), c(0, 1, 0)), rbind(c(0.1, 0.9, 0.8),                 c(0.05, 0.95, 0))) m$result() ## tf.Tensor(0.5, shape=(), dtype=float32) m$reset_state() m$update_state(rbind(c(0, 0, 1), c(0, 1, 0)), rbind(c(0.1, 0.9, 0.8),                c(0.05, 0.95, 0)),                sample_weight = c(0.7, 0.3)) m$result() ## tf.Tensor(0.3, shape=(), dtype=float32) # 0.3 model %>% compile(optimizer = 'sgd',                   loss = 'categorical_crossentropy',                   metrics = list(metric_categorical_accuracy()))"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_categorical_crossentropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the crossentropy metric between the labels and predictions. — metric_categorical_crossentropy","title":"Computes the crossentropy metric between the labels and predictions. — metric_categorical_crossentropy","text":"crossentropy metric class used multiple label classes (2 ). assumes labels one-hot encoded, e.g., labels values c(2, 0, 1), y_true rbind(c([0, 0, 1), c(1, 0, 0), c(0, 1, 0)).","code":""},{"path":"https://keras.posit.co/reference/metric_categorical_crossentropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the crossentropy metric between the labels and predictions. — metric_categorical_crossentropy","text":"","code":"metric_categorical_crossentropy(   y_true,   y_pred,   from_logits = FALSE,   label_smoothing = 0,   axis = -1L,   ...,   name = \"categorical_crossentropy\",   dtype = NULL )"},{"path":"https://keras.posit.co/reference/metric_categorical_crossentropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the crossentropy metric between the labels and predictions. — metric_categorical_crossentropy","text":"y_true Tensor one-hot true targets. y_pred Tensor predicted targets. from_logits (Optional) Whether output expected logits tensor. default, consider output encodes probability distribution. label_smoothing (Optional) Float [0, 1]. > 0, label values smoothed, meaning confidence label values relaxed. e.g. label_smoothing=0.2 means use value 0.1 label \"0\" 0.9 label \"1\". axis (Optional) Defaults -1. dimension along entropy computed. ... forward/backward compatability. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_categorical_crossentropy.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the crossentropy metric between the labels and predictions. — metric_categorical_crossentropy","text":"Standalone usage:           Usage compile() API:","code":"# EPSILON = 1e-7, y = y_true, y` = y_pred # y` = clip_op_clip_by_value(output, EPSILON, 1. - EPSILON) # y` = rbind(c(0.05, 0.95, EPSILON), c(0.1, 0.8, 0.1)) # xent = -sum(y * log(y'), axis = -1) #      = -((log 0.95), (log 0.1)) #      = [0.051, 2.302] # Reduced xent = (0.051 + 2.302) / 2 m <- metric_categorical_crossentropy() m$update_state(rbind(c(0, 1, 0), c(0, 0, 1)),                rbind(c(0.05, 0.95, 0), c(0.1, 0.8, 0.1))) m$result() ## tf.Tensor(1.1769392, shape=(), dtype=float32) # 1.1769392 m$reset_state() m$update_state(rbind(c(0, 1, 0), c(0, 0, 1)),                rbind(c(0.05, 0.95, 0), c(0.1, 0.8, 0.1)),                sample_weight = c(0.3, 0.7)) m$result() ## tf.Tensor(1.6271976, shape=(), dtype=float32) model %>% compile(   optimizer = 'sgd',   loss = 'mse',   metrics = list(metric_categorical_crossentropy()))"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_categorical_focal_crossentropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the categorical focal crossentropy loss. — metric_categorical_focal_crossentropy","title":"Computes the categorical focal crossentropy loss. — metric_categorical_focal_crossentropy","text":"Computes categorical focal crossentropy loss.","code":""},{"path":"https://keras.posit.co/reference/metric_categorical_focal_crossentropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the categorical focal crossentropy loss. — metric_categorical_focal_crossentropy","text":"","code":"metric_categorical_focal_crossentropy(   y_true,   y_pred,   alpha = 0.25,   gamma = 2,   from_logits = FALSE,   label_smoothing = 0,   axis = -1L )"},{"path":"https://keras.posit.co/reference/metric_categorical_focal_crossentropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the categorical focal crossentropy loss. — metric_categorical_focal_crossentropy","text":"y_true Tensor one-hot true targets. y_pred Tensor predicted targets. alpha weight balancing factor classes, default 0.25 mentioned reference. can list floats scalar. multi-class case, alpha may set inverse class frequency using compute_class_weight sklearn.utils. gamma focusing parameter, default 2.0 mentioned reference. helps gradually reduce importance given simple examples smooth manner. gamma = 0, focal effect categorical crossentropy. from_logits Whether y_pred expected logits tensor. default, assume y_pred encodes probability distribution. label_smoothing Float [0, 1]. > 0 smooth labels. example, 0.1, use 0.1 / num_classes non-target labels 0.9 + 0.1 / num_classes target labels. axis Defaults -1. dimension along entropy computed.","code":""},{"path":"https://keras.posit.co/reference/metric_categorical_focal_crossentropy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the categorical focal crossentropy loss. — metric_categorical_focal_crossentropy","text":"Categorical focal crossentropy loss value.","code":""},{"path":"https://keras.posit.co/reference/metric_categorical_focal_crossentropy.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the categorical focal crossentropy loss. — metric_categorical_focal_crossentropy","text":"","code":"y_true <- rbind(c(0, 1, 0), c(0, 0, 1)) y_pred <- rbind(c(0.05, 0.9, 0.05), c(0.1, 0.85, 0.05)) loss <- loss_categorical_focal_crossentropy(y_true, y_pred) loss ## tf.Tensor([2.63401289e-04 6.75912094e-01], shape=(2), dtype=float64)"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_categorical_hinge.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the categorical hinge metric between y_true and y_pred. — metric_categorical_hinge","title":"Computes the categorical hinge metric between y_true and y_pred. — metric_categorical_hinge","text":"Formula:   neg=maximum((1-y_true)*y_pred) pos=sum(y_true*y_pred)","code":"loss <- maximum(neg - pos + 1, 0)"},{"path":"https://keras.posit.co/reference/metric_categorical_hinge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the categorical hinge metric between y_true and y_pred. — metric_categorical_hinge","text":"","code":"metric_categorical_hinge(   y_true,   y_pred,   ...,   name = \"categorical_hinge\",   dtype = NULL )"},{"path":"https://keras.posit.co/reference/metric_categorical_hinge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the categorical hinge metric between y_true and y_pred. — metric_categorical_hinge","text":"y_true ground truth values. y_true values expected either {-1, +1} {0, 1} (.e. one-hot-encoded tensor) shape = [batch_size, d0, .. dN]. y_pred predicted values shape = [batch_size, d0, .. dN]. ... forward/backward compatability. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_categorical_hinge.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the categorical hinge metric between y_true and y_pred. — metric_categorical_hinge","text":"Categorical hinge loss values shape = [batch_size, d0, .. dN-1].","code":""},{"path":"https://keras.posit.co/reference/metric_categorical_hinge.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the categorical hinge metric between y_true and y_pred. — metric_categorical_hinge","text":"Standalone usage:","code":"m <- metric_categorical_hinge() m$update_state(rbind(c(0, 1), c(0, 0)), rbind(c(0.6, 0.4), c(0.4, 0.6))) m$result() ## tf.Tensor(1.4000001, shape=(), dtype=float32) m$reset_state() m$update_state(rbind(c(0, 1), c(0, 0)), rbind(c(0.6, 0.4), c(0.4, 0.6)),                sample_weight = c(1, 0)) m$result() ## tf.Tensor(1.2, shape=(), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_cosine_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the cosine similarity between the labels and predictions. — metric_cosine_similarity","title":"Computes the cosine similarity between the labels and predictions. — metric_cosine_similarity","text":"Formula:   See: Cosine Similarity. metric keeps average cosine similarity predictions labels stream data.","code":"loss <- sum(l2_norm(y_true) * l2_norm(y_pred))"},{"path":"https://keras.posit.co/reference/metric_cosine_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the cosine similarity between the labels and predictions. — metric_cosine_similarity","text":"","code":"metric_cosine_similarity(   ...,   name = \"cosine_similarity\",   dtype = NULL,   axis = -1L )"},{"path":"https://keras.posit.co/reference/metric_cosine_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the cosine similarity between the labels and predictions. — metric_cosine_similarity","text":"... forward/backward compatability. name (Optional) string name metric instance. dtype (Optional) data type metric result. axis (Optional) Defaults -1. dimension along cosine similarity computed.","code":""},{"path":"https://keras.posit.co/reference/metric_cosine_similarity.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the cosine similarity between the labels and predictions. — metric_cosine_similarity","text":"Standalone usage:         Usage compile() API:","code":"m <- metric_cosine_similarity(axis=2) m$update_state(rbind(c(0., 1.), c(1., 1.)), rbind(c(1., 0.), c(1., 1.))) m$result() ## tf.Tensor(0.5, shape=(), dtype=float32) m$reset_state() m$update_state(rbind(c(0., 1.), c(1., 1.)), rbind(c(1., 0.), c(1., 1.)),                sample_weight = c(0.3, 0.7)) m$result() ## tf.Tensor(0.7, shape=(), dtype=float32) model %>% compile(   optimizer = 'sgd',   loss = 'mse',   metrics = list(metric_cosine_similarity(axis=2)))"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_f1_score.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes F-1 Score. — metric_f1_score","title":"Computes F-1 Score. — metric_f1_score","text":"Formula:   harmonic mean precision recall. output range [0, 1]. works multi-class multi-label classification.","code":"f1_score <- 2 * (precision * recall) / (precision + recall)"},{"path":"https://keras.posit.co/reference/metric_f1_score.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes F-1 Score. — metric_f1_score","text":"","code":"metric_f1_score(   ...,   average = NULL,   threshold = NULL,   name = \"f1_score\",   dtype = NULL )"},{"path":"https://keras.posit.co/reference/metric_f1_score.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes F-1 Score. — metric_f1_score","text":"... forward/backward compatability. average Type averaging performed data. Acceptable values NULL, \"micro\", \"macro\" \"weighted\". Defaults NULL. NULL, averaging performed result() return score class. \"micro\", compute metrics globally counting total true positives, false negatives false positives. \"macro\", compute metrics label, return unweighted mean. take label imbalance account. \"weighted\", compute metrics label, return average weighted support (number true instances label). alters \"macro\" account label imbalance. can result score precision recall. threshold Elements y_pred greater threshold converted 1, rest 0. threshold NULL, argmax y_pred converted 1, rest 0. name Optional. String name metric instance. dtype Optional. Data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_f1_score.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes F-1 Score. — metric_f1_score","text":"F-1 Score: float.","code":""},{"path":"https://keras.posit.co/reference/metric_f1_score.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes F-1 Score. — metric_f1_score","text":"","code":"metric <- metric_f1_score(threshold = 0.5) y_true <- rbind(c(1, 1, 1),                 c(1, 0, 0),                 c(1, 1, 0)) y_pred <- rbind(c(0.2, 0.6, 0.7),                 c(0.2, 0.6, 0.6),                 c(0.6, 0.8, 0.0)) metric$update_state(y_true, y_pred) result <- metric$result() result ## tf.Tensor([0.49999997 0.79999995 0.66666657], shape=(3), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_false_negatives.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculates the number of false negatives. — metric_false_negatives","title":"Calculates the number of false negatives. — metric_false_negatives","text":"sample_weight given, calculates sum weights false negatives. metric creates one local variable, accumulator used keep track number false negatives. sample_weight NULL, weights default 1. Use sample_weight 0 mask values.","code":""},{"path":"https://keras.posit.co/reference/metric_false_negatives.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates the number of false negatives. — metric_false_negatives","text":"","code":"metric_false_negatives(..., thresholds = NULL, name = NULL, dtype = NULL)"},{"path":"https://keras.posit.co/reference/metric_false_negatives.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculates the number of false negatives. — metric_false_negatives","text":"... forward/backward compatability. thresholds (Optional) Defaults 0.5. float value, Python list float threshold values [0, 1]. threshold compared prediction values determine truth value predictions (.e., threshold TRUE, FALSE). used loss function sets from_logits=TRUE (.e. sigmoid applied predictions), thresholds set 0. One metric value generated threshold value. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_false_negatives.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates the number of false negatives. — metric_false_negatives","text":"Standalone usage:","code":"m <- metric_false_negatives() m$update_state(c(0, 1, 1, 1), c(0, 1, 0, 0)) m$result() ## tf.Tensor(2.0, shape=(), dtype=float32) m$reset_state() m$update_state(c(0, 1, 1, 1), c(0, 1, 0, 0), sample_weight=c(0, 0, 1, 0)) m$result() ## tf.Tensor(1.0, shape=(), dtype=float32) # 1.0"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_false_positives.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculates the number of false positives. — metric_false_positives","title":"Calculates the number of false positives. — metric_false_positives","text":"sample_weight given, calculates sum weights false positives. metric creates one local variable, accumulator used keep track number false positives. sample_weight NULL, weights default 1. Use sample_weight 0 mask values.","code":""},{"path":"https://keras.posit.co/reference/metric_false_positives.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates the number of false positives. — metric_false_positives","text":"","code":"metric_false_positives(..., thresholds = NULL, name = NULL, dtype = NULL)"},{"path":"https://keras.posit.co/reference/metric_false_positives.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculates the number of false positives. — metric_false_positives","text":"... forward/backward compatability. thresholds (Optional) Defaults 0.5. float value, Python list float threshold values [0, 1]. threshold compared prediction values determine truth value predictions (.e., threshold TRUE, FALSE). used loss function sets from_logits=TRUE (.e. sigmoid applied predictions), thresholds set 0. One metric value generated threshold value. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_false_positives.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates the number of false positives. — metric_false_positives","text":"Standalone usage:","code":"m <- metric_false_positives() m$update_state(c(0, 1, 0, 0), c(0, 0, 1, 1)) m$result() ## tf.Tensor(2.0, shape=(), dtype=float32) m$reset_state() m$update_state(c(0, 1, 0, 0), c(0, 0, 1, 1), sample_weight = c(0, 0, 1, 0)) m$result() ## tf.Tensor(1.0, shape=(), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_fbeta_score.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes F-Beta score. — metric_fbeta_score","title":"Computes F-Beta score. — metric_fbeta_score","text":"Formula:   weighted harmonic mean precision recall. output range [0, 1]. works multi-class multi-label classification.","code":"b2 <- beta^2 f_beta_score <- (1 + b2) * (precision * recall) / (precision * b2 + recall)"},{"path":"https://keras.posit.co/reference/metric_fbeta_score.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes F-Beta score. — metric_fbeta_score","text":"","code":"metric_fbeta_score(   ...,   average = NULL,   beta = 1,   threshold = NULL,   name = \"fbeta_score\",   dtype = NULL )"},{"path":"https://keras.posit.co/reference/metric_fbeta_score.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes F-Beta score. — metric_fbeta_score","text":"... forward/backward compatability. average Type averaging performed across per-class results multi-class case. Acceptable values NULL, \"micro\", \"macro\" \"weighted\". Defaults NULL. NULL, averaging performed result() return score class. \"micro\", compute metrics globally counting total true positives, false negatives false positives. \"macro\", compute metrics label, return unweighted mean. take label imbalance account. \"weighted\", compute metrics label, return average weighted support (number true instances label). alters \"macro\" account label imbalance. can result score precision recall. beta Determines weight given recall harmonic mean precision recall (see pseudocode equation ). Defaults 1. threshold Elements y_pred greater threshold converted 1, rest 0. threshold NULL, argmax y_pred converted 1, rest 0. name Optional. String name metric instance. dtype Optional. Data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_fbeta_score.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes F-Beta score. — metric_fbeta_score","text":"F-Beta Score: float.","code":""},{"path":"https://keras.posit.co/reference/metric_fbeta_score.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes F-Beta score. — metric_fbeta_score","text":"","code":"metric <- metric_fbeta_score(beta = 2.0, threshold = 0.5) y_true <- rbind(c(1, 1, 1),                 c(1, 0, 0),                 c(1, 1, 0)) y_pred <- rbind(c(0.2, 0.6, 0.7),                 c(0.2, 0.6, 0.6),                 c(0.6, 0.8, 0.0)) metric$update_state(y_true, y_pred) metric$result() ## tf.Tensor([0.3846154  0.90909094 0.8333332 ], shape=(3), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_hinge.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the hinge metric between y_true and y_pred. — metric_hinge","title":"Computes the hinge metric between y_true and y_pred. — metric_hinge","text":"Formula:   y_true values expected -1 1. binary (0 1) labels provided convert -1 1.","code":"loss <- mean(maximum(1 - y_true * y_pred, 0), axis=-1)"},{"path":"https://keras.posit.co/reference/metric_hinge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the hinge metric between y_true and y_pred. — metric_hinge","text":"","code":"metric_hinge(y_true, y_pred, ..., name = \"hinge\", dtype = NULL)"},{"path":"https://keras.posit.co/reference/metric_hinge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the hinge metric between y_true and y_pred. — metric_hinge","text":"y_true ground truth values. y_true values expected -1 1. binary (0 1) labels provided converted -1 1 shape = [batch_size, d0, .. dN]. y_pred predicted values shape = [batch_size, d0, .. dN]. ... forward/backward compatability. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_hinge.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the hinge metric between y_true and y_pred. — metric_hinge","text":"Standalone usage:","code":"m <- metric_hinge() m$update_state(rbind(c(0, 1), c(0, 0)), rbind(c(0.6, 0.4), c(0.4, 0.6))) m$result() ## tf.Tensor(1.3, shape=(), dtype=float32) m$reset_state() m$update_state(rbind(c(0, 1), c(0, 0)), rbind(c(0.6, 0.4), c(0.4, 0.6)),                sample_weight = c(1, 0)) m$result() ## tf.Tensor(1.1, shape=(), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_huber.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes Huber loss value. — metric_huber","title":"Computes Huber loss value. — metric_huber","text":"Formula:   See: Huber loss.","code":"for (x in error) {   if (abs(x) <= delta){     loss <- c(loss, (0.5 * x^2))   } else if (abs(x) > delta) {     loss <- c(loss, (delta * abs(x) - 0.5 * delta^2))   } } loss <- mean(loss)"},{"path":"https://keras.posit.co/reference/metric_huber.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes Huber loss value. — metric_huber","text":"","code":"metric_huber(y_true, y_pred, delta = 1)"},{"path":"https://keras.posit.co/reference/metric_huber.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes Huber loss value. — metric_huber","text":"y_true tensor true targets. y_pred tensor predicted targets. delta float, point Huber loss function changes quadratic linear. Defaults 1.0.","code":""},{"path":"https://keras.posit.co/reference/metric_huber.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes Huber loss value. — metric_huber","text":"","code":"Tensor with one scalar loss entry per sample."},{"path":"https://keras.posit.co/reference/metric_huber.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes Huber loss value. — metric_huber","text":"","code":"y_true <- rbind(c(0, 1), c(0, 0)) y_pred <- rbind(c(0.6, 0.4), c(0.4, 0.6)) loss <- loss_huber(y_true, y_pred)"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_iou.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the Intersection-Over-Union metric for specific target classes. — metric_iou","title":"Computes the Intersection-Over-Union metric for specific target classes. — metric_iou","text":"Formula:   Intersection--Union common evaluation metric semantic image segmentation. compute IoUs, predictions accumulated confusion matrix, weighted sample_weight metric calculated . sample_weight NULL, weights default 1. Use sample_weight 0 mask values. Note, class first computes IoUs individual classes, returns mean IoUs classes specified target_class_ids. target_class_ids one id value, IoU specific class returned.","code":"iou <- true_positives / (true_positives + false_positives + false_negatives)"},{"path":"https://keras.posit.co/reference/metric_iou.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the Intersection-Over-Union metric for specific target classes. — metric_iou","text":"","code":"metric_iou(   ...,   num_classes,   target_class_ids,   name = NULL,   dtype = NULL,   ignore_class = NULL,   sparse_y_true = TRUE,   sparse_y_pred = TRUE,   axis = -1L )"},{"path":"https://keras.posit.co/reference/metric_iou.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the Intersection-Over-Union metric for specific target classes. — metric_iou","text":"... forward/backward compatability. num_classes possible number labels prediction task can . target_class_ids list target class ids metric returned. compute IoU specific class, list single id value provided. name (Optional) string name metric instance. dtype (Optional) data type metric result. ignore_class Optional integer. ID class ignored metric computation. useful, example, segmentation problems featuring \"void\" class (commonly -1 255) segmentation maps. default (ignore_class=NULL), classes considered. sparse_y_true Whether labels encoded using integers dense floating point vectors. FALSE, argmax function used determine sample's likely associated label. sparse_y_pred Whether predictions encoded using integers dense floating point vectors. FALSE, argmax function used determine sample's likely associated label. axis (Optional) -1 dimension containing logits. Defaults -1.","code":""},{"path":"https://keras.posit.co/reference/metric_iou.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the Intersection-Over-Union metric for specific target classes. — metric_iou","text":"Standalone usage:         Usage compile() API:","code":"m <- metric_iou(num_classes = 2L, target_class_ids = list(0L)) m$update_state(c(0, 0, 1, 1), c(0, 1, 0, 1)) m$result() ## tf.Tensor(0.3333333, shape=(), dtype=float32) m$reset_state() m$update_state(c(0, 0, 1, 1), c(0, 1, 0, 1),                sample_weight = c(0.3, 0.3, 0.3, 0.1)) m$result() ## tf.Tensor(0.33333325, shape=(), dtype=float32) model %>% compile(   optimizer = 'sgd',   loss = 'mse',   metrics = list(metric_iou(num_classes = 2L, target_class_ids = list(0L))))"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_kl_divergence.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes Kullback-Leibler divergence metric between y_true and — metric_kl_divergence","title":"Computes Kullback-Leibler divergence metric between y_true and — metric_kl_divergence","text":"Formula:","code":"loss <- y_true * log(y_true / y_pred)"},{"path":"https://keras.posit.co/reference/metric_kl_divergence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes Kullback-Leibler divergence metric between y_true and — metric_kl_divergence","text":"","code":"metric_kl_divergence(y_true, y_pred, ..., name = \"kl_divergence\", dtype = NULL)"},{"path":"https://keras.posit.co/reference/metric_kl_divergence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes Kullback-Leibler divergence metric between y_true and — metric_kl_divergence","text":"y_true Tensor true targets. y_pred Tensor predicted targets. ... forward/backward compatability. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_kl_divergence.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes Kullback-Leibler divergence metric between y_true and — metric_kl_divergence","text":"Standalone usage:         Usage compile() API:","code":"m <- metric_kl_divergence() m$update_state(rbind(c(0, 1), c(0, 0)), rbind(c(0.6, 0.4), c(0.4, 0.6))) m$result() ## tf.Tensor(0.45814303, shape=(), dtype=float32) m$reset_state() m$update_state(rbind(c(0, 1), c(0, 0)), rbind(c(0.6, 0.4), c(0.4, 0.6)),                sample_weight = c(1, 0)) m$result() ## tf.Tensor(0.91628915, shape=(), dtype=float32) model %>% compile(optimizer = 'sgd',                   loss = 'mse',                   metrics = list(metric_kl_divergence()))"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_log_cosh.html","id":null,"dir":"Reference","previous_headings":"","what":"Logarithm of the hyperbolic cosine of the prediction error. — metric_log_cosh","title":"Logarithm of the hyperbolic cosine of the prediction error. — metric_log_cosh","text":"Formula:   Note log(cosh(x)) approximately equal (x ** 2) / 2 small x abs(x) - log(2) large x. means 'logcosh' works mostly like mean squared error, strongly affected occasional wildly incorrect prediction.","code":"loss <- mean(log(cosh(y_pred - y_true)), axis=-1)"},{"path":"https://keras.posit.co/reference/metric_log_cosh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Logarithm of the hyperbolic cosine of the prediction error. — metric_log_cosh","text":"","code":"metric_log_cosh(y_true, y_pred)"},{"path":"https://keras.posit.co/reference/metric_log_cosh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Logarithm of the hyperbolic cosine of the prediction error. — metric_log_cosh","text":"y_true Ground truth values shape = [batch_size, d0, .. dN]. y_pred predicted values shape = [batch_size, d0, .. dN].","code":""},{"path":"https://keras.posit.co/reference/metric_log_cosh.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Logarithm of the hyperbolic cosine of the prediction error. — metric_log_cosh","text":"","code":"Logcosh error values with shape = `[batch_size, d0, .. dN-1]`."},{"path":"https://keras.posit.co/reference/metric_log_cosh.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Logarithm of the hyperbolic cosine of the prediction error. — metric_log_cosh","text":"","code":"y_true <- rbind(c(0., 1.), c(0., 0.)) y_pred <- rbind(c(1., 1.), c(0., 0.)) loss <- metric_log_cosh(y_true, y_pred) loss ## tf.Tensor([ 2.16890413e-01 -1.90465432e-09], shape=(2), dtype=float64)"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_log_cosh_error.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the logarithm of the hyperbolic cosine of the prediction error. — metric_log_cosh_error","title":"Computes the logarithm of the hyperbolic cosine of the prediction error. — metric_log_cosh_error","text":"Formula:","code":"error <- y_pred - y_true logcosh <- mean(log((exp(error) + exp(-error))/2), axis=-1)"},{"path":"https://keras.posit.co/reference/metric_log_cosh_error.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the logarithm of the hyperbolic cosine of the prediction error. — metric_log_cosh_error","text":"","code":"metric_log_cosh_error(..., name = \"logcosh\", dtype = NULL)"},{"path":"https://keras.posit.co/reference/metric_log_cosh_error.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the logarithm of the hyperbolic cosine of the prediction error. — metric_log_cosh_error","text":"... forward/backward compatability. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_log_cosh_error.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the logarithm of the hyperbolic cosine of the prediction error. — metric_log_cosh_error","text":"Standalone usage:         Usage compile() API:","code":"m <- metric_log_cosh_error() m$update_state(rbind(c(0, 1), c(0, 0)), rbind(c(1, 1), c(0, 0))) m$result() ## tf.Tensor(0.108445205, shape=(), dtype=float32) m$reset_state() m$update_state(rbind(c(0, 1), c(0, 0)), rbind(c(1, 1), c(0, 0)),                sample_weight = c(1, 0)) m$result() ## tf.Tensor(0.21689041, shape=(), dtype=float32) model %>% compile(optimizer = 'sgd',                   loss = 'mse',                   metrics = list(metric_log_cosh_error()))"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_mean.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the (weighted) mean of the given values. — metric_mean","title":"Compute the (weighted) mean of the given values. — metric_mean","text":"example, values c(1, 3, 5, 7) mean 4. sample_weight specified c(1, 1, 0, 0) mean 2. metric creates two variables, total count. mean value returned simply total divided count.","code":""},{"path":"https://keras.posit.co/reference/metric_mean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the (weighted) mean of the given values. — metric_mean","text":"","code":"metric_mean(..., name = \"mean\", dtype = NULL)"},{"path":"https://keras.posit.co/reference/metric_mean.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the (weighted) mean of the given values. — metric_mean","text":"... forward/backward compatability. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_mean.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute the (weighted) mean of the given values. — metric_mean","text":"","code":"m <- metric_mean() m$update_state(c(1, 3, 5, 7)) m$result() ## tf.Tensor(4.0, shape=(), dtype=float32) m$reset_state() m$update_state(c(1, 3, 5, 7), sample_weight = c(1, 1, 0, 0)) m$result() ## tf.Tensor(2.0, shape=(), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_mean_absolute_error.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the mean absolute error between the labels and predictions. — metric_mean_absolute_error","title":"Computes the mean absolute error between the labels and predictions. — metric_mean_absolute_error","text":"Formula:","code":"loss <- mean(abs(y_true - y_pred))"},{"path":"https://keras.posit.co/reference/metric_mean_absolute_error.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the mean absolute error between the labels and predictions. — metric_mean_absolute_error","text":"","code":"metric_mean_absolute_error(   y_true,   y_pred,   ...,   name = \"mean_absolute_error\",   dtype = NULL )"},{"path":"https://keras.posit.co/reference/metric_mean_absolute_error.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the mean absolute error between the labels and predictions. — metric_mean_absolute_error","text":"y_true Ground truth values shape = [batch_size, d0, .. dN]. y_pred predicted values shape = [batch_size, d0, .. dN]. ... forward/backward compatability. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_mean_absolute_error.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the mean absolute error between the labels and predictions. — metric_mean_absolute_error","text":"Standalone usage:         Usage compile() API:","code":"m <- metric_mean_absolute_error() m$update_state(rbind(c(0, 1), c(0, 0)), rbind(c(1, 1), c(0, 0))) m$result() ## tf.Tensor(0.25, shape=(), dtype=float32) m$reset_state() m$update_state(rbind(c(0, 1), c(0, 0)), rbind(c(1, 1), c(0, 0)),                sample_weight = c(1, 0)) m$result() ## tf.Tensor(0.5, shape=(), dtype=float32) model %>% compile(     optimizer = 'sgd',     loss = 'mse',     metrics = list(metric_mean_absolute_error()))"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_mean_absolute_percentage_error.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes mean absolute percentage error between y_true and y_pred. — metric_mean_absolute_percentage_error","title":"Computes mean absolute percentage error between y_true and y_pred. — metric_mean_absolute_percentage_error","text":"Formula:   Division zero prevented dividing maximum(y_true, epsilon) epsilon = keras$backend$epsilon() (default 1e-7).","code":"loss <- 100 * mean(abs((y_true - y_pred) / y_true), axis=-1)"},{"path":"https://keras.posit.co/reference/metric_mean_absolute_percentage_error.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes mean absolute percentage error between y_true and y_pred. — metric_mean_absolute_percentage_error","text":"","code":"metric_mean_absolute_percentage_error(   y_true,   y_pred,   ...,   name = \"mean_absolute_percentage_error\",   dtype = NULL )"},{"path":"https://keras.posit.co/reference/metric_mean_absolute_percentage_error.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes mean absolute percentage error between y_true and y_pred. — metric_mean_absolute_percentage_error","text":"y_true Ground truth values shape = [batch_size, d0, .. dN]. y_pred predicted values shape = [batch_size, d0, .. dN]. ... forward/backward compatability. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_mean_absolute_percentage_error.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes mean absolute percentage error between y_true and y_pred. — metric_mean_absolute_percentage_error","text":"Standalone usage:         Usage compile() API:","code":"m <- metric_mean_absolute_percentage_error() m$update_state(rbind(c(0, 1), c(0, 0)), rbind(c(1, 1), c(0, 0))) m$result() ## tf.Tensor(250000000.0, shape=(), dtype=float32) m$reset_state() m$update_state(rbind(c(0, 1), c(0, 0)), rbind(c(1, 1), c(0, 0)),                sample_weight = c(1, 0)) m$result() ## tf.Tensor(500000000.0, shape=(), dtype=float32) model %>% compile(     optimizer = 'sgd',     loss = 'mse',     metrics = list(metric_mean_absolute_percentage_error()))"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_mean_iou.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the mean Intersection-Over-Union metric. — metric_mean_iou","title":"Computes the mean Intersection-Over-Union metric. — metric_mean_iou","text":"Formula:   Intersection--Union common evaluation metric semantic image segmentation. compute IoUs, predictions accumulated confusion matrix, weighted sample_weight metric calculated . sample_weight NULL, weights default 1. Use sample_weight 0 mask values. Note class first computes IoUs individual classes, returns mean values.","code":"iou <- true_positives / (true_positives + false_positives + false_negatives)"},{"path":"https://keras.posit.co/reference/metric_mean_iou.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the mean Intersection-Over-Union metric. — metric_mean_iou","text":"","code":"metric_mean_iou(   ...,   num_classes,   name = NULL,   dtype = NULL,   ignore_class = NULL,   sparse_y_true = TRUE,   sparse_y_pred = TRUE,   axis = -1L )"},{"path":"https://keras.posit.co/reference/metric_mean_iou.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the mean Intersection-Over-Union metric. — metric_mean_iou","text":"... forward/backward compatability. num_classes possible number labels prediction task can . value must provided, since confusion matrix dimension = [num_classes, num_classes] allocated. name (Optional) string name metric instance. dtype (Optional) data type metric result. ignore_class Optional integer. ID class ignored metric computation. useful, example, segmentation problems featuring \"void\" class (commonly -1 255) segmentation maps. default (ignore_class=NULL), classes considered. sparse_y_true Whether labels encoded using integers dense floating point vectors. FALSE, argmax function used determine sample's likely associated label. sparse_y_pred Whether predictions encoded using integers dense floating point vectors. FALSE, argmax function used determine sample's likely associated label. axis (Optional) dimension containing logits. Defaults -1.","code":""},{"path":"https://keras.posit.co/reference/metric_mean_iou.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the mean Intersection-Over-Union metric. — metric_mean_iou","text":"Standalone usage:         Usage compile() API:","code":"# cm = [[1, 1], #        [1, 1]] # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1] # iou = true_positives / (sum_row + sum_col - true_positives)) # result = (1 / (2 + 2 - 1) + 1 / (2 + 2 - 1)) / 2 = 0.33 m <- metric_mean_iou(num_classes = 2) m$update_state(c(0, 0, 1, 1), c(0, 1, 0, 1)) m$result() ## tf.Tensor(0.33333334, shape=(), dtype=float32) m$reset_state() m$update_state(c(0, 0, 1, 1), c(0, 1, 0, 1),                sample_weight=c(0.3, 0.3, 0.3, 0.1)) m$result() ## tf.Tensor(0.2380952, shape=(), dtype=float32) model %>% compile(   optimizer = 'sgd',   loss = 'mse',   metrics = list(metric_mean_iou(num_classes=2)))"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_mean_squared_error.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the mean squared error between y_true and y_pred. — metric_mean_squared_error","title":"Computes the mean squared error between y_true and y_pred. — metric_mean_squared_error","text":"Formula:","code":"loss <- mean(square(y_true - y_pred))"},{"path":"https://keras.posit.co/reference/metric_mean_squared_error.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the mean squared error between y_true and y_pred. — metric_mean_squared_error","text":"","code":"metric_mean_squared_error(   y_true,   y_pred,   ...,   name = \"mean_squared_error\",   dtype = NULL )"},{"path":"https://keras.posit.co/reference/metric_mean_squared_error.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the mean squared error between y_true and y_pred. — metric_mean_squared_error","text":"y_true Ground truth values shape = [batch_size, d0, .. dN]. y_pred predicted values shape = [batch_size, d0, .. dN]. ... forward/backward compatability. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_mean_squared_error.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the mean squared error between y_true and y_pred. — metric_mean_squared_error","text":"","code":"m <- metric_mean_squared_error() m$update_state(rbind(c(0, 1), c(0, 0)), rbind(c(1, 1), c(0, 0))) m$result() ## tf.Tensor(0.25, shape=(), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_mean_squared_logarithmic_error.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes mean squared logarithmic error between y_true and y_pred. — metric_mean_squared_logarithmic_error","title":"Computes mean squared logarithmic error between y_true and y_pred. — metric_mean_squared_logarithmic_error","text":"Formula:   Note y_pred y_true less equal 0. Negative values 0 values replaced keras$backend$epsilon() (default 1e-7).","code":"loss <- mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1)"},{"path":"https://keras.posit.co/reference/metric_mean_squared_logarithmic_error.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes mean squared logarithmic error between y_true and y_pred. — metric_mean_squared_logarithmic_error","text":"","code":"metric_mean_squared_logarithmic_error(   y_true,   y_pred,   ...,   name = \"mean_squared_logarithmic_error\",   dtype = NULL )"},{"path":"https://keras.posit.co/reference/metric_mean_squared_logarithmic_error.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes mean squared logarithmic error between y_true and y_pred. — metric_mean_squared_logarithmic_error","text":"y_true Ground truth values shape = [batch_size, d0, .. dN]. y_pred predicted values shape = [batch_size, d0, .. dN]. ... forward/backward compatability. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_mean_squared_logarithmic_error.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes mean squared logarithmic error between y_true and y_pred. — metric_mean_squared_logarithmic_error","text":"Standalone usage:         Usage compile() API:","code":"m <- metric_mean_squared_logarithmic_error() m$update_state(rbind(c(0, 1), c(0, 0)), rbind(c(1, 1), c(0, 0))) m$result() ## tf.Tensor(0.12011322, shape=(), dtype=float32) m$reset_state() m$update_state(rbind(c(0, 1), c(0, 0)), rbind(c(1, 1), c(0, 0)),                sample_weight = c(1, 0)) m$result() ## tf.Tensor(0.24022643, shape=(), dtype=float32) model %>% compile(   optimizer = 'sgd',   loss = 'mse',   metrics = list(metric_mean_squared_logarithmic_error()))"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_mean_wrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrap a stateless metric function with the Mean metric. — metric_mean_wrapper","title":"Wrap a stateless metric function with the Mean metric. — metric_mean_wrapper","text":"use class quickly build mean metric function. function needs signature fn(y_true, y_pred) return per-sample loss array. metric_mean_wrapper$result() return average metric value across samples seen far. example:","code":"mse <- function(y_true, y_pred) {   (y_true - y_pred)^2 }  mse_metric <- metric_mean_wrapper(fn = mse) mse_metric$update_state(c(0, 1), c(1, 1)) mse_metric$result() ## tf.Tensor(0.5, shape=(), dtype=float32)"},{"path":"https://keras.posit.co/reference/metric_mean_wrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrap a stateless metric function with the Mean metric. — metric_mean_wrapper","text":"","code":"metric_mean_wrapper(..., fn, name = NULL, dtype = NULL)"},{"path":"https://keras.posit.co/reference/metric_mean_wrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrap a stateless metric function with the Mean metric. — metric_mean_wrapper","text":"... Keyword arguments pass fn. fn metric function wrap, signature fn(y_true, y_pred). name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/metric_one_hot_iou.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the Intersection-Over-Union metric for one-hot encoded labels. — metric_one_hot_iou","title":"Computes the Intersection-Over-Union metric for one-hot encoded labels. — metric_one_hot_iou","text":"Formula:   Intersection--Union common evaluation metric semantic image segmentation. compute IoUs, predictions accumulated confusion matrix, weighted sample_weight metric calculated . sample_weight NULL, weights default 1. Use sample_weight 0 mask values. class can used compute IoU multi-class classification tasks labels one-hot encoded (last axis one dimension per class). Note predictions also shape. compute IoU, first labels predictions converted back integer format taking argmax class axis. computation steps base IoU class apply. Note, one channel labels predictions, class class IoU. case, use IoU instead. Also, make sure num_classes equal number classes data, avoid \"labels bound\" error confusion matrix computed.","code":"iou <- true_positives / (true_positives + false_positives + false_negatives)"},{"path":"https://keras.posit.co/reference/metric_one_hot_iou.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the Intersection-Over-Union metric for one-hot encoded labels. — metric_one_hot_iou","text":"","code":"metric_one_hot_iou(   ...,   num_classes,   target_class_ids,   name = NULL,   dtype = NULL,   ignore_class = NULL,   sparse_y_pred = FALSE,   axis = -1L )"},{"path":"https://keras.posit.co/reference/metric_one_hot_iou.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the Intersection-Over-Union metric for one-hot encoded labels. — metric_one_hot_iou","text":"... forward/backward compatability. num_classes possible number labels prediction task can . target_class_ids list list target class ids metric returned. compute IoU specific class, list (list) single id value provided. name (Optional) string name metric instance. dtype (Optional) data type metric result. ignore_class Optional integer. ID class ignored metric computation. useful, example, segmentation problems featuring \"void\" class (commonly -1 255) segmentation maps. default (ignore_class=NULL), classes considered. sparse_y_pred Whether predictions encoded using integers dense floating point vectors. FALSE, argmax function used determine sample's likely associated label. axis (Optional) dimension containing logits. Defaults -1.","code":""},{"path":"https://keras.posit.co/reference/metric_one_hot_iou.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the Intersection-Over-Union metric for one-hot encoded labels. — metric_one_hot_iou","text":"Standalone usage:     Usage compile() API:","code":"y_true <- rbind(c(0, 0, 1), c(1, 0, 0), c(0, 1, 0), c(1, 0, 0)) y_pred <- rbind(c(0.2, 0.3, 0.5), c(0.1, 0.2, 0.7), c(0.5, 0.3, 0.1),                 c(0.1, 0.4, 0.5)) sample_weight <- c(0.1, 0.2, 0.3, 0.4) m <- metric_one_hot_iou(num_classes = 3, target_class_ids = c(0, 2)) m$update_state(     y_true = y_true, y_pred = y_pred, sample_weight = sample_weight) m$result() ## tf.Tensor(0.07142855, shape=(), dtype=float32) model %>% compile(   optimizer = 'sgd',   loss = 'mse',   metrics = list(metric_one_hot_iou(     num_classes = 3L,     target_class_id = list(1L)   )) )"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_one_hot_mean_iou.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes mean Intersection-Over-Union metric for one-hot encoded labels. — metric_one_hot_mean_iou","title":"Computes mean Intersection-Over-Union metric for one-hot encoded labels. — metric_one_hot_mean_iou","text":"Formula:   Intersection--Union common evaluation metric semantic image segmentation. compute IoUs, predictions accumulated confusion matrix, weighted sample_weight metric calculated . sample_weight NULL, weights default 1. Use sample_weight 0 mask values. class can used compute mean IoU multi-class classification tasks labels one-hot encoded (last axis one dimension per class). Note predictions also shape. compute mean IoU, first labels predictions converted back integer format taking argmax class axis. computation steps base MeanIoU class apply. Note, one channel labels predictions, class class metric_mean_iou. case, use metric_mean_iou instead. Also, make sure num_classes equal number classes data, avoid \"labels bound\" error confusion matrix computed.","code":"iou <- true_positives / (true_positives + false_positives + false_negatives)"},{"path":"https://keras.posit.co/reference/metric_one_hot_mean_iou.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes mean Intersection-Over-Union metric for one-hot encoded labels. — metric_one_hot_mean_iou","text":"","code":"metric_one_hot_mean_iou(   ...,   num_classes,   name = NULL,   dtype = NULL,   ignore_class = NULL,   sparse_y_pred = FALSE,   axis = -1L )"},{"path":"https://keras.posit.co/reference/metric_one_hot_mean_iou.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes mean Intersection-Over-Union metric for one-hot encoded labels. — metric_one_hot_mean_iou","text":"... forward/backward compatability. num_classes possible number labels prediction task can . name (Optional) string name metric instance. dtype (Optional) data type metric result. ignore_class Optional integer. ID class ignored metric computation. useful, example, segmentation problems featuring \"void\" class (commonly -1 255) segmentation maps. default (ignore_class=NULL), classes considered. sparse_y_pred Whether predictions encoded using natural numbers probability distribution vectors. FALSE, argmax function used determine sample's likely associated label. axis (Optional) dimension containing logits. Defaults -1.","code":""},{"path":"https://keras.posit.co/reference/metric_one_hot_mean_iou.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes mean Intersection-Over-Union metric for one-hot encoded labels. — metric_one_hot_mean_iou","text":"Standalone usage:     Usage compile() API:","code":"y_true <- rbind(c(0, 0, 1), c(1, 0, 0), c(0, 1, 0), c(1, 0, 0)) y_pred <- rbind(c(0.2, 0.3, 0.5), c(0.1, 0.2, 0.7), c(0.5, 0.3, 0.1),                 c(0.1, 0.4, 0.5)) sample_weight <- c(0.1, 0.2, 0.3, 0.4) m <- metric_one_hot_mean_iou(num_classes = 3L) m$update_state(     y_true = y_true, y_pred = y_pred, sample_weight = sample_weight) m$result() ## tf.Tensor(0.047619034, shape=(), dtype=float32) model %>% compile(     optimizer = 'sgd',     loss = 'mse',     metrics = list(metric_one_hot_mean_iou(num_classes = 3L)))"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_poisson.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the Poisson metric between y_true and y_pred. — metric_poisson","title":"Computes the Poisson metric between y_true and y_pred. — metric_poisson","text":"Formula:","code":"metric <- y_pred - y_true * log(y_pred)"},{"path":"https://keras.posit.co/reference/metric_poisson.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the Poisson metric between y_true and y_pred. — metric_poisson","text":"","code":"metric_poisson(y_true, y_pred, ..., name = \"poisson\", dtype = NULL)"},{"path":"https://keras.posit.co/reference/metric_poisson.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the Poisson metric between y_true and y_pred. — metric_poisson","text":"y_true Ground truth values. shape = [batch_size, d0, .. dN]. y_pred predicted values. shape = [batch_size, d0, .. dN]. ... forward/backward compatability. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_poisson.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the Poisson metric between y_true and y_pred. — metric_poisson","text":"Standalone usage:         Usage compile() API:","code":"m <- metric_poisson() m$update_state(rbind(c(0, 1), c(0, 0)), rbind(c(1, 1), c(0, 0))) m$result() ## tf.Tensor(0.49999997, shape=(), dtype=float32) m$reset_state() m$update_state(rbind(c(0, 1), c(0, 0)), rbind(c(1, 1), c(0, 0)),                sample_weight = c(1, 0)) m$result() ## tf.Tensor(0.99999994, shape=(), dtype=float32) model %>% compile(   optimizer = 'sgd',   loss = 'mse',   metrics = list(metric_poisson()) )"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_precision.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the precision of the predictions with respect to the labels. — metric_precision","title":"Computes the precision of the predictions with respect to the labels. — metric_precision","text":"metric creates two local variables, true_positives false_positives used compute precision. value ultimately returned precision, idempotent operation simply divides true_positives sum true_positives false_positives. sample_weight NULL, weights default 1. Use sample_weight 0 mask values. top_k set, calculate precision often average class among top-k classes highest predicted values batch entry correct can found label entry. class_id specified, calculate precision considering entries batch class_id threshold /top-k highest predictions, computing fraction class_id indeed correct label.","code":""},{"path":"https://keras.posit.co/reference/metric_precision.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the precision of the predictions with respect to the labels. — metric_precision","text":"","code":"metric_precision(   ...,   thresholds = NULL,   top_k = NULL,   class_id = NULL,   name = NULL,   dtype = NULL )"},{"path":"https://keras.posit.co/reference/metric_precision.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the precision of the predictions with respect to the labels. — metric_precision","text":"... forward/backward compatability. thresholds (Optional) float value, Python list float threshold values [0, 1]. threshold compared prediction values determine truth value predictions (.e., threshold TRUE, FALSE). used loss function sets from_logits=TRUE (.e. sigmoid applied predictions), thresholds set 0. One metric value generated threshold value. neither thresholds top_k set, default calculate precision thresholds=0.5. top_k (Optional) Unset default. int value specifying top-k predictions consider calculating precision. class_id (Optional) Integer class ID want binary metrics. must half-open interval [0, num_classes), num_classes last dimension predictions. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_precision.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the precision of the predictions with respect to the labels. — metric_precision","text":"Standalone usage:                 Usage compile() API:   Usage loss from_logits=TRUE:","code":"m <- metric_precision() m$update_state(c(0, 1, 1, 1), c(1, 0, 1, 1)) m$result() ## tf.Tensor(0.6666667, shape=(), dtype=float32) m$reset_state() m$update_state(c(0, 1, 1, 1), c(1, 0, 1, 1), sample_weight=c(0, 0, 1, 0)) m$result() ## tf.Tensor(0.9999999, shape=(), dtype=float32) # With top_k=2, it will calculate precision over y_true[:2] # and y_pred[:2] m <- metric_precision(top_k = 2) m$update_state(c(0, 0, 1, 1), c(1, 1, 1, 1)) m$result() ## tf.Tensor(0.0, shape=(), dtype=float32) # With top_k=4, it will calculate precision over y_true[:4] # and y_pred[:4] m <- metric_precision(top_k = 4) m$update_state(c(0, 0, 1, 1), c(1, 1, 1, 1)) m$result() ## tf.Tensor(0.5, shape=(), dtype=float32) model %>% compile(optimizer = 'sgd',                   loss = 'mse',                   metrics = list(metric_precision())) model %>% compile(optimizer = 'adam',                   loss = loss_binary_crossentropy(from_logits = TRUE),                   metrics = list(metric_precision(thresholds = 0)))"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_precision_at_recall.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes best precision where recall is >= specified value. — metric_precision_at_recall","title":"Computes best precision where recall is >= specified value. — metric_precision_at_recall","text":"metric creates four local variables, true_positives, true_negatives, false_positives false_negatives used compute precision given recall. threshold given recall value computed used evaluate corresponding precision. sample_weight NULL, weights default 1. Use sample_weight 0 mask values. class_id specified, calculate precision considering entries batch class_id threshold predictions, computing fraction class_id indeed correct label.","code":""},{"path":"https://keras.posit.co/reference/metric_precision_at_recall.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes best precision where recall is >= specified value. — metric_precision_at_recall","text":"","code":"metric_precision_at_recall(   ...,   recall,   num_thresholds = 200L,   class_id = NULL,   name = NULL,   dtype = NULL )"},{"path":"https://keras.posit.co/reference/metric_precision_at_recall.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes best precision where recall is >= specified value. — metric_precision_at_recall","text":"... forward/backward compatability. recall scalar value range [0, 1]. num_thresholds (Optional) Defaults 200. number thresholds use matching given recall. class_id (Optional) Integer class ID want binary metrics. must half-open interval [0, num_classes), num_classes last dimension predictions. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_precision_at_recall.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes best precision where recall is >= specified value. — metric_precision_at_recall","text":"Standalone usage:     Usage compile() API:","code":"m <- metric_precision_at_recall(recall = 0.5) m$update_state(c(0, 0, 0, 1, 1), c(0, 0.3, 0.8, 0.3, 0.8)) # m$result() m$reset_state() m$update_state(c(0, 0, 0, 1, 1), c(0, 0.3, 0.8, 0.3, 0.8),                sample_weight = c(2, 2, 2, 1, 1)) # m$result() model %>% compile(   optimizer = 'sgd',   loss = 'mse',   metrics = list(metric_precision_at_recall(recall = 0.8)) )"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_r2_score.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes R2 score. — metric_r2_score","title":"Computes R2 score. — metric_r2_score","text":"Formula:   also called coefficient determination. indicates close fitted regression line ground-truth data. highest score possible 1.0. indicates predictors perfectly accounts variation target. score 0.0 indicates predictors account variation target. can also negative model worse random. metric can also compute \"Adjusted R2\" score.","code":"sum_squares_residuals <- sum((y_true - y_pred) ** 2) sum_squares <- sum((y_true - mean(y_true)) ** 2) R2 <- 1 - sum_squares_residuals / sum_squares"},{"path":"https://keras.posit.co/reference/metric_r2_score.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes R2 score. — metric_r2_score","text":"","code":"metric_r2_score(   ...,   class_aggregation = \"uniform_average\",   num_regressors = 0L,   name = \"r2_score\",   dtype = NULL )"},{"path":"https://keras.posit.co/reference/metric_r2_score.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes R2 score. — metric_r2_score","text":"... forward/backward compatability. class_aggregation Specifies aggregate scores corresponding different output classes (target dimensions), .e. different dimensions last axis predictions. Equivalent multioutput argument Scikit-Learn. one NULL (aggregation), \"uniform_average\", \"variance_weighted_average\". num_regressors Number independent regressors used (\"Adjusted R2\" score). 0 standard R2 score. Defaults 0. name Optional. string name metric instance. dtype Optional. data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_r2_score.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes R2 score. — metric_r2_score","text":"","code":"y_true <- rbind(1, 4, 3) y_pred <- rbind(2, 4, 4) metric <- metric_r2_score() metric$update_state(y_true, y_pred) metric$result() ## tf.Tensor(0.57142854, shape=(), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_recall.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the recall of the predictions with respect to the labels. — metric_recall","title":"Computes the recall of the predictions with respect to the labels. — metric_recall","text":"metric creates two local variables, true_positives false_negatives, used compute recall. value ultimately returned recall, idempotent operation simply divides true_positives sum true_positives false_negatives. sample_weight NULL, weights default 1. Use sample_weight 0 mask values. top_k set, recall computed often average class among labels batch entry top-k predictions. class_id specified, calculate recall considering entries batch class_id label, computing fraction class_id threshold /top-k predictions.","code":""},{"path":"https://keras.posit.co/reference/metric_recall.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the recall of the predictions with respect to the labels. — metric_recall","text":"","code":"metric_recall(   ...,   thresholds = NULL,   top_k = NULL,   class_id = NULL,   name = NULL,   dtype = NULL )"},{"path":"https://keras.posit.co/reference/metric_recall.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the recall of the predictions with respect to the labels. — metric_recall","text":"... forward/backward compatability. thresholds (Optional) float value, Python list float threshold values [0, 1]. threshold compared prediction values determine truth value predictions (.e., threshold TRUE, FALSE). used loss function sets from_logits=TRUE (.e. sigmoid applied predictions), thresholds set 0. One metric value generated threshold value. neither thresholds top_k set, default calculate recall thresholds=0.5. top_k (Optional) Unset default. int value specifying top-k predictions consider calculating recall. class_id (Optional) Integer class ID want binary metrics. must half-open interval [0, num_classes), num_classes last dimension predictions. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_recall.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the recall of the predictions with respect to the labels. — metric_recall","text":"Standalone usage:         Usage compile() API:   Usage loss from_logits=TRUE:","code":"m <- metric_recall() m$update_state(c(0, 1, 1, 1), c(1, 0, 1, 1)) m$result() ## tf.Tensor(0.6666667, shape=(), dtype=float32) m$reset_state() m$update_state(c(0, 1, 1, 1), c(1, 0, 1, 1), sample_weight = c(0, 0, 1, 0)) m$result() ## tf.Tensor(0.9999999, shape=(), dtype=float32) model %>% compile(optimizer = 'sgd',                   loss = 'mse',                   metrics = list(metric_recall())) model %>% compile(optimizer = 'adam',                   loss = loss_binary_crossentropy(from_logits=TRUE),                   metrics = list(metric_recall(thresholds=0)))"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_recall_at_precision.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes best recall where precision is >= specified value. — metric_recall_at_precision","title":"Computes best recall where precision is >= specified value. — metric_recall_at_precision","text":"given score-label-distribution required precision might achievable, case 0.0 returned recall. metric creates four local variables, true_positives, true_negatives, false_positives false_negatives used compute recall given precision. threshold given precision value computed used evaluate corresponding recall. sample_weight NULL, weights default 1. Use sample_weight 0 mask values. class_id specified, calculate precision considering entries batch class_id threshold predictions, computing fraction class_id indeed correct label.","code":""},{"path":"https://keras.posit.co/reference/metric_recall_at_precision.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes best recall where precision is >= specified value. — metric_recall_at_precision","text":"","code":"metric_recall_at_precision(   ...,   precision,   num_thresholds = 200L,   class_id = NULL,   name = NULL,   dtype = NULL )"},{"path":"https://keras.posit.co/reference/metric_recall_at_precision.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes best recall where precision is >= specified value. — metric_recall_at_precision","text":"... forward/backward compatability. precision scalar value range [0, 1]. num_thresholds (Optional) Defaults 200. number thresholds use matching given precision. class_id (Optional) Integer class ID want binary metrics. must half-open interval [0, num_classes), num_classes last dimension predictions. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_recall_at_precision.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes best recall where precision is >= specified value. — metric_recall_at_precision","text":"Standalone usage:     Usage compile() API:","code":"m <- metric_recall_at_precision(precision = 0.8) m$update_state(c(0, 0, 1, 1), c(0, 0.5, 0.3, 0.9)) # m$result() m$reset_state() m$update_state(c(0, 0, 1, 1), c(0, 0.5, 0.3, 0.9),                sample_weight = c(1, 0, 0, 1)) # m$result() model %>% compile(     optimizer = 'sgd',     loss = 'mse',     metrics = list(metric_recall_at_precision(precision = 0.8)))"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_root_mean_squared_error.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes root mean squared error metric between y_true and y_pred. — metric_root_mean_squared_error","title":"Computes root mean squared error metric between y_true and y_pred. — metric_root_mean_squared_error","text":"Formula:","code":"loss <- sqrt(mean((y_pred - y_true) ^ 2))"},{"path":"https://keras.posit.co/reference/metric_root_mean_squared_error.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes root mean squared error metric between y_true and y_pred. — metric_root_mean_squared_error","text":"","code":"metric_root_mean_squared_error(   ...,   name = \"root_mean_squared_error\",   dtype = NULL )"},{"path":"https://keras.posit.co/reference/metric_root_mean_squared_error.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes root mean squared error metric between y_true and y_pred. — metric_root_mean_squared_error","text":"... forward/backward compatability. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_root_mean_squared_error.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes root mean squared error metric between y_true and y_pred. — metric_root_mean_squared_error","text":"Standalone usage:         Usage compile() API:","code":"m <- metric_root_mean_squared_error() m$update_state(rbind(c(0, 1), c(0, 0)), rbind(c(1, 1), c(0, 0))) m$result() ## tf.Tensor(0.5, shape=(), dtype=float32) m$reset_state() m$update_state(rbind(c(0, 1), c(0, 0)), rbind(c(1, 1), c(0, 0)),                sample_weight = c(1, 0)) m$result() ## tf.Tensor(0.70710677, shape=(), dtype=float32) model %>% compile(   optimizer = 'sgd',   loss = 'mse',   metrics = list(metric_root_mean_squared_error()))"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_sensitivity_at_specificity.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes best sensitivity where specificity is >= specified value. — metric_sensitivity_at_specificity","title":"Computes best sensitivity where specificity is >= specified value. — metric_sensitivity_at_specificity","text":"Sensitivity measures proportion actual positives correctly identified (tp / (tp + fn)). Specificity measures proportion actual negatives correctly identified (tn / (tn + fp)). metric creates four local variables, true_positives, true_negatives, false_positives false_negatives used compute sensitivity given specificity. threshold given specificity value computed used evaluate corresponding sensitivity. sample_weight NULL, weights default 1. Use sample_weight 0 mask values. class_id specified, calculate precision considering entries batch class_id threshold predictions, computing fraction class_id indeed correct label. additional information specificity sensitivity, see following.","code":""},{"path":"https://keras.posit.co/reference/metric_sensitivity_at_specificity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes best sensitivity where specificity is >= specified value. — metric_sensitivity_at_specificity","text":"","code":"metric_sensitivity_at_specificity(   ...,   specificity,   num_thresholds = 200L,   class_id = NULL,   name = NULL,   dtype = NULL )"},{"path":"https://keras.posit.co/reference/metric_sensitivity_at_specificity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes best sensitivity where specificity is >= specified value. — metric_sensitivity_at_specificity","text":"... forward/backward compatability. specificity scalar value range [0, 1]. num_thresholds (Optional) Defaults 200. number thresholds use matching given specificity. class_id (Optional) Integer class ID want binary metrics. must half-open interval [0, num_classes), num_classes last dimension predictions. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_sensitivity_at_specificity.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes best sensitivity where specificity is >= specified value. — metric_sensitivity_at_specificity","text":"Standalone usage:     Usage compile() API:","code":"m <- metric_sensitivity_at_specificity(specificity = 0.5) m$update_state(c(0, 0, 0, 1, 1), c(0, 0.3, 0.8, 0.3, 0.8)) # m$result() m$reset_state() m$update_state(c(0, 0, 0, 1, 1), c(0, 0.3, 0.8, 0.3, 0.8),                sample_weight = c(1, 1, 2, 2, 1)) # m$result() model %>% compile(     optimizer = 'sgd',     loss = 'mse',     metrics = list(metric_sensitivity_at_specificity()))"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_sparse_categorical_accuracy.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculates how often predictions match integer labels. — metric_sparse_categorical_accuracy","title":"Calculates how often predictions match integer labels. — metric_sparse_categorical_accuracy","text":"can provide logits classes y_pred, since argmax logits probabilities . metric creates two local variables, total count used compute frequency y_pred matches y_true. frequency ultimately returned sparse categorical accuracy: idempotent operation simply divides total count. sample_weight NULL, weights default 1. Use sample_weight 0 mask values.","code":"acc <- sample_weight %*% (y_true == which.max(y_pred))"},{"path":"https://keras.posit.co/reference/metric_sparse_categorical_accuracy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates how often predictions match integer labels. — metric_sparse_categorical_accuracy","text":"","code":"metric_sparse_categorical_accuracy(   y_true,   y_pred,   ...,   name = \"sparse_categorical_accuracy\",   dtype = NULL )"},{"path":"https://keras.posit.co/reference/metric_sparse_categorical_accuracy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculates how often predictions match integer labels. — metric_sparse_categorical_accuracy","text":"y_true Tensor true targets. y_pred Tensor predicted targets. ... forward/backward compatability. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_sparse_categorical_accuracy.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates how often predictions match integer labels. — metric_sparse_categorical_accuracy","text":"Standalone usage:         Usage compile() API:","code":"m <- metric_sparse_categorical_accuracy() m$update_state(rbind(2L, 1L), rbind(c(0.1, 0.6, 0.3), c(0.05, 0.95, 0))) m$result() ## tf.Tensor(0.5, shape=(), dtype=float32) m$reset_state() m$update_state(rbind(2L, 1L), rbind(c(0.1, 0.6, 0.3), c(0.05, 0.95, 0)),                sample_weight = c(0.7, 0.3)) m$result() ## tf.Tensor(0.3, shape=(), dtype=float32) model %>% compile(optimizer = 'sgd',                   loss = 'sparse_categorical_crossentropy',                   metrics = list(metric_sparse_categorical_accuracy()))"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_sparse_categorical_crossentropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the crossentropy metric between the labels and predictions. — metric_sparse_categorical_crossentropy","title":"Computes the crossentropy metric between the labels and predictions. — metric_sparse_categorical_crossentropy","text":"Use crossentropy metric two label classes. expects labels provided integers. want provide labels one-hot encoded, please use metric_categorical_crossentropy() metric instead. num_classes floating point values per feature y_pred single floating point value per feature y_true.","code":""},{"path":"https://keras.posit.co/reference/metric_sparse_categorical_crossentropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the crossentropy metric between the labels and predictions. — metric_sparse_categorical_crossentropy","text":"","code":"metric_sparse_categorical_crossentropy(   y_true,   y_pred,   from_logits = FALSE,   ignore_class = NULL,   axis = -1L,   ...,   name = \"sparse_categorical_crossentropy\",   dtype = NULL )"},{"path":"https://keras.posit.co/reference/metric_sparse_categorical_crossentropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the crossentropy metric between the labels and predictions. — metric_sparse_categorical_crossentropy","text":"y_true Ground truth values. y_pred predicted values. from_logits (Optional) Whether output expected logits tensor. default, consider output encodes probability distribution. ignore_class Optional integer. ID class ignored loss computation. useful, example, segmentation problems featuring \"void\" class (commonly -1 255) segmentation maps. default (ignore_class=NULL), classes considered. axis (Optional) Defaults -1. dimension along entropy computed. ... forward/backward compatability. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_sparse_categorical_crossentropy.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the crossentropy metric between the labels and predictions. — metric_sparse_categorical_crossentropy","text":"Standalone usage:           Usage compile() API:","code":"m <- metric_sparse_categorical_crossentropy() m$update_state(c(1, 2),                rbind(c(0.05, 0.95, 0), c(0.1, 0.8, 0.1))) m$result() ## tf.Tensor(1.1769392, shape=(), dtype=float32) m$reset_state() m$update_state(c(1, 2),                rbind(c(0.05, 0.95, 0), c(0.1, 0.8, 0.1)),                sample_weight = c(0.3, 0.7)) m$result() ## tf.Tensor(1.6271976, shape=(), dtype=float32) # 1.6271976 model %>% compile(     optimizer = 'sgd',     loss = 'mse',     metrics = list(metric_sparse_categorical_crossentropy()))"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_sparse_top_k_categorical_accuracy.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes how often integer targets are in the top K predictions. — metric_sparse_top_k_categorical_accuracy","title":"Computes how often integer targets are in the top K predictions. — metric_sparse_top_k_categorical_accuracy","text":"Computes often integer targets top K predictions.","code":""},{"path":"https://keras.posit.co/reference/metric_sparse_top_k_categorical_accuracy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes how often integer targets are in the top K predictions. — metric_sparse_top_k_categorical_accuracy","text":"","code":"metric_sparse_top_k_categorical_accuracy(   y_true,   y_pred,   k = 5L,   ...,   name = \"sparse_top_k_categorical_accuracy\",   dtype = NULL )"},{"path":"https://keras.posit.co/reference/metric_sparse_top_k_categorical_accuracy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes how often integer targets are in the top K predictions. — metric_sparse_top_k_categorical_accuracy","text":"y_true Tensor true targets. y_pred Tensor predicted targets. k (Optional) Number top elements look computing accuracy. Defaults 5. ... forward/backward compatability. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_sparse_top_k_categorical_accuracy.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes how often integer targets are in the top K predictions. — metric_sparse_top_k_categorical_accuracy","text":"Standalone usage:         Usage compile() API:","code":"m <- metric_sparse_top_k_categorical_accuracy(k = 1L) m$update_state(   rbind(2, 1),   op_array(rbind(c(0.1, 0.9, 0.8), c(0.05, 0.95, 0)), dtype = \"float32\") ) m$result() ## tf.Tensor(0.5, shape=(), dtype=float32) m$reset_state() m$update_state(   rbind(2, 1),   op_array(rbind(c(0.1, 0.9, 0.8), c(0.05, 0.95, 0)), dtype = \"float32\"),   sample_weight = c(0.7, 0.3) ) m$result() ## tf.Tensor(0.3, shape=(), dtype=float32) model %>% compile(optimizer = 'sgd',                   loss = 'sparse_categorical_crossentropy',                   metrics = list(metric_sparse_top_k_categorical_accuracy()))"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_sparse_top_op_categorical_accuracy.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes how often integer targets are in the top K predictions. — metric_sparse_top_op_categorical_accuracy","title":"Computes how often integer targets are in the top K predictions. — metric_sparse_top_op_categorical_accuracy","text":"Computes often integer targets top K predictions.","code":""},{"path":"https://keras.posit.co/reference/metric_sparse_top_op_categorical_accuracy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes how often integer targets are in the top K predictions. — metric_sparse_top_op_categorical_accuracy","text":"","code":"metric_sparse_top_op_categorical_accuracy(   y_true,   y_pred,   k = 5L,   ...,   name = \"sparse_top_op_categorical_accuracy\",   dtype = NULL )"},{"path":"https://keras.posit.co/reference/metric_sparse_top_op_categorical_accuracy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes how often integer targets are in the top K predictions. — metric_sparse_top_op_categorical_accuracy","text":"y_true Tensor true targets. y_pred Tensor predicted targets. k (Optional) Number top elements look computing accuracy. Defaults 5. ... forward/backward compatability. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_sparse_top_op_categorical_accuracy.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes how often integer targets are in the top K predictions. — metric_sparse_top_op_categorical_accuracy","text":"Standalone usage:         Usage compile() API:","code":"m <- metric_sparse_top_op_categorical_accuracy(k = 1L) m$update_state(   rbind(2, 1),   op_array(rbind(c(0.1, 0.9, 0.8), c(0.05, 0.95, 0)), dtype = \"float32\") ) m$result() ## tf.Tensor(0.5, shape=(), dtype=float32) m$reset_state() m$update_state(   rbind(2, 1),   op_array(rbind(c(0.1, 0.9, 0.8), c(0.05, 0.95, 0)), dtype = \"float32\"),   sample_weight = c(0.7, 0.3) ) m$result() ## tf.Tensor(0.3, shape=(), dtype=float32) model %>% compile(optimizer = 'sgd',                   loss = 'sparse_categorical_crossentropy',                   metrics = list(metric_sparse_top_op_categorical_accuracy()))"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_specificity_at_sensitivity.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes best specificity where sensitivity is >= specified value. — metric_specificity_at_sensitivity","title":"Computes best specificity where sensitivity is >= specified value. — metric_specificity_at_sensitivity","text":"Sensitivity measures proportion actual positives correctly identified (tp / (tp + fn)). Specificity measures proportion actual negatives correctly identified (tn / (tn + fp)). metric creates four local variables, true_positives, true_negatives, false_positives false_negatives used compute specificity given sensitivity. threshold given sensitivity value computed used evaluate corresponding specificity. sample_weight NULL, weights default 1. Use sample_weight 0 mask values. class_id specified, calculate precision considering entries batch class_id threshold predictions, computing fraction class_id indeed correct label. additional information specificity sensitivity, see following.","code":""},{"path":"https://keras.posit.co/reference/metric_specificity_at_sensitivity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes best specificity where sensitivity is >= specified value. — metric_specificity_at_sensitivity","text":"","code":"metric_specificity_at_sensitivity(   ...,   sensitivity,   num_thresholds = 200L,   class_id = NULL,   name = NULL,   dtype = NULL )"},{"path":"https://keras.posit.co/reference/metric_specificity_at_sensitivity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes best specificity where sensitivity is >= specified value. — metric_specificity_at_sensitivity","text":"... forward/backward compatability. sensitivity scalar value range [0, 1]. num_thresholds (Optional) Defaults 200. number thresholds use matching given sensitivity. class_id (Optional) Integer class ID want binary metrics. must half-open interval [0, num_classes), num_classes last dimension predictions. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_specificity_at_sensitivity.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes best specificity where sensitivity is >= specified value. — metric_specificity_at_sensitivity","text":"Standalone usage:     Usage compile() API:","code":"m <- metric_specificity_at_sensitivity(sensitivity = 0.5) m$update_state(c(0, 0, 0, 1, 1), c(0, 0.3, 0.8, 0.3, 0.8)) # m$result() m$reset_state() m$update_state(c(0, 0, 0, 1, 1), c(0, 0.3, 0.8, 0.3, 0.8),                sample_weight = c(1, 1, 2, 2, 2)) # m$result() model %>% compile(   optimizer = 'sgd',   loss = 'mse',   metrics = list(metric_sensitivity_at_specificity()))"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_squared_hinge.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the hinge metric between y_true and y_pred. — metric_squared_hinge","title":"Computes the hinge metric between y_true and y_pred. — metric_squared_hinge","text":"Formula:   y_true values expected -1 1. binary (0 1) labels provided convert -1 1.","code":"loss <- mean(square(maximum(1 - y_true * y_pred, 0)))"},{"path":"https://keras.posit.co/reference/metric_squared_hinge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the hinge metric between y_true and y_pred. — metric_squared_hinge","text":"","code":"metric_squared_hinge(y_true, y_pred, ..., name = \"squared_hinge\", dtype = NULL)"},{"path":"https://keras.posit.co/reference/metric_squared_hinge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the hinge metric between y_true and y_pred. — metric_squared_hinge","text":"y_true ground truth values. y_true values expected -1 1. binary (0 1) labels provided convert -1 1 shape = [batch_size, d0, .. dN]. y_pred predicted values shape = [batch_size, d0, .. dN]. ... forward/backward compatability. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_squared_hinge.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the hinge metric between y_true and y_pred. — metric_squared_hinge","text":"Standalone usage:","code":"m <- metric_squared_hinge() m$update_state(rbind(c(0, 1), c(0, 0)), rbind(c(0.6, 0.4), c(0.4, 0.6))) m$result() ## tf.Tensor(1.86, shape=(), dtype=float32) m$reset_state() m$update_state(rbind(c(0, 1), c(0, 0)), rbind(c(0.6, 0.4), c(0.4, 0.6)),                sample_weight = c(1, 0)) m$result() ## tf.Tensor(1.46, shape=(), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_sum.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the (weighted) sum of the given values. — metric_sum","title":"Compute the (weighted) sum of the given values. — metric_sum","text":"example, values [1, 3, 5, 7] sum 16. sample_weight specified [1, 1, 0, 0] sum 4. metric creates one variable, total. ultimately returned sum value.","code":""},{"path":"https://keras.posit.co/reference/metric_sum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the (weighted) sum of the given values. — metric_sum","text":"","code":"metric_sum(..., name = \"sum\", dtype = NULL)"},{"path":"https://keras.posit.co/reference/metric_sum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the (weighted) sum of the given values. — metric_sum","text":"... forward/backward compatability. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_sum.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute the (weighted) sum of the given values. — metric_sum","text":"","code":"m <- metric_sum() m$update_state(c(1, 3, 5, 7)) m$result() ## tf.Tensor(16.0, shape=(), dtype=float32) m <- metric_sum() m$update_state(c(1, 3, 5, 7), sample_weight = c(1, 1, 0, 0)) m$result() ## tf.Tensor(4.0, shape=(), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_top_k_categorical_accuracy.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes how often targets are in the top K predictions. — metric_top_k_categorical_accuracy","title":"Computes how often targets are in the top K predictions. — metric_top_k_categorical_accuracy","text":"Computes often targets top K predictions.","code":""},{"path":"https://keras.posit.co/reference/metric_top_k_categorical_accuracy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes how often targets are in the top K predictions. — metric_top_k_categorical_accuracy","text":"","code":"metric_top_k_categorical_accuracy(   y_true,   y_pred,   k = 5L,   ...,   name = \"top_k_categorical_accuracy\",   dtype = NULL )"},{"path":"https://keras.posit.co/reference/metric_top_k_categorical_accuracy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes how often targets are in the top K predictions. — metric_top_k_categorical_accuracy","text":"y_true Tensor true targets. y_pred Tensor predicted targets. k (Optional) Number top elements look computing accuracy. Defaults 5. ... forward/backward compatability. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_top_k_categorical_accuracy.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes how often targets are in the top K predictions. — metric_top_k_categorical_accuracy","text":"Standalone usage:         Usage compile() API:","code":"m <- metric_top_k_categorical_accuracy(k = 1) m$update_state(   rbind(c(0, 0, 1), c(0, 1, 0)),   op_array(rbind(c(0.1, 0.9, 0.8), c(0.05, 0.95, 0)), dtype = \"float32\") ) m$result() ## tf.Tensor(0.5, shape=(), dtype=float32) m$reset_state() m$update_state(   rbind(c(0, 0, 1), c(0, 1, 0)),   op_array(rbind(c(0.1, 0.9, 0.8), c(0.05, 0.95, 0)), dtype = \"float32\"),   sample_weight = c(0.7, 0.3)) m$result() ## tf.Tensor(0.3, shape=(), dtype=float32) model.compile(optimizer = 'sgd',               loss = 'categorical_crossentropy',               metrics = list(metric_top_k_categorical_accuracy()))"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_top_op_categorical_accuracy.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes how often targets are in the top K predictions. — metric_top_op_categorical_accuracy","title":"Computes how often targets are in the top K predictions. — metric_top_op_categorical_accuracy","text":"Computes often targets top K predictions.","code":""},{"path":"https://keras.posit.co/reference/metric_top_op_categorical_accuracy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes how often targets are in the top K predictions. — metric_top_op_categorical_accuracy","text":"","code":"metric_top_op_categorical_accuracy(   y_true,   y_pred,   k = 5L,   ...,   name = \"top_op_categorical_accuracy\",   dtype = NULL )"},{"path":"https://keras.posit.co/reference/metric_top_op_categorical_accuracy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes how often targets are in the top K predictions. — metric_top_op_categorical_accuracy","text":"y_true Tensor true targets. y_pred Tensor predicted targets. k (Optional) Number top elements look computing accuracy. Defaults 5. ... forward/backward compatability. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_top_op_categorical_accuracy.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes how often targets are in the top K predictions. — metric_top_op_categorical_accuracy","text":"Standalone usage:         Usage compile() API:","code":"m <- metric_top_op_categorical_accuracy(k = 1) m$update_state(   rbind(c(0, 0, 1), c(0, 1, 0)),   op_array(rbind(c(0.1, 0.9, 0.8), c(0.05, 0.95, 0)), dtype = \"float32\") ) m$result() ## tf.Tensor(0.5, shape=(), dtype=float32) m$reset_state() m$update_state(   rbind(c(0, 0, 1), c(0, 1, 0)),   op_array(rbind(c(0.1, 0.9, 0.8), c(0.05, 0.95, 0)), dtype = \"float32\"),   sample_weight = c(0.7, 0.3)) m$result() ## tf.Tensor(0.3, shape=(), dtype=float32) model.compile(optimizer = 'sgd',               loss = 'categorical_crossentropy',               metrics = list(metric_top_op_categorical_accuracy()))"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_true_negatives.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculates the number of true negatives. — metric_true_negatives","title":"Calculates the number of true negatives. — metric_true_negatives","text":"sample_weight given, calculates sum weights true negatives. metric creates one local variable, accumulator used keep track number true negatives. sample_weight NULL, weights default 1. Use sample_weight 0 mask values.","code":""},{"path":"https://keras.posit.co/reference/metric_true_negatives.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates the number of true negatives. — metric_true_negatives","text":"","code":"metric_true_negatives(..., thresholds = NULL, name = NULL, dtype = NULL)"},{"path":"https://keras.posit.co/reference/metric_true_negatives.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculates the number of true negatives. — metric_true_negatives","text":"... forward/backward compatability. thresholds (Optional) Defaults 0.5. float value, Python list float threshold values [0, 1]. threshold compared prediction values determine truth value predictions (.e., threshold TRUE, FALSE). used loss function sets from_logits=TRUE (.e. sigmoid applied predictions), thresholds set 0. One metric value generated threshold value. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_true_negatives.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates the number of true negatives. — metric_true_negatives","text":"Standalone usage:","code":"m <- metric_true_negatives() m$update_state(c(0, 1, 0, 0), c(1, 1, 0, 0)) m$result() ## tf.Tensor(2.0, shape=(), dtype=float32) m$reset_state() m$update_state(c(0, 1, 0, 0), c(1, 1, 0, 0), sample_weight = c(0, 0, 1, 0)) m$result() ## tf.Tensor(1.0, shape=(), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/metric_true_positives.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculates the number of true positives. — metric_true_positives","title":"Calculates the number of true positives. — metric_true_positives","text":"sample_weight given, calculates sum weights true positives. metric creates one local variable, true_positives used keep track number true positives. sample_weight NULL, weights default 1. Use sample_weight 0 mask values.","code":""},{"path":"https://keras.posit.co/reference/metric_true_positives.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates the number of true positives. — metric_true_positives","text":"","code":"metric_true_positives(..., thresholds = NULL, name = NULL, dtype = NULL)"},{"path":"https://keras.posit.co/reference/metric_true_positives.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculates the number of true positives. — metric_true_positives","text":"... forward/backward compatability. thresholds (Optional) Defaults 0.5. float value, Python list float threshold values [0, 1]. threshold compared prediction values determine truth value predictions (.e., threshold TRUE, FALSE). used loss function sets from_logits=TRUE (.e. sigmoid applied predictions), thresholds set 0. One metric value generated threshold value. name (Optional) string name metric instance. dtype (Optional) data type metric result.","code":""},{"path":"https://keras.posit.co/reference/metric_true_positives.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates the number of true positives. — metric_true_positives","text":"Standalone usage:","code":"m <- metric_true_positives() m$update_state(c(0, 1, 1, 1), c(1, 0, 1, 1)) m$result() ## tf.Tensor(2.0, shape=(), dtype=float32) m$reset_state() m$update_state(c(0, 1, 1, 1), c(1, 0, 1, 1), sample_weight = c(0, 0, 1, 0)) m$result() ## tf.Tensor(1.0, shape=(), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/model_from_saved_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Load a Keras model from the Saved Model format — model_from_saved_model","title":"Load a Keras model from the Saved Model format — model_from_saved_model","text":"Load Keras model Saved Model format","code":""},{"path":"https://keras.posit.co/reference/model_from_saved_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load a Keras model from the Saved Model format — model_from_saved_model","text":"","code":"model_from_saved_model(saved_model_path, custom_objects = NULL)"},{"path":"https://keras.posit.co/reference/model_from_saved_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load a Keras model from the Saved Model format — model_from_saved_model","text":"saved_model_path string specifying path SavedModel directory. custom_objects Optional dictionary mapping string names custom classes functions (e.g. custom loss functions).","code":""},{"path":"https://keras.posit.co/reference/model_from_saved_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load a Keras model from the Saved Model format — model_from_saved_model","text":"Keras model.","code":""},{"path":"https://keras.posit.co/reference/model_from_saved_model.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Load a Keras model from the Saved Model format — model_from_saved_model","text":"functionality experimental works TensorFlow version >= \"2.0\".","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/model_to_dot.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a Keras model to dot format. — model_to_dot","title":"Convert a Keras model to dot format. — model_to_dot","text":"Convert Keras model dot format.","code":""},{"path":"https://keras.posit.co/reference/model_to_dot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a Keras model to dot format. — model_to_dot","text":"","code":"model_to_dot(   model,   show_shapes = FALSE,   show_dtype = FALSE,   show_layer_names = TRUE,   rankdir = \"TB\",   expand_nested = FALSE,   dpi = 200L,   subgraph = FALSE,   show_layer_activations = FALSE,   show_trainable = FALSE,   ... )"},{"path":"https://keras.posit.co/reference/model_to_dot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a Keras model to dot format. — model_to_dot","text":"model Keras model instance. show_shapes whether display shape information. show_dtype whether display layer dtypes. show_layer_names whether display layer names. rankdir rankdir argument passed PyDot, string specifying format plot: \"TB\" creates vertical plot; \"LR\" creates horizontal plot. expand_nested whether expand nested Functional models clusters. dpi Image resolution dots per inch. subgraph whether return pydot.Cluster instance. show_layer_activations Display layer activations (layers activation property). show_trainable whether display layer trainable. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/model_to_dot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a Keras model to dot format. — model_to_dot","text":"pydot.Dot instance representing Keras model pydot.Cluster instance representing nested model subgraph=TRUE.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/model_to_json.html","id":null,"dir":"Reference","previous_headings":"","what":"Model configuration as JSON — model_to_json","title":"Model configuration as JSON — model_to_json","text":"Save re-load models configurations JSON. Note representation include weights, architecture.","code":""},{"path":"https://keras.posit.co/reference/model_to_json.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model configuration as JSON — model_to_json","text":"","code":"model_to_json(object)  model_from_json(json, custom_objects = NULL)"},{"path":"https://keras.posit.co/reference/model_to_json.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model configuration as JSON — model_to_json","text":"object Model object save json JSON model configuration custom_objects Optional named list mapping names custom classes functions considered deserialization.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/model_to_saved_model.html","id":null,"dir":"Reference","previous_headings":"","what":"(Deprecated) Export to Saved Model format — model_to_saved_model","title":"(Deprecated) Export to Saved Model format — model_to_saved_model","text":"(Deprecated) Export Saved Model format","code":""},{"path":"https://keras.posit.co/reference/model_to_saved_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"(Deprecated) Export to Saved Model format — model_to_saved_model","text":"","code":"model_to_saved_model(   model,   saved_model_path,   custom_objects = NULL,   as_text = FALSE,   input_signature = NULL,   serving_only = FALSE )"},{"path":"https://keras.posit.co/reference/model_to_saved_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"(Deprecated) Export to Saved Model format — model_to_saved_model","text":"model Keras model saved. model subclassed, flag serving_only must set TRUE. saved_model_path string specifying path SavedModel directory. custom_objects Optional dictionary mapping string names custom classes functions (e.g. custom loss functions). as_text bool, FALSE default. Whether write SavedModel proto text format. Currently unavailable serving-mode. input_signature possibly nested sequence tf.TensorSpec objects, used specify expected model inputs. See tf.function details. serving_only bool, FALSE default. true, prediction graph saved.","code":""},{"path":"https://keras.posit.co/reference/model_to_saved_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"(Deprecated) Export to Saved Model format — model_to_saved_model","text":"Invisibly returns saved_model_path.","code":""},{"path":"https://keras.posit.co/reference/model_to_saved_model.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"(Deprecated) Export to Saved Model format — model_to_saved_model","text":"functionality experimental works TensorFlow version >= \"2.0\".","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/multi-assign.html","id":null,"dir":"Reference","previous_headings":"","what":"Assign values to names — %<-%","title":"Assign values to names — %<-%","text":"See %<-% details.","code":""},{"path":"https://keras.posit.co/reference/multi-assign.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assign values to names — %<-%","text":"","code":"x %<-% value"},{"path":"https://keras.posit.co/reference/new-classes.html","id":null,"dir":"Reference","previous_headings":"","what":"Define new keras types — new_metric_class","title":"Define new keras types — new_metric_class","text":"functions can used make custom objects fit family existing keras types. example, new_layer_class() return class constructor, object behaves like layer functions layer_dense(). new_callback_class() return object behaves similarly callback functions, like callback_reduce_lr_on_plateau(), . arguments default NULL value optional methods can provided.","code":""},{"path":"https://keras.posit.co/reference/new-classes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define new keras types — new_metric_class","text":"","code":"new_metric_class(classname, ..., initialize, update_state, result)  new_loss_class(   classname,   ...,   call = NULL,   inherit = keras3::keras$losses$Loss )  new_model_class(   classname,   ...,   initialize = NULL,   call = NULL,   train_step = NULL,   predict_step = NULL,   test_step = NULL,   compute_loss = NULL,   compute_metrics = NULL )"},{"path":"https://keras.posit.co/reference/new-classes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define new keras types — new_metric_class","text":"classname classname string. Convention classname CamelCase version constructor. ... Additional fields methods new type. initialize, update_state, result, call, train_step, predict_step, test_step, compute_loss, compute_metrics Optional methods can overridden. inherit class inherit .","code":""},{"path":"https://keras.posit.co/reference/new-classes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Define new keras types — new_metric_class","text":"new class generator object inherits appropriate Keras base class.","code":""},{"path":"https://keras.posit.co/reference/new-classes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Define new keras types — new_metric_class","text":"mark_active() decorator can used indicate functions become active properties class instances.","code":""},{"path":"https://keras.posit.co/reference/new_learning_rate_schedule_class.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a new learning rate schedule type — new_learning_rate_schedule_class","title":"Create a new learning rate schedule type — new_learning_rate_schedule_class","text":"Create new learning rate schedule type","code":""},{"path":"https://keras.posit.co/reference/new_learning_rate_schedule_class.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a new learning rate schedule type — new_learning_rate_schedule_class","text":"","code":"new_learning_rate_schedule_class(   classname,   ...,   initialize = NULL,   call,   get_config = NULL )"},{"path":"https://keras.posit.co/reference/new_learning_rate_schedule_class.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a new learning rate schedule type — new_learning_rate_schedule_class","text":"classname string ... methods properties schedule class initialize, get_config Additional recommended methods implement. https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/LearningRateSchedule call function takes step argument (scalar integer tensor, current training step count, returns new learning rate). tracking additional state, objects self private automatically injected scope function.","code":""},{"path":"https://keras.posit.co/reference/new_learning_rate_schedule_class.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a new learning rate schedule type — new_learning_rate_schedule_class","text":"LearningRateSchedule class generator.","code":""},{"path":"https://keras.posit.co/reference/normalize.html","id":null,"dir":"Reference","previous_headings":"","what":"Normalizes an array. — normalize","title":"Normalizes an array. — normalize","text":"input NumPy array, NumPy array returned. backend tensor, backend tensor returned.","code":""},{"path":"https://keras.posit.co/reference/normalize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Normalizes an array. — normalize","text":"","code":"normalize(x, axis = -1L, order = 2L)"},{"path":"https://keras.posit.co/reference/normalize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Normalizes an array. — normalize","text":"x Array normalize. axis axis along normalize. order Normalization order (e.g. order=2 L2 norm).","code":""},{"path":"https://keras.posit.co/reference/normalize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Normalizes an array. — normalize","text":"","code":"A normalized copy of the array."},{"path":[]},{"path":"https://keras.posit.co/reference/op_abs.html","id":null,"dir":"Reference","previous_headings":"","what":"Shorthand for keras.ops.absolute. — op_abs","title":"Shorthand for keras.ops.absolute. — op_abs","text":"Shorthand keras.ops.absolute.","code":""},{"path":"https://keras.posit.co/reference/op_abs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Shorthand for keras.ops.absolute. — op_abs","text":"","code":"op_abs(x)"},{"path":"https://keras.posit.co/reference/op_abs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Shorthand for keras.ops.absolute. — op_abs","text":"x see description","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/op_add.html","id":null,"dir":"Reference","previous_headings":"","what":"Add arguments element-wise. — op_add","title":"Add arguments element-wise. — op_add","text":"Add arguments element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_add.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add arguments element-wise. — op_add","text":"","code":"op_add(x1, x2)"},{"path":"https://keras.posit.co/reference/op_add.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add arguments element-wise. — op_add","text":"x1 First input tensor. x2 Second input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_add.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add arguments element-wise. — op_add","text":"tensor containing element-wise sum x1 x2.","code":""},{"path":"https://keras.posit.co/reference/op_add.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add arguments element-wise. — op_add","text":"op_add also broadcasts shapes:","code":"x1 <- op_convert_to_tensor(c(1, 4)) x2 <- op_convert_to_tensor(c(5, 6)) op_add(x1, x2) ## tf.Tensor([ 6. 10.], shape=(2), dtype=float32) # alias for x1 + x2 x1 + x2 ## tf.Tensor([ 6. 10.], shape=(2), dtype=float32) x1 <- op_convert_to_tensor(array(c(5, 5, 4, 6), dim =c(2, 2))) x2 <- op_convert_to_tensor(c(5, 6)) op_add(x1, x2) ## tf.Tensor( ## [[10. 10.] ##  [10. 12.]], shape=(2, 2), dtype=float64)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_all.html","id":null,"dir":"Reference","previous_headings":"","what":"Test whether all array elements along a given axis evaluate to TRUE. — op_all","title":"Test whether all array elements along a given axis evaluate to TRUE. — op_all","text":"Test whether array elements along given axis evaluate TRUE.","code":""},{"path":"https://keras.posit.co/reference/op_all.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test whether all array elements along a given axis evaluate to TRUE. — op_all","text":"","code":"op_all(x, axis = NULL, keepdims = FALSE)"},{"path":"https://keras.posit.co/reference/op_all.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test whether all array elements along a given axis evaluate to TRUE. — op_all","text":"x Input tensor. axis integer tuple integers represent axis along logical reduction performed. default (axis = NULL) perform logical dimensions input array. axis may negative, case counts last first axis. keepdims TRUE, axes reduced left result dimensions size one. option, result broadcast correctly input array. Defaults toFALSE.","code":""},{"path":"https://keras.posit.co/reference/op_all.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test whether all array elements along a given axis evaluate to TRUE. — op_all","text":"tensor containing logical reduction axis.","code":""},{"path":"https://keras.posit.co/reference/op_all.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Test whether all array elements along a given axis evaluate to TRUE. — op_all","text":"keepdims = TRUE outputs tensor dimensions reduced one.","code":"x <- op_convert_to_tensor(c(TRUE, FALSE)) op_all(x) ## tf.Tensor(False, shape=(), dtype=bool) (x <- op_convert_to_tensor(array(c(TRUE, FALSE, TRUE, TRUE, TRUE, TRUE), dim = c(3, 2)))) ## tf.Tensor( ## [[ True  True] ##  [False  True] ##  [ True  True]], shape=(3, 2), dtype=bool) op_all(x, axis = 1) ## tf.Tensor([False  True], shape=(2), dtype=bool) op_all(x, keepdims = TRUE) ## tf.Tensor([[False]], shape=(1, 1), dtype=bool)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_amax.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the maximum of a vector or maximum value along an axis. — op_amax","title":"Returns the maximum of a vector or maximum value along an axis. — op_amax","text":"Returns maximum vector maximum value along axis.","code":""},{"path":"https://keras.posit.co/reference/op_amax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the maximum of a vector or maximum value along an axis. — op_amax","text":"","code":"op_amax(x, axis = NULL, keepdims = FALSE)"},{"path":"https://keras.posit.co/reference/op_amax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the maximum of a vector or maximum value along an axis. — op_amax","text":"x Input tensor. axis Axis along compute maximum. default (axis = NULL), find maximum value dimensions input tensor. keepdims TRUE, axes reduced left result dimensions broadcast size original input tensor. Defaults FALSE.","code":""},{"path":"https://keras.posit.co/reference/op_amax.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the maximum of a vector or maximum value along an axis. — op_amax","text":"tensor maximum value. axis = NULL, result scalar value representing maximum element entire tensor. axis given, result tensor maximum values along specified axis.","code":""},{"path":"https://keras.posit.co/reference/op_amax.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Returns the maximum of a vector or maximum value along an axis. — op_amax","text":"","code":"(x <- op_convert_to_tensor(rbind(c(1, 3, 5), c(1, 5, 2)))) ## tf.Tensor( ## [[1. 3. 5.] ##  [1. 5. 2.]], shape=(2, 3), dtype=float64) op_amax(x) ## tf.Tensor(5.0, shape=(), dtype=float64) op_amax(x, axis = 1) ## tf.Tensor([1. 5. 5.], shape=(3), dtype=float64) op_amax(x, axis = 1, keepdims = TRUE) ## tf.Tensor([[1. 5. 5.]], shape=(1, 3), dtype=float64)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_amin.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the minimum of a vector or minimum value along an axis. — op_amin","title":"Returns the minimum of a vector or minimum value along an axis. — op_amin","text":"Returns minimum vector minimum value along axis.","code":""},{"path":"https://keras.posit.co/reference/op_amin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the minimum of a vector or minimum value along an axis. — op_amin","text":"","code":"op_amin(x, axis = NULL, keepdims = FALSE)"},{"path":"https://keras.posit.co/reference/op_amin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the minimum of a vector or minimum value along an axis. — op_amin","text":"x Input tensor. axis Axis along compute minimum. default (axis = NULL), find minimum value dimensions input tensor. keepdims TRUE, axes reduced left result dimensions broadcast size original input tensor. Defaults FALSE.","code":""},{"path":"https://keras.posit.co/reference/op_amin.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the minimum of a vector or minimum value along an axis. — op_amin","text":"tensor minimum value. axis = NULL, result scalar value representing minimum element entire tensor. axis given, result tensor minimum values along specified axis.","code":""},{"path":"https://keras.posit.co/reference/op_amin.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Returns the minimum of a vector or minimum value along an axis. — op_amin","text":"","code":"(x <- op_convert_to_tensor(rbind(c(1, 3, 5), c(1, 5, 2)))) ## tf.Tensor( ## [[1. 3. 5.] ##  [1. 5. 2.]], shape=(2, 3), dtype=float64) op_amin(x) ## tf.Tensor(1.0, shape=(), dtype=float64) op_amin(x, axis = 1) ## tf.Tensor([1. 3. 2.], shape=(3), dtype=float64) op_amin(x, axis = 1, keepdims = TRUE) ## tf.Tensor([[1. 3. 2.]], shape=(1, 3), dtype=float64)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_any.html","id":null,"dir":"Reference","previous_headings":"","what":"Test whether any array element along a given axis evaluates to TRUE. — op_any","title":"Test whether any array element along a given axis evaluates to TRUE. — op_any","text":"Test whether array element along given axis evaluates TRUE.","code":""},{"path":"https://keras.posit.co/reference/op_any.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test whether any array element along a given axis evaluates to TRUE. — op_any","text":"","code":"op_any(x, axis = NULL, keepdims = FALSE)"},{"path":"https://keras.posit.co/reference/op_any.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test whether any array element along a given axis evaluates to TRUE. — op_any","text":"x Input tensor. axis integer tuple integers represent axis along logical reduction performed. default (axis = NULL) perform logical dimensions input array. axis may negative, case counts last first axis. keepdims TRUE, axes reduced left result dimensions size one. option, result broadcast correctly input array. Defaults FALSE.","code":""},{"path":"https://keras.posit.co/reference/op_any.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test whether any array element along a given axis evaluates to TRUE. — op_any","text":"tensor containing logical reduction axis.","code":""},{"path":"https://keras.posit.co/reference/op_any.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Test whether any array element along a given axis evaluates to TRUE. — op_any","text":"keepdims = TRUE outputs tensor dimensions reduced one.","code":"x <- op_array(c(TRUE, FALSE)) op_any(x) ## tf.Tensor(True, shape=(), dtype=bool) (x <- op_reshape(c(FALSE, FALSE, FALSE,                   TRUE, FALSE, FALSE), c(2, 3))) ## tf.Tensor( ## [[False False False] ##  [ True False False]], shape=(2, 3), dtype=bool) op_any(x, axis = 1) ## tf.Tensor([ True False False], shape=(3), dtype=bool) op_any(x, axis = 2) ## tf.Tensor([False  True], shape=(2), dtype=bool) op_any(x, axis = -1) ## tf.Tensor([False  True], shape=(2), dtype=bool) op_any(x, keepdims = TRUE) ## tf.Tensor([[ True]], shape=(1, 1), dtype=bool) op_any(x, 1, keepdims = TRUE) ## tf.Tensor([[ True False False]], shape=(1, 3), dtype=bool)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_append.html","id":null,"dir":"Reference","previous_headings":"","what":"Append tensor x2 to the end of tensor x1. — op_append","title":"Append tensor x2 to the end of tensor x1. — op_append","text":"Append tensor x2 end tensor x1.","code":""},{"path":"https://keras.posit.co/reference/op_append.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Append tensor x2 to the end of tensor x1. — op_append","text":"","code":"op_append(x1, x2, axis = NULL)"},{"path":"https://keras.posit.co/reference/op_append.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Append tensor x2 to the end of tensor x1. — op_append","text":"x1 First input tensor. x2 Second input tensor. axis Axis along tensor x2 appended tensor x1. NULL, tensors flattened use.","code":""},{"path":"https://keras.posit.co/reference/op_append.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Append tensor x2 to the end of tensor x1. — op_append","text":"tensor values x2 appended x1.","code":""},{"path":"https://keras.posit.co/reference/op_append.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Append tensor x2 to the end of tensor x1. — op_append","text":"axis specified, x1 x2 must compatible shapes.","code":"x1 <- op_convert_to_tensor(c(1, 2, 3)) x2 <- op_convert_to_tensor(rbind(c(4, 5, 6), c(7, 8, 9))) op_append(x1, x2) ## tf.Tensor([1. 2. 3. 4. 5. 6. 7. 8. 9.], shape=(9), dtype=float64) x1 <- op_convert_to_tensor(rbind(c(1, 2, 3), c(4, 5, 6))) x2 <- op_convert_to_tensor(rbind(c(7, 8, 9))) op_append(x1, x2, axis = 1) ## tf.Tensor( ## [[1. 2. 3.] ##  [4. 5. 6.] ##  [7. 8. 9.]], shape=(3, 3), dtype=float64) x3 <- op_convert_to_tensor(c(7, 8, 9)) try(op_append(x1, x3, axis = 1)) ## Error in py_call_impl(callable, call_args$unnamed, call_args$named) : ##   tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Ranks of all input tensors should match: shape[0] = [2,3] vs. shape[1] = [3] [Op:ConcatV2] name: concat"},{"path":[]},{"path":"https://keras.posit.co/reference/op_arange.html","id":null,"dir":"Reference","previous_headings":"","what":"Return evenly spaced values within a given interval. — op_arange","title":"Return evenly spaced values within a given interval. — op_arange","text":"arange can called varying number positional arguments: arange(stop): Values generated within half-open interval [0, stop) (words, interval including start excluding stop). arange(start, stop): Values generated within half-open interval [start, stop). arange(start, stop, step): Values generated within half-open interval [start, stop), spacing values given step.","code":""},{"path":"https://keras.posit.co/reference/op_arange.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return evenly spaced values within a given interval. — op_arange","text":"","code":"op_arange(start, stop = NULL, step = 1L, dtype = NULL)"},{"path":"https://keras.posit.co/reference/op_arange.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return evenly spaced values within a given interval. — op_arange","text":"start Integer real, representing start interval. interval includes value. stop Integer real, representing end interval. interval include value, except cases step integer floating point round-affects length . Defaults NULL. step Integer real, represent spacing values. output , distance two adjacent values, [+1] - []. default step size 1. step specified position argument, start must also given. dtype type output array. dtype given, infer data type input arguments.","code":""},{"path":"https://keras.posit.co/reference/op_arange.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return evenly spaced values within a given interval. — op_arange","text":"Tensor evenly spaced values. floating point arguments, length result ceiling((stop - start)/step). floating point overflow, rule may result last element greater stop.","code":""},{"path":"https://keras.posit.co/reference/op_arange.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Return evenly spaced values within a given interval. — op_arange","text":"","code":"op_arange(3L) ## tf.Tensor([0 1 2], shape=(3), dtype=int32) op_arange(3) # float ## tf.Tensor([0. 1. 2.], shape=(3), dtype=float64) op_arange(3, dtype = 'int32') #int ## tf.Tensor([0 1 2], shape=(3), dtype=int32) op_arange(3L, 7L) ## tf.Tensor([3 4 5 6], shape=(4), dtype=int32) op_arange(3L, 7L, 2L) ## tf.Tensor([3 5], shape=(2), dtype=int32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_arccos.html","id":null,"dir":"Reference","previous_headings":"","what":"Trigonometric inverse cosine, element-wise. — op_arccos","title":"Trigonometric inverse cosine, element-wise. — op_arccos","text":"inverse cos , y = cos(x), x = arccos(y).","code":""},{"path":"https://keras.posit.co/reference/op_arccos.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Trigonometric inverse cosine, element-wise. — op_arccos","text":"","code":"op_arccos(x)"},{"path":"https://keras.posit.co/reference/op_arccos.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Trigonometric inverse cosine, element-wise. — op_arccos","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_arccos.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Trigonometric inverse cosine, element-wise. — op_arccos","text":"Tensor angle ray intersecting unit circle given x-coordinate radians [0, pi].","code":""},{"path":"https://keras.posit.co/reference/op_arccos.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Trigonometric inverse cosine, element-wise. — op_arccos","text":"","code":"x <- op_convert_to_tensor(c(1, -1)) op_arccos(x) ## tf.Tensor([0.        3.1415925], shape=(2), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_arccosh.html","id":null,"dir":"Reference","previous_headings":"","what":"Inverse hyperbolic cosine, element-wise. — op_arccosh","title":"Inverse hyperbolic cosine, element-wise. — op_arccosh","text":"Inverse hyperbolic cosine, element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_arccosh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Inverse hyperbolic cosine, element-wise. — op_arccosh","text":"","code":"op_arccosh(x)"},{"path":"https://keras.posit.co/reference/op_arccosh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Inverse hyperbolic cosine, element-wise. — op_arccosh","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_arccosh.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Inverse hyperbolic cosine, element-wise. — op_arccosh","text":"Output tensor shape x.","code":""},{"path":"https://keras.posit.co/reference/op_arccosh.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Inverse hyperbolic cosine, element-wise. — op_arccosh","text":"","code":"x <- op_convert_to_tensor(c(10, 100)) op_arccosh(x) ## tf.Tensor([2.993223 5.298292], shape=(2), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_arcsin.html","id":null,"dir":"Reference","previous_headings":"","what":"Inverse sine, element-wise. — op_arcsin","title":"Inverse sine, element-wise. — op_arcsin","text":"Inverse sine, element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_arcsin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Inverse sine, element-wise. — op_arcsin","text":"","code":"op_arcsin(x)"},{"path":"https://keras.posit.co/reference/op_arcsin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Inverse sine, element-wise. — op_arcsin","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_arcsin.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Inverse sine, element-wise. — op_arcsin","text":"Tensor inverse sine element x, radians closed interval [-pi/2, pi/2].","code":""},{"path":"https://keras.posit.co/reference/op_arcsin.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Inverse sine, element-wise. — op_arcsin","text":"","code":"x <- op_convert_to_tensor(c(1, -1, 0)) op_arcsin(x) ## tf.Tensor([ 1.5707963 -1.5707963  0.       ], shape=(3), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_arcsinh.html","id":null,"dir":"Reference","previous_headings":"","what":"Inverse hyperbolic sine, element-wise. — op_arcsinh","title":"Inverse hyperbolic sine, element-wise. — op_arcsinh","text":"Inverse hyperbolic sine, element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_arcsinh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Inverse hyperbolic sine, element-wise. — op_arcsinh","text":"","code":"op_arcsinh(x)"},{"path":"https://keras.posit.co/reference/op_arcsinh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Inverse hyperbolic sine, element-wise. — op_arcsinh","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_arcsinh.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Inverse hyperbolic sine, element-wise. — op_arcsinh","text":"Output tensor shape x.","code":""},{"path":"https://keras.posit.co/reference/op_arcsinh.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Inverse hyperbolic sine, element-wise. — op_arcsinh","text":"","code":"x <- op_convert_to_tensor(c(1, -1, 0)) op_arcsinh(x) ## tf.Tensor([ 0.8813736 -0.8813736  0.       ], shape=(3), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_arctan.html","id":null,"dir":"Reference","previous_headings":"","what":"Trigonometric inverse tangent, element-wise. — op_arctan","title":"Trigonometric inverse tangent, element-wise. — op_arctan","text":"Trigonometric inverse tangent, element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_arctan.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Trigonometric inverse tangent, element-wise. — op_arctan","text":"","code":"op_arctan(x)"},{"path":"https://keras.posit.co/reference/op_arctan.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Trigonometric inverse tangent, element-wise. — op_arctan","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_arctan.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Trigonometric inverse tangent, element-wise. — op_arctan","text":"Tensor inverse tangent element x, interval [-pi/2, pi/2].","code":""},{"path":"https://keras.posit.co/reference/op_arctan.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Trigonometric inverse tangent, element-wise. — op_arctan","text":"","code":"x <- op_convert_to_tensor(c(0, 1)) op_arctan(x) ## tf.Tensor([0.        0.7853982], shape=(2), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_arctan2.html","id":null,"dir":"Reference","previous_headings":"","what":"Element-wise arc tangent of x1/x2 choosing the quadrant correctly. — op_arctan2","title":"Element-wise arc tangent of x1/x2 choosing the quadrant correctly. — op_arctan2","text":"quadrant (.e., branch) chosen arctan2(x1, x2) signed angle radians ray ending origin passing point (1, 0), ray ending origin passing point (x2, x1). (Note role reversal: \"y-coordinate\" first function parameter, \"x-coordinate\" second.) IEEE convention, function defined x2 = +/-0 either x1 x2 = +/-inf.","code":""},{"path":"https://keras.posit.co/reference/op_arctan2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Element-wise arc tangent of x1/x2 choosing the quadrant correctly. — op_arctan2","text":"","code":"op_arctan2(x1, x2)"},{"path":"https://keras.posit.co/reference/op_arctan2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Element-wise arc tangent of x1/x2 choosing the quadrant correctly. — op_arctan2","text":"x1 First input tensor. x2 Second input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_arctan2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Element-wise arc tangent of x1/x2 choosing the quadrant correctly. — op_arctan2","text":"Tensor angles radians, range [-pi, pi].","code":""},{"path":"https://keras.posit.co/reference/op_arctan2.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Element-wise arc tangent of x1/x2 choosing the quadrant correctly. — op_arctan2","text":"Consider four points different quadrants:     Note order parameters. arctan2 defined also x2 = 0 several points, obtaining values range [-pi, pi]:","code":"x <- op_array(c(-1, 1, 1, -1)) y <- op_array(c(-1, -1, 1, 1)) op_arctan2(y, x) * 180 / pi ## tf.Tensor([-135.        -44.999996   44.999996  135.      ], shape=(4), dtype=float32) op_arctan2(     op_array(c(1, -1)),     op_array(c(0, 0)) ) ## tf.Tensor([ 1.5707964 -1.5707964], shape=(2), dtype=float32) op_arctan2(     op_array(c(0, 0, Inf)),     op_array(c(+0, -0, Inf)) ) ## tf.Tensor([0.        3.1415925 0.7853982], shape=(3), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_arctanh.html","id":null,"dir":"Reference","previous_headings":"","what":"Inverse hyperbolic tangent, element-wise. — op_arctanh","title":"Inverse hyperbolic tangent, element-wise. — op_arctanh","text":"Inverse hyperbolic tangent, element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_arctanh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Inverse hyperbolic tangent, element-wise. — op_arctanh","text":"","code":"op_arctanh(x)"},{"path":"https://keras.posit.co/reference/op_arctanh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Inverse hyperbolic tangent, element-wise. — op_arctanh","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_arctanh.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Inverse hyperbolic tangent, element-wise. — op_arctanh","text":"","code":"Output tensor of same shape as `x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_argmax.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the indices of the maximum values along an axis. — op_argmax","title":"Returns the indices of the maximum values along an axis. — op_argmax","text":"Returns indices maximum values along axis.","code":""},{"path":"https://keras.posit.co/reference/op_argmax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the indices of the maximum values along an axis. — op_argmax","text":"","code":"op_argmax(x, axis = NULL)"},{"path":"https://keras.posit.co/reference/op_argmax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the indices of the maximum values along an axis. — op_argmax","text":"x Input tensor. axis default, index flattened tensor, otherwise along specified axis.","code":""},{"path":"https://keras.posit.co/reference/op_argmax.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the indices of the maximum values along an axis. — op_argmax","text":"Tensor indices. shape x, dimension along axis removed.","code":""},{"path":"https://keras.posit.co/reference/op_argmax.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Returns the indices of the maximum values along an axis. — op_argmax","text":"","code":"x <- op_arange(6L) |> op_reshape(c(2, 3)) |> op_add(10) x ## tf.Tensor( ## [[10. 11. 12.] ##  [13. 14. 15.]], shape=(2, 3), dtype=float32) op_argmax(x) ## tf.Tensor(5, shape=(), dtype=int32) op_argmax(x, axis = 1) ## tf.Tensor([1 1 1], shape=(3), dtype=int32) op_argmax(x, axis = 2) ## tf.Tensor([2 2], shape=(2), dtype=int32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_argmin.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the indices of the minimum values along an axis. — op_argmin","title":"Returns the indices of the minimum values along an axis. — op_argmin","text":"Returns indices minimum values along axis.","code":""},{"path":"https://keras.posit.co/reference/op_argmin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the indices of the minimum values along an axis. — op_argmin","text":"","code":"op_argmin(x, axis = NULL)"},{"path":"https://keras.posit.co/reference/op_argmin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the indices of the minimum values along an axis. — op_argmin","text":"x Input tensor. axis default, index flattened tensor, otherwise along specified axis.","code":""},{"path":"https://keras.posit.co/reference/op_argmin.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the indices of the minimum values along an axis. — op_argmin","text":"Tensor indices. shape x, dimension along axis removed.","code":""},{"path":"https://keras.posit.co/reference/op_argmin.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Returns the indices of the minimum values along an axis. — op_argmin","text":"","code":"x <- op_arange(6L) |> op_reshape(c(2, 3)) |> op_add(10) x ## tf.Tensor( ## [[10. 11. 12.] ##  [13. 14. 15.]], shape=(2, 3), dtype=float32) op_argmin(x) ## tf.Tensor(0, shape=(), dtype=int32) op_argmin(x, axis = 1) ## tf.Tensor([0 0 0], shape=(3), dtype=int32) op_argmin(x, axis = 2) ## tf.Tensor([0 0], shape=(2), dtype=int32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_argsort.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the indices that would sort a tensor. — op_argsort","title":"Returns the indices that would sort a tensor. — op_argsort","text":"Returns indices sort tensor.","code":""},{"path":"https://keras.posit.co/reference/op_argsort.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the indices that would sort a tensor. — op_argsort","text":"","code":"op_argsort(x, axis = -1L)"},{"path":"https://keras.posit.co/reference/op_argsort.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the indices that would sort a tensor. — op_argsort","text":"x Input tensor. axis Axis along sort. Defaults -1 (last axis). NULL, flattened tensor used.","code":""},{"path":"https://keras.posit.co/reference/op_argsort.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the indices that would sort a tensor. — op_argsort","text":"Tensor indices sort x along specified axis.","code":""},{"path":"https://keras.posit.co/reference/op_argsort.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Returns the indices that would sort a tensor. — op_argsort","text":"One dimensional array:     Two-dimensional array:","code":"x <- op_array(c(3, 1, 2)) op_argsort(x) ## tf.Tensor([1 2 0], shape=(3), dtype=int32) x <- op_array(rbind(c(0, 3),                    c(3, 2),                    c(4, 5)), dtype = \"int32\") op_argsort(x, axis = 1) ## tf.Tensor( ## [[0 1] ##  [1 0] ##  [2 2]], shape=(3, 2), dtype=int32) op_argsort(x, axis = 2) ## tf.Tensor( ## [[0 1] ##  [1 0] ##  [0 1]], shape=(3, 2), dtype=int32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_array.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a tensor. — op_array","title":"Create a tensor. — op_array","text":"Create tensor.","code":""},{"path":"https://keras.posit.co/reference/op_array.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a tensor. — op_array","text":"","code":"op_array(x, dtype = NULL)"},{"path":"https://keras.posit.co/reference/op_array.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a tensor. — op_array","text":"x Input tensor. dtype desired data-type tensor.","code":""},{"path":"https://keras.posit.co/reference/op_array.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a tensor. — op_array","text":"tensor.","code":""},{"path":"https://keras.posit.co/reference/op_array.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a tensor. — op_array","text":"","code":"op_array(c(1, 2, 3)) ## tf.Tensor([1. 2. 3.], shape=(3), dtype=float32) op_array(c(1, 2, 3), dtype = \"float32\") ## tf.Tensor([1. 2. 3.], shape=(3), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_average.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the weighted average along the specified axis. — op_average","title":"Compute the weighted average along the specified axis. — op_average","text":"Compute weighted average along specified axis.","code":""},{"path":"https://keras.posit.co/reference/op_average.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the weighted average along the specified axis. — op_average","text":"","code":"op_average(x, axis = NULL, weights = NULL)"},{"path":"https://keras.posit.co/reference/op_average.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the weighted average along the specified axis. — op_average","text":"x Input tensor. axis Integer along average x. default, axis = NULL, average elements input tensor. axis negative counts last first axis. weights Tensor wieghts associated values x. value x contributes average according associated weight. weights array can either 1-D (case length must size along given axis) shape x. weights = NULL (default), data x assumed weight equal one. 1-D calculation : avg = sum(* weights) / sum(weights). constraint weights sum(weights) must 0.","code":""},{"path":"https://keras.posit.co/reference/op_average.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the weighted average along the specified axis. — op_average","text":"Return average along specified axis.","code":""},{"path":"https://keras.posit.co/reference/op_average.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute the weighted average along the specified axis. — op_average","text":"","code":"data <- op_arange(1, 5, dtype = \"int32\") data ## tf.Tensor([1 2 3 4], shape=(4), dtype=int32) op_average(data) ## tf.Tensor(2.5, shape=(), dtype=float32) op_average(   op_arange(1, 11),   weights = op_arange(10, 0, -1) ) ## tf.Tensor(4.0, shape=(), dtype=float64) data <- op_arange(6) |> op_reshape(c(3, 2)) data ## tf.Tensor( ## [[0. 1.] ##  [2. 3.] ##  [4. 5.]], shape=(3, 2), dtype=float64) op_average(   data,   axis = 2,   weights = op_array(c(1/4, 3/4)) ) ## tf.Tensor([0.75 2.75 4.75], shape=(3), dtype=float64) # Error: Axis must be specified when shapes of a and weights differ. try(op_average(   data,   weights = op_array(c(1/4, 3/4)) )) ## Error in py_call_impl(callable, call_args$unnamed, call_args$named) : ##   tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: 3, 2 ## 2"},{"path":[]},{"path":"https://keras.posit.co/reference/op_average_pool.html","id":null,"dir":"Reference","previous_headings":"","what":"Average pooling operation. — op_average_pool","title":"Average pooling operation. — op_average_pool","text":"Average pooling operation.","code":""},{"path":"https://keras.posit.co/reference/op_average_pool.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Average pooling operation. — op_average_pool","text":"","code":"op_average_pool(   inputs,   pool_size,   strides = NULL,   padding = \"valid\",   data_format = NULL )"},{"path":"https://keras.posit.co/reference/op_average_pool.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Average pooling operation. — op_average_pool","text":"inputs Tensor rank N+2. inputs shape (batch_size,) + inputs_spatial_shape + (num_channels,) data_format = \"channels_last\", (batch_size, num_channels) + inputs_spatial_shape data_format = \"channels_first\". Pooling happens spatial dimensions . pool_size int tuple/list integers size len(inputs_spatial_shape), specifying size pooling window spatial dimension input tensor. pool_size int, every spatial dimension shares pool_size. strides int tuple/list integers size len(inputs_spatial_shape). stride sliding window spatial dimension input tensor. strides int, every spatial dimension shares strides. padding string, either \"valid\" \"\". \"valid\" means padding applied, \"\" results padding evenly left/right /input output height/width dimension input strides = 1. data_format string, either \"channels_last\" \"channels_first\". data_format determines ordering dimensions inputs. data_format = \"channels_last\", inputs shape (batch_size, ..., channels) data_format = \"channels_first\", inputs shape (batch_size, channels, ...).","code":""},{"path":"https://keras.posit.co/reference/op_average_pool.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Average pooling operation. — op_average_pool","text":"","code":"A tensor of rank N+2, the result of the average pooling operation."},{"path":[]},{"path":"https://keras.posit.co/reference/op_binary_crossentropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes binary cross-entropy loss between target and output tensor. — op_binary_crossentropy","title":"Computes binary cross-entropy loss between target and output tensor. — op_binary_crossentropy","text":"binary cross-entropy loss commonly used binary classification tasks input sample belongs one two classes. measures dissimilarity target output probabilities logits.","code":""},{"path":"https://keras.posit.co/reference/op_binary_crossentropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes binary cross-entropy loss between target and output tensor. — op_binary_crossentropy","text":"","code":"op_binary_crossentropy(target, output, from_logits = FALSE)"},{"path":"https://keras.posit.co/reference/op_binary_crossentropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes binary cross-entropy loss between target and output tensor. — op_binary_crossentropy","text":"target target tensor representing true binary labels. shape match shape output tensor. output output tensor representing predicted probabilities logits. shape match shape target tensor. from_logits (optional) Whether output tensor logits probabilities. Set TRUE output represents logits; otherwise, set FALSE output represents probabilities. Defaults FALSE.","code":""},{"path":"https://keras.posit.co/reference/op_binary_crossentropy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes binary cross-entropy loss between target and output tensor. — op_binary_crossentropy","text":"Integer tensor: computed binary cross-entropy loss target output.","code":""},{"path":"https://keras.posit.co/reference/op_binary_crossentropy.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes binary cross-entropy loss between target and output tensor. — op_binary_crossentropy","text":"","code":"target <- op_array(c(0, 1, 1, 0)) output <- op_array(c(0.1, 0.9, 0.8, 0.2)) op_binary_crossentropy(target, output) ## tf.Tensor([0.10536054 0.10536054 0.22314355 0.22314355], shape=(4), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_bincount.html","id":null,"dir":"Reference","previous_headings":"","what":"Count the number of occurrences of each value in a tensor of integers. — op_bincount","title":"Count the number of occurrences of each value in a tensor of integers. — op_bincount","text":"Count number occurrences value tensor integers.","code":""},{"path":"https://keras.posit.co/reference/op_bincount.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Count the number of occurrences of each value in a tensor of integers. — op_bincount","text":"","code":"op_bincount(x, weights = NULL, minlength = 0L)"},{"path":"https://keras.posit.co/reference/op_bincount.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Count the number of occurrences of each value in a tensor of integers. — op_bincount","text":"x Input tensor. must dimension 1, must contain non-negative integer(s). weights Weight tensor. must length x. default value NULL. specified, x weighted , .e. n = x[], [n] += weight[] instead default behavior [n] += 1. minlength integer. default value 0. specified, least number bins output tensor. greater max(x) + 1, value output index higher max(x) set 0.","code":""},{"path":"https://keras.posit.co/reference/op_bincount.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Count the number of occurrences of each value in a tensor of integers. — op_bincount","text":"1D tensor element gives number occurrence(s) index value x. length maximum max(x) + 1 minlength.","code":""},{"path":"https://keras.posit.co/reference/op_bincount.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Count the number of occurrences of each value in a tensor of integers. — op_bincount","text":"","code":"(x <- op_array(c(1, 2, 2, 3), dtype = \"uint8\")) ## tf.Tensor([1 2 2 3], shape=(4), dtype=uint8) op_bincount(x) ## tf.Tensor([0 1 2 1], shape=(4), dtype=int32) (weights <- x / 2) ## tf.Tensor([0.5 1.  1.  1.5], shape=(4), dtype=float32) op_bincount(x, weights = weights) ## tf.Tensor([0.  0.5 2.  1.5], shape=(4), dtype=float32) minlength <- as.integer(op_max(x) + 1 + 2) # 6 op_bincount(x, minlength = minlength) ## tf.Tensor([0 1 2 1 0 0], shape=(6), dtype=int32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_broadcast_to.html","id":null,"dir":"Reference","previous_headings":"","what":"Broadcast a tensor to a new shape. — op_broadcast_to","title":"Broadcast a tensor to a new shape. — op_broadcast_to","text":"Broadcast tensor new shape.","code":""},{"path":"https://keras.posit.co/reference/op_broadcast_to.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Broadcast a tensor to a new shape. — op_broadcast_to","text":"","code":"op_broadcast_to(x, shape)"},{"path":"https://keras.posit.co/reference/op_broadcast_to.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Broadcast a tensor to a new shape. — op_broadcast_to","text":"x tensor broadcast. shape shape desired tensor.","code":""},{"path":"https://keras.posit.co/reference/op_broadcast_to.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Broadcast a tensor to a new shape. — op_broadcast_to","text":"tensor desired shape.","code":""},{"path":"https://keras.posit.co/reference/op_broadcast_to.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Broadcast a tensor to a new shape. — op_broadcast_to","text":"","code":"x <- op_array(c(1, 2, 3)) op_broadcast_to(x, shape = c(3, 3)) ## tf.Tensor( ## [[1. 2. 3.] ##  [1. 2. 3.] ##  [1. 2. 3.]], shape=(3, 3), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_cast.html","id":null,"dir":"Reference","previous_headings":"","what":"Cast a tensor to the desired dtype. — op_cast","title":"Cast a tensor to the desired dtype. — op_cast","text":"Cast tensor desired dtype.","code":""},{"path":"https://keras.posit.co/reference/op_cast.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cast a tensor to the desired dtype. — op_cast","text":"","code":"op_cast(x, dtype)"},{"path":"https://keras.posit.co/reference/op_cast.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cast a tensor to the desired dtype. — op_cast","text":"x tensor variable. dtype target type.","code":""},{"path":"https://keras.posit.co/reference/op_cast.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cast a tensor to the desired dtype. — op_cast","text":"tensor specified dtype.","code":""},{"path":"https://keras.posit.co/reference/op_cast.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cast a tensor to the desired dtype. — op_cast","text":"","code":"(x <- op_arange(4)) ## tf.Tensor([0. 1. 2. 3.], shape=(4), dtype=float64) op_cast(x, dtype = \"float16\") ## tf.Tensor([0. 1. 2. 3.], shape=(4), dtype=float16)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_categorical_crossentropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes categorical cross-entropy loss between target and output tensor. — op_categorical_crossentropy","title":"Computes categorical cross-entropy loss between target and output tensor. — op_categorical_crossentropy","text":"categorical cross-entropy loss commonly used multi-class classification tasks input sample can belong one multiple classes. measures dissimilarity target output probabilities logits.","code":""},{"path":"https://keras.posit.co/reference/op_categorical_crossentropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes categorical cross-entropy loss between target and output tensor. — op_categorical_crossentropy","text":"","code":"op_categorical_crossentropy(target, output, from_logits = FALSE, axis = -1L)"},{"path":"https://keras.posit.co/reference/op_categorical_crossentropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes categorical cross-entropy loss between target and output tensor. — op_categorical_crossentropy","text":"target target tensor representing true categorical labels. shape match shape output tensor except last dimension. output output tensor representing predicted probabilities logits. shape match shape target tensor except last dimension. from_logits (optional) Whether output tensor logits probabilities. Set TRUE output represents logits; otherwise, set FALSE output represents probabilities. Defaults FALSE. axis (optional) axis along categorical cross-entropy computed. Defaults -1, corresponds last dimension tensors.","code":""},{"path":"https://keras.posit.co/reference/op_categorical_crossentropy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes categorical cross-entropy loss between target and output tensor. — op_categorical_crossentropy","text":"Integer tensor: computed categorical cross-entropy loss target output.","code":""},{"path":"https://keras.posit.co/reference/op_categorical_crossentropy.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes categorical cross-entropy loss between target and output tensor. — op_categorical_crossentropy","text":"","code":"target <- op_array(rbind(c(1, 0, 0),                         c(0, 1, 0),                         c(0, 0, 1))) output <- op_array(rbind(c(0.9, 0.05, 0.05),                         c(0.1, 0.8, 0.1),                         c(0.2, 0.3, 0.5))) op_categorical_crossentropy(target, output) ## tf.Tensor([0.10536052 0.22314355 0.69314718], shape=(3), dtype=float64)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_ceil.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the ceiling of the input, element-wise. — op_ceil","title":"Return the ceiling of the input, element-wise. — op_ceil","text":"ceil scalar x smallest integer , >= x.","code":""},{"path":"https://keras.posit.co/reference/op_ceil.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the ceiling of the input, element-wise. — op_ceil","text":"","code":"op_ceil(x)"},{"path":"https://keras.posit.co/reference/op_ceil.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the ceiling of the input, element-wise. — op_ceil","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_ceil.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the ceiling of the input, element-wise. — op_ceil","text":"","code":"The ceiling of each element in `x`, with float dtype."},{"path":[]},{"path":"https://keras.posit.co/reference/op_clip.html","id":null,"dir":"Reference","previous_headings":"","what":"Clip (limit) the values in a tensor. — op_clip","title":"Clip (limit) the values in a tensor. — op_clip","text":"Given interval, values outside interval clipped interval edges. example, interval [0, 1] specified, values smaller 0 become 0, values larger 1 become 1.","code":""},{"path":"https://keras.posit.co/reference/op_clip.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clip (limit) the values in a tensor. — op_clip","text":"","code":"op_clip(x, x_min, x_max)"},{"path":"https://keras.posit.co/reference/op_clip.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clip (limit) the values in a tensor. — op_clip","text":"x Input tensor. x_min Minimum value. x_max Maximum value.","code":""},{"path":"https://keras.posit.co/reference/op_clip.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clip (limit) the values in a tensor. — op_clip","text":"","code":"The clipped tensor."},{"path":[]},{"path":"https://keras.posit.co/reference/op_concatenate.html","id":null,"dir":"Reference","previous_headings":"","what":"Join a sequence of tensors along an existing axis. — op_concatenate","title":"Join a sequence of tensors along an existing axis. — op_concatenate","text":"Join sequence tensors along existing axis.","code":""},{"path":"https://keras.posit.co/reference/op_concatenate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Join a sequence of tensors along an existing axis. — op_concatenate","text":"","code":"op_concatenate(xs, axis = 1L)"},{"path":"https://keras.posit.co/reference/op_concatenate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Join a sequence of tensors along an existing axis. — op_concatenate","text":"xs sequence tensors concatenate. axis axis along tensors joined. Defaults 0.","code":""},{"path":"https://keras.posit.co/reference/op_concatenate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Join a sequence of tensors along an existing axis. — op_concatenate","text":"","code":"The concatenated tensor."},{"path":[]},{"path":"https://keras.posit.co/reference/op_cond.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditionally applies true_fn or false_fn. — op_cond","title":"Conditionally applies true_fn or false_fn. — op_cond","text":"Conditionally applies true_fn false_fn.","code":""},{"path":"https://keras.posit.co/reference/op_cond.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditionally applies true_fn or false_fn. — op_cond","text":"","code":"op_cond(pred, true_fn, false_fn)"},{"path":"https://keras.posit.co/reference/op_cond.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditionally applies true_fn or false_fn. — op_cond","text":"pred Boolean scalar type true_fn Callable returning output pred == TRUE case. false_fn Callable returning output pred == FALSE case.","code":""},{"path":"https://keras.posit.co/reference/op_cond.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Conditionally applies true_fn or false_fn. — op_cond","text":"","code":"The output of either `true_fn` or `false_fn` depending on pred."},{"path":[]},{"path":"https://keras.posit.co/reference/op_conj.html","id":null,"dir":"Reference","previous_headings":"","what":"Shorthand for keras.ops.conjugate. — op_conj","title":"Shorthand for keras.ops.conjugate. — op_conj","text":"Shorthand keras.ops.conjugate.","code":""},{"path":"https://keras.posit.co/reference/op_conj.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Shorthand for keras.ops.conjugate. — op_conj","text":"","code":"op_conj(x)"},{"path":"https://keras.posit.co/reference/op_conj.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Shorthand for keras.ops.conjugate. — op_conj","text":"x see description","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/op_conjugate.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the complex conjugate, element-wise. — op_conjugate","title":"Returns the complex conjugate, element-wise. — op_conjugate","text":"complex conjugate complex number obtained changing sign imaginary part. keras.ops.conj shorthand function.","code":""},{"path":"https://keras.posit.co/reference/op_conjugate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the complex conjugate, element-wise. — op_conjugate","text":"","code":"op_conjugate(x)"},{"path":"https://keras.posit.co/reference/op_conjugate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the complex conjugate, element-wise. — op_conjugate","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_conjugate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the complex conjugate, element-wise. — op_conjugate","text":"","code":"The complex conjugate of each element in `x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_conv.html","id":null,"dir":"Reference","previous_headings":"","what":"General N-D convolution. — op_conv","title":"General N-D convolution. — op_conv","text":"ops supports 1D, 2D 3D convolution.","code":""},{"path":"https://keras.posit.co/reference/op_conv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"General N-D convolution. — op_conv","text":"","code":"op_conv(   inputs,   kernel,   strides = 1L,   padding = \"valid\",   data_format = NULL,   dilation_rate = 1L )"},{"path":"https://keras.posit.co/reference/op_conv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"General N-D convolution. — op_conv","text":"inputs Tensor rank N+2. inputs shape (batch_size,) + inputs_spatial_shape + (num_channels,) data_format = \"channels_last\", (batch_size, num_channels) + inputs_spatial_shape data_format = \"channels_first\". kernel Tensor rank N+2. kernel shape (kernel_spatial_shape, num_input_channels, num_output_channels). num_input_channels match number channels inputs. strides int int tuple/list len(inputs_spatial_shape), specifying strides convolution along spatial dimension. strides int, every spatial dimension shares strides. padding string, either \"valid\" \"\". \"valid\" means padding applied, \"\" results padding evenly left/right /input output height/width dimension input strides = 1. data_format string, either \"channels_last\" \"channels_first\". data_format determines ordering dimensions inputs. data_format = \"channels_last\", inputs shape (batch_size, ..., channels) data_format = \"channels_first\", inputs shape (batch_size, channels, ...). dilation_rate int int tuple/list len(inputs_spatial_shape), specifying dilation rate use dilated convolution. dilation_rate int, every spatial dimension shares dilation_rate.","code":""},{"path":"https://keras.posit.co/reference/op_conv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"General N-D convolution. — op_conv","text":"","code":"A tensor of rank N+2, the result of the conv operation."},{"path":[]},{"path":"https://keras.posit.co/reference/op_conv_transpose.html","id":null,"dir":"Reference","previous_headings":"","what":"General N-D convolution transpose. — op_conv_transpose","title":"General N-D convolution transpose. — op_conv_transpose","text":"Also known de-convolution. ops supports 1D, 2D 3D convolution.","code":""},{"path":"https://keras.posit.co/reference/op_conv_transpose.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"General N-D convolution transpose. — op_conv_transpose","text":"","code":"op_conv_transpose(   inputs,   kernel,   strides,   padding = \"valid\",   output_padding = NULL,   data_format = NULL,   dilation_rate = 1L )"},{"path":"https://keras.posit.co/reference/op_conv_transpose.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"General N-D convolution transpose. — op_conv_transpose","text":"inputs Tensor rank N+2. inputs shape (batch_size,) + inputs_spatial_shape + (num_channels,) data_format = \"channels_last\", (batch_size, num_channels) + inputs_spatial_shape data_format = \"channels_first\". kernel Tensor rank N+2. kernel shape [kernel_spatial_shape, num_output_channels, num_input_channels], num_input_channels match number channels inputs. strides int int tuple/list len(inputs_spatial_shape), specifying strides convolution along spatial dimension. strides int, every spatial dimension shares strides. padding string, either \"valid\" \"\". \"valid\" means padding applied, \"\" results padding evenly left/right /input output height/width dimension input strides = 1. output_padding int int tuple/list len(inputs_spatial_shape), specifying amount padding along height width output tensor. Can single integer specify value spatial dimensions. amount output padding along given dimension must lower stride along dimension. set NULL (default), output shape inferred. data_format string, either \"channels_last\" \"channels_first\". data_format determines ordering dimensions inputs. data_format = \"channels_last\", inputs shape (batch_size, ..., channels) data_format = \"channels_first\", inputs shape (batch_size, channels, ...). dilation_rate int int tuple/list len(inputs_spatial_shape), specifying dilation rate use dilated convolution. dilation_rate int, every spatial dimension shares dilation_rate.","code":""},{"path":"https://keras.posit.co/reference/op_conv_transpose.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"General N-D convolution transpose. — op_conv_transpose","text":"","code":"A tensor of rank N+2, the result of the conv operation."},{"path":[]},{"path":"https://keras.posit.co/reference/op_convert_to_numpy.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a tensor to a NumPy array. — op_convert_to_numpy","title":"Convert a tensor to a NumPy array. — op_convert_to_numpy","text":"Convert tensor NumPy array.","code":""},{"path":"https://keras.posit.co/reference/op_convert_to_numpy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a tensor to a NumPy array. — op_convert_to_numpy","text":"","code":"op_convert_to_numpy(x)"},{"path":"https://keras.posit.co/reference/op_convert_to_numpy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a tensor to a NumPy array. — op_convert_to_numpy","text":"x tensor.","code":""},{"path":"https://keras.posit.co/reference/op_convert_to_numpy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a tensor to a NumPy array. — op_convert_to_numpy","text":"","code":"A NumPy array."},{"path":[]},{"path":"https://keras.posit.co/reference/op_convert_to_tensor.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert an array to a tensor. — op_convert_to_tensor","title":"Convert an array to a tensor. — op_convert_to_tensor","text":"Convert array tensor.","code":""},{"path":"https://keras.posit.co/reference/op_convert_to_tensor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert an array to a tensor. — op_convert_to_tensor","text":"","code":"op_convert_to_tensor(x, dtype = NULL, sparse = NULL)"},{"path":"https://keras.posit.co/reference/op_convert_to_tensor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert an array to a tensor. — op_convert_to_tensor","text":"x array. dtype target type. sparse Whether keep sparse tensors. FALSE cause sparse tensors densified. default value NULL means sparse tensors kept backend supports .","code":""},{"path":"https://keras.posit.co/reference/op_convert_to_tensor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert an array to a tensor. — op_convert_to_tensor","text":"tensor specified dtype.","code":""},{"path":"https://keras.posit.co/reference/op_convert_to_tensor.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert an array to a tensor. — op_convert_to_tensor","text":"","code":"x <- array(c(1, 2, 3)) y <- op_convert_to_tensor(x) y ## tf.Tensor([1. 2. 3.], shape=(3), dtype=float64)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_copy.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns a copy of x. — op_copy","title":"Returns a copy of x. — op_copy","text":"Returns copy x.","code":""},{"path":"https://keras.posit.co/reference/op_copy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns a copy of x. — op_copy","text":"","code":"op_copy(x)"},{"path":"https://keras.posit.co/reference/op_copy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns a copy of x. — op_copy","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_copy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns a copy of x. — op_copy","text":"","code":"A copy of `x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_cos.html","id":null,"dir":"Reference","previous_headings":"","what":"Cosine, element-wise. — op_cos","title":"Cosine, element-wise. — op_cos","text":"Cosine, element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_cos.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cosine, element-wise. — op_cos","text":"","code":"op_cos(x)"},{"path":"https://keras.posit.co/reference/op_cos.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cosine, element-wise. — op_cos","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_cos.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cosine, element-wise. — op_cos","text":"","code":"The corresponding cosine values."},{"path":[]},{"path":"https://keras.posit.co/reference/op_cosh.html","id":null,"dir":"Reference","previous_headings":"","what":"Hyperbolic cosine, element-wise. — op_cosh","title":"Hyperbolic cosine, element-wise. — op_cosh","text":"Hyperbolic cosine, element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_cosh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperbolic cosine, element-wise. — op_cosh","text":"","code":"op_cosh(x)"},{"path":"https://keras.posit.co/reference/op_cosh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hyperbolic cosine, element-wise. — op_cosh","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_cosh.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Hyperbolic cosine, element-wise. — op_cosh","text":"","code":"Output tensor of same shape as `x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_count_nonzero.html","id":null,"dir":"Reference","previous_headings":"","what":"Counts the number of non-zero values in x along the given axis. — op_count_nonzero","title":"Counts the number of non-zero values in x along the given axis. — op_count_nonzero","text":"axis specified non-zeros tensor counted.","code":""},{"path":"https://keras.posit.co/reference/op_count_nonzero.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Counts the number of non-zero values in x along the given axis. — op_count_nonzero","text":"","code":"op_count_nonzero(x, axis = NULL)"},{"path":"https://keras.posit.co/reference/op_count_nonzero.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Counts the number of non-zero values in x along the given axis. — op_count_nonzero","text":"x Input tensor. axis Axis tuple axes along count number non-zeros. Defaults NULL.","code":""},{"path":"https://keras.posit.co/reference/op_count_nonzero.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Counts the number of non-zero values in x along the given axis. — op_count_nonzero","text":"integer tensor integers.","code":""},{"path":"https://keras.posit.co/reference/op_count_nonzero.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Counts the number of non-zero values in x along the given axis. — op_count_nonzero","text":"","code":"x <- op_array(rbind(c(0, 1, 7, 0),                    c(3, 0, 2, 19))) op_count_nonzero(x) ## tf.Tensor(5, shape=(), dtype=int32) op_count_nonzero(x, axis = 1) ## tf.Tensor([1 1 2 1], shape=(4), dtype=int32) op_count_nonzero(x, axis = 2) ## tf.Tensor([2 3], shape=(2), dtype=int32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_cross.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the cross product of two (arrays of) vectors. — op_cross","title":"Returns the cross product of two (arrays of) vectors. — op_cross","text":"cross product x1 x2 R^3 vector perpendicular x1 x2. x1 x2 arrays vectors, vectors defined last axis x1 x2 default, axes can dimensions 2 3. dimension either x1 x2 2, third component input vector assumed zero cross product calculated accordingly. cases input vectors dimension 2, z-component cross product returned.","code":""},{"path":"https://keras.posit.co/reference/op_cross.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the cross product of two (arrays of) vectors. — op_cross","text":"","code":"op_cross(x1, x2, axisa = -1L, axisb = -1L, axisc = -1L, axis = NULL)"},{"path":"https://keras.posit.co/reference/op_cross.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the cross product of two (arrays of) vectors. — op_cross","text":"x1 Components first vector(s). x2 Components second vector(s). axisa Axis x1 defines vector(s). Defaults -1. axisb Axis x2 defines vector(s). Defaults -1. axisc Axis result containing cross product vector(s). Ignored input vectors dimension 2, return scalar. default, last axis. axis defined, axis x1, x2 result defines vector(s) cross product(s). Overrides axisa, axisb axisc.","code":""},{"path":"https://keras.posit.co/reference/op_cross.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the cross product of two (arrays of) vectors. — op_cross","text":"","code":"Vector cross product(s)."},{"path":"https://keras.posit.co/reference/op_cross.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Returns the cross product of two (arrays of) vectors. — op_cross","text":"Torch backend support two dimensional vectors, arguments axisa, axisb axisc. Use axis instead.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/op_cumprod.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the cumulative product of elements along a given axis. — op_cumprod","title":"Return the cumulative product of elements along a given axis. — op_cumprod","text":"Return cumulative product elements along given axis.","code":""},{"path":"https://keras.posit.co/reference/op_cumprod.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the cumulative product of elements along a given axis. — op_cumprod","text":"","code":"op_cumprod(x, axis = NULL, dtype = NULL)"},{"path":"https://keras.posit.co/reference/op_cumprod.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the cumulative product of elements along a given axis. — op_cumprod","text":"x Input tensor. axis Axis along cumulative product computed. default input flattened. dtype dtype returned tensor. Defaults x$dtype.","code":""},{"path":"https://keras.posit.co/reference/op_cumprod.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the cumulative product of elements along a given axis. — op_cumprod","text":"","code":"Output tensor."},{"path":[]},{"path":"https://keras.posit.co/reference/op_cumsum.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the cumulative sum of elements along a given axis. — op_cumsum","title":"Returns the cumulative sum of elements along a given axis. — op_cumsum","text":"Returns cumulative sum elements along given axis.","code":""},{"path":"https://keras.posit.co/reference/op_cumsum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the cumulative sum of elements along a given axis. — op_cumsum","text":"","code":"op_cumsum(x, axis = NULL, dtype = NULL)"},{"path":"https://keras.posit.co/reference/op_cumsum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the cumulative sum of elements along a given axis. — op_cumsum","text":"x Input tensor. axis Axis along cumulative sum computed. default input flattened. dtype dtype returned tensor. Defaults x$dtype.","code":""},{"path":"https://keras.posit.co/reference/op_cumsum.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the cumulative sum of elements along a given axis. — op_cumsum","text":"","code":"Output tensor."},{"path":[]},{"path":"https://keras.posit.co/reference/op_depthwise_conv.html","id":null,"dir":"Reference","previous_headings":"","what":"General N-D depthwise convolution. — op_depthwise_conv","title":"General N-D depthwise convolution. — op_depthwise_conv","text":"ops supports 1D 2D depthwise convolution.","code":""},{"path":"https://keras.posit.co/reference/op_depthwise_conv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"General N-D depthwise convolution. — op_depthwise_conv","text":"","code":"op_depthwise_conv(   inputs,   kernel,   strides = 1L,   padding = \"valid\",   data_format = NULL,   dilation_rate = 1L )"},{"path":"https://keras.posit.co/reference/op_depthwise_conv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"General N-D depthwise convolution. — op_depthwise_conv","text":"inputs Tensor rank N+2. inputs shape (batch_size,) + inputs_spatial_shape + (num_channels,) data_format = \"channels_last\", (batch_size, num_channels) + inputs_spatial_shape data_format = \"channels_first\". kernel Tensor rank N+2. kernel shape [kernel_spatial_shape, num_input_channels, num_channels_multiplier], num_input_channels match number channels inputs. strides int int tuple/list len(inputs_spatial_shape), specifying strides convolution along spatial dimension. strides int, every spatial dimension shares strides. padding string, either \"valid\" \"\". \"valid\" means padding applied, \"\" results padding evenly left/right /input output height/width dimension input strides = 1. data_format string, either \"channels_last\" \"channels_first\". data_format determines ordering dimensions inputs. data_format = \"channels_last\", inputs shape (batch_size, ..., channels) data_format = \"channels_first\", inputs shape (batch_size, channels, ...). dilation_rate int int tuple/list len(inputs_spatial_shape), specifying dilation rate use dilated convolution. dilation_rate int, every spatial dimension shares dilation_rate.","code":""},{"path":"https://keras.posit.co/reference/op_depthwise_conv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"General N-D depthwise convolution. — op_depthwise_conv","text":"","code":"A tensor of rank N+2, the result of the depthwise conv operation."},{"path":[]},{"path":"https://keras.posit.co/reference/op_diag.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract a diagonal or construct a diagonal array. — op_diag","title":"Extract a diagonal or construct a diagonal array. — op_diag","text":"Extract diagonal construct diagonal array.","code":""},{"path":"https://keras.posit.co/reference/op_diag.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract a diagonal or construct a diagonal array. — op_diag","text":"","code":"op_diag(x, k = 0L)"},{"path":"https://keras.posit.co/reference/op_diag.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract a diagonal or construct a diagonal array. — op_diag","text":"x Input tensor. x 2-D, returns k-th diagonal x. x 1-D, return 2-D tensor x k-th diagonal. k diagonal consider. Defaults 0. Use k > 0 diagonals main diagonal, k < 0 diagonals main diagonal.","code":""},{"path":"https://keras.posit.co/reference/op_diag.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract a diagonal or construct a diagonal array. — op_diag","text":"extracted diagonal constructed diagonal tensor.","code":""},{"path":"https://keras.posit.co/reference/op_diag.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract a diagonal or construct a diagonal array. — op_diag","text":"","code":"x <- op_arange(9L) |> op_reshape(c(3, 3)) x ## tf.Tensor( ## [[0 1 2] ##  [3 4 5] ##  [6 7 8]], shape=(3, 3), dtype=int32) op_diag(x) ## tf.Tensor([0 4 8], shape=(3), dtype=int32) op_diag(x, k = 1) ## tf.Tensor([1 5], shape=(2), dtype=int32) op_diag(x, k = -1) ## tf.Tensor([3 7], shape=(2), dtype=int32) op_diag(op_diag(x)) ## tf.Tensor( ## [[0 0 0] ##  [0 4 0] ##  [0 0 8]], shape=(3, 3), dtype=int32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_diagonal.html","id":null,"dir":"Reference","previous_headings":"","what":"Return specified diagonals. — op_diagonal","title":"Return specified diagonals. — op_diagonal","text":"x 2-D, returns diagonal x given offset, .e., collection elements form x[, +offset]. x two dimensions, axes specified axis1 axis2 used determine 2-D sub-array whose diagonal returned. shape resulting array can determined removing axis1 axis2 appending index right equal size resulting diagonals.","code":""},{"path":"https://keras.posit.co/reference/op_diagonal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return specified diagonals. — op_diagonal","text":"","code":"op_diagonal(x, offset = 0L, axis1 = 1L, axis2 = 2L)"},{"path":"https://keras.posit.co/reference/op_diagonal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return specified diagonals. — op_diagonal","text":"x Input tensor. offset Offset diagonal main diagonal. Can positive negative. Defaults 0.(main diagonal). axis1 Axis used first axis 2-D sub-arrays. Defaults 1.(first axis). axis2 Axis used second axis 2-D sub-arrays. Defaults 2 (second axis).","code":""},{"path":"https://keras.posit.co/reference/op_diagonal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return specified diagonals. — op_diagonal","text":"Tensor diagonals.","code":""},{"path":"https://keras.posit.co/reference/op_diagonal.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Return specified diagonals. — op_diagonal","text":"","code":"x <- op_arange(4L) |> op_reshape(c(2, 2)) x ## tf.Tensor( ## [[0 1] ##  [2 3]], shape=(2, 2), dtype=int32) op_diagonal(x) ## tf.Tensor([0 3], shape=(2), dtype=int32) op_diagonal(x, offset = 1) ## tf.Tensor([1], shape=(1), dtype=int32) x <- op_array(1:8) |> op_reshape(c(2, 2, 2)) x ## tf.Tensor( ## [[[1 2] ##   [3 4]] ## ##  [[5 6] ##   [7 8]]], shape=(2, 2, 2), dtype=int32) x |> op_diagonal(0) ## tf.Tensor( ## [[1 7] ##  [2 8]], shape=(2, 2), dtype=int32) x |> op_diagonal(0, 1, 2) # same as above, the default ## tf.Tensor( ## [[1 7] ##  [2 8]], shape=(2, 2), dtype=int32) x |> op_diagonal(0, 2, 3) ## tf.Tensor( ## [[1 4] ##  [5 8]], shape=(2, 2), dtype=int32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_diff.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the n-th discrete difference along the given axis. — op_diff","title":"Calculate the n-th discrete difference along the given axis. — op_diff","text":"first difference given [] = [+1] - [] along given axis, higher differences calculated using diff recursively.","code":""},{"path":"https://keras.posit.co/reference/op_diff.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the n-th discrete difference along the given axis. — op_diff","text":"","code":"op_diff(a, n = 1L, axis = -1L)"},{"path":"https://keras.posit.co/reference/op_diff.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the n-th discrete difference along the given axis. — op_diff","text":"Input tensor. n number times values differenced. Defaults 1. axis Axis compute discrete difference(s) along. Defaults -1 (last axis).","code":""},{"path":"https://keras.posit.co/reference/op_diff.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the n-th discrete difference along the given axis. — op_diff","text":"Tensor diagonals.","code":""},{"path":"https://keras.posit.co/reference/op_diff.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the n-th discrete difference along the given axis. — op_diff","text":"","code":"x <- op_array(c(1, 2, 4, 7, 0)) op_diff(x) ## tf.Tensor([ 1.  2.  3. -7.], shape=(4), dtype=float32) op_diff(x, n = 2) ## tf.Tensor([  1.   1. -10.], shape=(3), dtype=float32) x <- op_array(rbind(c(1, 3, 6, 10),                   c(0, 5, 6, 8))) op_diff(x) ## tf.Tensor( ## [[2. 3. 4.] ##  [5. 1. 2.]], shape=(2, 3), dtype=float64) op_diff(x, axis = 1) ## tf.Tensor([[-1.  2.  0. -2.]], shape=(1, 4), dtype=float64)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_digitize.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the indices of the bins to which each value in x belongs. — op_digitize","title":"Returns the indices of the bins to which each value in x belongs. — op_digitize","text":"Returns indices bins value x belongs.","code":""},{"path":"https://keras.posit.co/reference/op_digitize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the indices of the bins to which each value in x belongs. — op_digitize","text":"","code":"op_digitize(x, bins)"},{"path":"https://keras.posit.co/reference/op_digitize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the indices of the bins to which each value in x belongs. — op_digitize","text":"x Input array binned. bins Array bins. one-dimensional monotonically increasing.","code":""},{"path":"https://keras.posit.co/reference/op_digitize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the indices of the bins to which each value in x belongs. — op_digitize","text":"Output array indices, shape x.","code":""},{"path":"https://keras.posit.co/reference/op_digitize.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Returns the indices of the bins to which each value in x belongs. — op_digitize","text":"","code":"x <- op_array(c(0.0, 1.0, 3.0, 1.6)) bins <- array(c(0.0, 3.0, 4.5, 7.0)) op_digitize(x, bins) ## tf.Tensor([1 1 2 1], shape=(4), dtype=int32) # array([1, 1, 2, 1])"},{"path":[]},{"path":"https://keras.posit.co/reference/op_divide.html","id":null,"dir":"Reference","previous_headings":"","what":"Divide arguments element-wise. — op_divide","title":"Divide arguments element-wise. — op_divide","text":"keras.ops.true_divide alias function.","code":""},{"path":"https://keras.posit.co/reference/op_divide.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Divide arguments element-wise. — op_divide","text":"","code":"op_divide(x1, x2)"},{"path":"https://keras.posit.co/reference/op_divide.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Divide arguments element-wise. — op_divide","text":"x1 First input tensor. x2 Second input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_divide.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Divide arguments element-wise. — op_divide","text":"","code":"Output tensor, the quotient `x1/x2`, element-wise."},{"path":[]},{"path":"https://keras.posit.co/reference/op_dot.html","id":null,"dir":"Reference","previous_headings":"","what":"Dot product of two tensors. — op_dot","title":"Dot product of two tensors. — op_dot","text":"x1 x2 1-D tensors, inner product vectors (without complex conjugation). x1 x2 2-D tensors, matrix multiplication. either x1 x2 0-D (scalar), equivalent x1 * x2. x1 N-D tensor x2 1-D tensor, sum product last axis x1 x2. x1 N-D tensor x2 M-D tensor (M >= 2), sum product last axis x1 second--last axis x2: dot(x1, x2)[,j,k,m] = sum([,j,:] * b[k,:,m]).","code":""},{"path":"https://keras.posit.co/reference/op_dot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dot product of two tensors. — op_dot","text":"","code":"op_dot(x1, x2)"},{"path":"https://keras.posit.co/reference/op_dot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dot product of two tensors. — op_dot","text":"x1 First argument. x2 Second argument.","code":""},{"path":"https://keras.posit.co/reference/op_dot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Dot product of two tensors. — op_dot","text":"","code":"Dot product of `x1` and `x2`."},{"path":"https://keras.posit.co/reference/op_dot.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Dot product of two tensors. — op_dot","text":"Torch backend accept 0-D tensors arguments.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/op_einsum.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluates the Einstein summation convention on the operands. — op_einsum","title":"Evaluates the Einstein summation convention on the operands. — op_einsum","text":"Evaluates Einstein summation convention operands.","code":""},{"path":"https://keras.posit.co/reference/op_einsum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluates the Einstein summation convention on the operands. — op_einsum","text":"","code":"op_einsum(subscripts, ...)"},{"path":"https://keras.posit.co/reference/op_einsum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluates the Einstein summation convention on the operands. — op_einsum","text":"subscripts Specifies subscripts summation comma separated list subscript labels. implicit (classical Einstein summation) calculation performed unless explicit indicator -> included well subscript labels precise output form. ... operands compute Einstein sum .","code":""},{"path":"https://keras.posit.co/reference/op_einsum.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluates the Einstein summation convention on the operands. — op_einsum","text":"calculation based Einstein summation convention.","code":""},{"path":"https://keras.posit.co/reference/op_einsum.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Evaluates the Einstein summation convention on the operands. — op_einsum","text":"Trace matrix:     Extract diagonal:     Sum axis:     higher dimensional tensors summing single axis can done ellipsis:     Compute matrix transpose reorder number axes:     Matrix vector multiplication:","code":"a <- op_arange(25) |> op_reshape(c(5, 5)) b <- op_arange(5) c <- op_arange(6) |> op_reshape(c(2, 3)) op_einsum(\"ii\", a) op_trace(a) ## tf.Tensor(60.0, shape=(), dtype=float64) ## tf.Tensor(60.0, shape=(), dtype=float64) op_einsum(\"ii -> i\", a) op_diag(a) ## tf.Tensor([ 0.  6. 12. 18. 24.], shape=(5), dtype=float64) ## tf.Tensor([ 0.  6. 12. 18. 24.], shape=(5), dtype=float64) op_einsum(\"ij -> i\", a) op_sum(a, axis = 2) ## tf.Tensor([ 10.  35.  60.  85. 110.], shape=(5), dtype=float64) ## tf.Tensor([ 10.  35.  60.  85. 110.], shape=(5), dtype=float64) op_einsum(\"...j -> ...\", a) op_sum(a, axis = -1) ## tf.Tensor([ 10.  35.  60.  85. 110.], shape=(5), dtype=float64) ## tf.Tensor([ 10.  35.  60.  85. 110.], shape=(5), dtype=float64) op_einsum(\"ji\", c) op_einsum(\"ij -> ji\", c) op_transpose(c) ## tf.Tensor( ## [[0. 3.] ##  [1. 4.] ##  [2. 5.]], shape=(3, 2), dtype=float64) ## tf.Tensor( ## [[0. 3.] ##  [1. 4.] ##  [2. 5.]], shape=(3, 2), dtype=float64) ## tf.Tensor( ## [[0. 3.] ##  [1. 4.] ##  [2. 5.]], shape=(3, 2), dtype=float64) op_einsum(\"ij, j\", a, b) op_einsum(\"...j, j\", a, b) a %*% b op_matmul(a, b) ## tf.Tensor([ 30.  80. 130. 180. 230.], shape=(5), dtype=float64) ## tf.Tensor([ 30.  80. 130. 180. 230.], shape=(5), dtype=float64) ## tf.Tensor([ 30.  80. 130. 180. 230.], shape=(5), dtype=float64) ## tf.Tensor([ 30.  80. 130. 180. 230.], shape=(5), dtype=float64)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_elu.html","id":null,"dir":"Reference","previous_headings":"","what":"Exponential Linear Unit activation function. — op_elu","title":"Exponential Linear Unit activation function. — op_elu","text":"defined : f(x) =  alpha * (exp(x) - 1.) x < 0, f(x) = x x >= 0.","code":""},{"path":"https://keras.posit.co/reference/op_elu.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Exponential Linear Unit activation function. — op_elu","text":"","code":"op_elu(x, alpha = 1)"},{"path":"https://keras.posit.co/reference/op_elu.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Exponential Linear Unit activation function. — op_elu","text":"x Input tensor. alpha scalar, slope positive section. Defaults 1.0.","code":""},{"path":"https://keras.posit.co/reference/op_elu.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Exponential Linear Unit activation function. — op_elu","text":"tensor shape x.","code":""},{"path":"https://keras.posit.co/reference/op_elu.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Exponential Linear Unit activation function. — op_elu","text":"","code":"x <- op_array(c(-1., 0., 1.)) op_elu(x) ## tf.Tensor([-0.63212055  0.          1.        ], shape=(3), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_empty.html","id":null,"dir":"Reference","previous_headings":"","what":"Return a tensor of given shape and type filled with uninitialized data. — op_empty","title":"Return a tensor of given shape and type filled with uninitialized data. — op_empty","text":"Return tensor given shape type filled uninitialized data.","code":""},{"path":"https://keras.posit.co/reference/op_empty.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return a tensor of given shape and type filled with uninitialized data. — op_empty","text":"","code":"op_empty(shape, dtype = NULL)"},{"path":"https://keras.posit.co/reference/op_empty.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return a tensor of given shape and type filled with uninitialized data. — op_empty","text":"shape Shape empty tensor. dtype Desired data type empty tensor.","code":""},{"path":"https://keras.posit.co/reference/op_empty.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return a tensor of given shape and type filled with uninitialized data. — op_empty","text":"","code":"The empty tensor."},{"path":[]},{"path":"https://keras.posit.co/reference/op_equal.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns (x1 == x2) element-wise. — op_equal","title":"Returns (x1 == x2) element-wise. — op_equal","text":"Returns (x1 == x2) element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_equal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns (x1 == x2) element-wise. — op_equal","text":"","code":"op_equal(x1, x2)"},{"path":"https://keras.posit.co/reference/op_equal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns (x1 == x2) element-wise. — op_equal","text":"x1 Tensor compare. x2 Tensor compare.","code":""},{"path":"https://keras.posit.co/reference/op_equal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns (x1 == x2) element-wise. — op_equal","text":"","code":"Output tensor, element-wise comparison of `x1` and `x2`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_erf.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the error function of x, element-wise. — op_erf","title":"Computes the error function of x, element-wise. — op_erf","text":"Computes error function x, element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_erf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the error function of x, element-wise. — op_erf","text":"","code":"op_erf(x)"},{"path":"https://keras.posit.co/reference/op_erf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the error function of x, element-wise. — op_erf","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_erf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the error function of x, element-wise. — op_erf","text":"tensor dtype x.","code":""},{"path":"https://keras.posit.co/reference/op_erf.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the error function of x, element-wise. — op_erf","text":"","code":"x <- op_array(c(-3, -2, -1, 0, 1)) op_erf(x) ## tf.Tensor([-0.99997795 -0.9953222  -0.84270084  0.          0.84270084], shape=(5), dtype=float32) # array([-0.99998 , -0.99532, -0.842701,  0.,  0.842701], dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_exp.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the exponential of all elements in the input tensor. — op_exp","title":"Calculate the exponential of all elements in the input tensor. — op_exp","text":"Calculate exponential elements input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_exp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the exponential of all elements in the input tensor. — op_exp","text":"","code":"op_exp(x)"},{"path":"https://keras.posit.co/reference/op_exp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the exponential of all elements in the input tensor. — op_exp","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_exp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the exponential of all elements in the input tensor. — op_exp","text":"","code":"Output tensor, element-wise exponential of `x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_expand_dims.html","id":null,"dir":"Reference","previous_headings":"","what":"Expand the shape of a tensor. — op_expand_dims","title":"Expand the shape of a tensor. — op_expand_dims","text":"Insert new axis axis position expanded tensor shape.","code":""},{"path":"https://keras.posit.co/reference/op_expand_dims.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Expand the shape of a tensor. — op_expand_dims","text":"","code":"op_expand_dims(x, axis)"},{"path":"https://keras.posit.co/reference/op_expand_dims.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Expand the shape of a tensor. — op_expand_dims","text":"x Input tensor. axis Position expanded axes new axis (axes) placed.","code":""},{"path":"https://keras.posit.co/reference/op_expand_dims.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Expand the shape of a tensor. — op_expand_dims","text":"","code":"Output tensor with the number of dimensions increased."},{"path":[]},{"path":"https://keras.posit.co/reference/op_expm1.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate exp(x) - 1 for all elements in the tensor. — op_expm1","title":"Calculate exp(x) - 1 for all elements in the tensor. — op_expm1","text":"Calculate exp(x) - 1 elements tensor.","code":""},{"path":"https://keras.posit.co/reference/op_expm1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate exp(x) - 1 for all elements in the tensor. — op_expm1","text":"","code":"op_expm1(x)"},{"path":"https://keras.posit.co/reference/op_expm1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate exp(x) - 1 for all elements in the tensor. — op_expm1","text":"x Input values.","code":""},{"path":"https://keras.posit.co/reference/op_expm1.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate exp(x) - 1 for all elements in the tensor. — op_expm1","text":"","code":"Output tensor, element-wise exponential minus one."},{"path":[]},{"path":"https://keras.posit.co/reference/op_extract_sequences.html","id":null,"dir":"Reference","previous_headings":"","what":"Expands the dimension of last axis into sequences of sequence_length. — op_extract_sequences","title":"Expands the dimension of last axis into sequences of sequence_length. — op_extract_sequences","text":"Slides window size sequence_length last axis input stride sequence_stride, replacing last axis [num_sequences, sequence_length] sequences. dimension along last axis N, number sequences can computed : num_sequences = 1 + (N - sequence_length) // sequence_stride","code":""},{"path":"https://keras.posit.co/reference/op_extract_sequences.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Expands the dimension of last axis into sequences of sequence_length. — op_extract_sequences","text":"","code":"op_extract_sequences(x, sequence_length, sequence_stride)"},{"path":"https://keras.posit.co/reference/op_extract_sequences.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Expands the dimension of last axis into sequences of sequence_length. — op_extract_sequences","text":"x Input tensor. sequence_length integer representing sequences length. sequence_stride integer representing sequences hop size.","code":""},{"path":"https://keras.posit.co/reference/op_extract_sequences.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Expands the dimension of last axis into sequences of sequence_length. — op_extract_sequences","text":"tensor sequences shape [..., num_sequences, sequence_length].","code":""},{"path":"https://keras.posit.co/reference/op_extract_sequences.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Expands the dimension of last axis into sequences of sequence_length. — op_extract_sequences","text":"","code":"x <- op_convert_to_tensor(1:6) op_extract_sequences(x, 3, 2) ## tf.Tensor( ## [[1 2 3] ##  [3 4 5]], shape=(2, 3), dtype=int32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_eye.html","id":null,"dir":"Reference","previous_headings":"","what":"Return a 2-D tensor with ones on the diagonal and zeros elsewhere. — op_eye","title":"Return a 2-D tensor with ones on the diagonal and zeros elsewhere. — op_eye","text":"Return 2-D tensor ones diagonal zeros elsewhere.","code":""},{"path":"https://keras.posit.co/reference/op_eye.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return a 2-D tensor with ones on the diagonal and zeros elsewhere. — op_eye","text":"","code":"op_eye(N, M = NULL, k = 0L, dtype = NULL)"},{"path":"https://keras.posit.co/reference/op_eye.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return a 2-D tensor with ones on the diagonal and zeros elsewhere. — op_eye","text":"N Number rows output. M Number columns output. NULL, defaults N. k Index diagonal: 0 (default) refers main diagonal, positive value refers upper diagonal, negative value lower diagonal. dtype Data type returned tensor.","code":""},{"path":"https://keras.posit.co/reference/op_eye.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return a 2-D tensor with ones on the diagonal and zeros elsewhere. — op_eye","text":"","code":"Tensor with ones on the k-th diagonal and zeros elsewhere."},{"path":[]},{"path":"https://keras.posit.co/reference/op_fft.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the Fast Fourier Transform along last axis of input. — op_fft","title":"Computes the Fast Fourier Transform along last axis of input. — op_fft","text":"Computes Fast Fourier Transform along last axis input.","code":""},{"path":"https://keras.posit.co/reference/op_fft.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the Fast Fourier Transform along last axis of input. — op_fft","text":"","code":"op_fft(x)"},{"path":"https://keras.posit.co/reference/op_fft.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the Fast Fourier Transform along last axis of input. — op_fft","text":"x list real imaginary parts input tensor. tensors provided floating type.","code":""},{"path":"https://keras.posit.co/reference/op_fft.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the Fast Fourier Transform along last axis of input. — op_fft","text":"list containing two tensors - real imaginary parts output tensor.","code":""},{"path":"https://keras.posit.co/reference/op_fft.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the Fast Fourier Transform along last axis of input. — op_fft","text":"","code":"x = c(op_array(c(1., 2.)),       op_array(c(0., 1.))) op_fft(x) ## [[1]] ## tf.Tensor([ 3. -1.], shape=(2), dtype=float32) ## ## [[2]] ## tf.Tensor([ 1. -1.], shape=(2), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_fft2.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the 2D Fast Fourier Transform along the last two axes of input. — op_fft2","title":"Computes the 2D Fast Fourier Transform along the last two axes of input. — op_fft2","text":"Computes 2D Fast Fourier Transform along last two axes input.","code":""},{"path":"https://keras.posit.co/reference/op_fft2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the 2D Fast Fourier Transform along the last two axes of input. — op_fft2","text":"","code":"op_fft2(x)"},{"path":"https://keras.posit.co/reference/op_fft2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the 2D Fast Fourier Transform along the last two axes of input. — op_fft2","text":"x list real imaginary parts input tensor. tensors provided floating type.","code":""},{"path":"https://keras.posit.co/reference/op_fft2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the 2D Fast Fourier Transform along the last two axes of input. — op_fft2","text":"list containing two tensors - real imaginary parts output.","code":""},{"path":"https://keras.posit.co/reference/op_fft2.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the 2D Fast Fourier Transform along the last two axes of input. — op_fft2","text":"","code":"x <- c(op_array(rbind(c(1, 2),                      c(2, 1))),        op_array(rbind(c(0, 1),                      c(1, 0)))) op_fft2(x) ## [[1]] ## tf.Tensor( ## [[ 6.  0.] ##  [ 0. -2.]], shape=(2, 2), dtype=float64) ## ## [[2]] ## tf.Tensor( ## [[ 2.  0.] ##  [ 0. -2.]], shape=(2, 2), dtype=float64)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_flip.html","id":null,"dir":"Reference","previous_headings":"","what":"Reverse the order of elements in the tensor along the given axis. — op_flip","title":"Reverse the order of elements in the tensor along the given axis. — op_flip","text":"shape tensor preserved, elements reordered.","code":""},{"path":"https://keras.posit.co/reference/op_flip.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reverse the order of elements in the tensor along the given axis. — op_flip","text":"","code":"op_flip(x, axis = NULL)"},{"path":"https://keras.posit.co/reference/op_flip.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reverse the order of elements in the tensor along the given axis. — op_flip","text":"x Input tensor. axis Axis axes along flip tensor. default, axis = NULL, flip axes input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_flip.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reverse the order of elements in the tensor along the given axis. — op_flip","text":"","code":"Output tensor with entries of `axis` reversed."},{"path":[]},{"path":"https://keras.posit.co/reference/op_floor.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the floor of the input, element-wise. — op_floor","title":"Return the floor of the input, element-wise. — op_floor","text":"floor scalar x largest integer , <= x.","code":""},{"path":"https://keras.posit.co/reference/op_floor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the floor of the input, element-wise. — op_floor","text":"","code":"op_floor(x)"},{"path":"https://keras.posit.co/reference/op_floor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the floor of the input, element-wise. — op_floor","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_floor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the floor of the input, element-wise. — op_floor","text":"","code":"Output tensor, element-wise floor of `x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_floor_divide.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the largest integer smaller or equal to the division of inputs. — op_floor_divide","title":"Returns the largest integer smaller or equal to the division of inputs. — op_floor_divide","text":"Returns largest integer smaller equal division inputs.","code":""},{"path":"https://keras.posit.co/reference/op_floor_divide.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the largest integer smaller or equal to the division of inputs. — op_floor_divide","text":"","code":"op_floor_divide(x1, x2)"},{"path":"https://keras.posit.co/reference/op_floor_divide.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the largest integer smaller or equal to the division of inputs. — op_floor_divide","text":"x1 Numerator. x2 Denominator.","code":""},{"path":"https://keras.posit.co/reference/op_floor_divide.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the largest integer smaller or equal to the division of inputs. — op_floor_divide","text":"Output tensor, y <- floor(x1/x2)","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/op_fori_loop.html","id":null,"dir":"Reference","previous_headings":"","what":"For loop implementation. — op_fori_loop","title":"For loop implementation. — op_fori_loop","text":"loop implementation.","code":""},{"path":"https://keras.posit.co/reference/op_fori_loop.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"For loop implementation. — op_fori_loop","text":"","code":"op_fori_loop(lower, upper, body_fun, init_val)"},{"path":"https://keras.posit.co/reference/op_fori_loop.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"For loop implementation. — op_fori_loop","text":"lower initial value loop variable. upper upper bound loop variable. body_fun callable represents loop body. Must take two arguments: loop variable loop state. loop state updated returned function. init_val initial value loop state.","code":""},{"path":"https://keras.posit.co/reference/op_fori_loop.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"For loop implementation. — op_fori_loop","text":"final state loop.","code":""},{"path":"https://keras.posit.co/reference/op_fori_loop.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"For loop implementation. — op_fori_loop","text":"","code":"lower <- 0L upper <- 10L body_fun <- function(i, state) state + i init_state <- 0L final_state <- op_fori_loop(lower, upper, body_fun, init_state) final_state ## tf.Tensor(45, shape=(), dtype=int32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_full.html","id":null,"dir":"Reference","previous_headings":"","what":"Return a new tensor of given shape and type, filled with fill_value. — op_full","title":"Return a new tensor of given shape and type, filled with fill_value. — op_full","text":"Return new tensor given shape type, filled fill_value.","code":""},{"path":"https://keras.posit.co/reference/op_full.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return a new tensor of given shape and type, filled with fill_value. — op_full","text":"","code":"op_full(shape, fill_value, dtype = NULL)"},{"path":"https://keras.posit.co/reference/op_full.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return a new tensor of given shape and type, filled with fill_value. — op_full","text":"shape Shape new tensor. fill_value Fill value. dtype Desired data type tensor.","code":""},{"path":"https://keras.posit.co/reference/op_full.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return a new tensor of given shape and type, filled with fill_value. — op_full","text":"Output tensor.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/op_full_like.html","id":null,"dir":"Reference","previous_headings":"","what":"Return a full tensor with the same shape and type as the given tensor. — op_full_like","title":"Return a full tensor with the same shape and type as the given tensor. — op_full_like","text":"Return full tensor shape type given tensor.","code":""},{"path":"https://keras.posit.co/reference/op_full_like.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return a full tensor with the same shape and type as the given tensor. — op_full_like","text":"","code":"op_full_like(x, fill_value, dtype = NULL)"},{"path":"https://keras.posit.co/reference/op_full_like.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return a full tensor with the same shape and type as the given tensor. — op_full_like","text":"x Input tensor. fill_value Fill value. dtype Overrides data type result.","code":""},{"path":"https://keras.posit.co/reference/op_full_like.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return a full tensor with the same shape and type as the given tensor. — op_full_like","text":"","code":"Tensor of `fill_value` with the same shape and type as `x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_gelu.html","id":null,"dir":"Reference","previous_headings":"","what":"Gaussian Error Linear Unit (GELU) activation function. — op_gelu","title":"Gaussian Error Linear Unit (GELU) activation function. — op_gelu","text":"approximate TRUE, defined : f(x) = 0.5 * x * (1 + tanh(sqrt(2 / pi) * (x + 0.044715 * x^3))) approximate FALSE, defined : f(x) = x * P(X <= x) = 0.5 * x * (1 + erf(x / sqrt(2))), P(X) ~ N(0, 1).","code":""},{"path":"https://keras.posit.co/reference/op_gelu.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gaussian Error Linear Unit (GELU) activation function. — op_gelu","text":"","code":"op_gelu(x, approximate = TRUE)"},{"path":"https://keras.posit.co/reference/op_gelu.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gaussian Error Linear Unit (GELU) activation function. — op_gelu","text":"x Input tensor. approximate Approximate version GELU activation. Defaults TRUE.","code":""},{"path":"https://keras.posit.co/reference/op_gelu.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Gaussian Error Linear Unit (GELU) activation function. — op_gelu","text":"tensor shape x.","code":""},{"path":"https://keras.posit.co/reference/op_gelu.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Gaussian Error Linear Unit (GELU) activation function. — op_gelu","text":"","code":"x <- op_array(c(-1., 0., 1.)) op_gelu(x) ## tf.Tensor([-0.15880796  0.          0.841192  ], shape=(3), dtype=float32) op_gelu(x, FALSE) ## tf.Tensor([-0.15865526  0.          0.8413447 ], shape=(3), dtype=float32) x <- seq(-5, 5, .1) plot(x, op_gelu(x),      type = \"l\", #, frame.plot = FALSE,      panel.first = grid())"},{"path":[]},{"path":"https://keras.posit.co/reference/op_get_item.html","id":null,"dir":"Reference","previous_headings":"","what":"Return x[key]. — op_get_item","title":"Return x[key]. — op_get_item","text":"Return x[key].","code":""},{"path":"https://keras.posit.co/reference/op_get_item.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return x[key]. — op_get_item","text":"","code":"op_get_item(x, key)"},{"path":"https://keras.posit.co/reference/op_get_item.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return x[key]. — op_get_item","text":"x see description key see description","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/op_greater.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the truth value of x1 > x2 element-wise. — op_greater","title":"Return the truth value of x1 > x2 element-wise. — op_greater","text":"Return truth value x1 > x2 element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_greater.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the truth value of x1 > x2 element-wise. — op_greater","text":"","code":"op_greater(x1, x2)"},{"path":"https://keras.posit.co/reference/op_greater.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the truth value of x1 > x2 element-wise. — op_greater","text":"x1 First input tensor. x2 Second input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_greater.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the truth value of x1 > x2 element-wise. — op_greater","text":"Output tensor, element-wise comparison x1 x2.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/op_greater_equal.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the truth value of x1 >= x2 element-wise. — op_greater_equal","title":"Return the truth value of x1 >= x2 element-wise. — op_greater_equal","text":"Return truth value x1 >= x2 element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_greater_equal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the truth value of x1 >= x2 element-wise. — op_greater_equal","text":"","code":"op_greater_equal(x1, x2)"},{"path":"https://keras.posit.co/reference/op_greater_equal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the truth value of x1 >= x2 element-wise. — op_greater_equal","text":"x1 First input tensor. x2 Second input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_greater_equal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the truth value of x1 >= x2 element-wise. — op_greater_equal","text":"","code":"Output tensor, element-wise comparison of `x1` and `x2`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_hard_sigmoid.html","id":null,"dir":"Reference","previous_headings":"","what":"Hard sigmoid activation function. — op_hard_sigmoid","title":"Hard sigmoid activation function. — op_hard_sigmoid","text":"defined : 0 x < -2.5, 1 x > 2.5, (0.2 * x) + 0.5 -2.5 <= x <= 2.5.","code":""},{"path":"https://keras.posit.co/reference/op_hard_sigmoid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hard sigmoid activation function. — op_hard_sigmoid","text":"","code":"op_hard_sigmoid(x)"},{"path":"https://keras.posit.co/reference/op_hard_sigmoid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hard sigmoid activation function. — op_hard_sigmoid","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_hard_sigmoid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Hard sigmoid activation function. — op_hard_sigmoid","text":"tensor shape x.","code":""},{"path":"https://keras.posit.co/reference/op_hard_sigmoid.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hard sigmoid activation function. — op_hard_sigmoid","text":"","code":"x <- op_array(c(-1., 0., 1.)) op_hard_sigmoid(x) ## tf.Tensor([0.33333334 0.5        0.6666667 ], shape=(3), dtype=float32) x <- as.array(seq(-5, 5, .1)) plot(x, op_hard_sigmoid(x),      type = 'l', panel.first = grid(), frame.plot = FALSE)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_hstack.html","id":null,"dir":"Reference","previous_headings":"","what":"Stack tensors in sequence horizontally (column wise). — op_hstack","title":"Stack tensors in sequence horizontally (column wise). — op_hstack","text":"equivalent concatenation along first axis 1-D tensors, along second axis tensors.","code":""},{"path":"https://keras.posit.co/reference/op_hstack.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Stack tensors in sequence horizontally (column wise). — op_hstack","text":"","code":"op_hstack(xs)"},{"path":"https://keras.posit.co/reference/op_hstack.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Stack tensors in sequence horizontally (column wise). — op_hstack","text":"xs Sequence tensors.","code":""},{"path":"https://keras.posit.co/reference/op_hstack.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Stack tensors in sequence horizontally (column wise). — op_hstack","text":"","code":"The tensor formed by stacking the given tensors."},{"path":[]},{"path":"https://keras.posit.co/reference/op_identity.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the identity tensor. — op_identity","title":"Return the identity tensor. — op_identity","text":"identity tensor square tensor ones main diagonal zeros elsewhere.","code":""},{"path":"https://keras.posit.co/reference/op_identity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the identity tensor. — op_identity","text":"","code":"op_identity(n, dtype = NULL)"},{"path":"https://keras.posit.co/reference/op_identity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the identity tensor. — op_identity","text":"n Number rows (columns) n x n output tensor. dtype Data type output tensor.","code":""},{"path":"https://keras.posit.co/reference/op_identity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the identity tensor. — op_identity","text":"","code":"The identity tensor."},{"path":[]},{"path":"https://keras.posit.co/reference/op_imag.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the imaginary part of the complex argument. — op_imag","title":"Return the imaginary part of the complex argument. — op_imag","text":"Return imaginary part complex argument.","code":""},{"path":"https://keras.posit.co/reference/op_imag.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the imaginary part of the complex argument. — op_imag","text":"","code":"op_imag(x)"},{"path":"https://keras.posit.co/reference/op_imag.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the imaginary part of the complex argument. — op_imag","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_imag.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the imaginary part of the complex argument. — op_imag","text":"","code":"The imaginary component of the complex argument."},{"path":[]},{"path":"https://keras.posit.co/reference/op_image_affine_transform.html","id":null,"dir":"Reference","previous_headings":"","what":"Applies the given transform(s) to the image(s). — op_image_affine_transform","title":"Applies the given transform(s) to the image(s). — op_image_affine_transform","text":"Applies given transform(s) image(s).","code":""},{"path":"https://keras.posit.co/reference/op_image_affine_transform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Applies the given transform(s) to the image(s). — op_image_affine_transform","text":"","code":"op_image_affine_transform(   image,   transform,   interpolation = \"bilinear\",   fill_mode = \"constant\",   fill_value = 0L,   data_format = \"channels_last\" )"},{"path":"https://keras.posit.co/reference/op_image_affine_transform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Applies the given transform(s) to the image(s). — op_image_affine_transform","text":"image Input image batch images. Must 3D 4D. transform Projective transform matrix/matrices. vector length 8 tensor size N x 8. one row transform [a0, a1, a2, b0, b1, b2, c0, c1], maps output point (x, y) transformed input point (x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k), k = c0 x + c1 y + 1. transform inverted compared transform mapping input points output points. Note gradients backpropagated transformation parameters. Note c0 c1 effective using TensorFlow backend considered 0 using backends. interpolation Interpolation method. Available methods \"nearest\", \"bilinear\". Defaults \"bilinear\". fill_mode Points outside boundaries input filled according given mode. Available methods \"constant\", \"nearest\", \"wrap\" \"reflect\". Defaults \"constant\". \"reflect\": (d c b | b c d | d c b ) input extended reflecting edge last pixel. \"constant\": (k k k k | b c d | k k k k) input extended filling values beyond edge constant value k specified fill_value. \"wrap\": (b c d | b c d | b c d) input extended wrapping around opposite edge. \"nearest\": (| b c d | d d d d) input extended nearest pixel. fill_value Value used points outside boundaries input fill_mode = \"constant\". Defaults 0. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, height, width, channels) \"channels_first\" corresponds inputs shape (batch, channels, height, weight). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\".","code":""},{"path":"https://keras.posit.co/reference/op_image_affine_transform.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Applies the given transform(s) to the image(s). — op_image_affine_transform","text":"Applied affine transform image batch images.","code":""},{"path":"https://keras.posit.co/reference/op_image_affine_transform.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Applies the given transform(s) to the image(s). — op_image_affine_transform","text":"","code":"x <- random_uniform(c(2, 64, 80, 3)) # batch of 2 RGB images transform <- op_array(rbind(c(1.5, 0, -20, 0, 1.5, -16, 0, 0),  # zoom                            c(1, 0, -20, 0, 1, -16, 0, 0)))  # translation)) y <- op_image_affine_transform(x, transform) shape(y) ## shape(2, 64, 80, 3) # (2, 64, 80, 3) x <- random_uniform(c(64, 80, 3)) # single RGB image transform <- op_array(c(1.0, 0.5, -20, 0.5, 1.0, -16, 0, 0))  # shear y <- op_image_affine_transform(x, transform) shape(y) ## shape(64, 80, 3) # (64, 80, 3) x <- random_uniform(c(2, 3, 64, 80)) # batch of 2 RGB images transform <- op_array(rbind(   c(1.5, 0,-20, 0, 1.5,-16, 0, 0),  # zoom   c(1, 0,-20, 0, 1,-16, 0, 0)  # translation )) y <- op_image_affine_transform(x, transform, data_format = \"channels_first\") shape(y) ## shape(2, 3, 64, 80) # (2, 3, 64, 80)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_image_extract_patches.html","id":null,"dir":"Reference","previous_headings":"","what":"Extracts patches from the image(s). — op_image_extract_patches","title":"Extracts patches from the image(s). — op_image_extract_patches","text":"Extracts patches image(s).","code":""},{"path":"https://keras.posit.co/reference/op_image_extract_patches.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extracts patches from the image(s). — op_image_extract_patches","text":"","code":"op_image_extract_patches(   image,   size,   strides = NULL,   dilation_rate = 1L,   padding = \"valid\",   data_format = \"channels_last\" )"},{"path":"https://keras.posit.co/reference/op_image_extract_patches.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extracts patches from the image(s). — op_image_extract_patches","text":"image Input image batch images. Must 3D 4D. size Patch size int list (patch_height, patch_width) strides strides along height width. specified, NULL, defaults value size. dilation_rate input stride, specifying far two consecutive patch samples input. value 1, strides must 1. NOTE: strides > 1 supported conjunction dilation_rate > 1 padding type padding algorithm use: \"\" \"valid\". data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, height, width, channels) \"channels_first\" corresponds inputs shape (batch, channels, height, weight). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\".","code":""},{"path":"https://keras.posit.co/reference/op_image_extract_patches.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extracts patches from the image(s). — op_image_extract_patches","text":"Extracted patches 3D (batched) 4D (batched)","code":""},{"path":"https://keras.posit.co/reference/op_image_extract_patches.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extracts patches from the image(s). — op_image_extract_patches","text":"","code":"image <- random_uniform(c(2, 20, 20, 3), dtype = \"float32\") # batch of 2 RGB images patches <- op_image_extract_patches(image, c(5, 5)) shape(patches) ## shape(2, 4, 4, 75) # (2, 4, 4, 75) image <- random_uniform(c(20, 20, 3), dtype = \"float32\") # 1 RGB image patches <- op_image_extract_patches(image, c(3, 3), c(1, 1)) shape(patches) ## shape(18, 18, 27) # (18, 18, 27)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_image_map_coordinates.html","id":null,"dir":"Reference","previous_headings":"","what":"Map the input array to new coordinates by interpolation.. — op_image_map_coordinates","title":"Map the input array to new coordinates by interpolation.. — op_image_map_coordinates","text":"Note interpolation near boundaries differs scipy function, fixed outstanding bug scipy/issues/2640.","code":""},{"path":"https://keras.posit.co/reference/op_image_map_coordinates.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Map the input array to new coordinates by interpolation.. — op_image_map_coordinates","text":"","code":"op_image_map_coordinates(   input,   coordinates,   order,   fill_mode = \"constant\",   fill_value = 0L )"},{"path":"https://keras.posit.co/reference/op_image_map_coordinates.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Map the input array to new coordinates by interpolation.. — op_image_map_coordinates","text":"input input array. coordinates coordinates input evaluated. order order spline interpolation. order must 0 1. 0 indicates nearest neighbor 1 indicates linear interpolation. fill_mode Points outside boundaries input filled according given mode. Available methods \"constant\", \"nearest\", \"wrap\" \"mirror\" \"reflect\". Defaults \"constant\". \"constant\": (k k k k | b c d | k k k k) input extended filling values beyond edge constant value k specified fill_value. \"nearest\": (| b c d | d d d d) input extended nearest pixel. \"wrap\": (b c d | b c d | b c d) input extended wrapping around opposite edge. \"mirror\": (c d c b | b c d | c b b) input extended mirroring edge. \"reflect\": (d c b | b c d | d c b ) input extended reflecting edge last pixel. fill_value Value used points outside boundaries input fill_mode = \"constant\". Defaults 0.","code":""},{"path":"https://keras.posit.co/reference/op_image_map_coordinates.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Map the input array to new coordinates by interpolation.. — op_image_map_coordinates","text":"","code":"Output image or batch of images."},{"path":[]},{"path":"https://keras.posit.co/reference/op_image_pad_images.html","id":null,"dir":"Reference","previous_headings":"","what":"Pad images with zeros to the specified height and width. — op_image_pad_images","title":"Pad images with zeros to the specified height and width. — op_image_pad_images","text":"Pad images zeros specified height width.","code":""},{"path":"https://keras.posit.co/reference/op_image_pad_images.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pad images with zeros to the specified height and width. — op_image_pad_images","text":"","code":"op_image_pad_images(   images,   top_padding = NULL,   left_padding = NULL,   target_height = NULL,   target_width = NULL,   bottom_padding = NULL,   right_padding = NULL )"},{"path":"https://keras.posit.co/reference/op_image_pad_images.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pad images with zeros to the specified height and width. — op_image_pad_images","text":"images 4D Tensor shape (batch, height, width, channels) 3D Tensor shape (height, width, channels). top_padding Number rows zeros add top. left_padding Number columns zeros add left. target_height Height output images. target_width Width output images. bottom_padding Number rows zeros add bottom. right_padding Number columns zeros add right.","code":""},{"path":"https://keras.posit.co/reference/op_image_pad_images.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pad images with zeros to the specified height and width. — op_image_pad_images","text":"images 4D, 4D float Tensor shape (batch, target_height, target_width, channels) images 3D, 3D float Tensor shape (target_height, target_width, channels)","code":""},{"path":"https://keras.posit.co/reference/op_image_pad_images.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Pad images with zeros to the specified height and width. — op_image_pad_images","text":"","code":"images <- random_uniform(c(15, 25, 3)) padded_images <- op_image_pad_images(     images, 2, 3, target_height = 20, target_width = 30 ) shape(padded_images) ## shape(20, 30, 3) batch_images <- random_uniform(c(2, 15, 25, 3)) padded_batch <- op_image_pad_images(batch_images, 2, 3,                                    target_height = 20,                                    target_width = 30) shape(padded_batch) ## shape(2, 20, 30, 3)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_image_resize.html","id":null,"dir":"Reference","previous_headings":"","what":"Resize images to size using the specified interpolation method. — op_image_resize","title":"Resize images to size using the specified interpolation method. — op_image_resize","text":"Resize images size using specified interpolation method.","code":""},{"path":"https://keras.posit.co/reference/op_image_resize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Resize images to size using the specified interpolation method. — op_image_resize","text":"","code":"op_image_resize(   image,   size,   interpolation = \"bilinear\",   antialias = FALSE,   data_format = \"channels_last\" )"},{"path":"https://keras.posit.co/reference/op_image_resize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Resize images to size using the specified interpolation method. — op_image_resize","text":"image Input image batch images. Must 3D 4D. size Size output image (height, width) format. interpolation Interpolation method. Available methods \"nearest\", \"bilinear\", \"bicubic\". Defaults \"bilinear\". antialias Whether use antialiasing filter downsampling image. Defaults FALSE. data_format string, either \"channels_last\" \"channels_first\". ordering dimensions inputs. \"channels_last\" corresponds inputs shape (batch, height, width, channels) \"channels_first\" corresponds inputs shape (batch, channels, height, weight). defaults image_data_format value found Keras config file ~/.keras/keras.json. never set , \"channels_last\".","code":""},{"path":"https://keras.posit.co/reference/op_image_resize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Resize images to size using the specified interpolation method. — op_image_resize","text":"Resized image batch images.","code":""},{"path":"https://keras.posit.co/reference/op_image_resize.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Resize images to size using the specified interpolation method. — op_image_resize","text":"","code":"x <- random_uniform(c(2, 4, 4, 3)) # batch of 2 RGB images y <- op_image_resize(x, c(2, 2)) shape(y) ## shape(2, 2, 2, 3) x <- random_uniform(c(4, 4, 3)) # single RGB image y <- op_image_resize(x, c(2, 2)) shape(y) ## shape(2, 2, 3) x <- random_uniform(c(2, 3, 4, 4)) # batch of 2 RGB images y <- op_image_resize(x, c(2, 2), data_format = \"channels_first\") shape(y) ## shape(2, 3, 2, 2)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_in_top_k.html","id":null,"dir":"Reference","previous_headings":"","what":"Checks if the targets are in the top-k predictions. — op_in_top_k","title":"Checks if the targets are in the top-k predictions. — op_in_top_k","text":"Checks targets top-k predictions.","code":""},{"path":"https://keras.posit.co/reference/op_in_top_k.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Checks if the targets are in the top-k predictions. — op_in_top_k","text":"","code":"op_in_top_k(targets, predictions, k)"},{"path":"https://keras.posit.co/reference/op_in_top_k.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Checks if the targets are in the top-k predictions. — op_in_top_k","text":"targets tensor true labels. predictions tensor predicted labels. k integer representing number predictions consider.","code":""},{"path":"https://keras.posit.co/reference/op_in_top_k.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Checks if the targets are in the top-k predictions. — op_in_top_k","text":"boolean tensor shape targets, element indicates whether corresponding target top-k predictions.","code":""},{"path":"https://keras.posit.co/reference/op_in_top_k.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Checks if the targets are in the top-k predictions. — op_in_top_k","text":"","code":"targets <- op_array(c(2, 5, 3), \"int32\") predictions <- op_array(dtype = \"float32\", rbind(   c(0.1, 0.4, 0.6, 0.9, 0.5),   c(0.1, 0.7, 0.9, 0.8, 0.3),   c(0.1, 0.6, 0.9, 0.9, 0.5) )) op_in_top_k(targets, predictions, k = 3L) ## tf.Tensor([ True False  True], shape=(3), dtype=bool)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_irfft.html","id":null,"dir":"Reference","previous_headings":"","what":"Inverse real-valued Fast Fourier transform along the last axis. — op_irfft","title":"Inverse real-valued Fast Fourier transform along the last axis. — op_irfft","text":"Computes inverse 1D Discrete Fourier Transform real-valued signal inner-dimension input. inner-dimension input assumed result RFFT: fft_length / 2 + 1 unique components DFT real-valued signal. fft_length provided, computed size inner-dimension input (fft_length = 2 * (inner - 1)). FFT length used compute odd, provided since inferred properly. Along axis IRFFT computed , fft_length / 2 + 1 smaller corresponding dimension input, dimension cropped. larger, dimension padded zeros.","code":""},{"path":"https://keras.posit.co/reference/op_irfft.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Inverse real-valued Fast Fourier transform along the last axis. — op_irfft","text":"","code":"op_irfft(x, fft_length = NULL)"},{"path":"https://keras.posit.co/reference/op_irfft.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Inverse real-valued Fast Fourier transform along the last axis. — op_irfft","text":"x List real imaginary parts input tensor. tensors list floating type. fft_length integer representing number fft length. specified, inferred length last axis x. Defaults NULL.","code":""},{"path":"https://keras.posit.co/reference/op_irfft.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Inverse real-valued Fast Fourier transform along the last axis. — op_irfft","text":"tensor containing inverse real-valued Fast Fourier Transform along last axis x.","code":""},{"path":"https://keras.posit.co/reference/op_irfft.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Inverse real-valued Fast Fourier transform along the last axis. — op_irfft","text":"","code":"real <- op_array(c(0, 1, 2, 3, 4)) imag <- op_array(c(0, 1, 2, 3, 4)) op_irfft(c(real, imag)) #> tf.Tensor( #> [ 2.         -2.0606601   0.5        -0.35355338  0.          0.06066012 #>  -0.5         0.35355338], shape=(8), dtype=float32) all.equal(op_irfft(op_rfft(real, 5), 5), real) #> [1] TRUE"},{"path":[]},{"path":"https://keras.posit.co/reference/op_is_tensor.html","id":null,"dir":"Reference","previous_headings":"","what":"Check whether the given object is a tensor. — op_is_tensor","title":"Check whether the given object is a tensor. — op_is_tensor","text":"Check whether given object tensor.","code":""},{"path":"https://keras.posit.co/reference/op_is_tensor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check whether the given object is a tensor. — op_is_tensor","text":"","code":"op_is_tensor(x)"},{"path":"https://keras.posit.co/reference/op_is_tensor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check whether the given object is a tensor. — op_is_tensor","text":"x variable.","code":""},{"path":"https://keras.posit.co/reference/op_is_tensor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check whether the given object is a tensor. — op_is_tensor","text":"TRUE x tensor, otherwise FALSE.","code":""},{"path":"https://keras.posit.co/reference/op_is_tensor.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Check whether the given object is a tensor. — op_is_tensor","text":"checks backend specific tensors passing TensorFlow tensor return FALSE backend PyTorch JAX.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/op_isclose.html","id":null,"dir":"Reference","previous_headings":"","what":"Return whether two tensors are element-wise almost equal. — op_isclose","title":"Return whether two tensors are element-wise almost equal. — op_isclose","text":"Return whether two tensors element-wise almost equal.","code":""},{"path":"https://keras.posit.co/reference/op_isclose.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return whether two tensors are element-wise almost equal. — op_isclose","text":"","code":"op_isclose(x1, x2)"},{"path":"https://keras.posit.co/reference/op_isclose.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return whether two tensors are element-wise almost equal. — op_isclose","text":"x1 First input tensor. x2 Second input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_isclose.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return whether two tensors are element-wise almost equal. — op_isclose","text":"","code":"Output boolean tensor."},{"path":[]},{"path":"https://keras.posit.co/reference/op_isfinite.html","id":null,"dir":"Reference","previous_headings":"","what":"Return whether a tensor is finite, element-wise. — op_isfinite","title":"Return whether a tensor is finite, element-wise. — op_isfinite","text":"Real values finite NaN, positive infinity, negative infinity. Complex values finite real imaginary parts finite.","code":""},{"path":"https://keras.posit.co/reference/op_isfinite.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return whether a tensor is finite, element-wise. — op_isfinite","text":"","code":"op_isfinite(x)"},{"path":"https://keras.posit.co/reference/op_isfinite.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return whether a tensor is finite, element-wise. — op_isfinite","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_isfinite.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return whether a tensor is finite, element-wise. — op_isfinite","text":"","code":"Output boolean tensor."},{"path":[]},{"path":"https://keras.posit.co/reference/op_isinf.html","id":null,"dir":"Reference","previous_headings":"","what":"Test element-wise for positive or negative infinity. — op_isinf","title":"Test element-wise for positive or negative infinity. — op_isinf","text":"Test element-wise positive negative infinity.","code":""},{"path":"https://keras.posit.co/reference/op_isinf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test element-wise for positive or negative infinity. — op_isinf","text":"","code":"op_isinf(x)"},{"path":"https://keras.posit.co/reference/op_isinf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test element-wise for positive or negative infinity. — op_isinf","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_isinf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test element-wise for positive or negative infinity. — op_isinf","text":"","code":"Output boolean tensor."},{"path":[]},{"path":"https://keras.posit.co/reference/op_isnan.html","id":null,"dir":"Reference","previous_headings":"","what":"Test element-wise for NaN and return result as a boolean tensor. — op_isnan","title":"Test element-wise for NaN and return result as a boolean tensor. — op_isnan","text":"Test element-wise NaN return result boolean tensor.","code":""},{"path":"https://keras.posit.co/reference/op_isnan.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test element-wise for NaN and return result as a boolean tensor. — op_isnan","text":"","code":"op_isnan(x)"},{"path":"https://keras.posit.co/reference/op_isnan.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test element-wise for NaN and return result as a boolean tensor. — op_isnan","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_isnan.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test element-wise for NaN and return result as a boolean tensor. — op_isnan","text":"","code":"Output boolean tensor."},{"path":[]},{"path":"https://keras.posit.co/reference/op_istft.html","id":null,"dir":"Reference","previous_headings":"","what":"Inverse Short-Time Fourier Transform along the last axis of the input. — op_istft","title":"Inverse Short-Time Fourier Transform along the last axis of the input. — op_istft","text":"reconstruct original waveform, parameters stft.","code":""},{"path":"https://keras.posit.co/reference/op_istft.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Inverse Short-Time Fourier Transform along the last axis of the input. — op_istft","text":"","code":"op_istft(   x,   sequence_length,   sequence_stride,   fft_length,   length = NULL,   window = \"hann\",   center = TRUE )"},{"path":"https://keras.posit.co/reference/op_istft.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Inverse Short-Time Fourier Transform along the last axis of the input. — op_istft","text":"x Tuple real imaginary parts input tensor. tensors list floating type. sequence_length integer representing sequence length. sequence_stride integer representing sequence hop size. fft_length integer representing size FFT produced stft. length integer representing output clipped exactly length. specified, padding clipping take place. Defaults NULL. window string, tensor window NULL. window string, available values \"hann\" \"hamming\". window tensor, used directly window length must sequence_length. window NULL, windowing used. Defaults \"hann\". center Whether x padded sides t-th sequence centered time t * sequence_stride. Defaults TRUE.","code":""},{"path":"https://keras.posit.co/reference/op_istft.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Inverse Short-Time Fourier Transform along the last axis of the input. — op_istft","text":"tensor containing inverse Short-Time Fourier Transform along last axis x.","code":""},{"path":"https://keras.posit.co/reference/op_istft.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Inverse Short-Time Fourier Transform along the last axis of the input. — op_istft","text":"","code":"x <- op_convert_to_tensor(c(0, 1, 2, 3, 4)) op_istft(op_stft(x, 1, 1, 1), 1, 1, 1) ## tf.Tensor([], shape=(0), dtype=float32) # array([0.0, 1.0, 2.0, 3.0, 4.0])"},{"path":[]},{"path":"https://keras.posit.co/reference/op_leaky_relu.html","id":null,"dir":"Reference","previous_headings":"","what":"Leaky version of a Rectified Linear Unit activation function. — op_leaky_relu","title":"Leaky version of a Rectified Linear Unit activation function. — op_leaky_relu","text":"allows small gradient unit active, defined : f(x) = alpha * x x < 0 f(x) = x x >= 0.","code":""},{"path":"https://keras.posit.co/reference/op_leaky_relu.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Leaky version of a Rectified Linear Unit activation function. — op_leaky_relu","text":"","code":"op_leaky_relu(x, negative_slope = 0.2)"},{"path":"https://keras.posit.co/reference/op_leaky_relu.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Leaky version of a Rectified Linear Unit activation function. — op_leaky_relu","text":"x Input tensor. negative_slope Slope activation function x < 0. Defaults 0.2.","code":""},{"path":"https://keras.posit.co/reference/op_leaky_relu.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Leaky version of a Rectified Linear Unit activation function. — op_leaky_relu","text":"tensor shape x.","code":""},{"path":"https://keras.posit.co/reference/op_leaky_relu.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Leaky version of a Rectified Linear Unit activation function. — op_leaky_relu","text":"","code":"x <- op_array(c(-1., 0., 1.)) op_leaky_relu(x) ## tf.Tensor([-0.2  0.   1. ], shape=(3), dtype=float32) # array([-0.2,  0. ,  1. ], shape=(3,), dtype=float64) x <- seq(-5, 5, .1) plot(x, op_leaky_relu(x),      type = 'l', panel.first = grid())"},{"path":[]},{"path":"https://keras.posit.co/reference/op_less.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the truth value of x1 < x2 element-wise. — op_less","title":"Return the truth value of x1 < x2 element-wise. — op_less","text":"Return truth value x1 < x2 element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_less.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the truth value of x1 < x2 element-wise. — op_less","text":"","code":"op_less(x1, x2)"},{"path":"https://keras.posit.co/reference/op_less.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the truth value of x1 < x2 element-wise. — op_less","text":"x1 First input tensor. x2 Second input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_less.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the truth value of x1 < x2 element-wise. — op_less","text":"","code":"Output tensor, element-wise comparison of `x1` and `x2`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_less_equal.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the truth value of x1 <= x2 element-wise. — op_less_equal","title":"Return the truth value of x1 <= x2 element-wise. — op_less_equal","text":"Return truth value x1 <= x2 element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_less_equal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the truth value of x1 <= x2 element-wise. — op_less_equal","text":"","code":"op_less_equal(x1, x2)"},{"path":"https://keras.posit.co/reference/op_less_equal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the truth value of x1 <= x2 element-wise. — op_less_equal","text":"x1 First input tensor. x2 Second input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_less_equal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the truth value of x1 <= x2 element-wise. — op_less_equal","text":"","code":"Output tensor, element-wise comparison of `x1` and `x2`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_linspace.html","id":null,"dir":"Reference","previous_headings":"","what":"Return evenly spaced numbers over a specified interval. — op_linspace","title":"Return evenly spaced numbers over a specified interval. — op_linspace","text":"Returns num evenly spaced samples, calculated interval [start, stop]. endpoint interval can optionally excluded.","code":""},{"path":"https://keras.posit.co/reference/op_linspace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return evenly spaced numbers over a specified interval. — op_linspace","text":"","code":"op_linspace(   start,   stop,   num = 50L,   endpoint = TRUE,   retstep = FALSE,   dtype = NULL,   axis = 1L )"},{"path":"https://keras.posit.co/reference/op_linspace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return evenly spaced numbers over a specified interval. — op_linspace","text":"start starting value sequence. stop end value sequence, unless endpoint set FALSE. case, sequence consists last num + 1 evenly spaced samples, stop excluded. Note step size changes endpoint FALSE. num Number samples generate. Defaults 50. Must non-negative. endpoint TRUE, stop last sample. Otherwise, included. Defaults toTRUE. retstep TRUE, return (samples, step), step spacing samples. dtype type output tensor. axis axis result store samples. Relevant start stop array-like. Defaults 0.","code":""},{"path":"https://keras.posit.co/reference/op_linspace.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return evenly spaced numbers over a specified interval. — op_linspace","text":"tensor evenly spaced numbers. retstep TRUE, returns (samples, step)","code":""},{"path":"https://keras.posit.co/reference/op_linspace.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Return evenly spaced numbers over a specified interval. — op_linspace","text":"Torch backend support axis argument.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/op_log.html","id":null,"dir":"Reference","previous_headings":"","what":"Natural logarithm, element-wise. — op_log","title":"Natural logarithm, element-wise. — op_log","text":"Natural logarithm, element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_log.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Natural logarithm, element-wise. — op_log","text":"","code":"op_log(x)"},{"path":"https://keras.posit.co/reference/op_log.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Natural logarithm, element-wise. — op_log","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_log.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Natural logarithm, element-wise. — op_log","text":"","code":"Output tensor, element-wise natural logarithm of `x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_log10.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the base 10 logarithm of the input tensor, element-wise. — op_log10","title":"Return the base 10 logarithm of the input tensor, element-wise. — op_log10","text":"Return base 10 logarithm input tensor, element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_log10.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the base 10 logarithm of the input tensor, element-wise. — op_log10","text":"","code":"op_log10(x)"},{"path":"https://keras.posit.co/reference/op_log10.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the base 10 logarithm of the input tensor, element-wise. — op_log10","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_log10.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the base 10 logarithm of the input tensor, element-wise. — op_log10","text":"","code":"Output tensor, element-wise base 10 logarithm of `x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_log1p.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the natural logarithm of one plus the x, element-wise. — op_log1p","title":"Returns the natural logarithm of one plus the x, element-wise. — op_log1p","text":"Calculates log(1 + x).","code":""},{"path":"https://keras.posit.co/reference/op_log1p.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the natural logarithm of one plus the x, element-wise. — op_log1p","text":"","code":"op_log1p(x)"},{"path":"https://keras.posit.co/reference/op_log1p.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the natural logarithm of one plus the x, element-wise. — op_log1p","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_log1p.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the natural logarithm of one plus the x, element-wise. — op_log1p","text":"","code":"Output tensor, element-wise natural logarithm of `1 + x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_log2.html","id":null,"dir":"Reference","previous_headings":"","what":"Base-2 logarithm of x, element-wise. — op_log2","title":"Base-2 logarithm of x, element-wise. — op_log2","text":"Base-2 logarithm x, element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_log2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Base-2 logarithm of x, element-wise. — op_log2","text":"","code":"op_log2(x)"},{"path":"https://keras.posit.co/reference/op_log2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Base-2 logarithm of x, element-wise. — op_log2","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_log2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Base-2 logarithm of x, element-wise. — op_log2","text":"","code":"Output tensor, element-wise base-2 logarithm of `x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_log_sigmoid.html","id":null,"dir":"Reference","previous_headings":"","what":"Logarithm of the sigmoid activation function. — op_log_sigmoid","title":"Logarithm of the sigmoid activation function. — op_log_sigmoid","text":"defined f(x) = log(1 / (1 + exp(-x))).","code":""},{"path":"https://keras.posit.co/reference/op_log_sigmoid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Logarithm of the sigmoid activation function. — op_log_sigmoid","text":"","code":"op_log_sigmoid(x)"},{"path":"https://keras.posit.co/reference/op_log_sigmoid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Logarithm of the sigmoid activation function. — op_log_sigmoid","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_log_sigmoid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Logarithm of the sigmoid activation function. — op_log_sigmoid","text":"tensor shape x.","code":""},{"path":"https://keras.posit.co/reference/op_log_sigmoid.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Logarithm of the sigmoid activation function. — op_log_sigmoid","text":"","code":"x <- op_convert_to_tensor(c(-0.541391, 0.0, 0.50, 5.0)) op_log_sigmoid(x) ## tf.Tensor([-1.0000418  -0.6931472  -0.474077   -0.00671535], shape=(4), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_log_softmax.html","id":null,"dir":"Reference","previous_headings":"","what":"Log-softmax activation function. — op_log_softmax","title":"Log-softmax activation function. — op_log_softmax","text":"defined : f(x) = x - max(x) - log(sum(exp(x - max(x))))","code":""},{"path":"https://keras.posit.co/reference/op_log_softmax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Log-softmax activation function. — op_log_softmax","text":"","code":"op_log_softmax(x, axis = -1L)"},{"path":"https://keras.posit.co/reference/op_log_softmax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Log-softmax activation function. — op_log_softmax","text":"x Input tensor. axis Integer, axis along log-softmax applied. Defaults -1.","code":""},{"path":"https://keras.posit.co/reference/op_log_softmax.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Log-softmax activation function. — op_log_softmax","text":"tensor shape x.","code":""},{"path":"https://keras.posit.co/reference/op_log_softmax.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Log-softmax activation function. — op_log_softmax","text":"","code":"x <- op_array(c(-1., 0., 1.)) op_log_softmax(x) ## tf.Tensor([-2.407606   -1.4076059  -0.40760595], shape=(3), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_logaddexp.html","id":null,"dir":"Reference","previous_headings":"","what":"Logarithm of the sum of exponentiations of the inputs. — op_logaddexp","title":"Logarithm of the sum of exponentiations of the inputs. — op_logaddexp","text":"Calculates log(exp(x1) + exp(x2)).","code":""},{"path":"https://keras.posit.co/reference/op_logaddexp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Logarithm of the sum of exponentiations of the inputs. — op_logaddexp","text":"","code":"op_logaddexp(x1, x2)"},{"path":"https://keras.posit.co/reference/op_logaddexp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Logarithm of the sum of exponentiations of the inputs. — op_logaddexp","text":"x1 Input tensor. x2 Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_logaddexp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Logarithm of the sum of exponentiations of the inputs. — op_logaddexp","text":"Output tensor, element-wise logarithm sum exponentiations inputs.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/op_logical_and.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the element-wise logical AND of the given input tensors. — op_logical_and","title":"Computes the element-wise logical AND of the given input tensors. — op_logical_and","text":"Zeros treated FALSE non-zeros treated TRUE.","code":""},{"path":"https://keras.posit.co/reference/op_logical_and.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the element-wise logical AND of the given input tensors. — op_logical_and","text":"","code":"op_logical_and(x1, x2)"},{"path":"https://keras.posit.co/reference/op_logical_and.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the element-wise logical AND of the given input tensors. — op_logical_and","text":"x1 Input tensor. x2 Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_logical_and.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the element-wise logical AND of the given input tensors. — op_logical_and","text":"","code":"Output tensor, element-wise logical AND of the inputs."},{"path":[]},{"path":"https://keras.posit.co/reference/op_logical_not.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the element-wise NOT of the given input tensor. — op_logical_not","title":"Computes the element-wise NOT of the given input tensor. — op_logical_not","text":"Zeros treated FALSE non-zeros treated TRUE.","code":""},{"path":"https://keras.posit.co/reference/op_logical_not.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the element-wise NOT of the given input tensor. — op_logical_not","text":"","code":"op_logical_not(x)"},{"path":"https://keras.posit.co/reference/op_logical_not.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the element-wise NOT of the given input tensor. — op_logical_not","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_logical_not.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the element-wise NOT of the given input tensor. — op_logical_not","text":"","code":"Output tensor, element-wise logical NOT of the input."},{"path":[]},{"path":"https://keras.posit.co/reference/op_logical_or.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the element-wise logical OR of the given input tensors. — op_logical_or","title":"Computes the element-wise logical OR of the given input tensors. — op_logical_or","text":"Zeros treated FALSE non-zeros treated TRUE.","code":""},{"path":"https://keras.posit.co/reference/op_logical_or.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the element-wise logical OR of the given input tensors. — op_logical_or","text":"","code":"op_logical_or(x1, x2)"},{"path":"https://keras.posit.co/reference/op_logical_or.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the element-wise logical OR of the given input tensors. — op_logical_or","text":"x1 Input tensor. x2 Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_logical_or.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the element-wise logical OR of the given input tensors. — op_logical_or","text":"","code":"Output tensor, element-wise logical OR of the inputs."},{"path":[]},{"path":"https://keras.posit.co/reference/op_logical_xor.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the truth value of x1 XOR x2, element-wise. — op_logical_xor","title":"Compute the truth value of x1 XOR x2, element-wise. — op_logical_xor","text":"Compute truth value x1 XOR x2, element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_logical_xor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the truth value of x1 XOR x2, element-wise. — op_logical_xor","text":"","code":"op_logical_xor(x1, x2)"},{"path":"https://keras.posit.co/reference/op_logical_xor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the truth value of x1 XOR x2, element-wise. — op_logical_xor","text":"x1 First input tensor. x2 Second input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_logical_xor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the truth value of x1 XOR x2, element-wise. — op_logical_xor","text":"","code":"Output boolean tensor."},{"path":[]},{"path":"https://keras.posit.co/reference/op_logspace.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns numbers spaced evenly on a log scale. — op_logspace","title":"Returns numbers spaced evenly on a log scale. — op_logspace","text":"linear space, sequence starts base ** start ends base ** stop (see endpoint ).","code":""},{"path":"https://keras.posit.co/reference/op_logspace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns numbers spaced evenly on a log scale. — op_logspace","text":"","code":"op_logspace(   start,   stop,   num = 50L,   endpoint = TRUE,   base = 10L,   dtype = NULL,   axis = 1L )"},{"path":"https://keras.posit.co/reference/op_logspace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns numbers spaced evenly on a log scale. — op_logspace","text":"start starting value sequence. stop final value sequence, unless endpoint FALSE. case, num + 1 values spaced interval log-space, last (sequence length num) returned. num Number samples generate. Defaults 50. endpoint TRUE, stop last sample. Otherwise, included. Defaults toTRUE. base base log space. Defaults 10. dtype type output tensor. axis axis result store samples. Relevant start stop array-like.","code":""},{"path":"https://keras.posit.co/reference/op_logspace.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns numbers spaced evenly on a log scale. — op_logspace","text":"","code":"A tensor of evenly spaced samples on a log scale."},{"path":"https://keras.posit.co/reference/op_logspace.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Returns numbers spaced evenly on a log scale. — op_logspace","text":"Torch backend support axis argument.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/op_logsumexp.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the logarithm of sum of exponentials of elements in a tensor. — op_logsumexp","title":"Computes the logarithm of sum of exponentials of elements in a tensor. — op_logsumexp","text":"Computes logarithm sum exponentials elements tensor.","code":""},{"path":"https://keras.posit.co/reference/op_logsumexp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the logarithm of sum of exponentials of elements in a tensor. — op_logsumexp","text":"","code":"op_logsumexp(x, axis = NULL, keepdims = FALSE)"},{"path":"https://keras.posit.co/reference/op_logsumexp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the logarithm of sum of exponentials of elements in a tensor. — op_logsumexp","text":"x Input tensor. axis integer list integers specifying axis/axes along compute sum. NULL, sum computed elements. Defaults toNULL. keepdims boolean indicating whether keep dimensions input tensor computing sum. Defaults toFALSE.","code":""},{"path":"https://keras.posit.co/reference/op_logsumexp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the logarithm of sum of exponentials of elements in a tensor. — op_logsumexp","text":"tensor containing logarithm sum exponentials elements x.","code":""},{"path":"https://keras.posit.co/reference/op_logsumexp.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the logarithm of sum of exponentials of elements in a tensor. — op_logsumexp","text":"","code":"x <- op_convert_to_tensor(c(1, 2, 3)) op_logsumexp(x) ## tf.Tensor(3.407606, shape=(), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_matmul.html","id":null,"dir":"Reference","previous_headings":"","what":"Matrix product of two tensors. — op_matmul","title":"Matrix product of two tensors. — op_matmul","text":"tensors 1-dimensional, dot product (scalar) returned. either tensor N-D, N > 2, treated stack matrices residing last two indexes broadcast accordingly. first tensor 1-D, promoted matrix prepending 1 dimensions. matrix multiplication prepended 1 removed. second tensor 1-D, promoted matrix appending 1 dimensions. matrix multiplication appended 1 removed.","code":""},{"path":"https://keras.posit.co/reference/op_matmul.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Matrix product of two tensors. — op_matmul","text":"","code":"op_matmul(x1, x2)"},{"path":"https://keras.posit.co/reference/op_matmul.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Matrix product of two tensors. — op_matmul","text":"x1 First tensor. x2 Second tensor.","code":""},{"path":"https://keras.posit.co/reference/op_matmul.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Matrix product of two tensors. — op_matmul","text":"","code":"Output tensor, matrix product of the inputs."},{"path":[]},{"path":"https://keras.posit.co/reference/op_max.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the maximum of a tensor or maximum along an axis. — op_max","title":"Return the maximum of a tensor or maximum along an axis. — op_max","text":"Return maximum tensor maximum along axis.","code":""},{"path":"https://keras.posit.co/reference/op_max.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the maximum of a tensor or maximum along an axis. — op_max","text":"","code":"op_max(x, axis = NULL, keepdims = FALSE, initial = NULL)"},{"path":"https://keras.posit.co/reference/op_max.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the maximum of a tensor or maximum along an axis. — op_max","text":"x Input tensor. axis Axis axes along operate. default, flattened input used. keepdims set TRUE, axes reduced left result dimensions size one. Defaults toFALSE. initial minimum value output element. Defaults toNULL.","code":""},{"path":"https://keras.posit.co/reference/op_max.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the maximum of a tensor or maximum along an axis. — op_max","text":"","code":"Maximum of `x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_max_pool.html","id":null,"dir":"Reference","previous_headings":"","what":"Max pooling operation. — op_max_pool","title":"Max pooling operation. — op_max_pool","text":"Max pooling operation.","code":""},{"path":"https://keras.posit.co/reference/op_max_pool.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Max pooling operation. — op_max_pool","text":"","code":"op_max_pool(   inputs,   pool_size,   strides = NULL,   padding = \"valid\",   data_format = NULL )"},{"path":"https://keras.posit.co/reference/op_max_pool.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Max pooling operation. — op_max_pool","text":"inputs Tensor rank N+2. inputs shape (batch_size,) + inputs_spatial_shape + (num_channels,) data_format = \"channels_last\", (batch_size, num_channels) + inputs_spatial_shape data_format = \"channels_first\". Pooling happens spatial dimensions . pool_size int tuple/list integers size len(inputs_spatial_shape), specifying size pooling window spatial dimension input tensor. pool_size int, every spatial dimension shares pool_size. strides int tuple/list integers size len(inputs_spatial_shape). stride sliding window spatial dimension input tensor. strides int, every spatial dimension shares strides. padding string, either \"valid\" \"\". \"valid\" means padding applied, \"\" results padding evenly left/right /input output height/width dimension input strides = 1. data_format string, either \"channels_last\" \"channels_first\". data_format determines ordering dimensions inputs. data_format = \"channels_last\", inputs shape (batch_size, ..., channels) data_format = \"channels_first\", inputs shape (batch_size, channels, ...).","code":""},{"path":"https://keras.posit.co/reference/op_max_pool.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Max pooling operation. — op_max_pool","text":"","code":"A tensor of rank N+2, the result of the max pooling operation."},{"path":[]},{"path":"https://keras.posit.co/reference/op_maximum.html","id":null,"dir":"Reference","previous_headings":"","what":"Element-wise maximum of x1 and x2. — op_maximum","title":"Element-wise maximum of x1 and x2. — op_maximum","text":"Element-wise maximum x1 x2.","code":""},{"path":"https://keras.posit.co/reference/op_maximum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Element-wise maximum of x1 and x2. — op_maximum","text":"","code":"op_maximum(x1, x2)"},{"path":"https://keras.posit.co/reference/op_maximum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Element-wise maximum of x1 and x2. — op_maximum","text":"x1 First tensor. x2 Second tensor.","code":""},{"path":"https://keras.posit.co/reference/op_maximum.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Element-wise maximum of x1 and x2. — op_maximum","text":"","code":"Output tensor, element-wise maximum of `x1` and `x2`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_mean.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the arithmetic mean along the specified axes. — op_mean","title":"Compute the arithmetic mean along the specified axes. — op_mean","text":"Compute arithmetic mean along specified axes.","code":""},{"path":"https://keras.posit.co/reference/op_mean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the arithmetic mean along the specified axes. — op_mean","text":"","code":"op_mean(x, axis = NULL, keepdims = FALSE)"},{"path":"https://keras.posit.co/reference/op_mean.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the arithmetic mean along the specified axes. — op_mean","text":"x Input tensor. axis Axis axes along means computed. default compute mean flattened tensor. keepdims set TRUE, axes reduced left result dimensions size one.","code":""},{"path":"https://keras.posit.co/reference/op_mean.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the arithmetic mean along the specified axes. — op_mean","text":"","code":"Output tensor containing the mean values."},{"path":[]},{"path":"https://keras.posit.co/reference/op_median.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the median along the specified axis. — op_median","title":"Compute the median along the specified axis. — op_median","text":"Compute median along specified axis.","code":""},{"path":"https://keras.posit.co/reference/op_median.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the median along the specified axis. — op_median","text":"","code":"op_median(x, axis = NULL, keepdims = FALSE)"},{"path":"https://keras.posit.co/reference/op_median.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the median along the specified axis. — op_median","text":"x Input tensor. axis Axis axes along medians computed. Defaults axis = NULL compute median(s) along flattened version array. keepdims set TRUE, axes reduce left result dimensions size one.","code":""},{"path":"https://keras.posit.co/reference/op_median.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the median along the specified axis. — op_median","text":"","code":"The output tensor."},{"path":[]},{"path":"https://keras.posit.co/reference/op_meshgrid.html","id":null,"dir":"Reference","previous_headings":"","what":"Creates grids of coordinates from coordinate vectors. — op_meshgrid","title":"Creates grids of coordinates from coordinate vectors. — op_meshgrid","text":"Given N 1-D tensors T0, T1, ..., TN-1 inputs corresponding lengths S0, S1, ..., SN-1, creates N N-dimensional tensors G0, G1, ..., GN-1 shape (S0, ..., SN-1) output Gi constructed expanding Ti result shape.","code":""},{"path":"https://keras.posit.co/reference/op_meshgrid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Creates grids of coordinates from coordinate vectors. — op_meshgrid","text":"","code":"op_meshgrid(..., indexing = \"xy\")"},{"path":"https://keras.posit.co/reference/op_meshgrid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creates grids of coordinates from coordinate vectors. — op_meshgrid","text":"... 1-D tensors representing coordinates grid. indexing Cartesian (\"xy\", default) matrix (\"ij\") indexing output.","code":""},{"path":"https://keras.posit.co/reference/op_meshgrid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Creates grids of coordinates from coordinate vectors. — op_meshgrid","text":"Sequence N tensors.","code":""},{"path":"https://keras.posit.co/reference/op_meshgrid.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Creates grids of coordinates from coordinate vectors. — op_meshgrid","text":"","code":"x <- op_array(c(1, 2, 3), \"int32\") y <- op_array(c(4, 5, 6), \"int32\") c(grid_x, grid_y) %<-% op_meshgrid(x, y, indexing = \"ij\") grid_x ## tf.Tensor( ## [[1 1 1] ##  [2 2 2] ##  [3 3 3]], shape=(3, 3), dtype=int32) # array([[1, 1, 1], #        [2, 2, 2], #        [3, 3, 3])) grid_y ## tf.Tensor( ## [[4 5 6] ##  [4 5 6] ##  [4 5 6]], shape=(3, 3), dtype=int32) # array([[4, 5, 6], #        [4, 5, 6], #        [4, 5, 6]))"},{"path":[]},{"path":"https://keras.posit.co/reference/op_min.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the minimum of a tensor or minimum along an axis. — op_min","title":"Return the minimum of a tensor or minimum along an axis. — op_min","text":"Return minimum tensor minimum along axis.","code":""},{"path":"https://keras.posit.co/reference/op_min.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the minimum of a tensor or minimum along an axis. — op_min","text":"","code":"op_min(x, axis = NULL, keepdims = FALSE, initial = NULL)"},{"path":"https://keras.posit.co/reference/op_min.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the minimum of a tensor or minimum along an axis. — op_min","text":"x Input tensor. axis Axis axes along operate. default, flattened input used. keepdims set TRUE, axes reduced left result dimensions size one. Defaults toFALSE. initial maximum value output element. Defaults toNULL.","code":""},{"path":"https://keras.posit.co/reference/op_min.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the minimum of a tensor or minimum along an axis. — op_min","text":"","code":"Minimum of `x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_minimum.html","id":null,"dir":"Reference","previous_headings":"","what":"Element-wise minimum of x1 and x2. — op_minimum","title":"Element-wise minimum of x1 and x2. — op_minimum","text":"Element-wise minimum x1 x2.","code":""},{"path":"https://keras.posit.co/reference/op_minimum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Element-wise minimum of x1 and x2. — op_minimum","text":"","code":"op_minimum(x1, x2)"},{"path":"https://keras.posit.co/reference/op_minimum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Element-wise minimum of x1 and x2. — op_minimum","text":"x1 First tensor. x2 Second tensor.","code":""},{"path":"https://keras.posit.co/reference/op_minimum.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Element-wise minimum of x1 and x2. — op_minimum","text":"","code":"Output tensor, element-wise minimum of `x1` and `x2`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_mod.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the element-wise remainder of division. — op_mod","title":"Returns the element-wise remainder of division. — op_mod","text":"Returns element-wise remainder division.","code":""},{"path":"https://keras.posit.co/reference/op_mod.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the element-wise remainder of division. — op_mod","text":"","code":"op_mod(x1, x2)"},{"path":"https://keras.posit.co/reference/op_mod.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns the element-wise remainder of division. — op_mod","text":"x1 First tensor. x2 Second tensor.","code":""},{"path":"https://keras.posit.co/reference/op_mod.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the element-wise remainder of division. — op_mod","text":"","code":"Output tensor, element-wise remainder of division."},{"path":[]},{"path":"https://keras.posit.co/reference/op_moments.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculates the mean and variance of x. — op_moments","title":"Calculates the mean and variance of x. — op_moments","text":"mean variance calculated aggregating contents x across axes. x 1-D axes = c(1) just mean variance vector.","code":""},{"path":"https://keras.posit.co/reference/op_moments.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates the mean and variance of x. — op_moments","text":"","code":"op_moments(x, axes, keepdims = FALSE, synchronized = FALSE)"},{"path":"https://keras.posit.co/reference/op_moments.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculates the mean and variance of x. — op_moments","text":"x Input tensor. axes list axes compute mean variance. keepdims set TRUE, axes reduced left result dimensions size one. synchronized applicable TensorFlow backend. TRUE, synchronizes global batch statistics (mean variance) across devices training step distributed training strategy. FALSE, replica uses local batch statistics.","code":""},{"path":"https://keras.posit.co/reference/op_moments.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculates the mean and variance of x. — op_moments","text":"list containing two tensors - mean variance.","code":""},{"path":"https://keras.posit.co/reference/op_moments.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculates the mean and variance of x. — op_moments","text":"","code":"x <- op_convert_to_tensor(c(0, 1, 2, 3, 100), dtype = \"float32\") op_moments(x, axes = c(1)) ## [[1]] ## tf.Tensor(21.2, shape=(), dtype=float32) ## ## [[2]] ## tf.Tensor(1553.36, shape=(), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_moveaxis.html","id":null,"dir":"Reference","previous_headings":"","what":"Move axes of a tensor to new positions. — op_moveaxis","title":"Move axes of a tensor to new positions. — op_moveaxis","text":"axes remain original order.","code":""},{"path":"https://keras.posit.co/reference/op_moveaxis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Move axes of a tensor to new positions. — op_moveaxis","text":"","code":"op_moveaxis(x, source, destination)"},{"path":"https://keras.posit.co/reference/op_moveaxis.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Move axes of a tensor to new positions. — op_moveaxis","text":"x Tensor whose axes reordered. source Original positions axes move. must unique. destination Destinations positions original axes. must also unique.","code":""},{"path":"https://keras.posit.co/reference/op_moveaxis.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Move axes of a tensor to new positions. — op_moveaxis","text":"","code":"Tensor with moved axes."},{"path":[]},{"path":"https://keras.posit.co/reference/op_multi_hot.html","id":null,"dir":"Reference","previous_headings":"","what":"Encodes integer labels as multi-hot vectors. — op_multi_hot","title":"Encodes integer labels as multi-hot vectors. — op_multi_hot","text":"function encodes integer labels multi-hot vectors, label mapped binary value resulting vector.","code":""},{"path":"https://keras.posit.co/reference/op_multi_hot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Encodes integer labels as multi-hot vectors. — op_multi_hot","text":"","code":"op_multi_hot(inputs, num_tokens, axis = -1L, dtype = NULL)"},{"path":"https://keras.posit.co/reference/op_multi_hot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Encodes integer labels as multi-hot vectors. — op_multi_hot","text":"inputs Tensor integer labels converted multi-hot vectors. num_tokens Integer, total number unique tokens classes. axis (optional) Axis along multi-hot encoding added. Defaults -1, corresponds last dimension. dtype (optional) data type resulting tensor. Default backend's float type.","code":""},{"path":"https://keras.posit.co/reference/op_multi_hot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Encodes integer labels as multi-hot vectors. — op_multi_hot","text":"Tensor: multi-hot encoded tensor.","code":""},{"path":"https://keras.posit.co/reference/op_multi_hot.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Encodes integer labels as multi-hot vectors. — op_multi_hot","text":"","code":"data <- op_convert_to_tensor(c(0, 4)) op_multi_hot(data, num_tokens = 5) ## tf.Tensor([1. 0. 0. 0. 1.], shape=(5), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_multiply.html","id":null,"dir":"Reference","previous_headings":"","what":"Multiply arguments element-wise. — op_multiply","title":"Multiply arguments element-wise. — op_multiply","text":"Multiply arguments element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_multiply.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multiply arguments element-wise. — op_multiply","text":"","code":"op_multiply(x1, x2)"},{"path":"https://keras.posit.co/reference/op_multiply.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multiply arguments element-wise. — op_multiply","text":"x1 First input tensor. x2 Second input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_multiply.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multiply arguments element-wise. — op_multiply","text":"","code":"Output tensor, element-wise product of `x1` and `x2`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_nan_to_num.html","id":null,"dir":"Reference","previous_headings":"","what":"Replace NaN with zero and infinity with large finite numbers. — op_nan_to_num","title":"Replace NaN with zero and infinity with large finite numbers. — op_nan_to_num","text":"Replace NaN zero infinity large finite numbers.","code":""},{"path":"https://keras.posit.co/reference/op_nan_to_num.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Replace NaN with zero and infinity with large finite numbers. — op_nan_to_num","text":"","code":"op_nan_to_num(x)"},{"path":"https://keras.posit.co/reference/op_nan_to_num.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Replace NaN with zero and infinity with large finite numbers. — op_nan_to_num","text":"x Input data.","code":""},{"path":"https://keras.posit.co/reference/op_nan_to_num.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Replace NaN with zero and infinity with large finite numbers. — op_nan_to_num","text":"","code":"`x`, with non-finite values replaced."},{"path":[]},{"path":"https://keras.posit.co/reference/op_ndim.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the number of dimensions of a tensor. — op_ndim","title":"Return the number of dimensions of a tensor. — op_ndim","text":"Return number dimensions tensor.","code":""},{"path":"https://keras.posit.co/reference/op_ndim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the number of dimensions of a tensor. — op_ndim","text":"","code":"op_ndim(x)"},{"path":"https://keras.posit.co/reference/op_ndim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the number of dimensions of a tensor. — op_ndim","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_ndim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the number of dimensions of a tensor. — op_ndim","text":"","code":"The number of dimensions in `x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_negative.html","id":null,"dir":"Reference","previous_headings":"","what":"Numerical negative, element-wise. — op_negative","title":"Numerical negative, element-wise. — op_negative","text":"Numerical negative, element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_negative.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Numerical negative, element-wise. — op_negative","text":"","code":"op_negative(x)"},{"path":"https://keras.posit.co/reference/op_negative.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Numerical negative, element-wise. — op_negative","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_negative.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Numerical negative, element-wise. — op_negative","text":"","code":"Output tensor, `y = -x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_nonzero.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the indices of the elements that are non-zero. — op_nonzero","title":"Return the indices of the elements that are non-zero. — op_nonzero","text":"Return indices elements non-zero.","code":""},{"path":"https://keras.posit.co/reference/op_nonzero.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the indices of the elements that are non-zero. — op_nonzero","text":"","code":"op_nonzero(x)"},{"path":"https://keras.posit.co/reference/op_nonzero.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the indices of the elements that are non-zero. — op_nonzero","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_nonzero.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the indices of the elements that are non-zero. — op_nonzero","text":"","code":"Indices of elements that are non-zero."},{"path":[]},{"path":"https://keras.posit.co/reference/op_not_equal.html","id":null,"dir":"Reference","previous_headings":"","what":"Return (x1 != x2) element-wise. — op_not_equal","title":"Return (x1 != x2) element-wise. — op_not_equal","text":"Return (x1 != x2) element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_not_equal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return (x1 != x2) element-wise. — op_not_equal","text":"","code":"op_not_equal(x1, x2)"},{"path":"https://keras.posit.co/reference/op_not_equal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return (x1 != x2) element-wise. — op_not_equal","text":"x1 First input tensor. x2 Second input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_not_equal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return (x1 != x2) element-wise. — op_not_equal","text":"","code":"Output tensor, element-wise comparsion of `x1` and `x2`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_one_hot.html","id":null,"dir":"Reference","previous_headings":"","what":"Converts integer tensor x into a one-hot tensor. — op_one_hot","title":"Converts integer tensor x into a one-hot tensor. — op_one_hot","text":"one-hot encoding representation integer value converted binary vector length equal num_classes, index corresponding integer value marked 1, indices marked 0.","code":""},{"path":"https://keras.posit.co/reference/op_one_hot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Converts integer tensor x into a one-hot tensor. — op_one_hot","text":"","code":"op_one_hot(x, num_classes, axis = -1L, dtype = NULL)"},{"path":"https://keras.posit.co/reference/op_one_hot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Converts integer tensor x into a one-hot tensor. — op_one_hot","text":"x Integer tensor encoded. shape can arbitrary, dtype integer. num_classes Number classes one-hot encoding. axis Axis along encoding performed. Defaults -1, represents last axis. dtype (Optional) Data type output tensor. provided, defaults default data type backend.","code":""},{"path":"https://keras.posit.co/reference/op_one_hot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Converts integer tensor x into a one-hot tensor. — op_one_hot","text":"Integer tensor: One-hot encoded tensor shape x except specified axis dimension, length num_classes. dtype output tensor determined dtype default data type backend.","code":""},{"path":"https://keras.posit.co/reference/op_one_hot.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Converts integer tensor x into a one-hot tensor. — op_one_hot","text":"","code":"x <- op_array(c(1, 3, 2, 0), \"int32\") op_one_hot(x, num_classes = 4) ## tf.Tensor( ## [[0. 1. 0. 0.] ##  [0. 0. 0. 1.] ##  [0. 0. 1. 0.] ##  [1. 0. 0. 0.]], shape=(4, 4), dtype=float32) # array([[0. 1. 0. 0.] #        [0. 0. 0. 1.] #        [0. 0. 1. 0.] #        [1. 0. 0. 0.]], shape=(4, 4), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_ones.html","id":null,"dir":"Reference","previous_headings":"","what":"Return a new tensor of given shape and type, filled with ones. — op_ones","title":"Return a new tensor of given shape and type, filled with ones. — op_ones","text":"Return new tensor given shape type, filled ones.","code":""},{"path":"https://keras.posit.co/reference/op_ones.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return a new tensor of given shape and type, filled with ones. — op_ones","text":"","code":"op_ones(shape, dtype = NULL)"},{"path":"https://keras.posit.co/reference/op_ones.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return a new tensor of given shape and type, filled with ones. — op_ones","text":"shape Shape new tensor. dtype Desired data type tensor.","code":""},{"path":"https://keras.posit.co/reference/op_ones.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return a new tensor of given shape and type, filled with ones. — op_ones","text":"","code":"Tensor of ones with the given shape and dtype."},{"path":[]},{"path":"https://keras.posit.co/reference/op_ones_like.html","id":null,"dir":"Reference","previous_headings":"","what":"Return a tensor of ones with the same shape and type of x. — op_ones_like","title":"Return a tensor of ones with the same shape and type of x. — op_ones_like","text":"Return tensor ones shape type x.","code":""},{"path":"https://keras.posit.co/reference/op_ones_like.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return a tensor of ones with the same shape and type of x. — op_ones_like","text":"","code":"op_ones_like(x, dtype = NULL)"},{"path":"https://keras.posit.co/reference/op_ones_like.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return a tensor of ones with the same shape and type of x. — op_ones_like","text":"x Input tensor. dtype Overrides data type result.","code":""},{"path":"https://keras.posit.co/reference/op_ones_like.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return a tensor of ones with the same shape and type of x. — op_ones_like","text":"","code":"A tensor of ones with the same shape and type as `x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_outer.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the outer product of two vectors. — op_outer","title":"Compute the outer product of two vectors. — op_outer","text":"Given two vectors x1 x2, outer product :","code":"out[i, j] = x1[i] * x2[j]"},{"path":"https://keras.posit.co/reference/op_outer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the outer product of two vectors. — op_outer","text":"","code":"op_outer(x1, x2)"},{"path":"https://keras.posit.co/reference/op_outer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the outer product of two vectors. — op_outer","text":"x1 First input tensor. x2 Second input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_outer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the outer product of two vectors. — op_outer","text":"","code":"Outer product of `x1` and `x2`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_pad.html","id":null,"dir":"Reference","previous_headings":"","what":"Pad a tensor. — op_pad","title":"Pad a tensor. — op_pad","text":"Pad tensor.","code":""},{"path":"https://keras.posit.co/reference/op_pad.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pad a tensor. — op_pad","text":"","code":"op_pad(x, pad_width, mode = \"constant\", constant_values = NULL)"},{"path":"https://keras.posit.co/reference/op_pad.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pad a tensor. — op_pad","text":"x Tensor pad. pad_width Number values padded edges axis. ((before_1, after_1), ...(before_N, after_N)) unique pad widths axis. ((, ),) yields pad axis. (pad,) int shortcut = = pad width axes. mode One \"constant\", \"edge\", \"linear_ramp\", \"maximum\", \"mean\", \"median\", \"minimum\", \"reflect\", \"symmetric\", \"wrap\", \"empty\", \"circular\". Defaults \"constant\". constant_values Value pad mode == \"constant\". Defaults 0. ValueError raised NULL mode != \"constant\".","code":""},{"path":"https://keras.posit.co/reference/op_pad.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pad a tensor. — op_pad","text":"","code":"Padded tensor."},{"path":"https://keras.posit.co/reference/op_pad.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Pad a tensor. — op_pad","text":"Torch backend supports modes \"constant\", \"reflect\", \"symmetric\" \"circular\". Torch backend supports \"circular\" mode. Note: Tensorflow backend supports modes \"constant\", \"reflect\" \"symmetric\".","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/op_power.html","id":null,"dir":"Reference","previous_headings":"","what":"First tensor elements raised to powers from second tensor, element-wise. — op_power","title":"First tensor elements raised to powers from second tensor, element-wise. — op_power","text":"First tensor elements raised powers second tensor, element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_power.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"First tensor elements raised to powers from second tensor, element-wise. — op_power","text":"","code":"op_power(x1, x2)"},{"path":"https://keras.posit.co/reference/op_power.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"First tensor elements raised to powers from second tensor, element-wise. — op_power","text":"x1 bases. x2 exponents.","code":""},{"path":"https://keras.posit.co/reference/op_power.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"First tensor elements raised to powers from second tensor, element-wise. — op_power","text":"","code":"Output tensor, the bases in `x1` raised to the exponents in `x2`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_prod.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the product of tensor elements over a given axis. — op_prod","title":"Return the product of tensor elements over a given axis. — op_prod","text":"Return product tensor elements given axis.","code":""},{"path":"https://keras.posit.co/reference/op_prod.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the product of tensor elements over a given axis. — op_prod","text":"","code":"op_prod(x, axis = NULL, keepdims = FALSE, dtype = NULL)"},{"path":"https://keras.posit.co/reference/op_prod.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the product of tensor elements over a given axis. — op_prod","text":"x Input tensor. axis Axis axes along product performed. default, axis = NULL, compute product elements input tensor. keepdims set TRUE, axes reduce left result dimensions size one. dtype Data type returned tensor.","code":""},{"path":"https://keras.posit.co/reference/op_prod.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the product of tensor elements over a given axis. — op_prod","text":"","code":"Product of elements of `x` over the given axis or axes."},{"path":[]},{"path":"https://keras.posit.co/reference/op_qr.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the QR decomposition of a tensor. — op_qr","title":"Computes the QR decomposition of a tensor. — op_qr","text":"Computes QR decomposition tensor.","code":""},{"path":"https://keras.posit.co/reference/op_qr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the QR decomposition of a tensor. — op_qr","text":"","code":"op_qr(x, mode = \"reduced\")"},{"path":"https://keras.posit.co/reference/op_qr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the QR decomposition of a tensor. — op_qr","text":"x Input tensor. mode string specifying mode QR decomposition. 'reduced': Returns reduced QR decomposition. (default) 'complete': Returns complete QR decomposition.","code":""},{"path":"https://keras.posit.co/reference/op_qr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the QR decomposition of a tensor. — op_qr","text":"list containing two tensors. first tensor represents orthogonal matrix Q, second tensor represents upper triangular matrix R.","code":""},{"path":"https://keras.posit.co/reference/op_qr.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the QR decomposition of a tensor. — op_qr","text":"","code":"x <- op_convert_to_tensor(rbind(c(1, 2), c(3, 4), c(5, 6))) c(q, r) %<-% op_qr(x) q ## tf.Tensor( ## [[-0.16903085  0.89708523] ##  [-0.50709255  0.27602622] ##  [-0.84515425 -0.34503278]], shape=(3, 2), dtype=float64)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_quantile.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the q-th quantile(s) of the data along the specified axis. — op_quantile","title":"Compute the q-th quantile(s) of the data along the specified axis. — op_quantile","text":"Compute q-th quantile(s) data along specified axis.","code":""},{"path":"https://keras.posit.co/reference/op_quantile.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the q-th quantile(s) of the data along the specified axis. — op_quantile","text":"","code":"op_quantile(x, q, axis = NULL, method = \"linear\", keepdims = FALSE)"},{"path":"https://keras.posit.co/reference/op_quantile.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the q-th quantile(s) of the data along the specified axis. — op_quantile","text":"x Input tensor. q Probability sequence probabilities quantiles compute. Values must 0 1 inclusive. axis Axis axes along quantiles computed. Defaults axis=NULL compute quantile(s) along flattened version array. method string specifies method use estimating quantile. Available methods \"linear\", \"lower\", \"higher\", \"midpoint\", \"nearest\". Defaults \"linear\". desired quantile lies two data points < j: \"linear\": + (j - ) * fraction, fraction fractional part index surrounded j. \"lower\": . \"higher\": j. \"midpoint\": (+ j) / 2 \"nearest\": j, whichever nearest. keepdims set TRUE, axes reduce left result dimensions size one.","code":""},{"path":"https://keras.posit.co/reference/op_quantile.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the q-th quantile(s) of the data along the specified axis. — op_quantile","text":"quantile(s). q single probability axis=NULL, result scalar. multiple probabilies levels given, first axis result corresponds quantiles. axes axes remain reduction x.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/op_ravel.html","id":null,"dir":"Reference","previous_headings":"","what":"Return a contiguous flattened tensor. — op_ravel","title":"Return a contiguous flattened tensor. — op_ravel","text":"1-D tensor, containing elements input, returned.","code":""},{"path":"https://keras.posit.co/reference/op_ravel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return a contiguous flattened tensor. — op_ravel","text":"","code":"op_ravel(x)"},{"path":"https://keras.posit.co/reference/op_ravel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return a contiguous flattened tensor. — op_ravel","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_ravel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return a contiguous flattened tensor. — op_ravel","text":"","code":"Output tensor."},{"path":[]},{"path":"https://keras.posit.co/reference/op_real.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the real part of the complex argument. — op_real","title":"Return the real part of the complex argument. — op_real","text":"Return real part complex argument.","code":""},{"path":"https://keras.posit.co/reference/op_real.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the real part of the complex argument. — op_real","text":"","code":"op_real(x)"},{"path":"https://keras.posit.co/reference/op_real.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the real part of the complex argument. — op_real","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_real.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the real part of the complex argument. — op_real","text":"","code":"The real component of the complex argument."},{"path":[]},{"path":"https://keras.posit.co/reference/op_reciprocal.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the reciprocal of the argument, element-wise. — op_reciprocal","title":"Return the reciprocal of the argument, element-wise. — op_reciprocal","text":"Calculates 1/x.","code":""},{"path":"https://keras.posit.co/reference/op_reciprocal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the reciprocal of the argument, element-wise. — op_reciprocal","text":"","code":"op_reciprocal(x)"},{"path":"https://keras.posit.co/reference/op_reciprocal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the reciprocal of the argument, element-wise. — op_reciprocal","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_reciprocal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the reciprocal of the argument, element-wise. — op_reciprocal","text":"","code":"Output tensor, element-wise reciprocal of `x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_relu.html","id":null,"dir":"Reference","previous_headings":"","what":"Rectified linear unit activation function. — op_relu","title":"Rectified linear unit activation function. — op_relu","text":"defined f(x) = max(0, x).","code":""},{"path":"https://keras.posit.co/reference/op_relu.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rectified linear unit activation function. — op_relu","text":"","code":"op_relu(x)"},{"path":"https://keras.posit.co/reference/op_relu.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rectified linear unit activation function. — op_relu","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_relu.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rectified linear unit activation function. — op_relu","text":"tensor shape x.","code":""},{"path":"https://keras.posit.co/reference/op_relu.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rectified linear unit activation function. — op_relu","text":"","code":"x1 <- op_convert_to_tensor(c(-1, 0, 1, 0.2)) op_relu(x1) ## tf.Tensor([0.  0.  1.  0.2], shape=(4), dtype=float32) x <- seq(-10, 10, .1) plot(x, op_relu(x))"},{"path":[]},{"path":"https://keras.posit.co/reference/op_relu6.html","id":null,"dir":"Reference","previous_headings":"","what":"Rectified linear unit activation function with upper bound of 6. — op_relu6","title":"Rectified linear unit activation function with upper bound of 6. — op_relu6","text":"defined f(x) = op_clip(x, 0, 6).","code":""},{"path":"https://keras.posit.co/reference/op_relu6.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rectified linear unit activation function with upper bound of 6. — op_relu6","text":"","code":"op_relu6(x)"},{"path":"https://keras.posit.co/reference/op_relu6.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rectified linear unit activation function with upper bound of 6. — op_relu6","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_relu6.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rectified linear unit activation function with upper bound of 6. — op_relu6","text":"tensor shape x.","code":""},{"path":"https://keras.posit.co/reference/op_relu6.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rectified linear unit activation function with upper bound of 6. — op_relu6","text":"","code":"x <- op_convert_to_tensor(c(-3, -2, 0.1, 0.2, 6, 8)) op_relu6(x) ## tf.Tensor([0.  0.  0.1 0.2 6.  6. ], shape=(6), dtype=float32) x <- seq(-10, 10, .1) plot(x, op_relu6(x))"},{"path":[]},{"path":"https://keras.posit.co/reference/op_repeat.html","id":null,"dir":"Reference","previous_headings":"","what":"Repeat each element of a tensor after themselves. — op_repeat","title":"Repeat each element of a tensor after themselves. — op_repeat","text":"Repeat element tensor .","code":""},{"path":"https://keras.posit.co/reference/op_repeat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Repeat each element of a tensor after themselves. — op_repeat","text":"","code":"op_repeat(x, repeats, axis = NULL)"},{"path":"https://keras.posit.co/reference/op_repeat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Repeat each element of a tensor after themselves. — op_repeat","text":"x Input tensor. repeats number repetitions element. axis axis along repeat values. default, use flattened input array, return flat output array.","code":""},{"path":"https://keras.posit.co/reference/op_repeat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Repeat each element of a tensor after themselves. — op_repeat","text":"","code":"Output tensor."},{"path":[]},{"path":"https://keras.posit.co/reference/op_reshape.html","id":null,"dir":"Reference","previous_headings":"","what":"Gives a new shape to a tensor without changing its data. — op_reshape","title":"Gives a new shape to a tensor without changing its data. — op_reshape","text":"Gives new shape tensor without changing data.","code":""},{"path":"https://keras.posit.co/reference/op_reshape.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gives a new shape to a tensor without changing its data. — op_reshape","text":"","code":"op_reshape(x, ..., new_shape = list(...))"},{"path":"https://keras.posit.co/reference/op_reshape.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gives a new shape to a tensor without changing its data. — op_reshape","text":"x Input tensor. ... forward/backward compatability. new_shape new shape compatible original shape. One shape dimension can -1 case value inferred length array remaining dimensions.","code":""},{"path":"https://keras.posit.co/reference/op_reshape.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Gives a new shape to a tensor without changing its data. — op_reshape","text":"","code":"The reshaped tensor."},{"path":[]},{"path":"https://keras.posit.co/reference/op_rfft.html","id":null,"dir":"Reference","previous_headings":"","what":"Real-valued Fast Fourier Transform along the last axis of the input. — op_rfft","title":"Real-valued Fast Fourier Transform along the last axis of the input. — op_rfft","text":"Computes 1D Discrete Fourier Transform real-valued signal inner-dimension input. Since Discrete Fourier Transform real-valued signal Hermitian-symmetric, RFFT returns fft_length / 2 + 1 unique components FFT: zero-frequency term, followed fft_length / 2 positive-frequency terms. Along axis RFFT computed , fft_length smaller corresponding dimension input, dimension cropped. larger, dimension padded zeros.","code":""},{"path":"https://keras.posit.co/reference/op_rfft.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Real-valued Fast Fourier Transform along the last axis of the input. — op_rfft","text":"","code":"op_rfft(x, fft_length = NULL)"},{"path":"https://keras.posit.co/reference/op_rfft.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Real-valued Fast Fourier Transform along the last axis of the input. — op_rfft","text":"x Input tensor. fft_length integer representing number fft length. specified, inferred length last axis x. Defaults NULL.","code":""},{"path":"https://keras.posit.co/reference/op_rfft.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Real-valued Fast Fourier Transform along the last axis of the input. — op_rfft","text":"list containing two tensors - real imaginary parts output.","code":""},{"path":"https://keras.posit.co/reference/op_rfft.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Real-valued Fast Fourier Transform along the last axis of the input. — op_rfft","text":"","code":"x <- op_convert_to_tensor(c(0, 1, 2, 3, 4)) op_rfft(x) ## [[1]] ## tf.Tensor([10.  -2.5 -2.5], shape=(3), dtype=float32) ## ## [[2]] ## tf.Tensor([0.        3.440955  0.8122992], shape=(3), dtype=float32) op_rfft(x, 3) ## [[1]] ## tf.Tensor([ 3.  -1.5], shape=(2), dtype=float32) ## ## [[2]] ## tf.Tensor([0.        0.8660254], shape=(2), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_roll.html","id":null,"dir":"Reference","previous_headings":"","what":"Roll tensor elements along a given axis. — op_roll","title":"Roll tensor elements along a given axis. — op_roll","text":"Elements roll beyond last position re-introduced first.","code":""},{"path":"https://keras.posit.co/reference/op_roll.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Roll tensor elements along a given axis. — op_roll","text":"","code":"op_roll(x, shift, axis = NULL)"},{"path":"https://keras.posit.co/reference/op_roll.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Roll tensor elements along a given axis. — op_roll","text":"x Input tensor. shift number places elements shifted. axis axis along elements shifted. default, array flattened shifting, original shape restored.","code":""},{"path":"https://keras.posit.co/reference/op_roll.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Roll tensor elements along a given axis. — op_roll","text":"","code":"Output tensor."},{"path":[]},{"path":"https://keras.posit.co/reference/op_round.html","id":null,"dir":"Reference","previous_headings":"","what":"Evenly round to the given number of decimals. — op_round","title":"Evenly round to the given number of decimals. — op_round","text":"Evenly round given number decimals.","code":""},{"path":"https://keras.posit.co/reference/op_round.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evenly round to the given number of decimals. — op_round","text":"","code":"op_round(x, decimals = 0L)"},{"path":"https://keras.posit.co/reference/op_round.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evenly round to the given number of decimals. — op_round","text":"x Input tensor. decimals Number decimal places round . Defaults 0.","code":""},{"path":"https://keras.posit.co/reference/op_round.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evenly round to the given number of decimals. — op_round","text":"","code":"Output tensor."},{"path":[]},{"path":"https://keras.posit.co/reference/op_rsqrt.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes reciprocal of square root of x element-wise. — op_rsqrt","title":"Computes reciprocal of square root of x element-wise. — op_rsqrt","text":"Computes reciprocal square root x element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_rsqrt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes reciprocal of square root of x element-wise. — op_rsqrt","text":"","code":"op_rsqrt(x)"},{"path":"https://keras.posit.co/reference/op_rsqrt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes reciprocal of square root of x element-wise. — op_rsqrt","text":"x input tensor","code":""},{"path":"https://keras.posit.co/reference/op_rsqrt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes reciprocal of square root of x element-wise. — op_rsqrt","text":"tensor dtype x.","code":""},{"path":"https://keras.posit.co/reference/op_rsqrt.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes reciprocal of square root of x element-wise. — op_rsqrt","text":"","code":"x <- op_convert_to_tensor(c(1, 10, 100)) op_rsqrt(x) ## tf.Tensor([1.         0.31622776 0.1       ], shape=(3), dtype=float32) # array([1, 0.31622776, 0.1], dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_scatter.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns a tensor of shape shape where indices are set to values. — op_scatter","title":"Returns a tensor of shape shape where indices are set to values. — op_scatter","text":"high level, operation zeros[indices] = updates returns output. equivalent :","code":"output <- op_scatter_update(op_zeros(shape), indices, values)"},{"path":"https://keras.posit.co/reference/op_scatter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns a tensor of shape shape where indices are set to values. — op_scatter","text":"","code":"op_scatter(indices, values, shape)"},{"path":"https://keras.posit.co/reference/op_scatter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns a tensor of shape shape where indices are set to values. — op_scatter","text":"indices tensor list specifying indices values values. values tensor, values set indices. shape Shape output tensor.","code":""},{"path":"https://keras.posit.co/reference/op_scatter.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Returns a tensor of shape shape where indices are set to values. — op_scatter","text":"","code":"indices <- rbind(c(1, 2), c(2, 2)) values <- op_array(c(1, 1)) op_scatter(indices, values, shape= c(2, 2)) ## tf.Tensor( ## [[0. 1.] ##  [0. 1.]], shape=(2, 2), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_scatter_update.html","id":null,"dir":"Reference","previous_headings":"","what":"Update inputs via updates at scattered (sparse) indices. — op_scatter_update","title":"Update inputs via updates at scattered (sparse) indices. — op_scatter_update","text":"high level, operation inputs[indices] = updates. Assume inputs tensor shape (D1, D2, ..., Dn), 2 main usages scatter_update. indices 2D tensor shape (num_updates, n), num_updates number updates perform, updates 1D tensor shape (num_updates). example, inputs op_zeros(c(4, 4, 4)), want update inputs[2, 3, 4] inputs[1, 2, 4] 1, can use:     2 indices 2D tensor shape (num_updates, k), num_updates number updates perform, k (k <= n) size index indices. updates n - k-D tensor shape (num_updates, inputs.shape[k:)). example, inputs = op_zeros(c(4, 4, 4)), want update inputs[1, 2, ] inputs[2, 3, ] [1, 1, 1, 1], indices shape (num_updates, 2) (k = 2), updates shape (num_updates, 4) (inputs.shape[2:] = 4). See code :","code":"inputs <- op_zeros(c(4, 4, 4)) indices <- rbind(c(2, 3, 4), c(1, 2, 4)) updates <- op_array(c(1, 1), \"float32\") op_scatter_update(inputs, indices, updates) ## tf.Tensor( ## [[[0. 0. 0. 0.] ##   [0. 0. 0. 1.] ##   [0. 0. 0. 0.] ##   [0. 0. 0. 0.]] ## ##  [[0. 0. 0. 0.] ##   [0. 0. 0. 0.] ##   [0. 0. 0. 1.] ##   [0. 0. 0. 0.]] ## ##  [[0. 0. 0. 0.] ##   [0. 0. 0. 0.] ##   [0. 0. 0. 0.] ##   [0. 0. 0. 0.]] ## ##  [[0. 0. 0. 0.] ##   [0. 0. 0. 0.] ##   [0. 0. 0. 0.] ##   [0. 0. 0. 0.]]], shape=(4, 4, 4), dtype=float32) inputs <- op_zeros(c(4, 4, 4)) indices <- rbind(c(2, 3), c(3, 4)) updates <- op_array(rbind(c(1, 1, 1, 1), c(1, 1, 1, 1)), \"float32\") op_scatter_update(inputs, indices, updates) ## tf.Tensor( ## [[[0. 0. 0. 0.] ##   [0. 0. 0. 0.] ##   [0. 0. 0. 0.] ##   [0. 0. 0. 0.]] ## ##  [[0. 0. 0. 0.] ##   [0. 0. 0. 0.] ##   [1. 1. 1. 1.] ##   [0. 0. 0. 0.]] ## ##  [[0. 0. 0. 0.] ##   [0. 0. 0. 0.] ##   [0. 0. 0. 0.] ##   [1. 1. 1. 1.]] ## ##  [[0. 0. 0. 0.] ##   [0. 0. 0. 0.] ##   [0. 0. 0. 0.] ##   [0. 0. 0. 0.]]], shape=(4, 4, 4), dtype=float32)"},{"path":"https://keras.posit.co/reference/op_scatter_update.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update inputs via updates at scattered (sparse) indices. — op_scatter_update","text":"","code":"op_scatter_update(inputs, indices, updates)"},{"path":"https://keras.posit.co/reference/op_scatter_update.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update inputs via updates at scattered (sparse) indices. — op_scatter_update","text":"inputs tensor, tensor updated. indices tensor list shape (N, inputs$ndim), specifying indices update. N number indices update, must equal first dimension updates. updates tensor, new values put inputs indices.","code":""},{"path":"https://keras.posit.co/reference/op_scatter_update.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update inputs via updates at scattered (sparse) indices. — op_scatter_update","text":"","code":"A tensor, has the same shape and dtype as `inputs`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_segment_max.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the max of segments in a tensor. — op_segment_max","title":"Computes the max of segments in a tensor. — op_segment_max","text":"Computes max segments tensor.","code":""},{"path":"https://keras.posit.co/reference/op_segment_max.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the max of segments in a tensor. — op_segment_max","text":"","code":"op_segment_max(data, segment_ids, num_segments = NULL, sorted = FALSE)"},{"path":"https://keras.posit.co/reference/op_segment_max.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the max of segments in a tensor. — op_segment_max","text":"data Input tensor. segment_ids 1-D tensor containing segment indices element data. num_segments integer representing total number segments. specified, inferred maximum value segment_ids. sorted boolean indicating whether segment_ids sorted. Defaults toFALSE.","code":""},{"path":"https://keras.posit.co/reference/op_segment_max.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the max of segments in a tensor. — op_segment_max","text":"tensor containing max segments, element represents max corresponding segment data.","code":""},{"path":"https://keras.posit.co/reference/op_segment_max.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the max of segments in a tensor. — op_segment_max","text":"","code":"data <- op_convert_to_tensor(c(1, 2, 10, 20, 100, 200)) segment_ids <- op_array(c(1, 1, 2, 2, 3, 3), \"int32\") num_segments <- 3 op_segment_max(data, segment_ids, num_segments) ## tf.Tensor([  2.  20. 200.], shape=(3), dtype=float32) # array([2, 20, 200], dtype=int32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_segment_sum.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes the sum of segments in a tensor. — op_segment_sum","title":"Computes the sum of segments in a tensor. — op_segment_sum","text":"Computes sum segments tensor.","code":""},{"path":"https://keras.posit.co/reference/op_segment_sum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes the sum of segments in a tensor. — op_segment_sum","text":"","code":"op_segment_sum(data, segment_ids, num_segments = NULL, sorted = FALSE)"},{"path":"https://keras.posit.co/reference/op_segment_sum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes the sum of segments in a tensor. — op_segment_sum","text":"data Input tensor. segment_ids 1-D tensor containing segment indices element data. num_segments integer representing total number segments. specified, inferred maximum value segment_ids. sorted boolean indicating whether segment_ids sorted. Defaults toFALSE.","code":""},{"path":"https://keras.posit.co/reference/op_segment_sum.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes the sum of segments in a tensor. — op_segment_sum","text":"tensor containing sum segments, element represents sum corresponding segment data.","code":""},{"path":"https://keras.posit.co/reference/op_segment_sum.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes the sum of segments in a tensor. — op_segment_sum","text":"","code":"data <- op_array(c(1, 2, 10, 20, 100, 200)) segment_ids <- op_array(c(1, 1, 2, 2, 3, 3), \"int32\") num_segments <- 3 op_segment_sum(data, segment_ids, num_segments) ## tf.Tensor([  3.  30. 300.], shape=(3), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_selu.html","id":null,"dir":"Reference","previous_headings":"","what":"Scaled Exponential Linear Unit (SELU) activation function. — op_selu","title":"Scaled Exponential Linear Unit (SELU) activation function. — op_selu","text":"defined : f(x) =  scale * alpha * (exp(x) - 1.) x < 0, f(x) = scale * x x >= 0.","code":""},{"path":"https://keras.posit.co/reference/op_selu.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scaled Exponential Linear Unit (SELU) activation function. — op_selu","text":"","code":"op_selu(x)"},{"path":"https://keras.posit.co/reference/op_selu.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scaled Exponential Linear Unit (SELU) activation function. — op_selu","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_selu.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Scaled Exponential Linear Unit (SELU) activation function. — op_selu","text":"tensor shape x.","code":""},{"path":"https://keras.posit.co/reference/op_selu.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scaled Exponential Linear Unit (SELU) activation function. — op_selu","text":"","code":"x <- op_array(c(-1, 0, 1)) op_selu(x) ## tf.Tensor([-1.1113307  0.         1.050701 ], shape=(3), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_separable_conv.html","id":null,"dir":"Reference","previous_headings":"","what":"General N-D separable convolution. — op_separable_conv","title":"General N-D separable convolution. — op_separable_conv","text":"ops supports 1D 2D separable convolution. separable_conv depthwise conv followed pointwise conv.","code":""},{"path":"https://keras.posit.co/reference/op_separable_conv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"General N-D separable convolution. — op_separable_conv","text":"","code":"op_separable_conv(   inputs,   depthwise_kernel,   pointwise_kernel,   strides = 1L,   padding = \"valid\",   data_format = NULL,   dilation_rate = 1L )"},{"path":"https://keras.posit.co/reference/op_separable_conv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"General N-D separable convolution. — op_separable_conv","text":"inputs Tensor rank N+2. inputs shape (batch_size,) + inputs_spatial_shape + (num_channels,) data_format=\"channels_last\", (batch_size, num_channels) + inputs_spatial_shape data_format=\"channels_first\". depthwise_kernel Tensor rank N+2. depthwise_kernel shape [kernel_spatial_shape, num_input_channels, num_channels_multiplier], num_input_channels match number channels inputs. pointwise_kernel Tensor rank N+2. pointwise_kernel shape (*ones_like(kernel_spatial_shape), num_input_channels * num_channels_multiplier, num_output_channels). strides int int tuple/list len(inputs_spatial_shape), specifying strides convolution along spatial dimension. strides int, every spatial dimension shares strides. padding string, either \"valid\" \"\". \"valid\" means padding applied, \"\" results padding evenly left/right /input output height/width dimension input strides=1. data_format string, either \"channels_last\" \"channels_first\". data_format determines ordering dimensions inputs. data_format=\"channels_last\", inputs shape (batch_size, ..., channels) data_format=\"channels_first\", inputs shape (batch_size, channels, ...). dilation_rate int int tuple/list len(inputs_spatial_shape), specifying dilation rate use dilated convolution. dilation_rate int, every spatial dimension shares dilation_rate.","code":""},{"path":"https://keras.posit.co/reference/op_separable_conv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"General N-D separable convolution. — op_separable_conv","text":"","code":"A tensor of rank N+2, the result of the depthwise conv operation."},{"path":[]},{"path":"https://keras.posit.co/reference/op_shape.html","id":null,"dir":"Reference","previous_headings":"","what":"Gets the shape of the tensor input. — op_shape","title":"Gets the shape of the tensor input. — op_shape","text":"Gets shape tensor input.","code":""},{"path":"https://keras.posit.co/reference/op_shape.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gets the shape of the tensor input. — op_shape","text":"","code":"op_shape(x)"},{"path":"https://keras.posit.co/reference/op_shape.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gets the shape of the tensor input. — op_shape","text":"x tensor. function try access shape attribute input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_shape.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Gets the shape of the tensor input. — op_shape","text":"list integers NULL values, indicating shape input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_shape.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Gets the shape of the tensor input. — op_shape","text":"TensorFlow backend, x tf.Tensor dynamic shape, dimensions dynamic context compiled function tf.Tensor value instead static integer value.","code":""},{"path":"https://keras.posit.co/reference/op_shape.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Gets the shape of the tensor input. — op_shape","text":"","code":"x <- op_zeros(c(8, 12)) op_shape(x) ## shape(8, 12)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_sigmoid.html","id":null,"dir":"Reference","previous_headings":"","what":"Sigmoid activation function. — op_sigmoid","title":"Sigmoid activation function. — op_sigmoid","text":"defined f(x) = 1 / (1 + exp(-x)).","code":""},{"path":"https://keras.posit.co/reference/op_sigmoid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sigmoid activation function. — op_sigmoid","text":"","code":"op_sigmoid(x)"},{"path":"https://keras.posit.co/reference/op_sigmoid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sigmoid activation function. — op_sigmoid","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_sigmoid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sigmoid activation function. — op_sigmoid","text":"tensor shape x.","code":""},{"path":"https://keras.posit.co/reference/op_sigmoid.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sigmoid activation function. — op_sigmoid","text":"","code":"x <- op_convert_to_tensor(c(-6, 1, 0, 1, 6)) op_sigmoid(x) ## tf.Tensor([0.00247262 0.7310586  0.5        0.7310586  0.99752736], shape=(5), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_sign.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns a tensor with the signs of the elements of x. — op_sign","title":"Returns a tensor with the signs of the elements of x. — op_sign","text":"Returns tensor signs elements x.","code":""},{"path":"https://keras.posit.co/reference/op_sign.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns a tensor with the signs of the elements of x. — op_sign","text":"","code":"op_sign(x)"},{"path":"https://keras.posit.co/reference/op_sign.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns a tensor with the signs of the elements of x. — op_sign","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_sign.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns a tensor with the signs of the elements of x. — op_sign","text":"","code":"Output tensor of same shape as `x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_silu.html","id":null,"dir":"Reference","previous_headings":"","what":"Sigmoid Linear Unit (SiLU) activation function, also known as Swish. — op_silu","title":"Sigmoid Linear Unit (SiLU) activation function, also known as Swish. — op_silu","text":"SiLU activation function computed sigmoid function multiplied input. defined f(x) = x * sigmoid(x).","code":""},{"path":"https://keras.posit.co/reference/op_silu.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sigmoid Linear Unit (SiLU) activation function, also known as Swish. — op_silu","text":"","code":"op_silu(x)"},{"path":"https://keras.posit.co/reference/op_silu.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sigmoid Linear Unit (SiLU) activation function, also known as Swish. — op_silu","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_silu.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sigmoid Linear Unit (SiLU) activation function, also known as Swish. — op_silu","text":"tensor shape x.","code":""},{"path":"https://keras.posit.co/reference/op_silu.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sigmoid Linear Unit (SiLU) activation function, also known as Swish. — op_silu","text":"","code":"x <- op_convert_to_tensor(c(-6, 1, 0, 1, 6)) op_sigmoid(x) ## tf.Tensor([0.00247262 0.7310586  0.5        0.7310586  0.99752736], shape=(5), dtype=float32) op_silu(x) ## tf.Tensor([-0.01483574  0.7310586   0.          0.7310586   5.985164  ], shape=(5), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_sin.html","id":null,"dir":"Reference","previous_headings":"","what":"Trigonomeric sine, element-wise. — op_sin","title":"Trigonomeric sine, element-wise. — op_sin","text":"Trigonomeric sine, element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_sin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Trigonomeric sine, element-wise. — op_sin","text":"","code":"op_sin(x)"},{"path":"https://keras.posit.co/reference/op_sin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Trigonomeric sine, element-wise. — op_sin","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_sin.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Trigonomeric sine, element-wise. — op_sin","text":"","code":"Output tensor of same shape as `x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_sinh.html","id":null,"dir":"Reference","previous_headings":"","what":"Hyperbolic sine, element-wise. — op_sinh","title":"Hyperbolic sine, element-wise. — op_sinh","text":"Hyperbolic sine, element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_sinh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperbolic sine, element-wise. — op_sinh","text":"","code":"op_sinh(x)"},{"path":"https://keras.posit.co/reference/op_sinh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hyperbolic sine, element-wise. — op_sinh","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_sinh.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Hyperbolic sine, element-wise. — op_sinh","text":"","code":"Output tensor of same shape as `x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_size.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the number of elements in a tensor. — op_size","title":"Return the number of elements in a tensor. — op_size","text":"Return number elements tensor.","code":""},{"path":"https://keras.posit.co/reference/op_size.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the number of elements in a tensor. — op_size","text":"","code":"op_size(x)"},{"path":"https://keras.posit.co/reference/op_size.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the number of elements in a tensor. — op_size","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_size.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the number of elements in a tensor. — op_size","text":"","code":"Number of elements in `x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_slice.html","id":null,"dir":"Reference","previous_headings":"","what":"Return a slice of an input tensor. — op_slice","title":"Return a slice of an input tensor. — op_slice","text":"high level, operation explicit replacement array slicing e.g. inputs[start_indices:(start_indices + shape)]. Unlike slicing via brackets, operation accept tensor start indices backends, useful indices dynamically computed via tensor operations.","code":"(inputs <- op_arange(5*5) |> op_reshape(c(5, 5))) ## tf.Tensor( ## [[ 0.  1.  2.  3.  4.] ##  [ 5.  6.  7.  8.  9.] ##  [10. 11. 12. 13. 14.] ##  [15. 16. 17. 18. 19.] ##  [20. 21. 22. 23. 24.]], shape=(5, 5), dtype=float64) start_indices <- c(3, 3) shape <- c(2, 2) op_slice(inputs, start_indices, shape) ## tf.Tensor( ## [[12. 13.] ##  [17. 18.]], shape=(2, 2), dtype=float64)"},{"path":"https://keras.posit.co/reference/op_slice.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return a slice of an input tensor. — op_slice","text":"","code":"op_slice(inputs, start_indices, shape)"},{"path":"https://keras.posit.co/reference/op_slice.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return a slice of an input tensor. — op_slice","text":"inputs tensor, tensor sliced. start_indices list length inputs$ndim, specifying starting indices updating. shape full shape returned slice.","code":""},{"path":"https://keras.posit.co/reference/op_slice.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return a slice of an input tensor. — op_slice","text":"tensor, shape dtype inputs.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/op_slice_update.html","id":null,"dir":"Reference","previous_headings":"","what":"Update an input by slicing in a tensor of updated values. — op_slice_update","title":"Update an input by slicing in a tensor of updated values. — op_slice_update","text":"high level, operation inputs[start_indices: start_indices + updates.shape] = updates. Assume inputs tensor shape (D1, D2, ..., Dn), start_indices must list n integers, specifying starting indices. updates must rank inputs, size dim must exceed Di - start_indices[]. example, 2D inputs inputs = op_zeros(c(5, 5)), want update intersection last 2 rows last 2 columns 1, .e., inputs[4:5, 4:5] = op_ones(c(2, 2)), can use code :","code":"inputs <- op_zeros(c(5, 5)) start_indices <- c(3, 3) updates <- op_ones(c(2, 2)) op_slice_update(inputs, start_indices, updates) ## tf.Tensor( ## [[0. 0. 0. 0. 0.] ##  [0. 0. 0. 0. 0.] ##  [0. 0. 1. 1. 0.] ##  [0. 0. 1. 1. 0.] ##  [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)"},{"path":"https://keras.posit.co/reference/op_slice_update.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update an input by slicing in a tensor of updated values. — op_slice_update","text":"","code":"op_slice_update(inputs, start_indices, updates)"},{"path":"https://keras.posit.co/reference/op_slice_update.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update an input by slicing in a tensor of updated values. — op_slice_update","text":"inputs tensor, tensor updated. start_indices list length inputs$ndim, specifying starting indices updating. updates tensor, new values put inputs indices. updates must rank inputs.","code":""},{"path":"https://keras.posit.co/reference/op_slice_update.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update an input by slicing in a tensor of updated values. — op_slice_update","text":"","code":"A tensor, has the same shape and dtype as `inputs`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_softmax.html","id":null,"dir":"Reference","previous_headings":"","what":"Softmax activation function. — op_softmax","title":"Softmax activation function. — op_softmax","text":"elements output vector lie within range (0, 1), total sum exactly 1 (excluding floating point rounding error). vector processed independently. axis argument specifies axis along function applied within input. defined : f(x) = exp(x) / sum(exp(x))","code":""},{"path":"https://keras.posit.co/reference/op_softmax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Softmax activation function. — op_softmax","text":"","code":"op_softmax(x, axis = -1L)"},{"path":"https://keras.posit.co/reference/op_softmax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Softmax activation function. — op_softmax","text":"x Input tensor. axis Integer, axis along softmax applied.","code":""},{"path":"https://keras.posit.co/reference/op_softmax.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Softmax activation function. — op_softmax","text":"tensor shape x.","code":""},{"path":"https://keras.posit.co/reference/op_softmax.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Softmax activation function. — op_softmax","text":"","code":"x <- op_array(c(-1, 0, 1)) op_softmax(x) ## tf.Tensor([0.09003057 0.24472848 0.66524094], shape=(3), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_softplus.html","id":null,"dir":"Reference","previous_headings":"","what":"Softplus activation function. — op_softplus","title":"Softplus activation function. — op_softplus","text":"defined f(x) = log(exp(x) + 1), log natural logarithm exp exponential function.","code":""},{"path":"https://keras.posit.co/reference/op_softplus.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Softplus activation function. — op_softplus","text":"","code":"op_softplus(x)"},{"path":"https://keras.posit.co/reference/op_softplus.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Softplus activation function. — op_softplus","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_softplus.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Softplus activation function. — op_softplus","text":"tensor shape x.","code":""},{"path":"https://keras.posit.co/reference/op_softplus.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Softplus activation function. — op_softplus","text":"","code":"x <- op_convert_to_tensor(c(-0.555, 0, 0.555)) op_softplus(x) ## tf.Tensor([0.45366603 0.6931472  1.008666  ], shape=(3), dtype=float32) x <- seq(-10, 10, .1) plot(x, op_softplus(x))"},{"path":[]},{"path":"https://keras.posit.co/reference/op_softsign.html","id":null,"dir":"Reference","previous_headings":"","what":"Softsign activation function. — op_softsign","title":"Softsign activation function. — op_softsign","text":"defined f(x) = x / (abs(x) + 1).","code":""},{"path":"https://keras.posit.co/reference/op_softsign.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Softsign activation function. — op_softsign","text":"","code":"op_softsign(x)"},{"path":"https://keras.posit.co/reference/op_softsign.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Softsign activation function. — op_softsign","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_softsign.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Softsign activation function. — op_softsign","text":"tensor shape x.","code":""},{"path":"https://keras.posit.co/reference/op_softsign.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Softsign activation function. — op_softsign","text":"","code":"x <- op_convert_to_tensor(c(-0.100, -10.0, 1.0, 0.0, 100.0)) op_softsign(x) ## tf.Tensor([-0.09090909 -0.90909094  0.5         0.          0.990099  ], shape=(5), dtype=float32) x <- seq(-10, 10, .1) plot(x, op_softsign(x), ylim = c(-1, 1))"},{"path":[]},{"path":"https://keras.posit.co/reference/op_solve.html","id":null,"dir":"Reference","previous_headings":"","what":"Solves for x in the equation a * x = b. — op_solve","title":"Solves for x in the equation a * x = b. — op_solve","text":"Solves x equation * x = b.","code":""},{"path":"https://keras.posit.co/reference/op_solve.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Solves for x in the equation a * x = b. — op_solve","text":"","code":"op_solve(a, b)"},{"path":"https://keras.posit.co/reference/op_solve.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Solves for x in the equation a * x = b. — op_solve","text":"Input tensor. b Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_solve.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Solves for x in the equation a * x = b. — op_solve","text":"tensor shape dtype .","code":""},{"path":"https://keras.posit.co/reference/op_solve.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Solves for x in the equation a * x = b. — op_solve","text":"","code":"a <- op_array(c(1, 2, 4, 5), dtype=\"float32\") |> op_reshape(c(2, 2)) b <- op_array(c(2, 4, 8, 10), dtype=\"float32\") |> op_reshape(c(2, 2)) op_solve(a, b) ## tf.Tensor( ## [[2. 0.] ##  [0. 2.]], shape=(2, 2), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_sort.html","id":null,"dir":"Reference","previous_headings":"","what":"Sorts the elements of x along a given axis in ascending order. — op_sort","title":"Sorts the elements of x along a given axis in ascending order. — op_sort","text":"Sorts elements x along given axis ascending order.","code":""},{"path":"https://keras.posit.co/reference/op_sort.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sorts the elements of x along a given axis in ascending order. — op_sort","text":"","code":"op_sort(x, axis = -1L)"},{"path":"https://keras.posit.co/reference/op_sort.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sorts the elements of x along a given axis in ascending order. — op_sort","text":"x Input tensor. axis Axis along sort. NULL, tensor flattened sorting. Defaults -1; last axis.","code":""},{"path":"https://keras.posit.co/reference/op_sort.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sorts the elements of x along a given axis in ascending order. — op_sort","text":"","code":"Sorted tensor."},{"path":[]},{"path":"https://keras.posit.co/reference/op_sparse_categorical_crossentropy.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes sparse categorical cross-entropy loss. — op_sparse_categorical_crossentropy","title":"Computes sparse categorical cross-entropy loss. — op_sparse_categorical_crossentropy","text":"sparse categorical cross-entropy loss similar categorical cross-entropy, used target tensor contains integer class labels instead one-hot encoded vectors. measures dissimilarity target output probabilities logits.","code":""},{"path":"https://keras.posit.co/reference/op_sparse_categorical_crossentropy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes sparse categorical cross-entropy loss. — op_sparse_categorical_crossentropy","text":"","code":"op_sparse_categorical_crossentropy(   target,   output,   from_logits = FALSE,   axis = -1L )"},{"path":"https://keras.posit.co/reference/op_sparse_categorical_crossentropy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes sparse categorical cross-entropy loss. — op_sparse_categorical_crossentropy","text":"target target tensor representing true class labels integers. shape match shape output tensor except last dimension. output output tensor representing predicted probabilities logits. shape match shape target tensor except last dimension. from_logits (optional) Whether output tensor logits probabilities. Set TRUE output represents logits; otherwise, set FALSE output represents probabilities. Defaults toFALSE. axis (optional) axis along sparse categorical cross-entropy computed. Defaults -1, corresponds last dimension tensors.","code":""},{"path":"https://keras.posit.co/reference/op_sparse_categorical_crossentropy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes sparse categorical cross-entropy loss. — op_sparse_categorical_crossentropy","text":"Integer tensor: computed sparse categorical cross-entropy loss target output.","code":""},{"path":"https://keras.posit.co/reference/op_sparse_categorical_crossentropy.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes sparse categorical cross-entropy loss. — op_sparse_categorical_crossentropy","text":"","code":"target <- op_array(c(0, 1, 2), dtype=\"int32\") output <- op_array(rbind(c(0.9, 0.05, 0.05),                         c(0.1, 0.8,  0.1),                         c(0.2, 0.3,  0.5))) op_sparse_categorical_crossentropy(target, output) ## tf.Tensor([0.10536052 0.22314355 0.69314718], shape=(3), dtype=float64)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_split.html","id":null,"dir":"Reference","previous_headings":"","what":"Split a tensor into chunks. — op_split","title":"Split a tensor into chunks. — op_split","text":"Split tensor chunks.","code":""},{"path":"https://keras.posit.co/reference/op_split.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split a tensor into chunks. — op_split","text":"","code":"op_split(x, indices_or_sections, axis = 1L)"},{"path":"https://keras.posit.co/reference/op_split.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split a tensor into chunks. — op_split","text":"x Input tensor. indices_or_sections Either integer indicating number sections along axis list integers indicating indices along axis tensor split. integer, N, tensor split N equal sections along axis. 1-D array sorted integers, entries indicate indices tensor split along axis. axis Axis along split. Defaults 0.","code":""},{"path":"https://keras.posit.co/reference/op_split.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Split a tensor into chunks. — op_split","text":"","code":"A list of tensors."},{"path":"https://keras.posit.co/reference/op_split.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Split a tensor into chunks. — op_split","text":"split result equal division using Torch backend.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/op_sqrt.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the non-negative square root of a tensor, element-wise. — op_sqrt","title":"Return the non-negative square root of a tensor, element-wise. — op_sqrt","text":"Return non-negative square root tensor, element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_sqrt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the non-negative square root of a tensor, element-wise. — op_sqrt","text":"","code":"op_sqrt(x)"},{"path":"https://keras.posit.co/reference/op_sqrt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the non-negative square root of a tensor, element-wise. — op_sqrt","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_sqrt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the non-negative square root of a tensor, element-wise. — op_sqrt","text":"","code":"Output tensor, the non-negative square root of `x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_square.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the element-wise square of the input. — op_square","title":"Return the element-wise square of the input. — op_square","text":"Return element-wise square input.","code":""},{"path":"https://keras.posit.co/reference/op_square.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the element-wise square of the input. — op_square","text":"","code":"op_square(x)"},{"path":"https://keras.posit.co/reference/op_square.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the element-wise square of the input. — op_square","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_square.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the element-wise square of the input. — op_square","text":"","code":"Output tensor, the square of `x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_squeeze.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove axes of length one from x. — op_squeeze","title":"Remove axes of length one from x. — op_squeeze","text":"Remove axes length one x.","code":""},{"path":"https://keras.posit.co/reference/op_squeeze.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove axes of length one from x. — op_squeeze","text":"","code":"op_squeeze(x, axis = NULL)"},{"path":"https://keras.posit.co/reference/op_squeeze.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove axes of length one from x. — op_squeeze","text":"x Input tensor. axis Select subset entries length one shape.","code":""},{"path":"https://keras.posit.co/reference/op_squeeze.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove axes of length one from x. — op_squeeze","text":"input tensor subset dimensions length 1 removed.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/op_stack.html","id":null,"dir":"Reference","previous_headings":"","what":"Join a sequence of tensors along a new axis. — op_stack","title":"Join a sequence of tensors along a new axis. — op_stack","text":"axis parameter specifies index new axis dimensions result.","code":""},{"path":"https://keras.posit.co/reference/op_stack.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Join a sequence of tensors along a new axis. — op_stack","text":"","code":"op_stack(x, axis = 1L)"},{"path":"https://keras.posit.co/reference/op_stack.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Join a sequence of tensors along a new axis. — op_stack","text":"x sequence tensors. axis Axis along stack. Defaults 0.","code":""},{"path":"https://keras.posit.co/reference/op_stack.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Join a sequence of tensors along a new axis. — op_stack","text":"","code":"The stacked tensor."},{"path":[]},{"path":"https://keras.posit.co/reference/op_std.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the standard deviation along the specified axis. — op_std","title":"Compute the standard deviation along the specified axis. — op_std","text":"Compute standard deviation along specified axis.","code":""},{"path":"https://keras.posit.co/reference/op_std.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the standard deviation along the specified axis. — op_std","text":"","code":"op_std(x, axis = NULL, keepdims = FALSE)"},{"path":"https://keras.posit.co/reference/op_std.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the standard deviation along the specified axis. — op_std","text":"x Input tensor. axis Axis along compute standard deviation. Default compute standard deviation flattened tensor. keepdims set TRUE, axes reduced left result dimensions size one.","code":""},{"path":"https://keras.posit.co/reference/op_std.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the standard deviation along the specified axis. — op_std","text":"","code":"Output tensor containing the standard deviation values."},{"path":[]},{"path":"https://keras.posit.co/reference/op_stft.html","id":null,"dir":"Reference","previous_headings":"","what":"Short-Time Fourier Transform along the last axis of the input. — op_stft","title":"Short-Time Fourier Transform along the last axis of the input. — op_stft","text":"STFT computes Fourier transform short overlapping windows input. giving frequency components signal change time.","code":""},{"path":"https://keras.posit.co/reference/op_stft.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Short-Time Fourier Transform along the last axis of the input. — op_stft","text":"","code":"op_stft(   x,   sequence_length,   sequence_stride,   fft_length,   window = \"hann\",   center = TRUE )"},{"path":"https://keras.posit.co/reference/op_stft.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Short-Time Fourier Transform along the last axis of the input. — op_stft","text":"x Input tensor. sequence_length integer representing sequence length. sequence_stride integer representing sequence hop size. fft_length integer representing size FFT apply. specified, uses smallest power 2 enclosing sequence_length. window string, tensor window NULL. window string, available values \"hann\" \"hamming\". window tensor, used directly window length must sequence_length. window NULL, windowing used. Defaults \"hann\". center Whether pad x sides t-th sequence centered time t * sequence_stride. Otherwise, t-th sequence begins time t * sequence_stride. Defaults TRUE.","code":""},{"path":"https://keras.posit.co/reference/op_stft.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Short-Time Fourier Transform along the last axis of the input. — op_stft","text":"list containing two tensors - real imaginary parts STFT output.","code":""},{"path":"https://keras.posit.co/reference/op_stft.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Short-Time Fourier Transform along the last axis of the input. — op_stft","text":"","code":"x <- op_array(c(0, 1, 2, 3, 4)) op_stft(x, 3, 2, 3) ## [[1]] ## tf.Tensor( ## [[ 0.  0.] ##  [ 2. -1.] ##  [ 4. -2.]], shape=(3, 2), dtype=float32) ## ## [[2]] ## tf.Tensor( ## [[ 0.         0.       ] ##  [ 0.        -1.7320508] ##  [ 0.        -3.4641016]], shape=(3, 2), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_stop_gradient.html","id":null,"dir":"Reference","previous_headings":"","what":"Stops gradient computation. — op_stop_gradient","title":"Stops gradient computation. — op_stop_gradient","text":"Stops gradient computation.","code":""},{"path":"https://keras.posit.co/reference/op_stop_gradient.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Stops gradient computation. — op_stop_gradient","text":"","code":"op_stop_gradient(variable)"},{"path":"https://keras.posit.co/reference/op_stop_gradient.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Stops gradient computation. — op_stop_gradient","text":"variable tensor variable gradient computation disabled.","code":""},{"path":"https://keras.posit.co/reference/op_stop_gradient.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Stops gradient computation. — op_stop_gradient","text":"variable gradient computation disabled.","code":""},{"path":"https://keras.posit.co/reference/op_stop_gradient.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Stops gradient computation. — op_stop_gradient","text":"","code":"var <- op_convert_to_tensor(c(1, 2, 3), dtype=\"float32\") var <- op_stop_gradient(var)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_subtract.html","id":null,"dir":"Reference","previous_headings":"","what":"Subtract arguments element-wise. — op_subtract","title":"Subtract arguments element-wise. — op_subtract","text":"Subtract arguments element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_subtract.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Subtract arguments element-wise. — op_subtract","text":"","code":"op_subtract(x1, x2)"},{"path":"https://keras.posit.co/reference/op_subtract.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Subtract arguments element-wise. — op_subtract","text":"x1 First input tensor. x2 Second input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_subtract.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Subtract arguments element-wise. — op_subtract","text":"","code":"Output tensor, element-wise difference of `x1` and `x2`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_sum.html","id":null,"dir":"Reference","previous_headings":"","what":"Sum of a tensor over the given axes. — op_sum","title":"Sum of a tensor over the given axes. — op_sum","text":"Sum tensor given axes.","code":""},{"path":"https://keras.posit.co/reference/op_sum.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sum of a tensor over the given axes. — op_sum","text":"","code":"op_sum(x, axis = NULL, keepdims = FALSE)"},{"path":"https://keras.posit.co/reference/op_sum.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sum of a tensor over the given axes. — op_sum","text":"x Input tensor. axis Axis axes along sum computed. default compute sum flattened tensor. keepdims set TRUE, axes reduced left result dimensions size one.","code":""},{"path":"https://keras.posit.co/reference/op_sum.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sum of a tensor over the given axes. — op_sum","text":"","code":"Output tensor containing the sum."},{"path":[]},{"path":"https://keras.posit.co/reference/op_swapaxes.html","id":null,"dir":"Reference","previous_headings":"","what":"Interchange two axes of a tensor. — op_swapaxes","title":"Interchange two axes of a tensor. — op_swapaxes","text":"Interchange two axes tensor.","code":""},{"path":"https://keras.posit.co/reference/op_swapaxes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interchange two axes of a tensor. — op_swapaxes","text":"","code":"op_swapaxes(x, axis1, axis2)"},{"path":"https://keras.posit.co/reference/op_swapaxes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Interchange two axes of a tensor. — op_swapaxes","text":"x Input tensor. axis1 First axis. axis2 Second axis.","code":""},{"path":"https://keras.posit.co/reference/op_swapaxes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Interchange two axes of a tensor. — op_swapaxes","text":"","code":"A tensor with the axes swapped."},{"path":[]},{"path":"https://keras.posit.co/reference/op_take.html","id":null,"dir":"Reference","previous_headings":"","what":"Take elements from a tensor along an axis. — op_take","title":"Take elements from a tensor along an axis. — op_take","text":"Take elements tensor along axis.","code":""},{"path":"https://keras.posit.co/reference/op_take.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Take elements from a tensor along an axis. — op_take","text":"","code":"op_take(x, indices, axis = NULL)"},{"path":"https://keras.posit.co/reference/op_take.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Take elements from a tensor along an axis. — op_take","text":"x Source tensor. indices indices values extract. axis axis select values. default, flattened input tensor used.","code":""},{"path":"https://keras.posit.co/reference/op_take.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Take elements from a tensor along an axis. — op_take","text":"","code":"The corresponding tensor of values."},{"path":[]},{"path":"https://keras.posit.co/reference/op_take_along_axis.html","id":null,"dir":"Reference","previous_headings":"","what":"Select values from x at the 1-D indices along the given axis. — op_take_along_axis","title":"Select values from x at the 1-D indices along the given axis. — op_take_along_axis","text":"Select values x 1-D indices along given axis.","code":""},{"path":"https://keras.posit.co/reference/op_take_along_axis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select values from x at the 1-D indices along the given axis. — op_take_along_axis","text":"","code":"op_take_along_axis(x, indices, axis = NULL)"},{"path":"https://keras.posit.co/reference/op_take_along_axis.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select values from x at the 1-D indices along the given axis. — op_take_along_axis","text":"x Source tensor. indices indices values extract. axis axis select values. default, flattened input tensor used.","code":""},{"path":"https://keras.posit.co/reference/op_take_along_axis.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select values from x at the 1-D indices along the given axis. — op_take_along_axis","text":"","code":"The corresponding tensor of values."},{"path":[]},{"path":"https://keras.posit.co/reference/op_tan.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute tangent, element-wise. — op_tan","title":"Compute tangent, element-wise. — op_tan","text":"Compute tangent, element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_tan.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute tangent, element-wise. — op_tan","text":"","code":"op_tan(x)"},{"path":"https://keras.posit.co/reference/op_tan.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute tangent, element-wise. — op_tan","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_tan.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute tangent, element-wise. — op_tan","text":"","code":"Output tensor of same shape as `x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_tanh.html","id":null,"dir":"Reference","previous_headings":"","what":"Hyperbolic tangent, element-wise. — op_tanh","title":"Hyperbolic tangent, element-wise. — op_tanh","text":"Hyperbolic tangent, element-wise.","code":""},{"path":"https://keras.posit.co/reference/op_tanh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperbolic tangent, element-wise. — op_tanh","text":"","code":"op_tanh(x)"},{"path":"https://keras.posit.co/reference/op_tanh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hyperbolic tangent, element-wise. — op_tanh","text":"x Input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_tanh.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Hyperbolic tangent, element-wise. — op_tanh","text":"","code":"Output tensor of same shape as `x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_tensordot.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the tensor dot product along specified axes. — op_tensordot","title":"Compute the tensor dot product along specified axes. — op_tensordot","text":"Compute tensor dot product along specified axes.","code":""},{"path":"https://keras.posit.co/reference/op_tensordot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the tensor dot product along specified axes. — op_tensordot","text":"","code":"op_tensordot(x1, x2, axes = 3L)"},{"path":"https://keras.posit.co/reference/op_tensordot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the tensor dot product along specified axes. — op_tensordot","text":"x1 First tensor. x2 Second tensor. axes integer, N, sum last N axes x1 first N axes x2 order. sizes corresponding axes must match. , list axes summed , first sequence applying x1, second x2. sequences must length.","code":""},{"path":"https://keras.posit.co/reference/op_tensordot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the tensor dot product along specified axes. — op_tensordot","text":"","code":"The tensor dot product of the inputs."},{"path":[]},{"path":"https://keras.posit.co/reference/op_tile.html","id":null,"dir":"Reference","previous_headings":"","what":"Repeat x the number of times given by repeats. — op_tile","title":"Repeat x the number of times given by repeats. — op_tile","text":"repeats length d, result dimension max(d, x.ndim). x.ndim < d, x promoted d-dimensional prepending new axes. x.ndim > d, repeats promoted x.ndim prepending 1's .","code":""},{"path":"https://keras.posit.co/reference/op_tile.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Repeat x the number of times given by repeats. — op_tile","text":"","code":"op_tile(x, repeats)"},{"path":"https://keras.posit.co/reference/op_tile.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Repeat x the number of times given by repeats. — op_tile","text":"x Input tensor. repeats number repetitions x along axis.","code":""},{"path":"https://keras.posit.co/reference/op_tile.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Repeat x the number of times given by repeats. — op_tile","text":"","code":"The tiled output tensor."},{"path":[]},{"path":"https://keras.posit.co/reference/op_top_k.html","id":null,"dir":"Reference","previous_headings":"","what":"Finds the top-k values and their indices in a tensor. — op_top_k","title":"Finds the top-k values and their indices in a tensor. — op_top_k","text":"Finds top-k values indices tensor.","code":""},{"path":"https://keras.posit.co/reference/op_top_k.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Finds the top-k values and their indices in a tensor. — op_top_k","text":"","code":"op_top_k(x, k, sorted = TRUE)"},{"path":"https://keras.posit.co/reference/op_top_k.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Finds the top-k values and their indices in a tensor. — op_top_k","text":"x Input tensor. k integer representing number top elements retrieve. sorted boolean indicating whether sort output descending order. Defaults toTRUE.","code":""},{"path":"https://keras.posit.co/reference/op_top_k.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Finds the top-k values and their indices in a tensor. — op_top_k","text":"list containing two tensors. first tensor contains top-k values, second tensor contains indices top-k values input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_top_k.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Finds the top-k values and their indices in a tensor. — op_top_k","text":"","code":"x <- op_array(c(5, 2, 7, 1, 9, 3), \"int32\") op_top_k(x, k = 3) ## $values ## tf.Tensor([9 7 5], shape=(3), dtype=int32) ## ## $indices ## tf.Tensor([4 2 0], shape=(3), dtype=int32) c(values, indices) %<-% op_top_k(x, k = 3) values ## tf.Tensor([9 7 5], shape=(3), dtype=int32) indices ## tf.Tensor([4 2 0], shape=(3), dtype=int32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_trace.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the sum along diagonals of the tensor. — op_trace","title":"Return the sum along diagonals of the tensor. — op_trace","text":"x 2-D, sum along diagonal given offset returned, .e., sum elements x[, +offset] . two dimensions, axes specified axis1 axis2 used determine 2-D sub-arrays whose traces returned. shape resulting tensor x axis1 axis2 removed.","code":""},{"path":"https://keras.posit.co/reference/op_trace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the sum along diagonals of the tensor. — op_trace","text":"","code":"op_trace(x, offset = 0L, axis1 = 0L, axis2 = 1L)"},{"path":"https://keras.posit.co/reference/op_trace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the sum along diagonals of the tensor. — op_trace","text":"x Input tensor. offset Offset diagonal main diagonal. Can positive negative. Defaults 0. axis1 Axis used first axis 2-D sub-arrays. Defaults 0.(first axis). axis2 Axis used second axis 2-D sub-arrays. Defaults 1 (second axis).","code":""},{"path":"https://keras.posit.co/reference/op_trace.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the sum along diagonals of the tensor. — op_trace","text":"x 2-D, sum diagonal returned. x larger dimensions, tensor sums along diagonals returned.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/op_transpose.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns a tensor with axes transposed. — op_transpose","title":"Returns a tensor with axes transposed. — op_transpose","text":"Returns tensor axes transposed.","code":""},{"path":"https://keras.posit.co/reference/op_transpose.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns a tensor with axes transposed. — op_transpose","text":"","code":"op_transpose(x, axes = NULL)"},{"path":"https://keras.posit.co/reference/op_transpose.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns a tensor with axes transposed. — op_transpose","text":"x Input tensor. axes Sequence integers. Permutation dimensions x. default, order axes reversed.","code":""},{"path":"https://keras.posit.co/reference/op_transpose.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns a tensor with axes transposed. — op_transpose","text":"x axes permuted.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/op_tri.html","id":null,"dir":"Reference","previous_headings":"","what":"Return a tensor with ones at and below a diagonal and zeros elsewhere. — op_tri","title":"Return a tensor with ones at and below a diagonal and zeros elsewhere. — op_tri","text":"Return tensor ones diagonal zeros elsewhere.","code":""},{"path":"https://keras.posit.co/reference/op_tri.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return a tensor with ones at and below a diagonal and zeros elsewhere. — op_tri","text":"","code":"op_tri(N, M = NULL, k = 0L, dtype = NULL)"},{"path":"https://keras.posit.co/reference/op_tri.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return a tensor with ones at and below a diagonal and zeros elsewhere. — op_tri","text":"N Number rows tensor. M Number columns tensor. k sub-diagonal array filled. k = 0 main diagonal, k < 0 , k > 0 . default 0. dtype Data type returned tensor. default \"float32\".","code":""},{"path":"https://keras.posit.co/reference/op_tri.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return a tensor with ones at and below a diagonal and zeros elsewhere. — op_tri","text":"Tensor lower triangle filled ones zeros elsewhere. T[, j] == 1 j <= + k, 0 otherwise.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/op_tril.html","id":null,"dir":"Reference","previous_headings":"","what":"Return lower triangle of a tensor. — op_tril","title":"Return lower triangle of a tensor. — op_tril","text":"tensors ndim exceeding 2, tril apply final two axes.","code":""},{"path":"https://keras.posit.co/reference/op_tril.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return lower triangle of a tensor. — op_tril","text":"","code":"op_tril(x, k = 0L)"},{"path":"https://keras.posit.co/reference/op_tril.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return lower triangle of a tensor. — op_tril","text":"x Input tensor. k Diagonal zero elements. Defaults 0. main diagonal. k < 0 , k > 0 .","code":""},{"path":"https://keras.posit.co/reference/op_tril.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return lower triangle of a tensor. — op_tril","text":"","code":"Lower triangle of `x`, of same shape and data type as `x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_triu.html","id":null,"dir":"Reference","previous_headings":"","what":"Return upper triangle of a tensor. — op_triu","title":"Return upper triangle of a tensor. — op_triu","text":"tensors ndim exceeding 2, triu apply final two axes.","code":""},{"path":"https://keras.posit.co/reference/op_triu.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return upper triangle of a tensor. — op_triu","text":"","code":"op_triu(x, k = 0L)"},{"path":"https://keras.posit.co/reference/op_triu.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return upper triangle of a tensor. — op_triu","text":"x Input tensor. k Diagonal zero elements. Defaults 0. main diagonal. k < 0 , k > 0 .","code":""},{"path":"https://keras.posit.co/reference/op_triu.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return upper triangle of a tensor. — op_triu","text":"","code":"Upper triangle of `x`, of same shape and data type as `x`."},{"path":[]},{"path":"https://keras.posit.co/reference/op_true_divide.html","id":null,"dir":"Reference","previous_headings":"","what":"Alias for keras.ops.divide. — op_true_divide","title":"Alias for keras.ops.divide. — op_true_divide","text":"Alias keras.ops.divide.","code":""},{"path":"https://keras.posit.co/reference/op_true_divide.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Alias for keras.ops.divide. — op_true_divide","text":"","code":"op_true_divide(x1, x2)"},{"path":"https://keras.posit.co/reference/op_true_divide.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Alias for keras.ops.divide. — op_true_divide","text":"x1 see description x2 see description","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/op_unstack.html","id":null,"dir":"Reference","previous_headings":"","what":"Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors. — op_unstack","title":"Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors. — op_unstack","text":"Unpacks given dimension rank-R tensor rank-(R-1) tensors.","code":""},{"path":"https://keras.posit.co/reference/op_unstack.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors. — op_unstack","text":"","code":"op_unstack(x, num = NULL, axis = 1L)"},{"path":"https://keras.posit.co/reference/op_unstack.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors. — op_unstack","text":"x input tensor. num length dimension axis. Automatically inferred NULL. axis axis along unpack.","code":""},{"path":"https://keras.posit.co/reference/op_unstack.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors. — op_unstack","text":"list tensors unpacked along given axis.","code":""},{"path":"https://keras.posit.co/reference/op_unstack.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors. — op_unstack","text":"[3, 4))]: R:3,%204))","code":"x <- op_array(rbind(c(1, 2),                    c(3, 4))) op_unstack(x, axis=1) ## [[1]] ## tf.Tensor([1. 2.], shape=(2), dtype=float64) ## ## [[2]] ## tf.Tensor([3. 4.], shape=(2), dtype=float64) op_unstack(x, axis=2) ## [[1]] ## tf.Tensor([1. 3.], shape=(2), dtype=float64) ## ## [[2]] ## tf.Tensor([2. 4.], shape=(2), dtype=float64) all.equal(op_unstack(x),           op_unstack(x, axis = 1)) ## [1] TRUE all.equal(op_unstack(x, axis = -1),           op_unstack(x, axis = 2)) ## [1] TRUE # [array([1, 2)), array([3, 4))]"},{"path":[]},{"path":"https://keras.posit.co/reference/op_var.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the variance along the specified axes. — op_var","title":"Compute the variance along the specified axes. — op_var","text":"Compute variance along specified axes.","code":""},{"path":"https://keras.posit.co/reference/op_var.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the variance along the specified axes. — op_var","text":"","code":"op_var(x, axis = NULL, keepdims = FALSE)"},{"path":"https://keras.posit.co/reference/op_var.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the variance along the specified axes. — op_var","text":"x Input tensor. axis Axis axes along variance computed. default compute variance flattened tensor. keepdims set TRUE, axes reduced left result dimensions size one.","code":""},{"path":"https://keras.posit.co/reference/op_var.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the variance along the specified axes. — op_var","text":"","code":"Output tensor containing the variance."},{"path":[]},{"path":"https://keras.posit.co/reference/op_vdot.html","id":null,"dir":"Reference","previous_headings":"","what":"Return the dot product of two vectors. — op_vdot","title":"Return the dot product of two vectors. — op_vdot","text":"first argument complex, complex conjugate first argument used calculation dot product. Multidimensional tensors flattened dot product taken.","code":""},{"path":"https://keras.posit.co/reference/op_vdot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return the dot product of two vectors. — op_vdot","text":"","code":"op_vdot(x1, x2)"},{"path":"https://keras.posit.co/reference/op_vdot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return the dot product of two vectors. — op_vdot","text":"x1 First input tensor. complex, complex conjugate taken calculation dot product. x2 Second input tensor.","code":""},{"path":"https://keras.posit.co/reference/op_vdot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return the dot product of two vectors. — op_vdot","text":"","code":"Output tensor."},{"path":[]},{"path":"https://keras.posit.co/reference/op_vectorized_map.html","id":null,"dir":"Reference","previous_headings":"","what":"Parallel map of function on axis 0 of tensor(s) elements. — op_vectorized_map","title":"Parallel map of function on axis 0 of tensor(s) elements. — op_vectorized_map","text":"Schematically, vectorized_map implements following, case single tensor input elements:   case iterable tensors elements, implements following:   case, function expected take input single list tensor arguments.","code":"op_vectorized_map <- function(elements, f) {   apply(elements, 1, f) } op_vectorized_map <- function(elements, f) {     batch_size <- elements[[1]] |> shape() |> _[[1]]     outputs <- vector(\"list\", batch_size)     outputs <- lapply(seq(batch_size), \\(index) {         f(lapply(elements, \\(e) e[index, all_dims()]))     }     op_stack(outputs) } (x <- op_arange(4*4) |> op_reshape(c(4,4))) ## tf.Tensor( ## [[ 0.  1.  2.  3.] ##  [ 4.  5.  6.  7.] ##  [ 8.  9. 10. 11.] ##  [12. 13. 14. 15.]], shape=(4, 4), dtype=float64) x |> op_vectorized_map(\\(row) {row + 10}) ## tf.Tensor( ## [[10. 11. 12. 13.] ##  [14. 15. 16. 17.] ##  [18. 19. 20. 21.] ##  [22. 23. 24. 25.]], shape=(4, 4), dtype=float64) list(x, x, x) |> op_vectorized_map(\\(rows) Reduce(`+`, rows)) ## tf.Tensor( ## [[ 0.  3.  6.  9.] ##  [12. 15. 18. 21.] ##  [24. 27. 30. 33.] ##  [36. 39. 42. 45.]], shape=(4, 4), dtype=float64)"},{"path":"https://keras.posit.co/reference/op_vectorized_map.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parallel map of function on axis 0 of tensor(s) elements. — op_vectorized_map","text":"","code":"op_vectorized_map(elements, f)"},{"path":"https://keras.posit.co/reference/op_vectorized_map.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parallel map of function on axis 0 of tensor(s) elements. — op_vectorized_map","text":"elements see description f function taking either tensor, list tensors.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/op_vstack.html","id":null,"dir":"Reference","previous_headings":"","what":"Stack tensors in sequence vertically (row wise). — op_vstack","title":"Stack tensors in sequence vertically (row wise). — op_vstack","text":"Stack tensors sequence vertically (row wise).","code":""},{"path":"https://keras.posit.co/reference/op_vstack.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Stack tensors in sequence vertically (row wise). — op_vstack","text":"","code":"op_vstack(xs)"},{"path":"https://keras.posit.co/reference/op_vstack.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Stack tensors in sequence vertically (row wise). — op_vstack","text":"xs Sequence tensors.","code":""},{"path":"https://keras.posit.co/reference/op_vstack.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Stack tensors in sequence vertically (row wise). — op_vstack","text":"","code":"Tensor formed by stacking the given tensors."},{"path":[]},{"path":"https://keras.posit.co/reference/op_where.html","id":null,"dir":"Reference","previous_headings":"","what":"Return elements chosen from x1 or x2 depending on condition. — op_where","title":"Return elements chosen from x1 or x2 depending on condition. — op_where","text":"Return elements chosen x1 x2 depending condition.","code":""},{"path":"https://keras.posit.co/reference/op_where.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return elements chosen from x1 or x2 depending on condition. — op_where","text":"","code":"op_where(condition, x1 = NULL, x2 = NULL)"},{"path":"https://keras.posit.co/reference/op_where.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return elements chosen from x1 or x2 depending on condition. — op_where","text":"condition TRUE, yield x1, otherwise yield x2. x1 Values choose condition TRUE. x2 Values choose condition FALSE.","code":""},{"path":"https://keras.posit.co/reference/op_where.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return elements chosen from x1 or x2 depending on condition. — op_where","text":"tensor elements x1 condition TRUE, elements x2 condition FALSE.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/op_while_loop.html","id":null,"dir":"Reference","previous_headings":"","what":"While loop implementation. — op_while_loop","title":"While loop implementation. — op_while_loop","text":"loop implementation.","code":""},{"path":"https://keras.posit.co/reference/op_while_loop.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"While loop implementation. — op_while_loop","text":"","code":"op_while_loop(cond, body, loop_vars, maximum_iterations = NULL)"},{"path":"https://keras.posit.co/reference/op_while_loop.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"While loop implementation. — op_while_loop","text":"cond callable represents termination condition loop. Must number args loop_vars, return bool. body callable represents loop body. Must number args loop_vars, return list length, shape dtype loop_vars. loop_vars list tensors, loop variables. maximum_iterations Optional maximum number iterations loop run. provided, cond output -ed additional condition ensuring number iterations executed greater maximum_iterations.","code":""},{"path":"https://keras.posit.co/reference/op_while_loop.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"While loop implementation. — op_while_loop","text":"list tensors, shape dtype loop_vars.","code":""},{"path":"https://keras.posit.co/reference/op_while_loop.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"While loop implementation. — op_while_loop","text":"","code":"i <- 0 loop_vars <- list(i)  # cond() must return a scalar bool cond <- function(i) i < 10L  # body must return same shape as loop_vars body <- function(i) list(i + 1L)  op_while_loop(cond, body, loop_vars) ## [[1]] ## tf.Tensor(10.0, shape=(), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/op_zeros.html","id":null,"dir":"Reference","previous_headings":"","what":"Return a new tensor of given shape and type, filled with zeros. — op_zeros","title":"Return a new tensor of given shape and type, filled with zeros. — op_zeros","text":"Return new tensor given shape type, filled zeros.","code":""},{"path":"https://keras.posit.co/reference/op_zeros.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return a new tensor of given shape and type, filled with zeros. — op_zeros","text":"","code":"op_zeros(shape, dtype = NULL)"},{"path":"https://keras.posit.co/reference/op_zeros.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return a new tensor of given shape and type, filled with zeros. — op_zeros","text":"shape Shape new tensor. dtype Desired data type tensor.","code":""},{"path":"https://keras.posit.co/reference/op_zeros.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return a new tensor of given shape and type, filled with zeros. — op_zeros","text":"","code":"Tensor of zeros with the given shape and dtype."},{"path":[]},{"path":"https://keras.posit.co/reference/op_zeros_like.html","id":null,"dir":"Reference","previous_headings":"","what":"Return a tensor of zeros with the same shape and type as x. — op_zeros_like","title":"Return a tensor of zeros with the same shape and type as x. — op_zeros_like","text":"Return tensor zeros shape type x.","code":""},{"path":"https://keras.posit.co/reference/op_zeros_like.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return a tensor of zeros with the same shape and type as x. — op_zeros_like","text":"","code":"op_zeros_like(x, dtype = NULL)"},{"path":"https://keras.posit.co/reference/op_zeros_like.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return a tensor of zeros with the same shape and type as x. — op_zeros_like","text":"x Input tensor. dtype Overrides data type result.","code":""},{"path":"https://keras.posit.co/reference/op_zeros_like.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return a tensor of zeros with the same shape and type as x. — op_zeros_like","text":"","code":"A tensor of zeros with the same shape and type as `x`."},{"path":[]},{"path":"https://keras.posit.co/reference/optimizer_adadelta.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimizer that implements the Adadelta algorithm. — optimizer_adadelta","title":"Optimizer that implements the Adadelta algorithm. — optimizer_adadelta","text":"Adadelta optimization stochastic gradient descent method based adaptive learning rate per dimension address two drawbacks: continual decay learning rates throughout training. need manually selected global learning rate. Adadelta robust extension Adagrad adapts learning rates based moving window gradient updates, instead accumulating past gradients. way, Adadelta continues learning even many updates done. Compared Adagrad, original version Adadelta set initial learning rate. version, initial learning rate can set, Keras optimizers.","code":""},{"path":"https://keras.posit.co/reference/optimizer_adadelta.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimizer that implements the Adadelta algorithm. — optimizer_adadelta","text":"","code":"optimizer_adadelta(   learning_rate = 0.001,   rho = 0.95,   epsilon = 1e-07,   weight_decay = NULL,   clipnorm = NULL,   clipvalue = NULL,   global_clipnorm = NULL,   use_ema = FALSE,   ema_momentum = 0.99,   ema_overwrite_frequency = NULL,   name = \"adadelta\",   ...,   loss_scale_factor = NULL )"},{"path":"https://keras.posit.co/reference/optimizer_adadelta.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimizer that implements the Adadelta algorithm. — optimizer_adadelta","text":"learning_rate float, keras.optimizers.schedules.LearningRateSchedule instance, callable takes arguments returns actual value use. learning rate. Defaults 0.001. Note Adadelta tends benefit higher initial learning rate values compared optimizers. match exact form original paper, use 1.0. rho floating point value. decay rate. Defaults 0.95. epsilon Small floating point value maintaining numerical stability. weight_decay Float. set, weight decay applied. clipnorm Float. set, gradient weight individually clipped norm higher value. clipvalue Float. set, gradient weight clipped higher value. global_clipnorm Float. set, gradient weights clipped global norm higher value. use_ema Boolean, defaults FALSE. TRUE, exponential moving average (EMA) applied. EMA consists computing exponential moving average weights model (weight values change training batch), periodically overwriting weights moving average. ema_momentum Float, defaults 0.99. used use_ema=TRUE. momentum use computing EMA model's weights: new_average = ema_momentum * old_average + (1 - ema_momentum) * current_variable_value. ema_overwrite_frequency Int NULL, defaults NULL. used use_ema=TRUE. Every ema_overwrite_frequency steps iterations, overwrite model variable moving average. NULL, optimizer overwrite model variables middle training, need explicitly overwrite variables end training calling optimizer.finalize_variable_values() (updates model variables -place). using built-fit() training loop, happens automatically last epoch, need anything. name String. name use momentum accumulator weights created optimizer. ... forward/backward compatability. loss_scale_factor Float NULL. float, scale factor multiplied loss computing gradients, inverse scale factor multiplied gradients updating variables. Useful preventing underflow mixed precision training. Alternately, keras.optimizers.LossScaleOptimizer automatically set loss scale factor.","code":""},{"path":"https://keras.posit.co/reference/optimizer_adadelta.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Optimizer that implements the Adadelta algorithm. — optimizer_adadelta","text":"Zeiler, 2012","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/optimizer_adafactor.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimizer that implements the Adafactor algorithm. — optimizer_adafactor","title":"Optimizer that implements the Adafactor algorithm. — optimizer_adafactor","text":"Adafactor commonly used NLP tasks, advantage taking less memory saves partial information previous gradients. default argument setup based original paper (see reference). gradients dimension > 2, Adafactor optimizer delete last 2 dimensions separately accumulator variables.","code":""},{"path":"https://keras.posit.co/reference/optimizer_adafactor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimizer that implements the Adafactor algorithm. — optimizer_adafactor","text":"","code":"optimizer_adafactor(   learning_rate = 0.001,   beta_2_decay = -0.8,   epsilon_1 = 1e-30,   epsilon_2 = 0.001,   clip_threshold = 1,   relative_step = TRUE,   weight_decay = NULL,   clipnorm = NULL,   clipvalue = NULL,   global_clipnorm = NULL,   use_ema = FALSE,   ema_momentum = 0.99,   ema_overwrite_frequency = NULL,   name = \"adafactor\",   ...,   loss_scale_factor = NULL )"},{"path":"https://keras.posit.co/reference/optimizer_adafactor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimizer that implements the Adafactor algorithm. — optimizer_adafactor","text":"learning_rate float, keras.optimizers.schedules.LearningRateSchedule instance, callable takes arguments returns actual value use. learning rate. Defaults 0.001. beta_2_decay float, defaults -0.8. decay rate beta_2. epsilon_1 float, defaults 1e-30. small offset keep denominator away 0. epsilon_2 float, defaults 1e-3. small offset avoid learning rate becoming small time. clip_threshold float, defaults 1.0. Clipping threshold. part Adafactor algorithm, independent clipnorm, clipvalue, global_clipnorm. relative_step bool, defaults TRUE. learning_rate constant relative_step=TRUE, learning rate adjusted based current iterations. default learning rate decay Adafactor. weight_decay Float. set, weight decay applied. clipnorm Float. set, gradient weight individually clipped norm higher value. clipvalue Float. set, gradient weight clipped higher value. global_clipnorm Float. set, gradient weights clipped global norm higher value. use_ema Boolean, defaults FALSE. TRUE, exponential moving average (EMA) applied. EMA consists computing exponential moving average weights model (weight values change training batch), periodically overwriting weights moving average. ema_momentum Float, defaults 0.99. used use_ema=TRUE. momentum use computing EMA model's weights: new_average = ema_momentum * old_average + (1 - ema_momentum) * current_variable_value. ema_overwrite_frequency Int NULL, defaults NULL. used use_ema=TRUE. Every ema_overwrite_frequency steps iterations, overwrite model variable moving average. NULL, optimizer overwrite model variables middle training, need explicitly overwrite variables end training calling optimizer.finalize_variable_values() (updates model variables -place). using built-fit() training loop, happens automatically last epoch, need anything. name String. name use momentum accumulator weights created optimizer. ... forward/backward compatability. loss_scale_factor Float NULL. float, scale factor multiplied loss computing gradients, inverse scale factor multiplied gradients updating variables. Useful preventing underflow mixed precision training. Alternately, keras.optimizers.LossScaleOptimizer automatically set loss scale factor.","code":""},{"path":"https://keras.posit.co/reference/optimizer_adafactor.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Optimizer that implements the Adafactor algorithm. — optimizer_adafactor","text":"Shazeer, Noam et al., 2018.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/optimizer_adagrad.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimizer that implements the Adagrad algorithm. — optimizer_adagrad","title":"Optimizer that implements the Adagrad algorithm. — optimizer_adagrad","text":"Adagrad optimizer parameter-specific learning rates, adapted relative frequently parameter gets updated training. updates parameter receives, smaller updates.","code":""},{"path":"https://keras.posit.co/reference/optimizer_adagrad.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimizer that implements the Adagrad algorithm. — optimizer_adagrad","text":"","code":"optimizer_adagrad(   learning_rate = 0.001,   initial_accumulator_value = 0.1,   epsilon = 1e-07,   weight_decay = NULL,   clipnorm = NULL,   clipvalue = NULL,   global_clipnorm = NULL,   use_ema = FALSE,   ema_momentum = 0.99,   ema_overwrite_frequency = NULL,   name = \"adagrad\",   ...,   loss_scale_factor = NULL )"},{"path":"https://keras.posit.co/reference/optimizer_adagrad.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimizer that implements the Adagrad algorithm. — optimizer_adagrad","text":"learning_rate float, keras.optimizers.schedules.LearningRateSchedule instance, callable takes arguments returns actual value use. learning rate. Defaults 0.001. Note Adagrad tends benefit higher initial learning rate values compared optimizers. match exact form original paper, use 1.0. initial_accumulator_value Floating point value. Starting value accumulators (per-parameter momentum values). Must non-negative. epsilon Small floating point value maintaining numerical stability. weight_decay Float. set, weight decay applied. clipnorm Float. set, gradient weight individually clipped norm higher value. clipvalue Float. set, gradient weight clipped higher value. global_clipnorm Float. set, gradient weights clipped global norm higher value. use_ema Boolean, defaults FALSE. TRUE, exponential moving average (EMA) applied. EMA consists computing exponential moving average weights model (weight values change training batch), periodically overwriting weights moving average. ema_momentum Float, defaults 0.99. used use_ema=TRUE. momentum use computing EMA model's weights: new_average = ema_momentum * old_average + (1 - ema_momentum) * current_variable_value. ema_overwrite_frequency Int NULL, defaults NULL. used use_ema=TRUE. Every ema_overwrite_frequency steps iterations, overwrite model variable moving average. NULL, optimizer overwrite model variables middle training, need explicitly overwrite variables end training calling optimizer.finalize_variable_values() (updates model variables -place). using built-fit() training loop, happens automatically last epoch, need anything. name String. name use momentum accumulator weights created optimizer. ... forward/backward compatability. loss_scale_factor Float NULL. float, scale factor multiplied loss computing gradients, inverse scale factor multiplied gradients updating variables. Useful preventing underflow mixed precision training. Alternately, keras.optimizers.LossScaleOptimizer automatically set loss scale factor.","code":""},{"path":"https://keras.posit.co/reference/optimizer_adagrad.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Optimizer that implements the Adagrad algorithm. — optimizer_adagrad","text":"Duchi et al., 2011.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/optimizer_adam.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimizer that implements the Adam algorithm. — optimizer_adam","title":"Optimizer that implements the Adam algorithm. — optimizer_adam","text":"Adam optimization stochastic gradient descent method based adaptive estimation first-order second-order moments. According Kingma et al., 2014, method \"computationally efficient, little memory requirement, invariant diagonal rescaling gradients, well suited problems large terms data/parameters\".","code":""},{"path":"https://keras.posit.co/reference/optimizer_adam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimizer that implements the Adam algorithm. — optimizer_adam","text":"","code":"optimizer_adam(   learning_rate = 0.001,   beta_1 = 0.9,   beta_2 = 0.999,   epsilon = 1e-07,   amsgrad = FALSE,   weight_decay = NULL,   clipnorm = NULL,   clipvalue = NULL,   global_clipnorm = NULL,   use_ema = FALSE,   ema_momentum = 0.99,   ema_overwrite_frequency = NULL,   name = \"adam\",   ...,   loss_scale_factor = NULL )"},{"path":"https://keras.posit.co/reference/optimizer_adam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimizer that implements the Adam algorithm. — optimizer_adam","text":"learning_rate float, keras.optimizers.schedules.LearningRateSchedule instance, callable takes arguments returns actual value use. learning rate. Defaults 0.001. beta_1 float value constant float tensor, callable takes arguments returns actual value use. exponential decay rate 1st moment estimates. Defaults 0.9. beta_2 float value constant float tensor, callable takes arguments returns actual value use. exponential decay rate 2nd moment estimates. Defaults 0.999. epsilon small constant numerical stability. epsilon \"epsilon hat\" Kingma Ba paper (formula just Section 2.1), epsilon Algorithm 1 paper. Defaults 1e-7. amsgrad Boolean. Whether apply AMSGrad variant algorithm paper \"Convergence Adam beyond\". Defaults FALSE. weight_decay Float. set, weight decay applied. clipnorm Float. set, gradient weight individually clipped norm higher value. clipvalue Float. set, gradient weight clipped higher value. global_clipnorm Float. set, gradient weights clipped global norm higher value. use_ema Boolean, defaults FALSE. TRUE, exponential moving average (EMA) applied. EMA consists computing exponential moving average weights model (weight values change training batch), periodically overwriting weights moving average. ema_momentum Float, defaults 0.99. used use_ema=TRUE. momentum use computing EMA model's weights: new_average = ema_momentum * old_average + (1 - ema_momentum) * current_variable_value. ema_overwrite_frequency Int NULL, defaults NULL. used use_ema=TRUE. Every ema_overwrite_frequency steps iterations, overwrite model variable moving average. NULL, optimizer overwrite model variables middle training, need explicitly overwrite variables end training calling optimizer.finalize_variable_values() (updates model variables -place). using built-fit() training loop, happens automatically last epoch, need anything. name String. name use momentum accumulator weights created optimizer. ... forward/backward compatability. loss_scale_factor Float NULL. float, scale factor multiplied loss computing gradients, inverse scale factor multiplied gradients updating variables. Useful preventing underflow mixed precision training. Alternately, keras.optimizers.LossScaleOptimizer automatically set loss scale factor.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/optimizer_adam_w.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimizer that implements the AdamW algorithm. — optimizer_adam_w","title":"Optimizer that implements the AdamW algorithm. — optimizer_adam_w","text":"AdamW optimization stochastic gradient descent method based adaptive estimation first-order second-order moments added method decay weights per techniques discussed paper, 'Decoupled Weight Decay Regularization' Loshchilov, Hutter et al., 2019. According Kingma et al., 2014, underying Adam method \"computationally efficient, little memory requirement, invariant diagonal rescaling gradients, well suited problems large terms data/parameters\".","code":""},{"path":"https://keras.posit.co/reference/optimizer_adam_w.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimizer that implements the AdamW algorithm. — optimizer_adam_w","text":"","code":"optimizer_adam_w(   learning_rate = 0.001,   weight_decay = 0.004,   beta_1 = 0.9,   beta_2 = 0.999,   epsilon = 1e-07,   amsgrad = FALSE,   clipnorm = NULL,   clipvalue = NULL,   global_clipnorm = NULL,   use_ema = FALSE,   ema_momentum = 0.99,   ema_overwrite_frequency = NULL,   name = \"adamw\",   ...,   loss_scale_factor = NULL )"},{"path":"https://keras.posit.co/reference/optimizer_adam_w.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimizer that implements the AdamW algorithm. — optimizer_adam_w","text":"learning_rate float, keras.optimizers.schedules.LearningRateSchedule instance, callable takes arguments returns actual value use. learning rate. Defaults 0.001. weight_decay Float. set, weight decay applied. beta_1 float value constant float tensor, callable takes arguments returns actual value use. exponential decay rate 1st moment estimates. Defaults 0.9. beta_2 float value constant float tensor, callable takes arguments returns actual value use. exponential decay rate 2nd moment estimates. Defaults 0.999. epsilon small constant numerical stability. epsilon \"epsilon hat\" Kingma Ba paper (formula just Section 2.1), epsilon Algorithm 1 paper. Defaults 1e-7. amsgrad Boolean. Whether apply AMSGrad variant algorithm paper \"Convergence Adam beyond\". Defaults FALSE. clipnorm Float. set, gradient weight individually clipped norm higher value. clipvalue Float. set, gradient weight clipped higher value. global_clipnorm Float. set, gradient weights clipped global norm higher value. use_ema Boolean, defaults FALSE. TRUE, exponential moving average (EMA) applied. EMA consists computing exponential moving average weights model (weight values change training batch), periodically overwriting weights moving average. ema_momentum Float, defaults 0.99. used use_ema=TRUE. momentum use computing EMA model's weights: new_average = ema_momentum * old_average + (1 - ema_momentum) * current_variable_value. ema_overwrite_frequency Int NULL, defaults NULL. used use_ema=TRUE. Every ema_overwrite_frequency steps iterations, overwrite model variable moving average. NULL, optimizer overwrite model variables middle training, need explicitly overwrite variables end training calling optimizer.finalize_variable_values() (updates model variables -place). using built-fit() training loop, happens automatically last epoch, need anything. name String. name use momentum accumulator weights created optimizer. ... forward/backward compatability. loss_scale_factor Float NULL. float, scale factor multiplied loss computing gradients, inverse scale factor multiplied gradients updating variables. Useful preventing underflow mixed precision training. Alternately, keras.optimizers.LossScaleOptimizer automatically set loss scale factor.","code":""},{"path":"https://keras.posit.co/reference/optimizer_adam_w.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimizer that implements the AdamW algorithm. — optimizer_adam_w","text":"Loshchilov et al., 2019 Kingma et al., 2014 adam Reddi et al., 2018 amsgrad.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/optimizer_adamax.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimizer that implements the Adamax algorithm. — optimizer_adamax","title":"Optimizer that implements the Adamax algorithm. — optimizer_adamax","text":"Adamax, variant Adam based infinity norm, first-order gradient-based optimization method. Due capability adjusting learning rate based data characteristics, suited learn time-variant process, e.g., speech data dynamically changed noise conditions. Default parameters follow provided paper (see references ). Initialization:   update rule parameter w gradient g described end section 7.1 paper (see referenece section):","code":"m <- 0  # Initialize initial 1st moment vector u <- 0  # Initialize the exponentially weighted infinity norm t <- 0  # Initialize timestep t <-  t + 1 m <- beta1 * m + (1 - beta) * g u <- max(beta2 * u, abs(g)) current_lr <- learning_rate / (1 - beta1 ** t) w <- w - current_lr * m / (u + epsilon)"},{"path":"https://keras.posit.co/reference/optimizer_adamax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimizer that implements the Adamax algorithm. — optimizer_adamax","text":"","code":"optimizer_adamax(   learning_rate = 0.001,   beta_1 = 0.9,   beta_2 = 0.999,   epsilon = 1e-07,   weight_decay = NULL,   clipnorm = NULL,   clipvalue = NULL,   global_clipnorm = NULL,   use_ema = FALSE,   ema_momentum = 0.99,   ema_overwrite_frequency = NULL,   name = \"adamax\",   ...,   loss_scale_factor = NULL )"},{"path":"https://keras.posit.co/reference/optimizer_adamax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimizer that implements the Adamax algorithm. — optimizer_adamax","text":"learning_rate float, keras.optimizers.schedules.LearningRateSchedule instance, callable takes arguments returns actual value use. learning rate. Defaults 0.001. beta_1 float value constant float tensor. exponential decay rate 1st moment estimates. beta_2 float value constant float tensor. exponential decay rate exponentially weighted infinity norm. epsilon small constant numerical stability. name: String. name use momentum accumulator weights created optimizer. weight_decay Float. set, weight decay applied. clipnorm Float. set, gradient weight individually clipped norm higher value. clipvalue Float. set, gradient weight clipped higher value. global_clipnorm Float. set, gradient weights clipped global norm higher value. use_ema Boolean, defaults FALSE. TRUE, exponential moving average (EMA) applied. EMA consists computing exponential moving average weights model (weight values change training batch), periodically overwriting weights moving average. ema_momentum Float, defaults 0.99. used use_ema=TRUE. momentum use computing EMA model's weights: new_average = ema_momentum * old_average + (1 - ema_momentum) * current_variable_value. ema_overwrite_frequency Int NULL, defaults NULL. used use_ema=TRUE. Every ema_overwrite_frequency steps iterations, overwrite model variable moving average. NULL, optimizer overwrite model variables middle training, need explicitly overwrite variables end training calling optimizer.finalize_variable_values() (updates model variables -place). using built-fit() training loop, happens automatically last epoch, need anything. name String, name object ... forward/backward compatability. loss_scale_factor Float NULL. float, scale factor multiplied loss computing gradients, inverse scale factor multiplied gradients updating variables. Useful preventing underflow mixed precision training. Alternately, keras.optimizers.LossScaleOptimizer automatically set loss scale factor.","code":""},{"path":"https://keras.posit.co/reference/optimizer_adamax.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Optimizer that implements the Adamax algorithm. — optimizer_adamax","text":"Kingma et al., 2014","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/optimizer_ftrl.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimizer that implements the FTRL algorithm. — optimizer_ftrl","title":"Optimizer that implements the FTRL algorithm. — optimizer_ftrl","text":"\"Follow Regularized Leader\" (FTRL) optimization algorithm developed Google click-rate prediction early 2010s. suitable shallow models large sparse feature spaces. algorithm described McMahan et al., 2013. Keras version support online L2 regularization (L2 regularization described paper ) shrinkage-type L2 regularization (addition L2 penalty loss function). Initialization:   Update rule one variable w:   Notation: lr learning rate g gradient variable lambda_1 L1 regularization strength lambda_2 L2 regularization strength lr_power power scale n. Check documentation l2_shrinkage_regularization_strength parameter details shrinkage enabled, case gradient replaced gradient shrinkage.","code":"n <- 0 sigma <- 0 z <- 0 prev_n <- n n <- n + g^2 sigma <- (n^(-lr_power) - prev_n^(-lr_power)) / lr z <- z + g - sigma * w if (abs(z) < lambda_1) {   w <- 0 } else {   w <- (sgn(z) * lambda_1 - z) / ((beta + sqrt(n)) / alpha + lambda_2) }"},{"path":"https://keras.posit.co/reference/optimizer_ftrl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimizer that implements the FTRL algorithm. — optimizer_ftrl","text":"","code":"optimizer_ftrl(   learning_rate = 0.001,   learning_rate_power = -0.5,   initial_accumulator_value = 0.1,   l1_regularization_strength = 0,   l2_regularization_strength = 0,   l2_shrinkage_regularization_strength = 0,   beta = 0,   weight_decay = NULL,   clipnorm = NULL,   clipvalue = NULL,   global_clipnorm = NULL,   use_ema = FALSE,   ema_momentum = 0.99,   ema_overwrite_frequency = NULL,   name = \"ftrl\",   ...,   loss_scale_factor = NULL )"},{"path":"https://keras.posit.co/reference/optimizer_ftrl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimizer that implements the FTRL algorithm. — optimizer_ftrl","text":"learning_rate float, keras.optimizers.schedules.LearningRateSchedule instance, callable takes arguments returns actual value use. learning rate. Defaults 0.001. learning_rate_power float value, must less equal zero. Controls learning rate decreases training. Use zero fixed learning rate. initial_accumulator_value starting value accumulators. zero positive values allowed. l1_regularization_strength float value, must greater equal zero. Defaults 0.0. l2_regularization_strength float value, must greater equal zero. Defaults 0.0. l2_shrinkage_regularization_strength float value, must greater equal zero. differs L2 L2 stabilization penalty, whereas L2 shrinkage magnitude penalty. input sparse shrinkage happen active weights. beta float value, representing beta value paper. Defaults 0.0. weight_decay Float. set, weight decay applied. clipnorm Float. set, gradient weight individually clipped norm higher value. clipvalue Float. set, gradient weight clipped higher value. global_clipnorm Float. set, gradient weights clipped global norm higher value. use_ema Boolean, defaults FALSE. TRUE, exponential moving average (EMA) applied. EMA consists computing exponential moving average weights model (weight values change training batch), periodically overwriting weights moving average. ema_momentum Float, defaults 0.99. used use_ema=TRUE. momentum use computing EMA model's weights: new_average = ema_momentum * old_average + (1 - ema_momentum) * current_variable_value. ema_overwrite_frequency Int NULL, defaults NULL. used use_ema=TRUE. Every ema_overwrite_frequency steps iterations, overwrite model variable moving average. NULL, optimizer overwrite model variables middle training, need explicitly overwrite variables end training calling optimizer$finalize_variable_values() (updates model variables -place). using built-fit() training loop, happens automatically last epoch, need anything. name String. name use momentum accumulator weights created optimizer. ... forward/backward compatability. loss_scale_factor Float NULL. float, scale factor multiplied loss computing gradients, inverse scale factor multiplied gradients updating variables. Useful preventing underflow mixed precision training. Alternately, optimizer_loss_scale automatically set loss scale factor.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/optimizer_lion.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimizer that implements the Lion algorithm. — optimizer_lion","title":"Optimizer that implements the Lion algorithm. — optimizer_lion","text":"Lion optimizer stochastic-gradient-descent method uses sign operator control magnitude update, unlike adaptive optimizers Adam rely second-order moments. make Lion memory-efficient keeps track momentum. According authors (see reference), performance gain Adam grows batch size. update Lion produced sign operation, resulting larger norm, suitable learning rate Lion typically 3-10x smaller AdamW. weight decay Lion turn 3-10x larger AdamW maintain similar strength (lr * wd).","code":""},{"path":"https://keras.posit.co/reference/optimizer_lion.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimizer that implements the Lion algorithm. — optimizer_lion","text":"","code":"optimizer_lion(   learning_rate = 0.001,   beta_1 = 0.9,   beta_2 = 0.99,   weight_decay = NULL,   clipnorm = NULL,   clipvalue = NULL,   global_clipnorm = NULL,   use_ema = FALSE,   ema_momentum = 0.99,   ema_overwrite_frequency = NULL,   name = \"lion\",   ...,   loss_scale_factor = NULL )"},{"path":"https://keras.posit.co/reference/optimizer_lion.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimizer that implements the Lion algorithm. — optimizer_lion","text":"learning_rate float, keras.optimizers.schedules.LearningRateSchedule instance, callable takes arguments returns actual value use. learning rate. Defaults 0.001. beta_1 float value constant float tensor, callable takes arguments returns actual value use. rate combine current gradient 1st moment estimate. Defaults 0.9. beta_2 float value constant float tensor, callable takes arguments returns actual value use. exponential decay rate 1st moment estimate. Defaults 0.99. weight_decay Float. set, weight decay applied. clipnorm Float. set, gradient weight individually clipped norm higher value. clipvalue Float. set, gradient weight clipped higher value. global_clipnorm Float. set, gradient weights clipped global norm higher value. use_ema Boolean, defaults FALSE. TRUE, exponential moving average (EMA) applied. EMA consists computing exponential moving average weights model (weight values change training batch), periodically overwriting weights moving average. ema_momentum Float, defaults 0.99. used use_ema=TRUE. momentum use computing EMA model's weights: new_average = ema_momentum * old_average + (1 - ema_momentum) * current_variable_value. ema_overwrite_frequency Int NULL, defaults NULL. used use_ema=TRUE. Every ema_overwrite_frequency steps iterations, overwrite model variable moving average. NULL, optimizer overwrite model variables middle training, need explicitly overwrite variables end training calling optimizer.finalize_variable_values() (updates model variables -place). using built-fit() training loop, happens automatically last epoch, need anything. name String. name use momentum accumulator weights created optimizer. ... forward/backward compatability. loss_scale_factor Float NULL. float, scale factor multiplied loss computing gradients, inverse scale factor multiplied gradients updating variables. Useful preventing underflow mixed precision training. Alternately, keras.optimizers.LossScaleOptimizer automatically set loss scale factor.","code":""},{"path":"https://keras.posit.co/reference/optimizer_lion.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimizer that implements the Lion algorithm. — optimizer_lion","text":"Chen et al., 2023 Authors' implementation","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/optimizer_loss_scale.html","id":null,"dir":"Reference","previous_headings":"","what":"An optimizer that dynamically scales the loss to prevent underflow. — optimizer_loss_scale","title":"An optimizer that dynamically scales the loss to prevent underflow. — optimizer_loss_scale","text":"Loss scaling technique prevent numeric underflow intermediate gradients float16 used. prevent underflow, loss multiplied (\"scaled\") certain factor called \"loss scale\", causes intermediate gradients scaled loss scale well. final gradients divided (\"unscaled\") loss scale bring back original value. LossScaleOptimizer wraps another optimizer applies dynamic loss scaling . loss scale dynamically updated time follows: train step, nonfinite gradient encountered, loss scale halved, train step skipped. dynamic_growth_steps ocurred since last time loss scale updated, nonfinite gradients occurred, loss scale doubled.","code":""},{"path":"https://keras.posit.co/reference/optimizer_loss_scale.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An optimizer that dynamically scales the loss to prevent underflow. — optimizer_loss_scale","text":"","code":"optimizer_loss_scale(   inner_optimizer,   initial_scale = 32768,   dynamic_growth_steps = 2000L,   ...,   name = NULL,   weight_decay = NULL,   clipnorm = NULL,   clipvalue = NULL,   global_clipnorm = NULL,   use_ema = NULL,   ema_momentum = NULL,   ema_overwrite_frequency = NULL,   loss_scale_factor = NULL )"},{"path":"https://keras.posit.co/reference/optimizer_loss_scale.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An optimizer that dynamically scales the loss to prevent underflow. — optimizer_loss_scale","text":"inner_optimizer keras.optimizers.Optimizer instance wrap. initial_scale Float. initial loss scale. scale updated training. recommended high number, loss scale high gets lowered far quickly loss scale low gets raised. dynamic_growth_steps Int. often update scale upwards. every dynamic_growth_steps steps finite gradients, loss scale doubled. ... forward/backward compatability. name String. name use momentum accumulator weights created optimizer. weight_decay Float. set, weight decay applied. clipnorm Float. set, gradient weight individually clipped norm higher value. clipvalue Float. set, gradient weight clipped higher value. global_clipnorm Float. set, gradient weights clipped global norm higher value. use_ema Boolean, defaults FALSE. TRUE, exponential moving average (EMA) applied. EMA consists computing exponential moving average weights model (weight values change training batch), periodically overwriting weights moving average. ema_momentum Float, defaults 0.99. used use_ema=TRUE. momentum use computing EMA model's weights: new_average = ema_momentum * old_average + (1 - ema_momentum) * current_variable_value. ema_overwrite_frequency Int NULL, defaults NULL. used use_ema=TRUE. Every ema_overwrite_frequency steps iterations, overwrite model variable moving average. NULL, optimizer overwrite model variables middle training, need explicitly overwrite variables end training calling optimizer.finalize_variable_values() (updates model variables -place). using built-fit() training loop, happens automatically last epoch, need anything. loss_scale_factor Float NULL. float, scale factor multiplied loss computing gradients, inverse scale factor multiplied gradients updating variables. Useful preventing underflow mixed precision training. Alternately, keras.optimizers.LossScaleOptimizer automatically set loss scale factor.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/optimizer_nadam.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimizer that implements the Nadam algorithm. — optimizer_nadam","title":"Optimizer that implements the Nadam algorithm. — optimizer_nadam","text":"Much like Adam essentially RMSprop momentum, Nadam Adam Nesterov momentum.","code":""},{"path":"https://keras.posit.co/reference/optimizer_nadam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimizer that implements the Nadam algorithm. — optimizer_nadam","text":"","code":"optimizer_nadam(   learning_rate = 0.001,   beta_1 = 0.9,   beta_2 = 0.999,   epsilon = 1e-07,   weight_decay = NULL,   clipnorm = NULL,   clipvalue = NULL,   global_clipnorm = NULL,   use_ema = FALSE,   ema_momentum = 0.99,   ema_overwrite_frequency = NULL,   name = \"nadam\",   ...,   loss_scale_factor = NULL )"},{"path":"https://keras.posit.co/reference/optimizer_nadam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimizer that implements the Nadam algorithm. — optimizer_nadam","text":"learning_rate float, keras.optimizers.schedules.LearningRateSchedule instance, callable takes arguments returns actual value use. learning rate. Defaults 0.001. beta_1 float value constant float tensor, callable takes arguments returns actual value use. exponential decay rate 1st moment estimates. Defaults 0.9. beta_2 float value constant float tensor, callable takes arguments returns actual value use. exponential decay rate 2nd moment estimates. Defaults 0.999. epsilon small constant numerical stability. epsilon \"epsilon hat\" Kingma Ba paper (formula just Section 2.1), epsilon Algorithm 1 paper. Defaults 1e-7. weight_decay Float. set, weight decay applied. clipnorm Float. set, gradient weight individually clipped norm higher value. clipvalue Float. set, gradient weight clipped higher value. global_clipnorm Float. set, gradient weights clipped global norm higher value. use_ema Boolean, defaults FALSE. TRUE, exponential moving average (EMA) applied. EMA consists computing exponential moving average weights model (weight values change training batch), periodically overwriting weights moving average. ema_momentum Float, defaults 0.99. used use_ema=TRUE. momentum use computing EMA model's weights: new_average = ema_momentum * old_average + (1 - ema_momentum) * current_variable_value. ema_overwrite_frequency Int NULL, defaults NULL. used use_ema=TRUE. Every ema_overwrite_frequency steps iterations, overwrite model variable moving average. NULL, optimizer overwrite model variables middle training, need explicitly overwrite variables end training calling optimizer.finalize_variable_values() (updates model variables -place). using built-fit() training loop, happens automatically last epoch, need anything. name String. name use momentum accumulator weights created optimizer. ... forward/backward compatability. loss_scale_factor Float NULL. float, scale factor multiplied loss computing gradients, inverse scale factor multiplied gradients updating variables. Useful preventing underflow mixed precision training. Alternately, keras.optimizers.LossScaleOptimizer automatically set loss scale factor.","code":""},{"path":"https://keras.posit.co/reference/optimizer_nadam.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Optimizer that implements the Nadam algorithm. — optimizer_nadam","text":"Dozat, 2015.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/optimizer_rmsprop.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimizer that implements the RMSprop algorithm. — optimizer_rmsprop","title":"Optimizer that implements the RMSprop algorithm. — optimizer_rmsprop","text":"gist RMSprop : Maintain moving (discounted) average square gradients Divide gradient root average implementation RMSprop uses plain momentum, Nesterov momentum. centered version additionally maintains moving average gradients, uses average estimate variance.","code":""},{"path":"https://keras.posit.co/reference/optimizer_rmsprop.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimizer that implements the RMSprop algorithm. — optimizer_rmsprop","text":"","code":"optimizer_rmsprop(   learning_rate = 0.001,   rho = 0.9,   momentum = 0,   epsilon = 1e-07,   centered = FALSE,   weight_decay = NULL,   clipnorm = NULL,   clipvalue = NULL,   global_clipnorm = NULL,   use_ema = FALSE,   ema_momentum = 0.99,   ema_overwrite_frequency = 100L,   name = \"rmsprop\",   ...,   loss_scale_factor = NULL )"},{"path":"https://keras.posit.co/reference/optimizer_rmsprop.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimizer that implements the RMSprop algorithm. — optimizer_rmsprop","text":"learning_rate float, learning_rate_schedule_* instance, callable takes arguments returns actual value use. learning rate. Defaults 0.001. rho float, defaults 0.9. Discounting factor old gradients. momentum float, defaults 0.0. 0.0., optimizer tracks momentum value, decay rate equals 1 - momentum. epsilon small constant numerical stability. epsilon \"epsilon hat\" Kingma Ba paper (formula just Section 2.1), epsilon Algorithm 1 paper. Defaults 1e-7. centered Boolean. TRUE, gradients normalized estimated variance gradient; FALSE, uncentered second moment. Setting TRUE may help training, slightly expensive terms computation memory. Defaults FALSE. weight_decay Float. set, weight decay applied. clipnorm Float. set, gradient weight individually clipped norm higher value. clipvalue Float. set, gradient weight clipped higher value. global_clipnorm Float. set, gradient weights clipped global norm higher value. use_ema Boolean, defaults FALSE. TRUE, exponential moving average (EMA) applied. EMA consists computing exponential moving average weights model (weight values change training batch), periodically overwriting weights moving average. ema_momentum Float, defaults 0.99. used use_ema=TRUE. momentum use computing EMA model's weights: new_average = ema_momentum * old_average + (1 - ema_momentum) * current_variable_value. ema_overwrite_frequency Int NULL, defaults NULL. used use_ema=TRUE. Every ema_overwrite_frequency steps iterations, overwrite model variable moving average. NULL, optimizer overwrite model variables middle training, need explicitly overwrite variables end training calling optimizer.finalize_variable_values() (updates model variables -place). using built-fit() training loop, happens automatically last epoch, need anything. name String. name use momentum accumulator weights created optimizer. ... forward/backward compatability. loss_scale_factor Float NULL. float, scale factor multiplied loss computing gradients, inverse scale factor multiplied gradients updating variables. Useful preventing underflow mixed precision training. Alternately, keras.optimizers.LossScaleOptimizer automatically set loss scale factor.","code":""},{"path":"https://keras.posit.co/reference/optimizer_rmsprop.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimizer that implements the RMSprop algorithm. — optimizer_rmsprop","text":"","code":"opt <- optimizer_rmsprop(learning_rate=0.1)"},{"path":"https://keras.posit.co/reference/optimizer_rmsprop.html","id":"reference","dir":"Reference","previous_headings":"","what":"Reference","title":"Optimizer that implements the RMSprop algorithm. — optimizer_rmsprop","text":"Hinton, 2012","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/optimizer_sgd.html","id":null,"dir":"Reference","previous_headings":"","what":"Gradient descent (with momentum) optimizer. — optimizer_sgd","title":"Gradient descent (with momentum) optimizer. — optimizer_sgd","text":"Update rule parameter w gradient g momentum 0:   Update rule momentum larger 0:   nesterov=TRUE, rule becomes:","code":"w <- w - learning_rate * g velocity <- momentum * velocity - learning_rate * g w <- w + velocity velocity <- momentum * velocity - learning_rate * g w <- w + momentum * velocity - learning_rate * g"},{"path":"https://keras.posit.co/reference/optimizer_sgd.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gradient descent (with momentum) optimizer. — optimizer_sgd","text":"","code":"optimizer_sgd(   learning_rate = 0.01,   momentum = 0,   nesterov = FALSE,   weight_decay = NULL,   clipnorm = NULL,   clipvalue = NULL,   global_clipnorm = NULL,   use_ema = FALSE,   ema_momentum = 0.99,   ema_overwrite_frequency = NULL,   name = \"SGD\",   ...,   loss_scale_factor = NULL )"},{"path":"https://keras.posit.co/reference/optimizer_sgd.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gradient descent (with momentum) optimizer. — optimizer_sgd","text":"learning_rate float, learning_rate_schedule_* instance, callable takes arguments returns actual value use. learning rate. Defaults 0.01. momentum float hyperparameter >= 0 accelerates gradient descent relevant direction dampens oscillations. 0 vanilla gradient descent. Defaults 0.0. nesterov boolean. Whether apply Nesterov momentum. Defaults FALSE. weight_decay Float. set, weight decay applied. clipnorm Float. set, gradient weight individually clipped norm higher value. clipvalue Float. set, gradient weight clipped higher value. global_clipnorm Float. set, gradient weights clipped global norm higher value. use_ema Boolean, defaults FALSE. TRUE, exponential moving average (EMA) applied. EMA consists computing exponential moving average weights model (weight values change training batch), periodically overwriting weights moving average. ema_momentum Float, defaults 0.99. used use_ema=TRUE. momentum use computing EMA model's weights: new_average = ema_momentum * old_average + (1 - ema_momentum) * current_variable_value. ema_overwrite_frequency Int NULL, defaults NULL. used use_ema=TRUE. Every ema_overwrite_frequency steps iterations, overwrite model variable moving average. NULL, optimizer overwrite model variables middle training, need explicitly overwrite variables end training calling optimizer$finalize_variable_values() (updates model variables -place). using built-fit() training loop, happens automatically last epoch, need anything. name String. name use momentum accumulator weights created optimizer. ... forward/backward compatability. loss_scale_factor Float NULL. float, scale factor multiplied loss computing gradients, inverse scale factor multiplied gradients updating variables. Useful preventing underflow mixed precision training. Alternately, optimizer_loss_scale() automatically set loss scale factor.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/pack_x_y_sample_weight.html","id":null,"dir":"Reference","previous_headings":"","what":"Packs user-provided data into a list. — pack_x_y_sample_weight","title":"Packs user-provided data into a list. — pack_x_y_sample_weight","text":"convenience utility packing data list formats fit() uses.","code":""},{"path":"https://keras.posit.co/reference/pack_x_y_sample_weight.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Packs user-provided data into a list. — pack_x_y_sample_weight","text":"","code":"pack_x_y_sample_weight(x, y = NULL, sample_weight = NULL)"},{"path":"https://keras.posit.co/reference/pack_x_y_sample_weight.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Packs user-provided data into a list. — pack_x_y_sample_weight","text":"x Features pass Model. y Ground-truth targets pass Model. sample_weight Sample weight element.","code":""},{"path":"https://keras.posit.co/reference/pack_x_y_sample_weight.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Packs user-provided data into a list. — pack_x_y_sample_weight","text":"","code":"List in the format used in `fit()`."},{"path":"https://keras.posit.co/reference/pack_x_y_sample_weight.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Packs user-provided data into a list. — pack_x_y_sample_weight","text":"Standalone usage:","code":"x <- op_ones(c(10, 1)) data <- pack_x_y_sample_weight(x)  # TRUE y <- op_ones(c(10, 1)) data <- pack_x_y_sample_weight(x, y)"},{"path":[]},{"path":"https://keras.posit.co/reference/pad_sequences.html","id":null,"dir":"Reference","previous_headings":"","what":"Pads sequences to the same length. — pad_sequences","title":"Pads sequences to the same length. — pad_sequences","text":"function transforms list (length num_samples) sequences (lists integers) 2D NumPy array shape (num_samples, num_timesteps). num_timesteps either maxlen argument provided, length longest sequence list. Sequences shorter num_timesteps padded value num_timesteps long. Sequences longer num_timesteps truncated fit desired length. position padding truncation happens determined arguments padding truncating, respectively. Pre-padding removing values beginning sequence default.","code":"sequence <- list(c(1), c(2, 3), c(4, 5, 6)) pad_sequences(sequence) ##      [,1] [,2] [,3] ## [1,]    0    0    1 ## [2,]    0    2    3 ## [3,]    4    5    6 pad_sequences(sequence, value=-1) ##      [,1] [,2] [,3] ## [1,]   -1   -1    1 ## [2,]   -1    2    3 ## [3,]    4    5    6 pad_sequences(sequence, padding='post') ##      [,1] [,2] [,3] ## [1,]    1    0    0 ## [2,]    2    3    0 ## [3,]    4    5    6 pad_sequences(sequence, maxlen=2) ##      [,1] [,2] ## [1,]    0    1 ## [2,]    2    3 ## [3,]    5    6"},{"path":"https://keras.posit.co/reference/pad_sequences.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pads sequences to the same length. — pad_sequences","text":"","code":"pad_sequences(   sequences,   maxlen = NULL,   dtype = \"int32\",   padding = \"pre\",   truncating = \"pre\",   value = 0 )"},{"path":"https://keras.posit.co/reference/pad_sequences.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pads sequences to the same length. — pad_sequences","text":"sequences List sequences (sequence list integers). maxlen Optional Int, maximum length sequences. provided, sequences padded length longest individual sequence. dtype (Optional, defaults \"int32\"). Type output sequences. pad sequences variable length strings, can use object. padding String, \"pre\" \"post\" (optional, defaults \"pre\"): pad either sequence. truncating String, \"pre\" \"post\" (optional, defaults \"pre\"): remove values sequences larger maxlen, either beginning end sequences. value Float String, padding value. (Optional, defaults 0.)","code":""},{"path":"https://keras.posit.co/reference/pad_sequences.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pads sequences to the same length. — pad_sequences","text":"","code":"Array with shape `(len(sequences), maxlen)`"},{"path":[]},{"path":"https://keras.posit.co/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See %>% details.","code":""},{"path":"https://keras.posit.co/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://keras.posit.co/reference/plot.keras.models.model.Model.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot a Keras model — plot.keras.models.model.Model","title":"Plot a Keras model — plot.keras.models.model.Model","text":"Plot Keras model","code":""},{"path":"https://keras.posit.co/reference/plot.keras.models.model.Model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot a Keras model — plot.keras.models.model.Model","text":"","code":"# S3 method for keras.models.model.Model plot(   x,   show_shapes = FALSE,   show_dtype = FALSE,   show_layer_names = TRUE,   ...,   rankdir = \"TB\",   expand_nested = FALSE,   dpi = 96,   layer_range = NULL,   show_layer_activations = FALSE,   to_file = NULL )"},{"path":"https://keras.posit.co/reference/plot.keras.models.model.Model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot a Keras model — plot.keras.models.model.Model","text":"x Keras model instance show_shapes whether display shape information. show_dtype whether display layer dtypes. show_layer_names whether display layer names. ... passed keras$utils$plot_model(). Used forward backward compatibility. rankdir string specifying format plot: 'TB' creates vertical plot; 'LR' creates horizontal plot. (argument passed PyDot) expand_nested Whether expand nested models clusters. dpi Dots per inch. Increase value image text appears excessively pixelated. layer_range list containing two character strings, starting layer name ending layer name (inclusive) indicating range layers plot generated. also accepts regex patterns instead exact name. case, start predicate first element matches layer_range[1] end predicate last element matches layer_range[2]. default NULL considers layers model. Note must pass range resultant subgraph must complete. show_layer_activations Display layer activations (layers activation property). to_file File name plot image. NULL (default), model drawn default graphics device. Otherwise, file saved.","code":""},{"path":"https://keras.posit.co/reference/plot.keras.models.model.Model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot a Keras model — plot.keras.models.model.Model","text":"Nothing, called side effects.","code":""},{"path":"https://keras.posit.co/reference/plot.keras.models.model.Model.html","id":"raises","dir":"Reference","previous_headings":"","what":"Raises","title":"Plot a Keras model — plot.keras.models.model.Model","text":"ValueError: plot_model called model built, unless input_shape =  argument supplied keras_model_sequential().","code":""},{"path":"https://keras.posit.co/reference/plot.keras.models.model.Model.html","id":"requirements","dir":"Reference","previous_headings":"","what":"Requirements","title":"Plot a Keras model — plot.keras.models.model.Model","text":"function requires pydot graphviz. pydot default installed install_keras(), installed tensorflow means, can install pydot directly :   conda environment, can install graphviz :   Otherwise can install graphviz : https://graphviz.gitlab.io/download/","code":"reticulate::py_install(\"pydot\", pip = TRUE) reticulate::conda_install(packages = \"graphviz\") # Restart the R session after install."},{"path":"https://keras.posit.co/reference/plot.keras_training_history.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot training history — plot.keras_training_history","title":"Plot training history — plot.keras_training_history","text":"Plots metrics recorded training.","code":""},{"path":"https://keras.posit.co/reference/plot.keras_training_history.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot training history — plot.keras_training_history","text":"","code":"# S3 method for keras_training_history plot(   x,   y,   metrics = NULL,   method = c(\"auto\", \"ggplot2\", \"base\"),   smooth = getOption(\"keras.plot.history.smooth\", TRUE),   theme_bw = getOption(\"keras.plot.history.theme_bw\", FALSE),   ... )"},{"path":"https://keras.posit.co/reference/plot.keras_training_history.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot training history — plot.keras_training_history","text":"x Training history object returned fit.keras.models.model.Model(). y Unused. metrics One metrics plot (e.g. c('loss', 'accuracy')). Defaults plotting captured metrics. method Method use plotting. default \"auto\" use ggplot2 available, otherwise use base graphics. smooth Whether loess smooth added plot, available ggplot2 method. number epochs smaller ten, forced false. theme_bw Use ggplot2::theme_bw() plot history black white. ... Additional parameters pass plot() method.","code":""},{"path":"https://keras.posit.co/reference/pop_layer.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove the last layer in a model — pop_layer","title":"Remove the last layer in a model — pop_layer","text":"Remove last layer model","code":""},{"path":"https://keras.posit.co/reference/pop_layer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove the last layer in a model — pop_layer","text":"","code":"pop_layer(object)"},{"path":"https://keras.posit.co/reference/pop_layer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove the last layer in a model — pop_layer","text":"object Keras model object","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/predict.keras.models.model.Model.html","id":null,"dir":"Reference","previous_headings":"","what":"Generates output predictions for the input samples. — predict.keras.models.model.Model","title":"Generates output predictions for the input samples. — predict.keras.models.model.Model","text":"Generates output predictions input samples.","code":""},{"path":"https://keras.posit.co/reference/predict.keras.models.model.Model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generates output predictions for the input samples. — predict.keras.models.model.Model","text":"","code":"# S3 method for keras.models.model.Model predict(   object,   x,   ...,   batch_size = NULL,   verbose = getOption(\"keras.verbose\", default = \"auto\"),   steps = NULL,   callbacks = NULL )"},{"path":"https://keras.posit.co/reference/predict.keras.models.model.Model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generates output predictions for the input samples. — predict.keras.models.model.Model","text":"object Keras model object x Input samples. : array (array-like), list arrays (case model multiple inputs). tensor, list tensors (case model multiple inputs). TF Dataset. ... forward/backward compatability. batch_size Integer NULL. Number samples per batch. unspecified, batch_size default 32. specify batch_size data form TF Dataset generator (since generate batches). verbose \"auto\", 0, 1, 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. \"auto\" becomes 1 cases, 2 knitr render running distributed training server. Note progress bar particularly useful logged file, verbose=2 recommended running interactively (e.g., production environment). Defaults \"auto\". steps Total number steps (batches samples) declaring prediction round finished. Ignored default value NULL. x TF Dataset steps NULL, predict() run input dataset exhausted. callbacks List Callback instances. List callbacks apply prediction.","code":""},{"path":"https://keras.posit.co/reference/predict.keras.models.model.Model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generates output predictions for the input samples. — predict.keras.models.model.Model","text":"R array(s) predictions.","code":""},{"path":"https://keras.posit.co/reference/predict.keras.models.model.Model.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generates output predictions for the input samples. — predict.keras.models.model.Model","text":"Computation done batches. method designed batch processing large numbers inputs. intended use inside loops iterate data process small numbers inputs time. small numbers inputs fit one batch, directly call model model$call faster execution, e.g., model(x), model(x, training = FALSE) layers BatchNormalization behave differently inference.","code":""},{"path":"https://keras.posit.co/reference/predict.keras.models.model.Model.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Generates output predictions for the input samples. — predict.keras.models.model.Model","text":"See FAQ entry details difference Model methods predict() call().","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/predict_generator.html","id":null,"dir":"Reference","previous_headings":"","what":"(Deprecated) Generates predictions for the input samples from a data generator. — predict_generator","title":"(Deprecated) Generates predictions for the input samples from a data generator. — predict_generator","text":"generator return kind data accepted predict_on_batch().","code":""},{"path":"https://keras.posit.co/reference/predict_generator.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"(Deprecated) Generates predictions for the input samples from a data generator. — predict_generator","text":"","code":"predict_generator(   object,   generator,   steps,   max_queue_size = 10,   workers = 1,   verbose = 0,   callbacks = NULL )"},{"path":"https://keras.posit.co/reference/predict_generator.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"(Deprecated) Generates predictions for the input samples from a data generator. — predict_generator","text":"object Keras model object generator Generator yielding batches input samples. steps Total number steps (batches samples) yield generator stopping. max_queue_size Maximum size generator queue. unspecified, max_queue_size default 10. workers Maximum number threads use parallel processing. Note parallel processing performed native Keras generators (e.g. flow_images_from_directory()) R based generators must run main thread. verbose verbosity mode, 0 1. callbacks List callbacks apply prediction.","code":""},{"path":"https://keras.posit.co/reference/predict_generator.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"(Deprecated) Generates predictions for the input samples from a data generator. — predict_generator","text":"Numpy array(s) predictions.","code":""},{"path":"https://keras.posit.co/reference/predict_generator.html","id":"raises","dir":"Reference","previous_headings":"","what":"Raises","title":"(Deprecated) Generates predictions for the input samples from a data generator. — predict_generator","text":"ValueError: case generator yields data invalid format.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/predict_on_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns predictions for a single batch of samples. — predict_on_batch","title":"Returns predictions for a single batch of samples. — predict_on_batch","text":"Returns predictions single batch samples.","code":""},{"path":"https://keras.posit.co/reference/predict_on_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns predictions for a single batch of samples. — predict_on_batch","text":"","code":"predict_on_batch(object, x)"},{"path":"https://keras.posit.co/reference/predict_on_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns predictions for a single batch of samples. — predict_on_batch","text":"object Keras model object x Input data. must array-like.","code":""},{"path":"https://keras.posit.co/reference/predict_on_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns predictions for a single batch of samples. — predict_on_batch","text":"Array(s) predictions.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/predict_proba.html","id":null,"dir":"Reference","previous_headings":"","what":"(Deprecated) Generates probability or class probability predictions for the input samples. — predict_proba","title":"(Deprecated) Generates probability or class probability predictions for the input samples. — predict_proba","text":"functions removed Tensorflow version 2.6. See details update code:","code":""},{"path":"https://keras.posit.co/reference/predict_proba.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"(Deprecated) Generates probability or class probability predictions for the input samples. — predict_proba","text":"","code":"predict_proba(object, x, batch_size = NULL, verbose = 0, steps = NULL)  predict_classes(object, x, batch_size = NULL, verbose = 0, steps = NULL)"},{"path":"https://keras.posit.co/reference/predict_proba.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"(Deprecated) Generates probability or class probability predictions for the input samples. — predict_proba","text":"object Keras model object x Input data (vector, matrix, array). can also pass tfdataset generator returning list (inputs, targets) (inputs, targets, sample_weights). batch_size Integer. unspecified, default 32. verbose Verbosity mode, 0, 1, 2, \"auto\". \"auto\" defaults 1 cases defaults verbose=2 used ParameterServerStrategy interactive logging disabled. steps Total number steps (batches samples) declaring evaluation round finished. default NULL equal number samples dataset divided batch size.","code":""},{"path":"https://keras.posit.co/reference/predict_proba.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"(Deprecated) Generates probability or class probability predictions for the input samples. — predict_proba","text":"update code: predict_proba(): use predict() directly. predict_classes(): model multi-class classification: (e.g. uses softmax last-layer activation).   model binary classification (e.g. uses sigmoid last-layer activation).   input samples processed batch batch.","code":"model %>% predict(x) %>% k_argmax() model %>% predict(x) %>% `>`(0.5) %>% k_cast(\"int32\")"},{"path":[]},{"path":"https://keras.posit.co/reference/process_utils.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocessing and postprocessing utilities — process_utils","title":"Preprocessing and postprocessing utilities — process_utils","text":"functions used preprocess postprocess inputs outputs Keras applications.","code":""},{"path":"https://keras.posit.co/reference/process_utils.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocessing and postprocessing utilities — process_utils","text":"","code":"application_preprocess_inputs(model, x, ..., data_format = NULL)  application_decode_predictions(model, preds, top = 5L, ...)"},{"path":"https://keras.posit.co/reference/process_utils.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocessing and postprocessing utilities — process_utils","text":"model Keras model initialized using application_ function. x batch inputs model. ... Additional arguments passed preprocessing decoding function. data_format Optional data format image tensor/array. NULL means global setting config_image_data_format() used (unless changed , uses \"channels_last\"). Defaults NULL. preds batch outputs model. top number top predictions return.","code":""},{"path":"https://keras.posit.co/reference/process_utils.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocessing and postprocessing utilities — process_utils","text":"list decoded predictions case application_decode_predictions(). batch preprocessed inputs case application_preprocess_inputs().","code":""},{"path":"https://keras.posit.co/reference/process_utils.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Preprocessing and postprocessing utilities — process_utils","text":"application_preprocess_inputs(): Pre-process inputs used model application_decode_predictions(): Decode predictions model","code":""},{"path":"https://keras.posit.co/reference/process_utils.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preprocessing and postprocessing utilities — process_utils","text":"","code":"if (FALSE) { model <- application_convnext_tiny()  inputs <- random_normal(c(32, 224, 224, 3)) processed_inputs <- application_preprocess_inputs(model, inputs)  preds <- random_normal(c(32, 1000)) decoded_preds <- application_decode_predictions(model, preds)  }"},{"path":"https://keras.posit.co/reference/random_array.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a Random Array — random_array","title":"Generate a Random Array — random_array","text":"function generates R array random numbers. dimensions array specified user. generation function random numbers can also customized.","code":""},{"path":"https://keras.posit.co/reference/random_array.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a Random Array — random_array","text":"","code":"random_array(..., gen = stats::runif)"},{"path":"https://keras.posit.co/reference/random_array.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a Random Array — random_array","text":"... Dimensions array separate integers single vector. gen function generating random numbers, defaulting runif.","code":""},{"path":"https://keras.posit.co/reference/random_array.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a Random Array — random_array","text":"Returns array specified dimensions filled random numbers.","code":""},{"path":"https://keras.posit.co/reference/random_array.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate a Random Array — random_array","text":"","code":"# Create a 3x3 matrix with random numbers from uniform distribution random_array(3, 3) #> Error in random_array(3, 3): could not find function \"random_array\"  # Create a 2x2x2 array with random numbers from normal distribution random_array(2, 2, 2, gen = rnorm) #> Error in random_array(2, 2, 2, gen = rnorm): could not find function \"random_array\"  # Create a 2x2 array with a sequence of integers. random_array(2, 2, gen = seq) #> Error in random_array(2, 2, gen = seq): could not find function \"random_array\""},{"path":"https://keras.posit.co/reference/random_categorical.html","id":null,"dir":"Reference","previous_headings":"","what":"Draws samples from a categorical distribution. — random_categorical","title":"Draws samples from a categorical distribution. — random_categorical","text":"function takes input logits, 2-D input tensor shape (batch_size, num_classes). row input represents categorical distribution, column index containing log-probability given class. function output 2-D tensor shape (batch_size, num_samples), row contains samples corresponding row logits. column index contains independent samples drawn input distribution.","code":""},{"path":"https://keras.posit.co/reference/random_categorical.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Draws samples from a categorical distribution. — random_categorical","text":"","code":"random_categorical(logits, num_samples, dtype = \"int32\", seed = NULL)"},{"path":"https://keras.posit.co/reference/random_categorical.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Draws samples from a categorical distribution. — random_categorical","text":"logits 2-D Tensor shape (batch_size, num_classes). row define categorical distibution unnormalized log-probabilities classes. num_samples Int, number independent samples draw row input. second dimension output tensor's shape. dtype Optional dtype output tensor. seed Python integer instance keras.random.SeedGenerator. Used make behavior initializer deterministic. Note initializer seeded integer NULL (unseeded) produce random values across multiple calls. get different random values across multiple calls, use seed instance keras.random.SeedGenerator.","code":""},{"path":"https://keras.posit.co/reference/random_categorical.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Draws samples from a categorical distribution. — random_categorical","text":"","code":"A 2-D tensor with (batch_size, num_samples)."},{"path":[]},{"path":"https://keras.posit.co/reference/random_dropout.html","id":null,"dir":"Reference","previous_headings":"","what":"random dropout — random_dropout","title":"random dropout — random_dropout","text":"Randomly set portion values tensor 0.","code":""},{"path":"https://keras.posit.co/reference/random_dropout.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"random dropout — random_dropout","text":"","code":"random_dropout(inputs, rate, noise_shape = NULL, seed = NULL)"},{"path":"https://keras.posit.co/reference/random_dropout.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"random dropout — random_dropout","text":"inputs see description rate see description noise_shape see description seed Initial seed random number generator","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/random_gamma.html","id":null,"dir":"Reference","previous_headings":"","what":"Draw random samples from the Gamma distribution. — random_gamma","title":"Draw random samples from the Gamma distribution. — random_gamma","text":"Draw random samples Gamma distribution.","code":""},{"path":"https://keras.posit.co/reference/random_gamma.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Draw random samples from the Gamma distribution. — random_gamma","text":"","code":"random_gamma(shape, alpha, dtype = NULL, seed = NULL)"},{"path":"https://keras.posit.co/reference/random_gamma.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Draw random samples from the Gamma distribution. — random_gamma","text":"shape shape random values generate. alpha Float, parameter distribution. dtype Optional dtype tensor. floating point types supported. specified, keras.config.floatx() used, defaults float32 unless configured otherwise (via keras.config.set_floatx(float_dtype)). seed Python integer instance keras.random.SeedGenerator. Used make behavior initializer deterministic. Note initializer seeded integer NULL (unseeded) produce random values across multiple calls. get different random values across multiple calls, use seed instance keras.random.SeedGenerator.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/random_integer.html","id":null,"dir":"Reference","previous_headings":"","what":"Draw random integers from a uniform distribution. — random_integer","title":"Draw random integers from a uniform distribution. — random_integer","text":"generated values follow uniform distribution range [minval, maxval). lower bound minval included range, upper bound maxval excluded. dtype must integer type.","code":""},{"path":"https://keras.posit.co/reference/random_integer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Draw random integers from a uniform distribution. — random_integer","text":"","code":"random_integer(shape, minval, maxval, dtype = \"int32\", seed = NULL)"},{"path":"https://keras.posit.co/reference/random_integer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Draw random integers from a uniform distribution. — random_integer","text":"shape shape random values generate. minval Float, defaults 0. Lower bound range random values generate (inclusive). maxval Float, defaults 1. Upper bound range random values generate (exclusive). dtype Optional dtype tensor. integer types supported. specified, keras.config.floatx() used, defaults float32 unless configured otherwise (via keras.config.set_floatx(float_dtype)) seed Python integer instance keras.random.SeedGenerator. Used make behavior initializer deterministic. Note initializer seeded integer NULL (unseeded) produce random values across multiple calls. get different random values across multiple calls, use seed instance keras.random.SeedGenerator.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/random_normal.html","id":null,"dir":"Reference","previous_headings":"","what":"Draw random samples from a normal (Gaussian) distribution. — random_normal","title":"Draw random samples from a normal (Gaussian) distribution. — random_normal","text":"Draw random samples normal (Gaussian) distribution.","code":""},{"path":"https://keras.posit.co/reference/random_normal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Draw random samples from a normal (Gaussian) distribution. — random_normal","text":"","code":"random_normal(shape, mean = 0, stddev = 1, dtype = NULL, seed = NULL)"},{"path":"https://keras.posit.co/reference/random_normal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Draw random samples from a normal (Gaussian) distribution. — random_normal","text":"shape shape random values generate. mean Float, defaults 0. Mean random values generate. stddev Float, defaults 1. Standard deviation random values generate. dtype Optional dtype tensor. floating point types supported. specified, keras.config.floatx() used, defaults float32 unless configured otherwise (via keras.config.set_floatx(float_dtype)). seed Python integer instance keras.random.SeedGenerator. Used make behavior initializer deterministic. Note initializer seeded integer NULL (unseeded) produce random values across multiple calls. get different random values across multiple calls, use seed instance keras.random.SeedGenerator.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/random_seed_generator.html","id":null,"dir":"Reference","previous_headings":"","what":"Generates variable seeds upon each call to a RNG-using function. — random_seed_generator","title":"Generates variable seeds upon each call to a RNG-using function. — random_seed_generator","text":"Keras, RNG-using methods (random_normal()) stateless, meaning pass integer seed (seed = 42), return values call. order get different values call, must use SeedGenerator instead seed argument. SeedGenerator object stateful.","code":""},{"path":"https://keras.posit.co/reference/random_seed_generator.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generates variable seeds upon each call to a RNG-using function. — random_seed_generator","text":"","code":"random_seed_generator(seed = NULL, ...)"},{"path":"https://keras.posit.co/reference/random_seed_generator.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generates variable seeds upon each call to a RNG-using function. — random_seed_generator","text":"seed Initial seed random number generator ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/random_seed_generator.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generates variable seeds upon each call to a RNG-using function. — random_seed_generator","text":"Usage layer:","code":"seed_gen <- random_seed_generator(seed = 42) values <- random_normal(shape = c(2, 3), seed = seed_gen) new_values <- random_normal(shape = c(2, 3), seed = seed_gen) layer_dropout2 <- new_layer_class(   \"dropout2\",   initialize = function(...) {     super$initialize(...)     self$seed_generator <- random_seed_generator(seed = 1337)   },   call = function(x, training = FALSE) {     if (training) {       return(random_dropout(x, rate = 0.5, seed = self$seed_generator))     }     return(x)   } )  out <- layer_dropout(rate = 0.8) out(op_ones(10), training = TRUE) ## tf.Tensor([0. 5. 5. 0. 0. 0. 0. 0. 0. 0.], shape=(10), dtype=float32)"},{"path":[]},{"path":"https://keras.posit.co/reference/random_shuffle.html","id":null,"dir":"Reference","previous_headings":"","what":"Shuffle the elements of a tensor uniformly at random along an axis. — random_shuffle","title":"Shuffle the elements of a tensor uniformly at random along an axis. — random_shuffle","text":"Shuffle elements tensor uniformly random along axis.","code":""},{"path":"https://keras.posit.co/reference/random_shuffle.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Shuffle the elements of a tensor uniformly at random along an axis. — random_shuffle","text":"","code":"random_shuffle(x, axis = 1L, seed = NULL)"},{"path":"https://keras.posit.co/reference/random_shuffle.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Shuffle the elements of a tensor uniformly at random along an axis. — random_shuffle","text":"x tensor shuffled. axis integer specifying axis along shuffle. Defaults 0. seed Python integer instance keras.random.SeedGenerator. Used make behavior initializer deterministic. Note initializer seeded integer NULL (unseeded) produce random values across multiple calls. get different random values across multiple calls, use seed instance keras.random.SeedGenerator.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/random_truncated_normal.html","id":null,"dir":"Reference","previous_headings":"","what":"Draw samples from a truncated normal distribution. — random_truncated_normal","title":"Draw samples from a truncated normal distribution. — random_truncated_normal","text":"values drawn normal distribution specified mean standard deviation, discarding re-drawing samples two standard deviations mean.","code":""},{"path":"https://keras.posit.co/reference/random_truncated_normal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Draw samples from a truncated normal distribution. — random_truncated_normal","text":"","code":"random_truncated_normal(shape, mean = 0, stddev = 1, dtype = NULL, seed = NULL)"},{"path":"https://keras.posit.co/reference/random_truncated_normal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Draw samples from a truncated normal distribution. — random_truncated_normal","text":"shape shape random values generate. mean Float, defaults 0. Mean random values generate. stddev Float, defaults 1. Standard deviation random values generate. dtype Optional dtype tensor. floating point types supported. specified, keras.config.floatx() used, defaults float32 unless configured otherwise (via keras.config.set_floatx(float_dtype)) seed Python integer instance keras.random.SeedGenerator. Used make behavior initializer deterministic. Note initializer seeded integer NULL (unseeded) produce random values across multiple calls. get different random values across multiple calls, use seed instance keras.random.SeedGenerator.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/random_uniform.html","id":null,"dir":"Reference","previous_headings":"","what":"Draw samples from a uniform distribution. — random_uniform","title":"Draw samples from a uniform distribution. — random_uniform","text":"generated values follow uniform distribution range [minval, maxval). lower bound minval included range, upper bound maxval excluded. dtype must floating point type, default range [0, 1).","code":""},{"path":"https://keras.posit.co/reference/random_uniform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Draw samples from a uniform distribution. — random_uniform","text":"","code":"random_uniform(shape, minval = 0, maxval = 1, dtype = NULL, seed = NULL)"},{"path":"https://keras.posit.co/reference/random_uniform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Draw samples from a uniform distribution. — random_uniform","text":"shape shape random values generate. minval Float, defaults 0. Lower bound range random values generate (inclusive). maxval Float, defaults 1. Upper bound range random values generate (exclusive). dtype Optional dtype tensor. floating point types supported. specified, keras.config.floatx() used, defaults float32 unless configured otherwise (via keras.config.set_floatx(float_dtype)) seed Python integer instance keras.random.SeedGenerator. Used make behavior initializer deterministic. Note initializer seeded integer NULL (unseeded) produce random values across multiple calls. get different random values across multiple calls, use seed instance keras.random.SeedGenerator.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. generics compile, fit magrittr %<>% reticulate array_reshape, np_array, tuple, use_condaenv, use_python, use_virtualenv tensorflow all_dims, as_tensor, evaluate, export_savedmodel, tensorboard, use_session_with_seed tfruns flag_boolean, flag_integer, flag_numeric, flag_string, flags, run_dir","code":""},{"path":"https://keras.posit.co/reference/register_custom_object.html","id":null,"dir":"Reference","previous_headings":"","what":"Registers an object with the Keras serialization framework. — register_custom_object","title":"Registers an object with the Keras serialization framework. — register_custom_object","text":"decorator injects decorated class function Keras custom object registry, can serialized deserialized without needing entry user-provided custom_objects argument. also injects function Keras call get object's serializable string key. Note serialized deserialized, classes must implement get_config() method. Functions requirement. object registered key 'package>name' name, defaults object name passed.","code":""},{"path":"https://keras.posit.co/reference/register_custom_object.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Registers an object with the Keras serialization framework. — register_custom_object","text":"","code":"register_custom_object(object, name = NULL, package = NULL)"},{"path":"https://keras.posit.co/reference/register_custom_object.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Registers an object with the Keras serialization framework. — register_custom_object","text":"name name serialize class package. package package class belongs . used key (\"package>name\") identify class. Defaults current package name, \"Custom\" outside package.","code":""},{"path":"https://keras.posit.co/reference/register_custom_object.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Registers an object with the Keras serialization framework. — register_custom_object","text":"","code":"# Note that `'my_package'` is used as the `package` argument here, and since # the `name` argument is not provided, `'MyDense'` is used as the `name`. layer_my_dense <- Layer(\"MyDense\") register_custom_object(layer_my_dense, package = \"my_package\")  MyDense <- environment(layer_my_dense)$Layer # the python class obj stopifnot(exprs = {   get_registered_object('my_package>MyDense') == MyDense   get_registered_name(MyDense) == 'my_package>MyDense' })"},{"path":[]},{"path":"https://keras.posit.co/reference/register_keras_serializable.html","id":null,"dir":"Reference","previous_headings":"","what":"Registers an object with the Keras serialization framework. — register_keras_serializable","title":"Registers an object with the Keras serialization framework. — register_keras_serializable","text":"function registers custom class function Keras custom object registry, can serialized deserialized without needing entry user-provided custom_objects argument. also injects function Keras call get object's serializable string key. Note serialized deserialized, classes must implement get_config() method. Functions requirement. object registered key 'package>name' name, defaults object name passed.","code":""},{"path":"https://keras.posit.co/reference/register_keras_serializable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Registers an object with the Keras serialization framework. — register_keras_serializable","text":"","code":"register_keras_serializable(object, name = NULL, package = NULL)"},{"path":"https://keras.posit.co/reference/register_keras_serializable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Registers an object with the Keras serialization framework. — register_keras_serializable","text":"object keras object. name name serialize class package. package package class belongs . used key (\"package>name\") identify class. Defaults current package name, \"Custom\" outside package.","code":""},{"path":"https://keras.posit.co/reference/register_keras_serializable.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Registers an object with the Keras serialization framework. — register_keras_serializable","text":"","code":"# Note that `'my_package'` is used as the `package` argument here, and since # the `name` argument is not provided, `'MyDense'` is used as the `name`. layer_my_dense <- Layer(\"MyDense\") register_keras_serializable(layer_my_dense, package = \"my_package\")  MyDense <- environment(layer_my_dense)$Layer # the python class obj stopifnot(exprs = {   get_registered_object('my_package>MyDense') == MyDense   get_registered_name(MyDense) == 'my_package>MyDense' })"},{"path":[]},{"path":"https://keras.posit.co/reference/regularizer_l1.html","id":null,"dir":"Reference","previous_headings":"","what":"A regularizer that applies a L1 regularization penalty. — regularizer_l1","title":"A regularizer that applies a L1 regularization penalty. — regularizer_l1","text":"L1 regularization penalty computed : loss = l1 * reduce_sum(abs(x)) L1 may passed layer string identifier:   case, default value used l1=0.01.","code":"dense <- layer_dense(units = 3, kernel_regularizer = 'l1')"},{"path":"https://keras.posit.co/reference/regularizer_l1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A regularizer that applies a L1 regularization penalty. — regularizer_l1","text":"","code":"regularizer_l1(l1 = 0.01)"},{"path":"https://keras.posit.co/reference/regularizer_l1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A regularizer that applies a L1 regularization penalty. — regularizer_l1","text":"l1 float, L1 regularization factor.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/regularizer_l1_l2.html","id":null,"dir":"Reference","previous_headings":"","what":"A regularizer that applies both L1 and L2 regularization penalties. — regularizer_l1_l2","title":"A regularizer that applies both L1 and L2 regularization penalties. — regularizer_l1_l2","text":"L1 regularization penalty computed : loss = l1 * reduce_sum(abs(x)) L2 regularization penalty computed loss = l2 * reduce_sum(square(x)) L1L2 may passed layer string identifier:   case, default values used l1=0.01 l2=0.01.","code":"dense <- layer_dense(units = 3, kernel_regularizer = 'L1L2')"},{"path":"https://keras.posit.co/reference/regularizer_l1_l2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A regularizer that applies both L1 and L2 regularization penalties. — regularizer_l1_l2","text":"","code":"regularizer_l1_l2(l1 = 0, l2 = 0)"},{"path":"https://keras.posit.co/reference/regularizer_l1_l2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A regularizer that applies both L1 and L2 regularization penalties. — regularizer_l1_l2","text":"l1 float, L1 regularization factor. l2 float, L2 regularization factor.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/regularizer_l2.html","id":null,"dir":"Reference","previous_headings":"","what":"A regularizer that applies a L2 regularization penalty. — regularizer_l2","title":"A regularizer that applies a L2 regularization penalty. — regularizer_l2","text":"L2 regularization penalty computed : loss = l2 * reduce_sum(square(x)) L2 may passed layer string identifier:   case, default value used l2=0.01.","code":"dense <- layer_dense(units = 3, kernel_regularizer='l2')"},{"path":"https://keras.posit.co/reference/regularizer_l2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A regularizer that applies a L2 regularization penalty. — regularizer_l2","text":"","code":"regularizer_l2(l2 = 0.01)"},{"path":"https://keras.posit.co/reference/regularizer_l2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A regularizer that applies a L2 regularization penalty. — regularizer_l2","text":"l2 float, L2 regularization factor.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/regularizer_orthogonal.html","id":null,"dir":"Reference","previous_headings":"","what":"Regularizer that encourages input vectors to be orthogonal to each other. — regularizer_orthogonal","title":"Regularizer that encourages input vectors to be orthogonal to each other. — regularizer_orthogonal","text":"can applied either rows matrix (mode=\"rows\") columns (mode=\"columns\"). applied Dense kernel shape (input_dim, units), rows mode seek make feature vectors (.e. basis output space) orthogonal .","code":""},{"path":"https://keras.posit.co/reference/regularizer_orthogonal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Regularizer that encourages input vectors to be orthogonal to each other. — regularizer_orthogonal","text":"","code":"regularizer_orthogonal(factor = 0.01, mode = \"rows\")"},{"path":"https://keras.posit.co/reference/regularizer_orthogonal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Regularizer that encourages input vectors to be orthogonal to each other. — regularizer_orthogonal","text":"factor Float. regularization factor. regularization penalty proportional factor times mean dot products L2-normalized rows (mode=\"rows\", columns mode=\"columns\") inputs, excluding product row/column .  Defaults 0.01. mode String, one {\"rows\", \"columns\"}. Defaults \"rows\". rows mode, regularization effect seeks make rows input orthogonal . columns mode, seeks make columns input orthogonal .","code":""},{"path":"https://keras.posit.co/reference/regularizer_orthogonal.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Regularizer that encourages input vectors to be orthogonal to each other. — regularizer_orthogonal","text":"","code":"regularizer <- regularizer_orthogonal(factor=0.01) layer <- layer_dense(units=4, kernel_regularizer=regularizer)"},{"path":[]},{"path":"https://keras.posit.co/reference/reset_states.html","id":null,"dir":"Reference","previous_headings":"","what":"Reset the states for a layer — reset_states","title":"Reset the states for a layer — reset_states","text":"Reset states layer","code":""},{"path":"https://keras.posit.co/reference/reset_states.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reset the states for a layer — reset_states","text":"","code":"reset_states(object)"},{"path":"https://keras.posit.co/reference/reset_states.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reset the states for a layer — reset_states","text":"object Model layer object","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/save_keras_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Saves a model as a .keras file. — save_keras_model","title":"Saves a model as a .keras file. — save_keras_model","text":"Saves model .keras file.","code":""},{"path":"https://keras.posit.co/reference/save_keras_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Saves a model as a .keras file. — save_keras_model","text":"","code":"save_keras_model(model, filepath, overwrite = FALSE, ...)"},{"path":"https://keras.posit.co/reference/save_keras_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Saves a model as a .keras file. — save_keras_model","text":"filepath string, Path save model. Must end .keras. overwrite Whether overwrite existing model target location, instead ask user via interactive prompt. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/save_keras_model.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Saves a model as a .keras file. — save_keras_model","text":"saved .keras file contains: model's configuration (architecture) model's weights model's optimizer's state () Thus models can reinstantiated exact state.","code":"model <- keras_model_sequential(input_shape = c(3)) |>   layer_dense(5) |>   layer_activation_softmax()  model |> save_keras_model(\"model.keras\") loaded_model <- load_keras_model(\"model.keras\") x <- random_uniform(c(10, 3)) stopifnot(all.equal(   model |> predict(x),   loaded_model |> predict(x) )) zip::zip_list(\"model.keras\")[, \"filename\"] ## [1] \"metadata.json\"    \"config.json\"      \"model.weights.h5\""},{"path":[]},{"path":"https://keras.posit.co/reference/save_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Saves a model as a .keras file. — save_model","title":"Saves a model as a .keras file. — save_model","text":"Saves model .keras file.","code":""},{"path":"https://keras.posit.co/reference/save_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Saves a model as a .keras file. — save_model","text":"","code":"save_model(model, filepath = NULL, overwrite = FALSE, ...)"},{"path":"https://keras.posit.co/reference/save_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Saves a model as a .keras file. — save_model","text":"model keras model. filepath string, Path save model. Must end .keras. overwrite Whether overwrite existing model target location, instead ask user via interactive prompt. ... forward/backward compatability.","code":""},{"path":"https://keras.posit.co/reference/save_model.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Saves a model as a .keras file. — save_model","text":"saved .keras file contains: model's configuration (architecture) model's weights model's optimizer's state () Thus models can reinstantiated exact state.","code":"model <- keras_model_sequential(input_shape = c(3)) |>   layer_dense(5) |>   layer_activation_softmax()  model |> save_model(\"model.keras\") loaded_model <- load_model(\"model.keras\") x <- random_uniform(c(10, 3)) stopifnot(all.equal(   model |> predict(x),   loaded_model |> predict(x) )) zip::zip_list(\"model.keras\")[, \"filename\"] ## [1] \"metadata.json\"    \"config.json\"      \"model.weights.h5\""},{"path":[]},{"path":"https://keras.posit.co/reference/save_model_config.html","id":null,"dir":"Reference","previous_headings":"","what":"Save Model configuration as JSON — save_model_config","title":"Save Model configuration as JSON — save_model_config","text":"Save re-load models configurations JSON. Note representation include weights, architecture.","code":""},{"path":"https://keras.posit.co/reference/save_model_config.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save Model configuration as JSON — save_model_config","text":"","code":"save_model_config(model, filepath = NULL, overwrite = FALSE)  load_model_config(filepath, custom_objects = NULL)"},{"path":"https://keras.posit.co/reference/save_model_config.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save Model configuration as JSON — save_model_config","text":"model Model object save filepath path json file model config. overwrite Whether overwrite existing model configuration json filepath, instead ask user via interactive prompt. custom_objects Optional named list mapping names custom classes functions considered deserialization.","code":""},{"path":"https://keras.posit.co/reference/save_model_config.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Save Model configuration as JSON — save_model_config","text":"Note: save_model_config() serializes model JSON using serialize_keras_object(), get_config(). serialize_keras_object() returns superset get_config(), additional information needed create class object needed restore model. See example extract get_config() value saved model.","code":"model <- keras_model_sequential(input_shape = 10) |> layer_dense(10) file <- tempfile(\"model-config-\", fileext = \".json\") save_model_config(model, file)  # load a new model instance with the same architecture but different weights model2 <- load_model_config(file)  stopifnot(exprs = {   all.equal(get_config(model), get_config(model2))    # To extract the `get_config()` value from a saved model config:   all.equal(       get_config(model),       structure(jsonlite::read_json(file)$config,                 \"__class__\" = keras_model_sequential()$`__class__`)   ) })"},{"path":[]},{"path":"https://keras.posit.co/reference/save_model_weights.html","id":null,"dir":"Reference","previous_headings":"","what":"Saves all layer weights to a .weights.h5 file. — save_model_weights","title":"Saves all layer weights to a .weights.h5 file. — save_model_weights","text":"Saves layer weights .weights.h5 file.","code":""},{"path":"https://keras.posit.co/reference/save_model_weights.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Saves all layer weights to a .weights.h5 file. — save_model_weights","text":"","code":"save_model_weights(model, filepath, overwrite = FALSE)"},{"path":"https://keras.posit.co/reference/save_model_weights.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Saves all layer weights to a .weights.h5 file. — save_model_weights","text":"model keras Model object filepath string. Path save model. Must end .weights.h5. overwrite Whether overwrite existing model target location, instead ask user via interactive prompt.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/sequential_model_input_layer.html","id":null,"dir":"Reference","previous_headings":"","what":"sequential_model_input_layer — sequential_model_input_layer","title":"sequential_model_input_layer — sequential_model_input_layer","text":"sequential_model_input_layer","code":""},{"path":"https://keras.posit.co/reference/sequential_model_input_layer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"sequential_model_input_layer — sequential_model_input_layer","text":"","code":"sequential_model_input_layer(   input_shape = NULL,   batch_size = NULL,   dtype = NULL,   input_tensor = NULL,   sparse = NULL,   name = NULL,   ragged = NULL,   type_spec = NULL,   ...,   input_layer_name = NULL )"},{"path":"https://keras.posit.co/reference/sequential_model_input_layer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"sequential_model_input_layer — sequential_model_input_layer","text":"input_shape integer vector dimensions (including batch axis), tf$TensorShape instance (also including batch axis). batch_size Optional input batch size (integer NULL). dtype Optional datatype input. provided, Keras default float type used. input_tensor Optional tensor use layer input. set, layer use tf$TypeSpec tensor rather creating new placeholder tensor. sparse Boolean, whether placeholder created meant sparse. Default FALSE. ragged Boolean, whether placeholder created meant ragged. case, values 'NULL' 'shape' argument represent ragged dimensions. information RaggedTensors, see guide. Default FALSE. type_spec tf$TypeSpec object create Input . tf$TypeSpec represents entire batch. provided, args except name must NULL. ... additional arguments passed keras$layers$InputLayer. input_layer_name, name Optional name input layer (string).","code":""},{"path":"https://keras.posit.co/reference/serialize_keras_object.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve the config list by serializing the Keras object. — serialize_keras_object","title":"Retrieve the config list by serializing the Keras object. — serialize_keras_object","text":"serialize_keras_object() serializes Keras object named list represents object, reciprocal function deserialize_keras_object(). See deserialize_keras_object() information config format.","code":""},{"path":"https://keras.posit.co/reference/serialize_keras_object.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve the config list by serializing the Keras object. — serialize_keras_object","text":"","code":"serialize_keras_object(obj)"},{"path":"https://keras.posit.co/reference/serialize_keras_object.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve the config list by serializing the Keras object. — serialize_keras_object","text":"obj Keras object serialize.","code":""},{"path":"https://keras.posit.co/reference/serialize_keras_object.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve the config list by serializing the Keras object. — serialize_keras_object","text":"named list represents object config. config expected contain simple types , can saved json. object can deserialized config via deserialize_keras_object().","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/serialize_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Serialize a model to an R object — serialize_model","title":"Serialize a model to an R object — serialize_model","text":"Model objects external references Keras objects saved restored across R sessions. serialize_model() unserialize_model() functions provide facilities convert Keras models R objects persistence within R data files.","code":""},{"path":"https://keras.posit.co/reference/serialize_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Serialize a model to an R object — serialize_model","text":"","code":"serialize_model(model, ...)  unserialize_model(model, custom_objects = NULL, compile = TRUE)"},{"path":"https://keras.posit.co/reference/serialize_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Serialize a model to an R object — serialize_model","text":"model Keras model R \"raw\" object containing serialized Keras model.","code":""},{"path":"https://keras.posit.co/reference/serialize_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Serialize a model to an R object — serialize_model","text":"serialize_model() returns R \"raw\" object containing hdf5 version Keras model. unserialize_model() returns Keras model.","code":""},{"path":"https://keras.posit.co/reference/serialize_model.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Serialize a model to an R object — serialize_model","text":"save_model() function enables saving Keras models external hdf5 files.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/set_random_seed.html","id":null,"dir":"Reference","previous_headings":"","what":"Sets all random seeds (Python, NumPy, and backend framework, e.g. TF). — set_random_seed","title":"Sets all random seeds (Python, NumPy, and backend framework, e.g. TF). — set_random_seed","text":"can use utility make almost Keras program fully deterministic. limitations apply cases network communications involved (e.g. parameter server distribution), creates additional sources randomness, certain non-deterministic cuDNN ops involved. Calling utility equivalent following Python:   Note TensorFlow seed set even using TensorFlow backend framework, since many workflows leverage tf$data pipelines (feature random shuffling). Likewise many workflows might leverage NumPy APIs.","code":"import random import numpy as np from keras.utils.module_utils import tensorflow as tf random.seed(seed) np.random.seed(seed) tf.random.set_seed(seed)"},{"path":"https://keras.posit.co/reference/set_random_seed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sets all random seeds (Python, NumPy, and backend framework, e.g. TF). — set_random_seed","text":"","code":"set_random_seed(seed)"},{"path":"https://keras.posit.co/reference/set_random_seed.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sets all random seeds (Python, NumPy, and backend framework, e.g. TF). — set_random_seed","text":"seed Integer, random seed use.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/shape.html","id":null,"dir":"Reference","previous_headings":"","what":"Tensor shape utility — shape","title":"Tensor shape utility — shape","text":"function can used create get shape object.","code":""},{"path":"https://keras.posit.co/reference/shape.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tensor shape utility — shape","text":"","code":"shape(...)  # S3 method for keras_shape format(x, ...)  # S3 method for keras_shape print(x, ...)  # S3 method for keras_shape [(x, ...)  # S3 method for keras_shape as.integer(x, ...)  # S3 method for keras_shape as.list(x, ...)"},{"path":"https://keras.posit.co/reference/shape.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tensor shape utility — shape","text":"... shape specification. Numerics, NULL tensors valid. NULL, NA, -1L can used specify unspecified dim size. Tensors dispatched k_shape() extract tensor shape. Values wrapped () used asis (see examples). objects coerced via .integer(). x 'keras_shape' object","code":""},{"path":"https://keras.posit.co/reference/shape.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tensor shape utility — shape","text":"list \"keras_shape\" class attribute. element list either ) NULL, b) integer c) scalar integer tensor (e.g., supplied TF tensor unspecified dimension function traced).","code":""},{"path":"https://keras.posit.co/reference/shape.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tensor shape utility — shape","text":"3 ways specify unknown dimension     functions take 'shape' argument also coerce shape()     can also use shape() get shape tensor             Combine expand shapes         graph mode, shape might contain scalar integer tensor unknown axes.     useful pattern unpack shape() %<-%, like :   unpacking shape() graph mode, want reassemble axes shape(), wrap tensors () use tensor , rather shape tensor.","code":"shape(1, 2, 3) ## shape(1, 2, 3) shape(NA,   2, 3) shape(NULL, 2, 3) shape(-1,   2, 3) ## shape(NA, 2, 3) ## shape(NA, 2, 3) ## shape(NA, 2, 3) layer_input(c(1, 2, 3)) layer_input(shape(1, 2, 3)) ## <KerasTensor shape=(None, 1, 2, 3), dtype=float32, sparse=None, name=keras_tensor> ## <KerasTensor shape=(None, 1, 2, 3), dtype=float32, sparse=None, name=keras_tensor_1> symbolic_tensor <- layer_input(shape(1, 2, 3)) shape(symbolic_tensor) ## shape(NA, 1, 2, 3) eager_tensor <- op_ones(c(1,2,3)) shape(eager_tensor) ## shape(1, 2, 3) op_shape(eager_tensor) ## shape(1, 2, 3) shape(symbolic_tensor, 4) ## shape(NA, 1, 2, 3, 4) shape(5, symbolic_tensor, 4) ## shape(5, NA, 1, 2, 3, 4) tfn <- tensorflow::tf_function(function(x) {   print(shape(x))   x }, input_signature = list(tensorflow::tf$TensorSpec(shape(1, NA, 3)))) invisible(tfn(op_ones(shape(1, 2, 3)))) ## shape(1, Tensor(\"strided_slice:0\", shape=(), dtype=int32), 3) c(batch_size, seq_len, channels) %<-% shape(x) echo_print <- function(x) { message(\"> \", deparse(substitute(x))); print(x) } tfn <- tensorflow::tf_function(function(x) {   c(axis1, axis2, axis3) %<-% shape(x)   str(list(axis1 = axis1, axis2 = axis2, axis3 = axis3))    echo_print(shape(axis2))               # resolve axis2 tensor shape   echo_print(shape(axis1, axis2, axis3)) # resolve axis2 tensor shape    echo_print(shape(I(axis2)))               # use axis2 tensor as axis value   echo_print(shape(axis1, I(axis2), axis3)) # use axis2 tensor as axis value   x }, input_signature = list(tensorflow::tf$TensorSpec(shape(1, NA, 3)))) invisible(tfn(op_ones(shape(1, 2, 3)))) ## List of 3 ##  $ axis1: int 1 ##  $ axis2:<tf.Tensor 'strided_slice:0' shape=() dtype=int32> ##  $ axis3: int 3 ## > shape(axis2) ## shape() ## > shape(axis1, axis2, axis3) ## shape(1, 3) ## > shape(I(axis2)) ## shape(Tensor(\"strided_slice:0\", shape=(), dtype=int32)) ## > shape(axis1, I(axis2), axis3) ## shape(1, Tensor(\"strided_slice:0\", shape=(), dtype=int32), 3)"},{"path":[]},{"path":"https://keras.posit.co/reference/split_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Splits a dataset into a left half and a right half (e.g. train / test). — split_dataset","title":"Splits a dataset into a left half and a right half (e.g. train / test). — split_dataset","text":"Splits dataset left half right half (e.g. train / test).","code":""},{"path":"https://keras.posit.co/reference/split_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Splits a dataset into a left half and a right half (e.g. train / test). — split_dataset","text":"","code":"split_dataset(   dataset,   left_size = NULL,   right_size = NULL,   shuffle = FALSE,   seed = NULL )"},{"path":"https://keras.posit.co/reference/split_dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Splits a dataset into a left half and a right half (e.g. train / test). — split_dataset","text":"dataset tf$data$Dataset, torch$utils$data$Dataset object, list arrays length. left_size float (range [0, 1]), signifies fraction data pack left dataset. integer, signifies number samples pack left dataset. NULL, defaults complement right_size. Defaults NULL. right_size float (range [0, 1]), signifies fraction data pack right dataset. integer, signifies number samples pack right dataset. NULL, defaults complement left_size. Defaults NULL. shuffle Boolean, whether shuffle data splitting . seed random seed shuffling.","code":""},{"path":"https://keras.posit.co/reference/split_dataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Splits a dataset into a left half and a right half (e.g. train / test). — split_dataset","text":"list two tf$data$Dataset objects: left right splits.","code":""},{"path":"https://keras.posit.co/reference/split_dataset.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Splits a dataset into a left half and a right half (e.g. train / test). — split_dataset","text":"","code":"data <- random_uniform(c(1000, 4)) c(left_ds, right_ds) %<-% split_dataset(list(data$numpy()), left_size = 0.8) left_ds$cardinality() ## tf.Tensor(800, shape=(), dtype=int64) right_ds$cardinality() ## tf.Tensor(200, shape=(), dtype=int64)"},{"path":[]},{"path":"https://keras.posit.co/reference/summary.keras.models.model.Model.html","id":null,"dir":"Reference","previous_headings":"","what":"Print a summary of a Keras Model — summary.keras.models.model.Model","title":"Print a summary of a Keras Model — summary.keras.models.model.Model","text":"Print summary Keras Model","code":""},{"path":"https://keras.posit.co/reference/summary.keras.models.model.Model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print a summary of a Keras Model — summary.keras.models.model.Model","text":"","code":"# S3 method for keras.models.model.Model summary(object, ...)  # S3 method for keras.models.model.Model format(   x,   line_length = getOption(\"width\"),   positions = NULL,   expand_nested = FALSE,   show_trainable = x$built && as.logical(length(x$non_trainable_weights)),   ...,   layer_range = NULL,   compact = TRUE )  # S3 method for keras.models.model.Model print(x, ...)"},{"path":"https://keras.posit.co/reference/summary.keras.models.model.Model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print a summary of a Keras Model — summary.keras.models.model.Model","text":"object, x Keras model instance ... summary() print(), passed format(). format(), passed model$summary(). line_length Total length printed lines positions Relative absolute positions log elements line. provided, defaults c(0.33, 0.55, 0.67, 1.0). expand_nested Whether expand nested models. provided, defaults FALSE. show_trainable Whether show layer trainable. provided, defaults FALSE. layer_range list, tuple, vector 2 strings, starting layer name ending layer name (inclusive) indicating range layers printed summary. also accepts regex patterns instead exact name. case, start predicate first element matches layer_range[[1]] end predicate last element matches layer_range[[1]]. default NULL considers layers model. compact Whether remove white-space lines model summary. (Default TRUE)","code":""},{"path":"https://keras.posit.co/reference/summary.keras.models.model.Model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print a summary of a Keras Model — summary.keras.models.model.Model","text":"format() returns length 1 character vector. print() returns model object invisibly. summary() returns output format() invisibly printing .","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/test_on_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Test the model on a single batch of samples. — test_on_batch","title":"Test the model on a single batch of samples. — test_on_batch","text":"Test model single batch samples.","code":""},{"path":"https://keras.posit.co/reference/test_on_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test the model on a single batch of samples. — test_on_batch","text":"","code":"test_on_batch(object, x, y = NULL, sample_weight = NULL, ...)"},{"path":"https://keras.posit.co/reference/test_on_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test the model on a single batch of samples. — test_on_batch","text":"object Keras model object x Input data. Must array-like. y Target data. Must array-like. sample_weight Optional array length x, containing weights apply model's loss sample. case temporal data, can pass 2D array shape (samples, sequence_length), apply different weight every timestep every sample. ... forward/backward compatability","code":""},{"path":"https://keras.posit.co/reference/test_on_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test the model on a single batch of samples. — test_on_batch","text":"scalar loss value (metrics), named list loss metric values (metrics).","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/text_dataset_from_directory.html","id":null,"dir":"Reference","previous_headings":"","what":"Generates a tf.data.Dataset from text files in a directory. — text_dataset_from_directory","title":"Generates a tf.data.Dataset from text files in a directory. — text_dataset_from_directory","text":"directory structure :   calling text_dataset_from_directory(main_directory, labels='inferred') return tf.data.Dataset yields batches texts subdirectories class_a class_b, together labels 0 1 (0 corresponding class_a 1 corresponding class_b). .txt files supported time.","code":"main_directory/ ...class_a/ ......a_text_1.txt ......a_text_2.txt ...class_b/ ......b_text_1.txt ......b_text_2.txt"},{"path":"https://keras.posit.co/reference/text_dataset_from_directory.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generates a tf.data.Dataset from text files in a directory. — text_dataset_from_directory","text":"","code":"text_dataset_from_directory(   directory,   labels = \"inferred\",   label_mode = \"int\",   class_names = NULL,   batch_size = 32L,   max_length = NULL,   shuffle = TRUE,   seed = NULL,   validation_split = NULL,   subset = NULL,   follow_links = FALSE )"},{"path":"https://keras.posit.co/reference/text_dataset_from_directory.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generates a tf.data.Dataset from text files in a directory. — text_dataset_from_directory","text":"directory Directory data located. labels \"inferred\", contain subdirectories, containing text files class. Otherwise, directory structure ignored. labels Either \"inferred\" (labels generated directory structure), NULL (labels), list/tuple integer labels size number text files found directory. Labels sorted according alphanumeric order text file paths (obtained via os.walk(directory) Python). label_mode String describing encoding labels. Options : \"int\": means labels encoded integers (e.g. sparse_categorical_crossentropy loss). \"categorical\" means labels encoded categorical vector (e.g. categorical_crossentropy loss). \"binary\" means labels (can 2) encoded float32 scalars values 0 1 (e.g. binary_crossentropy). NULL (labels). class_names valid \"labels\" \"inferred\". explicit list class names (must match names subdirectories). Used control order classes (otherwise alphanumerical order used). batch_size Size batches data. Defaults 32. NULL, data batched (dataset yield individual samples). max_length Maximum size text string. Texts longer truncated max_length. shuffle Whether shuffle data. Defaults TRUE. set FALSE, sorts data alphanumeric order. seed Optional random seed shuffling transformations. validation_split Optional float 0 1, fraction data reserve validation. subset Subset data return. One \"training\", \"validation\" \"\". used validation_split set. subset=\"\", utility returns tuple two datasets (training validation datasets respectively). follow_links Whether visits subdirectories pointed symlinks. Defaults FALSE.","code":""},{"path":"https://keras.posit.co/reference/text_dataset_from_directory.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generates a tf.data.Dataset from text files in a directory. — text_dataset_from_directory","text":"tf.data.Dataset object. label_mode NULL, yields string tensors shape (batch_size,), containing contents batch text files. Otherwise, yields tuple (texts, labels), texts shape (batch_size,) labels follows format described . Rules regarding labels format: label_mode int, labels int32 tensor shape (batch_size,). label_mode binary, labels float32 tensor 1s 0s shape (batch_size, 1). label_mode categorical, labels float32 tensor shape (batch_size, num_classes), representing one-hot encoding class index.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/time_distributed.html","id":null,"dir":"Reference","previous_headings":"","what":"This layer wrapper allows to apply a layer to every temporal slice of an input — time_distributed","title":"This layer wrapper allows to apply a layer to every temporal slice of an input — time_distributed","text":"layer wrapper allows apply layer every temporal slice input","code":""},{"path":"https://keras.posit.co/reference/time_distributed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"This layer wrapper allows to apply a layer to every temporal slice of an input — time_distributed","text":"","code":"time_distributed(object, layer, ...)"},{"path":"https://keras.posit.co/reference/time_distributed.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"This layer wrapper allows to apply a layer to every temporal slice of an input — time_distributed","text":"object Object compose layer . tensor, array, sequential model. layer tf.keras.layers.Layer instance. ... standard layer arguments.","code":""},{"path":"https://keras.posit.co/reference/time_distributed.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"This layer wrapper allows to apply a layer to every temporal slice of an input — time_distributed","text":"Every input least 3D, dimension index one first input considered temporal dimension. Consider batch 32 video samples, sample 128x128 RGB image channels_last data format, across 10 timesteps. batch input shape (32, 10, 128, 128, 3). can use TimeDistributed apply Conv2D layer 10 timesteps, independently:   TimeDistributed applies instance Conv2D timestamps, set weights used timestamp.","code":"input <- layer_input(c(10, 128, 128, 3)) conv_layer <- layer_conv_2d(filters = 64, kernel_size = c(3, 3)) output <- input %>% time_distributed(conv_layer) shape(output) # shape(NA, 10, 126, 126, 64)"},{"path":[]},{"path":"https://keras.posit.co/reference/timeseries_dataset_from_array.html","id":null,"dir":"Reference","previous_headings":"","what":"Creates a dataset of sliding windows over a timeseries provided as array. — timeseries_dataset_from_array","title":"Creates a dataset of sliding windows over a timeseries provided as array. — timeseries_dataset_from_array","text":"function takes sequence data-points gathered equal intervals, along time series parameters length sequences/windows, spacing two sequence/windows, etc., produce batches timeseries inputs targets.","code":""},{"path":"https://keras.posit.co/reference/timeseries_dataset_from_array.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Creates a dataset of sliding windows over a timeseries provided as array. — timeseries_dataset_from_array","text":"","code":"timeseries_dataset_from_array(   data,   targets,   sequence_length,   sequence_stride = 1L,   sampling_rate = 1L,   batch_size = 128L,   shuffle = FALSE,   seed = NULL,   start_index = NULL,   end_index = NULL )"},{"path":"https://keras.posit.co/reference/timeseries_dataset_from_array.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creates a dataset of sliding windows over a timeseries provided as array. — timeseries_dataset_from_array","text":"data array eager tensor containing consecutive data points (timesteps). first dimension expected time dimension. targets Targets corresponding timesteps data. targets[] target corresponding window starts index (see example 2 ). Pass NULL target data (case dataset yield input data). sequence_length Length output sequences (number timesteps). sequence_stride Period successive output sequences. stride s, output samples start index data[], data[+ s], data[+ 2 * s], etc. sampling_rate Period successive individual timesteps within sequences. rate r, timesteps data[], data[+ r], ... data[+ sequence_length] used creating sample sequence. batch_size Number timeseries samples batch (except maybe last one). NULL, data batched (dataset yield individual samples). shuffle Whether shuffle output samples, instead draw chronological order. seed Optional int; random seed shuffling. start_index Optional int; data points earlier (exclusive) start_index used output sequences. useful reserve part data test validation. end_index Optional int; data points later (exclusive) end_index used output sequences. useful reserve part data test validation.","code":""},{"path":"https://keras.posit.co/reference/timeseries_dataset_from_array.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Creates a dataset of sliding windows over a timeseries provided as array. — timeseries_dataset_from_array","text":"tf$data$Dataset instance. targets passed, dataset yields list (batch_of_sequences, batch_of_targets). , dataset yields batch_of_sequences. Example 1: Consider indices [0, 1, ... 98]. sequence_length=10,  sampling_rate=2, sequence_stride=3, shuffle=FALSE, dataset yield batches sequences composed following indices:   case last 2 data points discarded since full sequence can generated include (next sequence started index 81, thus last step gone 98). Example 2: Temporal regression. Consider array data scalar values, shape (steps,). generate dataset uses past 10 timesteps predict next timestep, use:     Example 3: Temporal regression many--many architectures. Consider two arrays scalar values X Y, shape (100,). resulting dataset consist samples 20 timestamps . samples overlap. generate dataset uses current timestamp predict corresponding target timestep, use:","code":"First sequence:  [0  2  4  6  8 10 12 14 16 18] Second sequence: [3  5  7  9 11 13 15 17 19 21] Third sequence:  [6  8 10 12 14 16 18 20 22 24] ... Last sequence:   [78 80 82 84 86 88 90 92 94 96] data <- op_array(1:20) input_data <- data[1:10] targets <- data[11:20] dataset <- timeseries_dataset_from_array(   input_data, targets, sequence_length=10) iter <- reticulate::as_iterator(dataset) reticulate::iter_next(iter) ## [[1]] ## tf.Tensor([[ 1  2  3  4  5  6  7  8  9 10]], shape=(1, 10), dtype=int32) ## ## [[2]] ## tf.Tensor([11], shape=(1), dtype=int32) X <- op_array(1:100) Y <- X*2  sample_length <- 20 input_dataset <- timeseries_dataset_from_array(     X, NULL, sequence_length=sample_length, sequence_stride=sample_length) target_dataset <- timeseries_dataset_from_array(     Y, NULL, sequence_length=sample_length, sequence_stride=sample_length)   inputs <- reticulate::as_iterator(input_dataset) %>% reticulate::iter_next() targets <- reticulate::as_iterator(target_dataset) %>% reticulate::iter_next()"},{"path":[]},{"path":"https://keras.posit.co/reference/to_categorical.html","id":null,"dir":"Reference","previous_headings":"","what":"Converts a class vector (integers) to binary class matrix. — to_categorical","title":"Converts a class vector (integers) to binary class matrix. — to_categorical","text":"E.g. use categorical_crossentropy.","code":""},{"path":"https://keras.posit.co/reference/to_categorical.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Converts a class vector (integers) to binary class matrix. — to_categorical","text":"","code":"to_categorical(x, num_classes = NULL)"},{"path":"https://keras.posit.co/reference/to_categorical.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Converts a class vector (integers) to binary class matrix. — to_categorical","text":"x Array-like class values converted matrix (integers 0 num_classes - 1). num_classes Total number classes. NULL, inferred max(x) + 1. Defaults NULL.","code":""},{"path":"https://keras.posit.co/reference/to_categorical.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Converts a class vector (integers) to binary class matrix. — to_categorical","text":"binary matrix representation input NumPy array. class axis placed last.","code":""},{"path":"https://keras.posit.co/reference/to_categorical.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Converts a class vector (integers) to binary class matrix. — to_categorical","text":"","code":"a <- to_categorical(c(0, 1, 2, 3), num_classes=4) print(a) ##      [,1] [,2] [,3] [,4] ## [1,]    1    0    0    0 ## [2,]    0    1    0    0 ## [3,]    0    0    1    0 ## [4,]    0    0    0    1 b <- array(c(.9, .04, .03, .03,               .3, .45, .15, .13,               .04, .01, .94, .05,               .12, .21, .5, .17),               dim = c(4, 4)) loss <- op_categorical_crossentropy(a, b) loss ## tf.Tensor([0.41284522 0.45601739 0.54430155 0.80437282], shape=(4), dtype=float64) loss <- op_categorical_crossentropy(a, a) loss ## tf.Tensor([1.00000005e-07 1.00000005e-07 1.00000005e-07 1.00000005e-07], shape=(4), dtype=float64)"},{"path":[]},{"path":"https://keras.posit.co/reference/train_on_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Runs a single gradient update on a single batch of data. — train_on_batch","title":"Runs a single gradient update on a single batch of data. — train_on_batch","text":"Runs single gradient update single batch data.","code":""},{"path":"https://keras.posit.co/reference/train_on_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Runs a single gradient update on a single batch of data. — train_on_batch","text":"","code":"train_on_batch(object, x, y = NULL, sample_weight = NULL, class_weight = NULL)"},{"path":"https://keras.posit.co/reference/train_on_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Runs a single gradient update on a single batch of data. — train_on_batch","text":"object Keras model object x Input data. Must array-like. y Target data. Must array-like. sample_weight Optional array length x, containing weights apply model's loss sample. case temporal data, can pass 2D array shape (samples, sequence_length), apply different weight every timestep every sample. class_weight Optional named list mapping class indices (integers, 0-based) weight (float) apply model's loss samples class training. can useful tell model \"pay attention\" samples -represented class. class_weight specified targets rank 2 greater, either y must one-hot encoded, explicit final dimension 1 must included sparse class labels.","code":""},{"path":"https://keras.posit.co/reference/train_on_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Runs a single gradient update on a single batch of data. — train_on_batch","text":"scalar loss value (metrics), named list loss metric values (metrics). property model$metrics_names give display labels scalar outputs.","code":""},{"path":[]},{"path":"https://keras.posit.co/reference/unpack_x_y_sample_weight.html","id":null,"dir":"Reference","previous_headings":"","what":"Unpacks user-provided data list. — unpack_x_y_sample_weight","title":"Unpacks user-provided data list. — unpack_x_y_sample_weight","text":"convenience utility used overriding $train_step, $test_step, $predict_step. utility makes easy support data form (x,), (x, y), (x, y, sample_weight).","code":""},{"path":"https://keras.posit.co/reference/unpack_x_y_sample_weight.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Unpacks user-provided data list. — unpack_x_y_sample_weight","text":"","code":"unpack_x_y_sample_weight(data)"},{"path":"https://keras.posit.co/reference/unpack_x_y_sample_weight.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Unpacks user-provided data list. — unpack_x_y_sample_weight","text":"data list form (x,), (x, y), (x, y, sample_weight).","code":""},{"path":"https://keras.posit.co/reference/unpack_x_y_sample_weight.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Unpacks user-provided data list. — unpack_x_y_sample_weight","text":"unpacked list, NULLs y sample_weight provided.","code":""},{"path":"https://keras.posit.co/reference/unpack_x_y_sample_weight.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Unpacks user-provided data list. — unpack_x_y_sample_weight","text":"Standalone usage:","code":"features_batch <- op_ones(c(10, 5)) labels_batch <- op_zeros(c(10, 5)) data <- list(features_batch, labels_batch) # `y` and `sample_weight` will default to `NULL` if not provided. c(x, y, sample_weight) %<-% unpack_x_y_sample_weight(data)"},{"path":[]},{"path":"https://keras.posit.co/reference/use_implementation.html","id":null,"dir":"Reference","previous_headings":"","what":"Select a Keras implementation and backend — use_implementation","title":"Select a Keras implementation and backend — use_implementation","text":"Select Keras implementation backend","code":""},{"path":"https://keras.posit.co/reference/use_implementation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select a Keras implementation and backend — use_implementation","text":"","code":"use_implementation(implementation = c(\"keras\", \"tensorflow\"))  use_backend(backend = c(\"tensorflow\", \"cntk\", \"theano\", \"plaidml\"))"},{"path":"https://keras.posit.co/reference/use_implementation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select a Keras implementation and backend — use_implementation","text":"implementation One \"keras\" \"tensorflow\" (defaults \"keras\"). backend One \"tensorflow\", \"cntk\", \"theano\" (defaults \"tensorflow\")","code":""},{"path":"https://keras.posit.co/reference/use_implementation.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Select a Keras implementation and backend — use_implementation","text":"Keras multiple implementations (original keras implementation implementation native TensorFlow) supports multiple backends (\"tensorflow\", \"cntk\", \"theano\", \"plaidml\"). functions allow switching various implementations backends. functions called library(keras3) calling functions within package (see example). default implementation backend suitable use cases. \"tensorflow\" implementation useful using Keras conjunction TensorFlow Estimators (tfestimators R package).","code":""},{"path":"https://keras.posit.co/reference/use_implementation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select a Keras implementation and backend — use_implementation","text":"","code":"if (FALSE) { # use the tensorflow implementation library(keras3) use_implementation(\"tensorflow\")  # use the cntk backend library(keras3) use_backend(\"theano\") }"},{"path":"https://keras.posit.co/reference/with_custom_object_scope.html","id":null,"dir":"Reference","previous_headings":"","what":"Provide a scope with mappings of names to custom objects — with_custom_object_scope","title":"Provide a scope with mappings of names to custom objects — with_custom_object_scope","text":"Provide scope mappings names custom objects","code":""},{"path":"https://keras.posit.co/reference/with_custom_object_scope.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Provide a scope with mappings of names to custom objects — with_custom_object_scope","text":"","code":"with_custom_object_scope(objects, expr)"},{"path":"https://keras.posit.co/reference/with_custom_object_scope.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Provide a scope with mappings of names to custom objects — with_custom_object_scope","text":"objects Named list objects expr Expression evaluate","code":""},{"path":"https://keras.posit.co/reference/with_custom_object_scope.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Provide a scope with mappings of names to custom objects — with_custom_object_scope","text":"many elements Keras models can customized user objects (e.g. losses, metrics, regularizers, etc.). loading saved models use functions typically need explicitily map names user objects via custom_objects parmaeter. with_custom_object_scope() function provides alternative lets create named alias user object applies entire block code, automatically recognized loading saved models.","code":""},{"path":"https://keras.posit.co/reference/with_custom_object_scope.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Provide a scope with mappings of names to custom objects — with_custom_object_scope","text":"","code":"if (FALSE) { # define custom metric metric_top_3_categorical_accuracy <-   custom_metric(\"top_3_categorical_accuracy\", function(y_true, y_pred) {     metric_top_k_categorical_accuracy(y_true, y_pred, k = 3)   })  with_custom_object_scope(c(top_k_acc = sparse_top_k_cat_acc), {    # ...define model...    # compile model (refer to \"top_k_acc\" by name)   model |> compile(     loss = \"binary_crossentropy\",     optimizer = optimizer_nadam(),     metrics = c(\"top_k_acc\")   )    # save the model   model |> save_model(\"my_model.keras\")    # loading the model within the custom object scope doesn't   # require explicitly providing the custom_object   reloaded_model <- load_model(\"my_model.keras\") }) }"},{"path":"https://keras.posit.co/reference/zip_lists.html","id":null,"dir":"Reference","previous_headings":"","what":"zip lists — zip_lists","title":"zip lists — zip_lists","text":"conceptually similar zip() Python, R functions purrr::transpose() data.table::transpose() (albeit, accepting elements ... instead single list), one crucial difference: provided objects named, matching done names, positions.","code":""},{"path":"https://keras.posit.co/reference/zip_lists.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"zip lists — zip_lists","text":"","code":"zip_lists(...)"},{"path":"https://keras.posit.co/reference/zip_lists.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"zip lists — zip_lists","text":"... R lists atomic vectors, optionally named.","code":""},{"path":"https://keras.posit.co/reference/zip_lists.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"zip lists — zip_lists","text":"inverted list","code":""},{"path":"https://keras.posit.co/reference/zip_lists.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"zip lists — zip_lists","text":"arguments supplied must length. positional matching required, arguments provided must unnamed. matching names, arguments must set names, can different orders.","code":""},{"path":"https://keras.posit.co/reference/zip_lists.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"zip lists — zip_lists","text":"","code":"gradients <- list(\"grad_for_wt_1\", \"grad_for_wt_2\", \"grad_for_wt_3\") weights <- list(\"weight_1\", \"weight_2\", \"weight_3\") str(zip_lists(gradients, weights)) #> List of 3 #>  $ :List of 2 #>   ..$ : chr \"grad_for_wt_1\" #>   ..$ : chr \"weight_1\" #>  $ :List of 2 #>   ..$ : chr \"grad_for_wt_2\" #>   ..$ : chr \"weight_2\" #>  $ :List of 2 #>   ..$ : chr \"grad_for_wt_3\" #>   ..$ : chr \"weight_3\" str(zip_lists(gradient = gradients, weight = weights)) #> List of 3 #>  $ :List of 2 #>   ..$ gradient: chr \"grad_for_wt_1\" #>   ..$ weight  : chr \"weight_1\" #>  $ :List of 2 #>   ..$ gradient: chr \"grad_for_wt_2\" #>   ..$ weight  : chr \"weight_2\" #>  $ :List of 2 #>   ..$ gradient: chr \"grad_for_wt_3\" #>   ..$ weight  : chr \"weight_3\"  names(gradients) <- names(weights) <- paste0(\"layer_\", 1:3) str(zip_lists(gradients, weights[c(3, 1, 2)])) #> List of 3 #>  $ layer_1:List of 2 #>   ..$ : chr \"grad_for_wt_1\" #>   ..$ : chr \"weight_1\" #>  $ layer_2:List of 2 #>   ..$ : chr \"grad_for_wt_2\" #>   ..$ : chr \"weight_2\" #>  $ layer_3:List of 2 #>   ..$ : chr \"grad_for_wt_3\" #>   ..$ : chr \"weight_3\"  names(gradients) <- paste0(\"gradient_\", 1:3) try(zip_lists(gradients, weights)) # error, names don't match #> Error in zip_lists(gradients, weights) :  #>   All names of arguments provided to `zip_lists()` must match. Call `unname()` on each argument if you want positional matching # call unname directly for positional matching str(zip_lists(unname(gradients), unname(weights))) #> List of 3 #>  $ :List of 2 #>   ..$ : chr \"grad_for_wt_1\" #>   ..$ : chr \"weight_1\" #>  $ :List of 2 #>   ..$ : chr \"grad_for_wt_2\" #>   ..$ : chr \"weight_2\" #>  $ :List of 2 #>   ..$ : chr \"grad_for_wt_3\" #>   ..$ : chr \"weight_3\""}]
