diff --git a/vignettes-src/writing_your_own_callbacks/2-translated.Rmd b/vignettes-src/writing_your_own_callbacks/2-translated.Rmd
index 3551c5dc..d0ad0ce9 100644
--- a/vignettes-src/writing_your_own_callbacks/2-translated.Rmd
+++ b/vignettes-src/writing_your_own_callbacks/2-translated.Rmd
@@ -7,11 +7,3 @@ description: Complete guide to writing new Keras callbacks.
 accelerator: GPU
-output: rmarkdown::html_vignette
-vignette: >
-  %\VignetteIndexEntry{Writing your own callbacks}
-  %\VignetteEngine{knitr::rmarkdown}
-  %\VignetteEncoding{UTF-8}
-knit: >
-  (function(input, encoding) rmarkdown::render(
-    input, encoding=encoding,
-    output_file='03-rendered.md')))
+knit: keras:::knit_vignette
 ---
@@ -31,5 +23,6 @@ started.
 
-```python
-import numpy as np
-import keras as keras
+```{r}
+library(keras)
+`add<-` <- `+`
+envir::import_from(dplyr, last)
 ```
@@ -46,5 +39,5 @@ model methods:
 
-- `keras.Model.fit()`
-- `keras.Model.evaluate()`
-- `keras.Model.predict()`
+- `fit()`
+- `evaluate()`
+- `predict()`
 
@@ -54,3 +47,3 @@ model methods:
 
-#### `on_(train|test|predict)_begin(self, logs=None)`
+#### `on_(train|test|predict)_begin(logs = NULL)`
 
@@ -58,3 +51,3 @@ Called at the beginning of `fit`/`evaluate`/`predict`.
 
-#### `on_(train|test|predict)_end(self, logs=None)`
+#### `on_(train|test|predict)_end(logs = NULL)`
 
@@ -64,3 +57,3 @@ Called at the end of `fit`/`evaluate`/`predict`.
 
-#### `on_(train|test|predict)_batch_begin(self, batch, logs=None)`
+#### `on_(train|test|predict)_batch_begin(batch, logs = NULL)`
 
@@ -68,6 +61,6 @@ Called right before processing a batch during training/testing/predicting.
 
-#### `on_(train|test|predict)_batch_end(self, batch, logs=None)`
+#### `on_(train|test|predict)_batch_end(batch, logs = NULL)`
 
 Called at the end of training/testing/predicting a batch. Within this method, `logs` is
-a dict containing the metrics results.
+a named list containing the metrics results.
 
@@ -75,3 +68,3 @@ a dict containing the metrics results.
 
-#### `on_epoch_begin(self, epoch, logs=None)`
+#### `on_epoch_begin(epoch, logs = NULL)`
 
@@ -79,3 +72,3 @@ Called at the beginning of an epoch during training.
 
-#### `on_epoch_end(self, epoch, logs=None)`
+#### `on_epoch_end(epoch, logs = NULL)`
 
@@ -88,13 +81,14 @@ define a simple Sequential Keras model:
 
-```python
+```{r}
 # Define the Keras model to add callbacks to
-def get_model():
-    model = keras.Sequential()
-    model.add(keras.layers.Dense(1))
-    model.compile(
-        optimizer=keras.optimizers.RMSprop(learning_rate=0.1),
-        loss="mean_squared_error",
-        metrics=["mean_absolute_error"],
-    )
-    return model
+get_model <- function() {
+  model <- keras_model_sequential()
+  model |> layer_dense(units = 1)
+  model |> compile(
+    optimizer = optimizer_rmsprop(learning_rate = 0.1),
+    loss = "mean_squared_error",
+    metrics = "mean_absolute_error"
+  )
+  model
+}
 ```
@@ -103,13 +97,21 @@ Then, load the MNIST data for training and testing from Keras datasets API:
 
-```python
+```{r}
 # Load example MNIST data and pre-process it
-(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
-x_train = x_train.reshape(-1, 784).astype("float32") / 255.0
-x_test = x_test.reshape(-1, 784).astype("float32") / 255.0
-
-# Limit the data to 1000 samples
-x_train = x_train[:1000]
-y_train = y_train[:1000]
-x_test = x_test[:1000]
-y_test = y_test[:1000]
+mnist <- dataset_mnist()
+
+flatten_and_rescale <- function(x) {
+  x <- array_reshape(x, c(-1, 784))
+  x <- x / 255
+  x
+}
+
+mnist$train$x <- flatten_and_rescale(mnist$train$x)
+mnist$test$x  <- flatten_and_rescale(mnist$test$x)
+
+# limit to 500 samples
+n <- 200
+mnist$train$x <- mnist$train$x[1:n,]
+mnist$train$y <- mnist$train$y[1:n]
+mnist$test$x  <- mnist$test$x[1:n,]
+mnist$test$y  <- mnist$test$y[1:n]
 ```
@@ -124,83 +126,29 @@ Now, define a simple custom callback that logs:
 
-```python
-class CustomCallback(keras.callbacks.Callback):
-    def on_train_begin(self, logs=None):
-        keys = list(logs.keys())
-        print("Starting training; got log keys: {}".format(keys))
-
-    def on_train_end(self, logs=None):
-        keys = list(logs.keys())
-        print("Stop training; got log keys: {}".format(keys))
-
-    def on_epoch_begin(self, epoch, logs=None):
-        keys = list(logs.keys())
-        print(
-            "Start epoch {} of training; got log keys: {}".format(epoch, keys)
-        )
-
-    def on_epoch_end(self, epoch, logs=None):
-        keys = list(logs.keys())
-        print("End epoch {} of training; got log keys: {}".format(epoch, keys))
-
-    def on_test_begin(self, logs=None):
-        keys = list(logs.keys())
-        print("Start testing; got log keys: {}".format(keys))
-
-    def on_test_end(self, logs=None):
-        keys = list(logs.keys())
-        print("Stop testing; got log keys: {}".format(keys))
-
-    def on_predict_begin(self, logs=None):
-        keys = list(logs.keys())
-        print("Start predicting; got log keys: {}".format(keys))
-
-    def on_predict_end(self, logs=None):
-        keys = list(logs.keys())
-        print("Stop predicting; got log keys: {}".format(keys))
-
-    def on_train_batch_begin(self, batch, logs=None):
-        keys = list(logs.keys())
-        print(
-            "...Training: start of batch {}; got log keys: {}".format(
-                batch, keys
-            )
-        )
-
-    def on_train_batch_end(self, batch, logs=None):
-        keys = list(logs.keys())
-        print(
-            "...Training: end of batch {}; got log keys: {}".format(batch, keys)
-        )
-
-    def on_test_batch_begin(self, batch, logs=None):
-        keys = list(logs.keys())
-        print(
-            "...Evaluating: start of batch {}; got log keys: {}".format(
-                batch, keys
-            )
-        )
-
-    def on_test_batch_end(self, batch, logs=None):
-        keys = list(logs.keys())
-        print(
-            "...Evaluating: end of batch {}; got log keys: {}".format(
-                batch, keys
-            )
-        )
-
-    def on_predict_batch_begin(self, batch, logs=None):
-        keys = list(logs.keys())
-        print(
-            "...Predicting: start of batch {}; got log keys: {}".format(
-                batch, keys
-            )
-        )
-
-    def on_predict_batch_end(self, batch, logs=None):
-        keys = list(logs.keys())
-        print(
-            "...Predicting: end of batch {}; got log keys: {}".format(
-                batch, keys
-            )
-        )
+```{r}
+show <- function(msg, logs) {
+  cat(glue::glue(msg, .envir = parent.frame()),
+      "got logs: ", sep = "; ")
+  str(logs); cat("\n")
+}
+
+callback_custom <- new_callback_class(
+  "CustomCallback",
+  on_train_begin         = function(logs = NULL) show("Starting training", logs),
+  on_epoch_begin         = function(epoch, logs = NULL) show("Start epoch {epoch} of training", logs),
+  on_train_batch_begin   = function(batch, logs = NULL) show("...Training: start of batch {batch}", logs),
+  on_train_batch_end     = function(batch, logs = NULL) show("...Training: end of batch {batch}",  logs),
+  on_epoch_end           = function(epoch, logs = NULL) show("End epoch {epoch} of training", logs),
+  on_train_end           = function(logs = NULL) show("Stop training", logs),
+
+
+  on_test_begin          = function(logs = NULL) show("Start testing", logs),
+  on_test_batch_begin    = function(batch, logs = NULL) show("...Evaluating: start of batch {batch}", logs),
+  on_test_batch_end      = function(batch, logs = NULL) show("...Evaluating: end of batch {batch}", logs),
+  on_test_end            = function(logs = NULL) show("Stop testing", logs),
+
+  on_predict_begin       = function(logs = NULL) show("Start predicting", logs),
+  on_predict_end         = function(logs = NULL) show("Stop predicting", logs),
+  on_predict_batch_begin = function(batch, logs = NULL) show("...Predicting: start of batch {batch}", logs),
+  on_predict_batch_end   = function(batch, logs = NULL) show("...Predicting: end of batch {batch}", logs),
+)
 ```
@@ -208,20 +156,21 @@ class CustomCallback(keras.callbacks.Callback):
 Let's try it out:
-
-```python
-model = get_model()
-model.fit(
-    x_train,
-    y_train,
-    batch_size=128,
-    epochs=1,
-    verbose=0,
-    validation_split=0.5,
-    callbacks=[CustomCallback()],
+```{r}
+model <- get_model()
+model |> fit(
+  mnist$train$x, mnist$train$y,
+  epochs = 2, verbose = 0, validation_split = 0.5,
+  callbacks = list(callback_custom())
 )
 
-res = model.evaluate(
-    x_test, y_test, batch_size=128, verbose=0, callbacks=[CustomCallback()]
+res <- model |> evaluate(
+  mnist$test$x, mnist$test$y,
+  verbose = 0,
+  callbacks = list(callback_custom())
 )
 
-res = model.predict(x_test, batch_size=128, callbacks=[CustomCallback()])
+res <- model |> predict(
+  mnist$test$x, 
+  verbose = 0,
+  callbacks = list(callback_custom())
+)
 ```
@@ -229,51 +178,41 @@ res = model.predict(x_test, batch_size=128, callbacks=[CustomCallback()])
 ### Usage of `logs` dict
-
-The `logs` dict contains the loss value, and all the metrics at the end of a batch or
+The `logs` named list contains the loss value, and all the metrics at the end of a batch or
 epoch. Example includes the loss and mean absolute error.
+```{r}
+callback_print_loss_and_mae <- new_callback_class(
+  "LossAndErrorPrintingCallback",
+
+  on_train_batch_end = function(batch, logs = NULL)
+    cat(sprintf("Up to batch %i, the average loss is %7.2f.\n",
+                batch,  logs$loss)),
+
+  on_test_batch_end = function(batch, logs = NULL)
+    cat(sprintf("Up to batch %i, the average loss is %7.2f.\n",
+                batch, logs$loss)),
+
+  on_epoch_end = function(epoch, logs = NULL)
+    cat(sprintf(
+      "The average loss for epoch %2i is %9.2f and mean absolute error is %7.2f.\n",
+      epoch, logs$loss, logs$mean_absolute_error
+    ))
+)
+
 
-```python
-class LossAndErrorPrintingCallback(keras.callbacks.Callback):
-    def on_train_batch_end(self, batch, logs=None):
-        print(
-            "Up to batch {}, the average loss is {:7.2f}.".format(
-                batch, logs["loss"]
-            )
-        )
-
-    def on_test_batch_end(self, batch, logs=None):
-        print(
-            "Up to batch {}, the average loss is {:7.2f}.".format(
-                batch, logs["loss"]
-            )
-        )
-
-    def on_epoch_end(self, epoch, logs=None):
-        print(
-            "The average loss for epoch {} is {:7.2f} "
-            "and mean absolute error is {:7.2f}.".format(
-                epoch, logs["loss"], logs["mean_absolute_error"]
-            )
-        )
-
-
-model = get_model()
-model.fit(
-    x_train,
-    y_train,
-    batch_size=128,
-    epochs=2,
-    verbose=0,
-    callbacks=[LossAndErrorPrintingCallback()],
+model <- get_model()
+model |> fit(
+  mnist$train$x, mnist$train$y,
+  epochs = 2, verbose = 0,
+  callbacks = list(callback_print_loss_and_mae())
 )
 
-res = model.evaluate(
-    x_test,
-    y_test,
-    batch_size=128,
-    verbose=0,
-    callbacks=[LossAndErrorPrintingCallback()],
+res = model |> evaluate(
+  mnist$test$x, mnist$test$y,
+  verbose = 0,
+  callbacks = list(callback_print_loss_and_mae())
 )
 ```
+For more information about callbacks, you can check out the [Keras callback API documentation](https://keras.io/api/callbacks/).
+
 
-## Usage of `self.model` attribute
+## Usage of `self$model` attribute
 
@@ -281,11 +220,11 @@ In addition to receiving log information when one of their methods is called,
 callbacks have access to the model associated with the current round of
-training/evaluation/inference: `self.model`.
+training/evaluation/inference: `self$model`.
 
-Here are a few of the things you can do with `self.model` in a callback:
+Here are of few of the things you can do with `self$model` in a callback:
 
-- Set `self.model.stop_training = True` to immediately interrupt training.
-- Mutate hyperparameters of the optimizer (available as `self.model.optimizer`),
-such as `self.model.optimizer.learning_rate`.
+- Set `self$model$stop_training <- TRUE` to immediately interrupt training.
+- Mutate hyperparameters of the optimizer (available as `self$model$optimizer`),
+such as `self$model$optimizer$learning_rate`.
 - Save the model at period intervals.
-- Record the output of `model.predict()` on a few test samples at the end of each
+- Record the output of `model |> predict()` on a few test samples at the end of each
 epoch, to use as a sanity check during training.
@@ -302,3 +241,3 @@ Let's see this in action in a couple of examples.
 This first example shows the creation of a `Callback` that stops training when the
-minimum of loss has been reached, by setting the attribute `self.model.stop_training`
+minimum of loss has been reached, by setting the attribute `self$model$stop_training`
 (boolean). Optionally, you can provide an argument `patience` to specify how many
@@ -306,7 +245,9 @@ epochs we should wait before stopping after having reached a local minimum.
 
-`keras.callbacks.EarlyStopping` provides a more complete and general implementation.
+`callback_early_stopping()` provides a more complete and general implementation.
 
-```python
-class EarlyStoppingAtMinLoss(keras.callbacks.Callback):
-    """Stop training when the loss is at its min, i.e. the loss stops decreasing.
+```{r}
+callback_early_stopping_at_min_loss <- new_callback_class(
+  "EarlyStoppingAtMinLoss",
+  `__doc__` =
+    "Stop training when the loss is at its min, i.e. the loss stops decreasing.
 
@@ -315,46 +256,55 @@ class EarlyStoppingAtMinLoss(keras.callbacks.Callback):
         number of no improvement, training stops.
-    """
-
-    def __init__(self, patience=0):
-        super().__init__()
-        self.patience = patience
-        # best_weights to store the weights at which the minimum loss occurs.
-        self.best_weights = None
-
-    def on_train_begin(self, logs=None):
-        # The number of epoch it has waited when loss is no longer minimum.
-        self.wait = 0
-        # The epoch the training stops at.
-        self.stopped_epoch = 0
-        # Initialize the best as infinity.
-        self.best = np.Inf
-
-    def on_epoch_end(self, epoch, logs=None):
-        current = logs.get("loss")
-        if np.less(current, self.best):
-            self.best = current
-            self.wait = 0
-            # Record the best weights if current results is better (less).
-            self.best_weights = self.model.get_weights()
-        else:
-            self.wait += 1
-            if self.wait >= self.patience:
-                self.stopped_epoch = epoch
-                self.model.stop_training = True
-                print("Restoring model weights from the end of the best epoch.")
-                self.model.set_weights(self.best_weights)
-
-    def on_train_end(self, logs=None):
-        if self.stopped_epoch > 0:
-            print(f"Epoch {self.stopped_epoch + 1}: early stopping")
-
-
-model = get_model()
-model.fit(
-    x_train,
-    y_train,
-    batch_size=64,
-    epochs=30,
-    verbose=0,
-    callbacks=[LossAndErrorPrintingCallback(), EarlyStoppingAtMinLoss()],
+    ",
+
+  initialize = function(patience = 0) {
+    super$initialize()
+    self$patience <- patience
+    # best_weights to store the weights at which the minimum loss occurs.
+    self$best_weights <- NULL
+  },
+
+  on_train_begin = function(logs = NULL) {
+    # The number of epoch it has waited when loss is no longer minimum.
+    self$wait <- 0
+    # The epoch the training stops at.
+    self$stopped_epoch <- 0
+    # Initialize the best as infinity.
+    self$best <- Inf
+  },
+
+  on_epoch_end = function(epoch, logs = NULL) {
+    current <- logs$loss
+    if (current < self$best) {
+      self$best <- current
+      self$wait <- 0
+      # Record the best weights if current results is better (less).
+      self$best_weights <- self$model$get_weights()
+    } else {
+      add(self$wait) <- 1
+      if (self$wait >= self$patience) {
+        self$stopped_epoch <- epoch
+        model <- self$model
+        model$stop_training <- TRUE
+        cat("Restoring model weights from the end of the best epoch.\n")
+        model$set_weights(self$best_weights)
+      }
+    }
+  },
+
+  on_train_end = function(logs = NULL)
+    if (self$stopped_epoch > 0)
+      cat(sprintf("Epoch %05d: early stopping\n", self$stopped_epoch + 1))
+)
+
+
+model <- get_model()
+model |> fit(
+  mnist$train$x,
+  mnist$train$y,
+  batch_size = 64,
+  steps_per_epoch = 5,
+  epochs = 30,
+  verbose = 0,
+  callbacks = list(callback_print_loss_and_mae(),
+                   callback_early_stopping_at_min_loss())
 )
@@ -362,2 +312,3 @@ model.fit(
 
+
 ### Learning rate scheduling
@@ -367,7 +318,9 @@ learning rate of the optimizer during the course of training.
 
-See `callbacks.LearningRateScheduler` for a more general implementations.
+See `keras$callbacks$LearningRateScheduler` for a more general implementations (in RStudio, press F1 while the cursor is over `LearningRateScheduler` and a browser will open to [this page](https://www.tensorflow.org/versions/r2.5/api_docs/python/tf/keras/callbacks/LearningRateScheduler)).
 
-```python
-class CustomLearningRateScheduler(keras.callbacks.Callback):
-    """Learning rate scheduler which sets the learning rate according to schedule.
+```{r}
+callback_custom_learning_rate_scheduler <- new_callback_class(
+  "CustomLearningRateScheduler",
+  `__doc__` =
+  "Learning rate scheduler which sets the learning rate according to schedule.
 
@@ -377,52 +330,55 @@ class CustomLearningRateScheduler(keras.callbacks.Callback):
             as inputs and returns a new learning rate as output (float).
-    """
-
-    def __init__(self, schedule):
-        super().__init__()
-        self.schedule = schedule
-
-    def on_epoch_begin(self, epoch, logs=None):
-        if not hasattr(self.model.optimizer, "learning_rate"):
-            raise ValueError('Optimizer must have a "learning_rate" attribute.')
-        # Get the current learning rate from model's optimizer.
-        lr = self.model.optimizer.learning_rate
-        # Call schedule function to get the scheduled learning rate.
-        scheduled_lr = self.schedule(epoch, lr)
-        # Set the value back to the optimizer before this epoch starts
-        self.model.optimizer.learning_rate = scheduled_lr
-        print(
-            f"\nEpoch {epoch}: Learning rate is {float(np.array(scheduled_lr))}."
-        )
-
-
-LR_SCHEDULE = [
-    # (epoch to start, learning rate) tuples
-    (3, 0.05),
-    (6, 0.01),
-    (9, 0.005),
-    (12, 0.001),
-]
-
-
-def lr_schedule(epoch, lr):
-    """Helper function to retrieve the scheduled learning rate based on epoch."""
-    if epoch < LR_SCHEDULE[0][0] or epoch > LR_SCHEDULE[-1][0]:
-        return lr
-    for i in range(len(LR_SCHEDULE)):
-        if epoch == LR_SCHEDULE[i][0]:
-            return LR_SCHEDULE[i][1]
-    return lr
-
-
-model = get_model()
-model.fit(
-    x_train,
-    y_train,
-    batch_size=64,
-    epochs=15,
-    verbose=0,
-    callbacks=[
-        LossAndErrorPrintingCallback(),
-        CustomLearningRateScheduler(lr_schedule),
-    ],
+    ",
+
+  initialize = function(schedule) {
+    super$initialize()
+    self$schedule <- schedule
+  },
+
+  on_epoch_begin = function(epoch, logs = NULL) {
+    ## When in doubt about what types of objects are in scope (e.g., self$model)
+    ## use a debugger to interact with the actual objects at the console!
+    # browser()
+
+    if (!"learning_rate" %in% names(self$model$optimizer))
+      stop('Optimizer must have a "learning_rate" attribute.')
+
+    # # Get the current learning rate from model's optimizer.
+    # use as.numeric() to convert the keras variablea to an R numeric
+    lr <- as.numeric(self$model$optimizer$learning_rate)
+    # # Call schedule function to get the scheduled learning rate.
+    scheduled_lr <- self$schedule(epoch, lr)
+    # # Set the value back to the optimizer before this epoch starts
+    optimizer <- self$model$optimizer
+    optimizer$learning_rate <- scheduled_lr
+    cat(sprintf("\nEpoch %03d: Learning rate is %6.4f.\n", epoch, scheduled_lr))
+  }
+)
+
+LR_SCHEDULE <- tibble::tribble(
+  ~start_epoch, ~learning_rate,
+             0,            0.1,
+             3,           0.05,
+             6,           0.01,
+             9,          0.005,
+            12,          0.001,
+  )
+
+lr_schedule <- function(epoch, learning_rate) {
+  "Helper function to retrieve the scheduled learning rate based on epoch."
+  with(LR_SCHEDULE, learning_rate[last(which(epoch >= start_epoch))])
+}
+
+model <- get_model()
+model |> fit(
+  mnist$train$x,
+  mnist$train$y,
+  batch_size = 64,
+  steps_per_epoch = 5,
+  epochs = 15,
+  verbose = 0,
+  callbacks = list(
+    callback_print_loss_and_mae(),
+    callback_custom_learning_rate_scheduler(lr_schedule)
+  )
 )
