---
title: 'Imbalanced classification: credit card fraud detection'
author: '[fchollet](https://twitter.com/fchollet)'
date-created: 2019/05/28
last-modified: 2020/04/17
description: Demonstration of how to handle highly imbalanced classification problems.
accelerator: GPU
output: rmarkdown::html_vignette
domain: structured data
category: basic
knit: ({source(here::here("tools/knit.R")); knit_vignette)
---

## Introduction

This example looks at the
[Kaggle Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud/)
dataset to demonstrate how
to train a classification model on data with highly imbalanced classes.

```{r}
library(keras3)
use_backend("jax")
```

You can manually download "creditcard.csv" from
[Kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud/). Alternatively, you
can create a Kaggle API key, and then download the dataset using the Kaggle
command line utility. To do so, navigate to the Kaggle website in a web browser,
log in, and go to the My Account page. In your account settings, youâ€™ll find an
API section. Clicking the Create New API Token button will generate a
kaggle.json key file and will download it to your machine.

```{r, eval = FALSE}
library(fs)
dir_create("~/.kaggle")
file_move("~/Downloads/kaggle.json", "~/.kaggle/")
file_chmod("~/.kaggle/kaggle.json", "0600") # mark file only readable by yourself

# install the kaggle package
reticulate::py_install("kaggle", pip = TRUE)
```

Then you can download using `kaggle` from the command line, and unzip it go 
get "creditcard.csv":
```{r, eval = FALSE}
system('kaggle datasets download -d mlg-ulb/creditcardfraud')
zip::unzip("creditcardfraud.zip")
```
The first time you try to download the data, you may get a "403
Forbidden" error. That's because you need to accept the terms associated
with the dataset before you download it---you'll have to go to
<http://www.kaggle.com/mlg-ulb/creditcardfraud/rules> (while logged into your
Kaggle account) and click the I Understand and Accept button. You only
need to do this once.

Once you've acquired the data, we'll load it

## First, load the data

```{r}
df <- readr::read_csv("creditcard.csv")
```


```{r}
library(dplyr, warn.conflicts = FALSE)
glimpse(df)
features <- df |> select(-last_col()) |> as.matrix() #|> np_array("float32")
targets <- df |> pull(last_col()) |> as.integer() |> as.matrix()     #|> np_array("uint8")

# storage.mode(targets) <- "integer"

str(features)
str(targets)
# features$shape
# targets$shape
```

## Prepare a validation set

```{r, results='hold'}
num_samples <- nrow(features)
num_val_samples <- round(num_samples * 0.2)
val_idx <- sample.int(num_samples, num_val_samples)

train_features <- features[-val_idx, ]
train_targets <- targets[-val_idx, ]
val_features <- features[val_idx, ]
val_targets <- targets[val_idx, ]

cat("Number of training samples:", nrow(train_features), "\n")
cat("Number of validation samples:", nrow(val_features), "\n")
```

## Analyze class imbalance in the targets

```{r}
counts <- table(train_targets)
cat(sprintf("Number of positive samples in training data: %i (%.2f%% of total)",
            counts["1"], 100 * counts["1"] / sum(counts)))

weight_for_0 = 1 / counts["0"]
weight_for_1 = 1 / counts["1"]
```
```{r}
library(reticulate)
py$train_features <- np_array(train_features)
py$val_features <- np_array(val_features)
```

## Normalize the data using training set statistics
```{r}
train_features %<>% scale()
val_features %<>% scale(center = attr(train_features, "scaled:center"),
                        scale = attr(train_features, "scaled:scale"))
```

## Build a binary classification model

```{r}
model <-
  keras_model_sequential(input_shape = ncol(train_features)) |>
  layer_dense(256, activation = "relu") |>
  layer_dense(256, activation = "relu") |>
  layer_dropout(0.3) |>
  layer_dense(256, activation = "relu") |>
  layer_dropout(0.3) |>
  layer_dense(1, activation = "sigmoid")

model
```

## Train the model with `class_weight` argument

```{r}
metrics <- list(
  metric_false_negatives(name = "fn"),
  metric_false_positives(name = "fp"),
  metric_true_negatives(name = "tn"),
  metric_true_positives(name = "tp"),
  metric_precision(name = "precision"),
  metric_recall(name = "recall")
)

model |> compile(optimizer = optimizer_adam(1e-2),
                 loss = "binary_crossentropy",
                 metrics = metrics)

callbacks <- list(
  callback_model_checkpoint("fraud_model_at_epoch_{epoch}.keras")
)

class_weight <- list("0" = weight_for_0, "1" = weight_for_1)

history <- model |> fit(
  train_features,
  train_targets,
  batch_size = 2048,
  epochs = 30,
  verbose = 2,
  callbacks = callbacks,
  validation_data = list(val_features, val_targets),
  class_weight = class_weight
)

plot(history)
```

## Conclusions

At the end of training, out of 56,961 validation transactions, we are:

- Correctly identifying 66 of them as fraudulent
- Missing 9 fraudulent transactions
- At the cost of incorrectly flagging 441 legitimate transactions

In the real world, one would put an even higher weight on class 1,
so as to reflect that False Negatives are more costly than False Positives.

Next time your credit card gets  declined in an online purchase -- this is why.
